Evaluation Metrics:
BLEU: 0.1129
ROUGE-L: 0.2461
METEOR: 0.3135
ChrF: 0.1696
Computing SIDE scores for each instance...

 Average SIDE score across all instances: 0.4340
 Updated CSV saved to: /home/saima/projects/prompt-project/codet5+/dataset/funcom-python-final/inference-results-no-SIDE/CBLEU_benchmark_inference_results-test.csv
 
Instance-level BLEU-4 scores saved to /home/saima/projects/prompt-project/codet5+/dataset/statistical_tests/Python/no-side/CBLEU/CBLEU_bleu-4.csv
Instance-level ROUGE-L scores saved to /home/saima/projects/prompt-project/codet5+/dataset/statistical_tests/Python/no-side/CBLEU/CBLEU_rouge-l.csv
Instance-level METEOR scores saved to /home/saima/projects/prompt-project/codet5+/dataset/statistical_tests/Python/no-side/CBLEU/CBLEU_meteor.csv
Instance-level ChrF scores saved to /home/saima/projects/prompt-project/codet5+/dataset/statistical_tests/Python/no-side/CBLEU/CBLEU_chrf.csv

=== Overall Corpus-Level Scores (ICPC script) ===
BLEU-4: 0.1175
ROUGE-L: 0.2602
METEOR: 0.3135
ChrF: 0.1741
