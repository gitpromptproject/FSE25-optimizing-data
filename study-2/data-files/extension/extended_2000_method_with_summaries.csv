id,function_name,codeFunctions,originalComment,codeComment
1501,make_fastmcp_streamable_http_app,"def make_fastmcp_streamable_http_app():
    

    mcp = FastMCP(name=""NoAuthServer"")

    # Add a simple tool
    @mcp.tool(description=""A simple echo tool"")
    def echo(message: str) -> str:
        return f""Echo: {message}""

    # Create the StreamableHTTP app
    app: Starlette = mcp.streamable_http_app()

    return mcp, app",Create a FastMCP server with StreamableHTTP transport.,??? Create a streamable HTTP app with an echo tool using FastMCP ???
1502,delete_namespace,"def delete_namespace(self, namespace: str):
        
        try:
            self.core_v1_api.delete_namespace(name=namespace)
            self.wait_for_namespace_deletion(namespace)
            print(f""Namespace '{namespace}' deleted successfully."")
        except ApiException as e:
            if e.status == 404:
                print(f""Namespace '{namespace}' not found."")
            else:
                print(f""Error deleting namespace '{namespace}': {e}"")",Delete a specified namespace.,???Delete a specified namespace and handle potential errors.???
1503,log_memory_summary,"def log_memory_summary(self, curr_snapshot_dir):
        
        summary = torch.cuda.memory_summary(device=None, abbreviated=False)
        allocated_memory = torch.cuda.memory_allocated()

        # Save the memory summary to a file
        with open(f""{curr_snapshot_dir}/rank{self.world_info.rank}_memory_summary.txt"", ""w"") as summary_file:
            summary_file.write(summary)

        # Save the allocated memory as a text log
        with open(f""{curr_snapshot_dir}/rank{self.world_info.rank}_memory_allocated.txt"", ""w"") as alloc_file:
            alloc_file.write(f""Allocated memory: {allocated_memory / 1024 ** 2:.2f} MB\n"")

        # log this information using the logger
        self.logger.info(f""Memory summary and allocation saved for rank {self.world_info.rank} at step {self.step_num}"")",Log memory summary and memory allocated,???Log and save GPU memory usage summary and allocation details to files.???
1504,mcp_to_gemini_tool,"def mcp_to_gemini_tool(tool: McpTool) -> types.Tool:
  
  return types.Tool(
      function_declarations=[{
          ""name"": tool.name,
          ""description"": tool.description,
          ""parameters"": types.Schema.from_json_schema(
              json_schema=types.JSONSchema(
                  **_filter_to_supported_schema(tool.inputSchema)
              )
          ),
      }]
  )",Translates an MCP tool to a Google GenAI tool.,???Convert MCP tool to Gemini tool format with schema adaptation.???
1505,get_dummy_request,"def get_dummy_request(project_id: str) -> CountTokensRequest:
    
    return CountTokensRequest(
        contents=[{""role"": ""user"", ""parts"": [{""text"": ""Hi""}]}],
        endpoint=f""projects/{project_id}/locations/global/publishers/google/models/gemini-2.0-flash"",
    )",Creates a simple test request for Gemini.,???Create a token count request for a specific project using a predefined message template.???
1506,inspect_tool_executor,"def inspect_tool_executor():
    
    print(""\n⚡ Inspecting ToolExecutor..."")
    
    try:
        from chuk_tool_processor.execution.tool_executor import ToolExecutor
        
        sig = inspect.signature(ToolExecutor.__init__)
        print(f""   ToolExecutor.__init__ signature: {sig}"")
        
        for param_name, param in sig.parameters.items():
            if param.default != inspect.Parameter.empty:
                print(f""   - {param_name} default: {param.default}"")
                
        print(f""   ToolExecutor source: {inspect.getfile(ToolExecutor)}"")
        
    except ImportError as e:
        print(f""   Could not import ToolExecutor: {e}"")",Inspect ToolExecutor for timeout settings.,???Inspect and display ToolExecutor's initialization signature and source file path.???
1507,_get_server_port,"def _get_server_port(self) -> int:
        
        # Use requested port if specified
        if self.requested_port is not None:
            if not self._check_port_available(self.requested_port):
                raise RuntimeError(f""Requested port {self.requested_port} is not available"")
            return self.requested_port

        # Find a free port
        for _ in range(10):  # Try up to 10 times
            port = random.randint(49152, 65535)
            if self._check_port_available(port):
                return port

        raise RuntimeError(""Could not find an available port"")",Get an available port for the server.,"???Determine an available server port, using a requested one if possible.???"
1508,validate_project_ref,"def validate_project_ref(cls, v: str) -> str:
        
        if v.startswith(""127.0.0.1""):
            # Local development - allow default format
            return v

        # Remote project - must be 20 chars
        if len(v) != 20:
            logger.error(""Invalid Supabase project ref format"")
            raise ValueError(
                ""Invalid Supabase project ref format. ""
                ""Remote project refs must be exactly 20 characters long. ""
                f""Got {len(v)} characters instead.""
            )
        return v",Validate the project ref format.,???Validate project reference format for local or remote environments???
1509,objects,"def objects(grid: Grid, univalued: Boolean, diagonal: Boolean, without_bg: Boolean) -> Objects:
    
    bg = mostcolor(grid) if without_bg else None
    objs = set()
    occupied = set()
    h, w = len(grid), len(grid[0])
    unvisited = asindices(grid)
    diagfun = neighbors if diagonal else dneighbors
    for loc in unvisited:
        if loc in occupied:
            continue
        val = grid[loc[0]][loc[1]]
        if val == bg:
            continue
        obj = {(val, loc)}
        cands = {loc}
        while len(cands) > 0:
            neighborhood = set()
            for cand in cands:
                v = grid[cand[0]][cand[1]]
                if (val == v) if univalued else (v != bg):
                    obj.add((v, cand))
                    occupied.add(cand)
                    neighborhood |= {(i, j) for i, j in diagfun(cand) if 0 <= i < h and 0 <= j < w}
            cands = neighborhood - occupied
        objs.add(frozenset(obj))
    return frozenset(objs)",objects occurring on the grid,???Identify distinct objects in a grid based on connectivity and value criteria.???
1510,get_log_summary,"def get_log_summary(self, markdown=False):
        
        if self.response:
            if self.response.get(""data_visualization_function_path""):
                log_details = f
                if markdown:
                    return Markdown(log_details)
                else:
                    return log_details","Logs a summary of the agent's operations, if logging is enabled.","???Returns a log summary, optionally formatted in Markdown, based on response data.???"
1511,prepare_deepscaler_dataset,"def prepare_deepscaler_dataset(seed: int = 42) -> dict[str, Dataset | None]:
    
    # Load the original dataset for training
    train_ds = load_dataset(""agentica-org/DeepScaleR-Preview-Dataset"", split=""train"")

    # Load hendrydong/aime24 dataset for validation
    val_ds = load_dataset(""HuggingFaceH4/aime_2024"", split=""train"")

    # Shuffle the training dataset with the specified seed
    train_ds = train_ds.shuffle(seed=seed)

    # Format the examples, removing original columns
    train_formatted = train_ds.map(format_math, remove_columns=train_ds.column_names)
    val_formatted = val_ds.map(format_math, remove_columns=val_ds.column_names)

    # Compute accuracy 16 times per sample (matching the DeepScaleR evaluation setting)
    val_repeated = []
    for _ in range(16):
        val_repeated.extend(val_formatted)
    val_formatted = val_formatted.from_list(val_repeated)

    return {
        ""train"": train_formatted,
        ""validation"": val_formatted,
    }",Load and split the DeepScaler dataset into train and test sets.,???Prepare and format datasets for training and validation with repeated evaluation???
1512,word_to_stresses,"def word_to_stresses(word: str) -> list[list[int]]:
    
    pronunciations = phones_for_word(word)
    stresses = {tuple(int(stress) for stress in pronouncing.stresses(pronunc)) for pronunc in pronunciations}
    return [list(pronunc_stresses) for pronunc_stresses in stresses]","Convert a word to a list of stresses, for each valid pronunciation.",???Convert word into a list of unique stress patterns using phonetic data.???
1513,do_BDC,"def do_BDC(self, tag: PDFStackT, props: PDFStackT) -> None:
        
        if isinstance(tag, PSLiteral):
            self.device.begin_tag(tag, props)
        else:
            log.warning(
                f""Cannot begin marked-content sequence with property list because {tag!r} is not a PSLiteral""
            )",Begin marked-content sequence with property list,"???Begin tagged content if tag is a literal, else log warning.???"
1514,validate_param,"def validate_param(self, param_name, param_value):
        
        valid_params = {
            'fft_size': lambda x: max(256, int(2 ** np.round(np.log2(x)))) if x > 0 else 256,
            'min_frequency': lambda x: max(20.0, min(x, 20000.0)),
            'max_frequency': lambda x: max(20.0, min(x, 20000.0)),
            'num_bars': lambda x: max(1, int(x)),
            'num_points': lambda x: max(3, int(x)),
            'smoothing': lambda x: np.clip(x, 0.0, 1.0),
            'rotation': lambda x: x % 360.0,
            'curvature': lambda x: max(0.0, x),
            'separation': lambda x: max(0.0, x),
            'max_height': lambda x: max(10.0, x),
            'min_height': lambda x: max(0.0, x),
            'position_x': lambda x: np.clip(x, 0.0, 1.0),
            'position_y': lambda x: np.clip(x, 0.0, 1.0),
            'reflect': lambda x: bool(x),
            'line_width': lambda x: max(1, int(x)),
            'radius': lambda x: max(1.0, x),
            'base_radius': lambda x: max(1.0, x),
            'amplitude_scale': lambda x: max(0.0, x),
        }

        if param_name in valid_params:
            return valid_params[param_name](param_value)
        else:
            return param_value",Ensure that modulated parameter values stay within valid ranges.,???Validate and adjust parameter values based on predefined constraints and rules.???
1515,find_by_fingerprint,"def find_by_fingerprint(self, tool_name: str, fingerprint: str) -> Optional[str]:
        
        tool_name_sanitized = tool_name.replace(""-"", ""_"").replace("" "", ""_"").lower()
        manifest = self._load_manifest()
        versions = manifest.get(tool_name_sanitized, {}).get(""versions"", {})
        for version_data in versions.values():
            if version_data.get(""fingerprint"") == fingerprint:
                return version_data.get(""module_path"")
        return None",Return module path if a version with the given fingerprint exists.,???Locate module path by matching tool fingerprint in manifest data.???
1516,set_scrollbars_hidden,"def set_scrollbars_hidden(
    hidden: bool,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""hidden""] = hidden
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Emulation.setScrollbarsHidden"",
        ""params"": params,
    }
    json = yield cmd_dict",EXPERIMENTAL** :param hidden: Whether scrollbars should be always hidden.,??? Function to toggle scrollbar visibility using a command dictionary generator. ???
1517,blacklist_keys,"def blacklist_keys(d: Any, replace_with: str, *keys: re.Pattern[str]) -> Any:
    

    def _maybe_replace_value(k: Any, v: Any) -> Any:
        if isinstance(k, str) and any(p.match(k) for p in keys):
            return replace_with
        return _blacklist_keys_inner(v)

    def _blacklist_keys_inner(d: Any) -> Any:
        if isinstance(d, dict):
            return {k: _maybe_replace_value(k, v) for k, v in d.items()}  # pyright: ignore [reportUnknownVariableType]
        if isinstance(d, list):
            return [_blacklist_keys_inner(v) for v in d]  # pyright: ignore [reportUnknownVariableType]
        return d

    return _blacklist_keys_inner(d)",Recursively remove keys from a dictionary that match any of the provided patterns.,???Recursively replace dictionary keys matching patterns with a specified string.???
1518,get_conversation_context,"def get_conversation_context(self, chat_id: str) -> str:
        
        if chat_id is None:
            return """"
        system_prompt_conversation_context = ""\n\nPrevious conversation history (in chronological order):\n""
        # Get last 10 messages (will be in DESC order)
        conversation_messages = self.message_store.find_messages(
            message_type=""agent_response"", chat_id=chat_id, limit=10
        )

        # Sort by timestamp and reverse to get chronological order
        conversation_messages.sort(key=lambda x: x[""timestamp""], reverse=True)
        conversation_messages.reverse()

        # Build conversation history
        for msg in conversation_messages:
            if msg.get(""original_query""):  # Ensure we have both question and answer
                system_prompt_conversation_context += f""User: {msg['original_query']}\n""
                system_prompt_conversation_context += f""Assistant: {msg['message']}\n\n""
        # print(""system_prompt_conversation_context: "", system_prompt_conversation_context)
        return system_prompt_conversation_context",Get conversation context from the chat ID,???Retrieve and format the last ten chat messages for context.???
1519,generate_results_dict,"def generate_results_dict(model_name, t_onnx, t_engine, model_info):
        
        layers, params, gradients, flops = model_info
        return {
            ""model/name"": model_name,
            ""model/parameters"": params,
            ""model/GFLOPs"": round(flops, 3),
            ""model/speed_ONNX(ms)"": round(t_onnx[0], 3),
            ""model/speed_TensorRT(ms)"": round(t_engine[0], 3),
        }","Generates a dictionary of profiling results including model name, parameters, GFLOPs, and speed metrics.","??? 
Create a dictionary summarizing model performance metrics and specifications. 
???"
1520,process_tool_response,"def process_tool_response(result: CallToolResult) -> list:
    
    response = []
    for content in result.content:
        if isinstance(content, types.TextContent):
            text = content.text
            if isinstance(text, str):
                try:
                    text = json.loads(text)
                except json.JSONDecodeError:
                    pass
            response.append(text)
        elif isinstance(content, types.ImageContent):
            image_data = f""data:{content.mimeType};base64,{content.data}""
            response.append(image_data)
        elif isinstance(content, types.EmbeddedResource):
            # TODO: Handle embedded resources
            response.append(""Embedded resource not supported yet."")
    return response",Universal response processor for all tool endpoints,???Transform tool response content into a list of text and image data.???
1521,task_inverse,"def task_inverse(task_result: Optional[dict[str, list[int]]]) -> Optional[dict[str, list[int]]]:
    
    if task_result is None:
        return None
    return {""input"": task_result[""output""], ""output"": task_result[""input""]}",Swap the input and output arrays of a task result.,"???  
Swap input and output lists in a task result dictionary.  
???"
1522,create_client,"def create_client(provider: LLMProvider, api_key: str, model: str) -> BaseAnthropicClient:
        
        if provider == LLMProvider.ANTHROPIC:
            return AnthropicDirectClient(api_key, model)
        elif provider == LLMProvider.VERTEX:
            return AnthropicVertexClient(model)
        elif provider == LLMProvider.BEDROCK:
            return AnthropicBedrockClient(model)
        raise ValueError(f""Unsupported provider: {provider}"")",Create an appropriate client based on the provider.,"???  
Instantiate AI client based on specified provider and model type.  
???"
1523,find_available_port,"def find_available_port(start_port: int, max_attempts: int = 100) -> int:
    
    import socket
    
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('0.0.0.0', port))
                return port
        except OSError:
            continue
    raise OSError(f""Could not find an available port in range {start_port}-{start_port + max_attempts}"")",Find an available port starting from start_port,???Find an available network port starting from a specified port number.???
1524,validate_secure_endpoint,"def validate_secure_endpoint(endpoint_url):
    
    try:
        import requests
        from urllib.parse import urlparse

        if not endpoint_url:
            return False, ""Endpoint URL is required""

        # Validate HTTPS protocol
        parsed_url = urlparse(endpoint_url)
        if parsed_url.scheme != ""https"":
            return (
                False,
            )

        if not parsed_url.netloc:
            return False, ""Invalid endpoint URL format""

        # Check if the endpoint is reachable
        response = requests.get(f""{endpoint_url}/health"", timeout=10)
        if response.status_code == 200:
            return True, ""Secure endpoint is reachable and uses HTTPS""
        else:
            return False, f""Endpoint returned status code {response.status_code}""
    except requests.exceptions.ConnectionError:
        return False, ""Cannot connect to secure endpoint""
    except requests.exceptions.Timeout:
        return False, ""Connection to secure endpoint timed out""
    except Exception as e:
        return False, f""Error validating secure endpoint: {str(e)}""",Validate secure endpoint by checking if it uses HTTPS and is reachable.,???Validate and verify HTTPS endpoint accessibility and security???
1525,add,"def add(cls, name: str, desc: Optional[str] = None) -> Callable[[R], R]:
        

        # Add the registry to the registry of registries
        cls._add_to_registry_of_registries()

        def _add(
            inner_self: T,
            inner_name: str = name,
            inner_desc: Optional[str] = desc,
            inner_cls: Type[BaseRegistry] = cls,
        ) -> T:
            

            existing = inner_cls.get(inner_name, raise_on_missing=False)

            if existing and existing != inner_self:
                if inner_self.__module__ == ""__main__"":
                    return inner_self

                raise ValueError(f""Tagger {inner_name} already exists"")
            inner_cls._get_storage()[inner_name] = (inner_self, inner_desc)
            return inner_self

        return _add",Add a class to the registry.,???Register and validate unique entries in a class-based registry system.???
1526,_generate_default_name,"def _generate_default_name(
        self, route: openapi.HTTPRoute, mcp_type: MCPType
    ) -> str:
        
        name = """"

        # First check if there's a custom mapping for this operationId
        if route.operation_id:
            if route.operation_id in self._mcp_names:
                name = self._mcp_names[route.operation_id]
            else:
                # If there's a double underscore in the operationId, use the first part
                name = route.operation_id.split(""__"")[0]
        else:
            name = route.summary or f""{route.method}_{route.path}""

        name = _slugify(name)

        # Truncate to 56 characters maximum
        if len(name) > 56:
            name = name[:56]

        return name",Generate a default name from the route using the configured strategy.,"???Generate a default name for an API route using operation ID or summary, ensuring uniqueness and length constraints.???"
1527,validate_path,"def validate_path(self, path: str) -> None:
        
        parts = path.split('.')
        current = self.base_config

        for i, part in enumerate(parts):
            if not isinstance(current, dict):
                current_path = '.'.join(parts[:i])
                raise click.BadParameter(f""Cannot navigate through non-dictionary value at '{current_path}'"")
            if part not in current:
                if i == len(parts) - 1:
                    current[part] = _Placeholder()
                else:
                    current[part] = {}

            current = current[part]",Validate if a path exists in base config,???Ensure nested dictionary path exists or create placeholders if missing???
1528,generate_sequences,"def generate_sequences(self, lm_inputs: DataProto) -> DataProto:
        

        messages_list = lm_inputs.non_tensor_batch['messages_list'].tolist()
        results, failed_messages = self.llm.run_batch(
            messages_list=messages_list,
            **self.llm_kwargs
        )
        assert not failed_messages, f""Failed to generate responses for the following messages: {failed_messages}""

        texts = [result[""response""] for result in results]
        print(f'[DEBUG] texts: {texts}')
        lm_outputs = DataProto()
        lm_outputs.non_tensor_batch = {
			'response_texts': texts,
			'env_ids': lm_inputs.non_tensor_batch['env_ids'],
			'group_ids': lm_inputs.non_tensor_batch['group_ids']
		} # this is a bit hard-coded to bypass the __init__ check in DataProto
        lm_outputs.meta_info = lm_inputs.meta_info
        
        return lm_outputs","Convert the input ids to text, make API calls to generate responses, and create a DataProto with the results.",???Generate response sequences from input messages using a language model.???
1529,download_dub_file,"def download_dub_file(
        self, dubbing_id: str, language_code: str, output_path: str
    ) -> Optional[str]:
        
        try:
            with open(output_path, ""wb"") as file:
                for chunk in self.client.dubbing.get_dubbed_file(
                    dubbing_id, language_code
                ):
                    file.write(chunk)
            return output_path
        except Exception as e:
            print(f""Error downloading dubbed file: {str(e)}"")
            return None",Download the dubbed file.,"???Download a dubbed file by ID and language, saving it to a specified path.???"
1530,generate,"def generate(self, prompt: str) -> str:
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{""role"": ""user"", ""content"": prompt}]
        )
        content = response.choices[0].message.content
        if content is None:
            raise ValueError(""OpenAI response content is None"")
        return content",Generate a response based on the given prompt.,"??? 
Fetch AI-generated text response based on user input prompt. 
???"
1531,_error_code_to_http_status,"def _error_code_to_http_status(self, error_code: int) -> int:
        
        error_map = {
            -32700: 400,  # Parse error
            -32600: 400,  # Invalid Request
            -32601: 404,  # Method not found
            -32602: 400,  # Invalid params
            -32603: 500,  # Internal error
        }
        return error_map.get(error_code, 500)",Map JSON-RPC error codes to HTTP status codes.,"???  
Map error codes to corresponding HTTP status responses.  
???"
1532,get_auth_headers,"def get_auth_headers(self) -> Dict[str, str]:
        
        headers = {}
        if self.auth_token:
            headers['Authorization'] = f'Bearer {self.auth_token}'
        return headers",Get headers with authorization if token is available.,???Generate authorization headers using a bearer token if available.???
1533,_extract_final_answer,"def _extract_final_answer(text: str) -> str:
    
    if ""####"" in text:
        match = re.search(r""####\s*(.*?)(?:\s*$|\n)"", text)
        if match:
            return match.group(1).strip()

    if ""\\boxed{"" in text:
        match = re.search(r""\\boxed\{([^}]+)\}"", text)
        if match:
            return match.group(1).strip()

    return text","Extract the final answer from text with various formats (GSM8K, boxed, etc).",???Extracts a final answer from text using specific delimiters.???
1534,extract_text_from_file,"def extract_text_from_file(file_path):
    
    try:
        # Expand ~ to user's home directory if present
        file_path = os.path.expanduser(file_path)

        if file_path.lower().endswith("".pdf""):
            # Handle PDF file
            doc = fitz.open(file_path)
            text = """"
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        elif file_path.lower().endswith(("".txt"", "".py"", "".md"")):
            # Handle text-based files
            with open(file_path, ""r"", encoding=""utf-8"") as f:
                return f.read()
        else:
            raise ValueError(
                ""Unsupported file format. Only PDF, TXT, PY, and MD files are supported.""
            )
    except Exception as e:
        print(f""Error reading file: {str(e)}"")
        return """"","Extract text from a PDF, TXT, Python, or Markdown file.","???Extracts text content from PDF or text-based files, handling errors gracefully.???"
1535,supported_languages_data,"def supported_languages_data() -> List[Dict[str, Any]]:
    
    return [
        {""code"": ""en"", ""name"": ""English"", ""nativeName"": ""English""},
        {""code"": ""ja"", ""name"": ""Japanese"", ""nativeName"": ""日本語""},
        {""code"": ""zh"", ""name"": ""Chinese"", ""nativeName"": ""中文""},
        {""code"": ""ko"", ""name"": ""Korean"", ""nativeName"": ""한국어""},
    ]",Return a list of supported languages.,???Function returns a list of dictionaries with language codes and names.???
1536,get_repo_path,"def get_repo_path(self, owner: str, repo: str, sha: str) -> str:
        
        if not self.config.cache_repos:
            # Caching disabled, use temporary directory
            return self._clone_to_temp(owner, repo, sha)

        repo_cache_dir = self.cache_dir / owner / repo

        # Check if we have a valid cached version
        if self._is_cache_valid(repo_cache_dir, sha):
            if not self.quiet:
                print(f""Using cached repository: {repo_cache_dir}"")
            self._checkout_sha(repo_cache_dir, sha)
            return str(repo_cache_dir)

        # Need to clone or update cache
        return self._update_cache(owner, repo, sha, repo_cache_dir)","Get repository path, using cache if available and valid.",???Determine repository path by checking cache or cloning if necessary???
1537,convert_types,"def convert_types(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        
        # Convert string numbers to integers
        for field in [""page"", ""per_page""]:
            if field in data and isinstance(data[field], str):
                try:
                    data[field] = int(data[field])
                except ValueError:
                    raise ValueError(f""{field} must be a valid integer"")

        # Convert various string representations to booleans
        for field in [""regexp"", ""case"", ""words""]:
            if field in data:
                if isinstance(data[field], str):
                    data[field] = data[field].lower() in [""true"", ""1"", ""yes"", ""on""]

        return data",Convert input types before validation,???Transform dictionary values from strings to integers or booleans based on field names.???
1538,delete_agent_action_groups,"def delete_agent_action_groups(self, agent_id):
        
        try:
            # List action groups
            response = self.bedrock_agent.list_agent_action_groups(
                agentId=agent_id,
                agentVersion='DRAFT'
            )
            
            action_groups = response.get('agentActionGroupSummaries', [])
            print(f""Found {len(action_groups)} action groups for agent {agent_id}"")
            
            # Delete each action group
            for action_group in action_groups:
                try:
                    print(f""Deleting action group: {action_group['actionGroupName']}"")
                    self.bedrock_agent.delete_agent_action_group(
                        agentId=agent_id,
                        agentVersion='DRAFT',
                        actionGroupId=action_group['actionGroupId']
                    )
                except Exception as e:
                    print(f""Error deleting action group {action_group['actionGroupName']}: {str(e)}"")
            
            return True
        except Exception as e:
            print(f""Error listing action groups for agent {agent_id}: {str(e)}"")
            return False",Delete all action groups for an agent,"???Delete all action groups for a specified agent, handling errors.???"
1539,set_show_container_query_overlays,"def set_show_container_query_overlays(
    container_query_highlight_configs: typing.List[ContainerQueryHighlightConfig],
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""containerQueryHighlightConfigs""] = [
        i.to_json() for i in container_query_highlight_configs
    ]
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Overlay.setShowContainerQueryOverlays"",
        ""params"": params,
    }
    json = yield cmd_dict",:param container_query_highlight_configs: An array of node identifiers and descriptors for the highlight appearance.,???Configure and send overlay settings for container queries in a web environment.???
1540,from_file,"def from_file(cls, file_path: Union[str, Path]) -> OpenAPISpecParser:
        
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f""No file found at {file_path}"")
        with path.open(""r"") as file:
            return cls.from_string(file.read())",Load an OpenAPI spec from a local file.,???Parse OpenAPI specification from a file path into a parser object.???
1541,reactivity_explicitly_disabled,"def reactivity_explicitly_disabled(config_path: str = ""preswald.toml"") -> bool:
    
    try:
        return read_disable_reactivity(config_path)
    except Exception as e:
        logger.warning(f""[is_app_reactivity_disabled] Failed to read config: {e}"")
        return False",Check if reactivity is disabled in project configuration.,"???Determine if reactivity is disabled by reading a configuration file, with error handling.???"
1542,load_global_config,"def load_global_config():
    
    # Lazy import to avoid circular dependencies
    from quantalogic_codeact.codeact.agent_config import AgentConfig
    
    # Use the low-level utility to load the YAML data
    config_data = load_yaml_config(GLOBAL_CONFIG_PATH)
    
    # If the file was empty or not found, use defaults
    if not config_data:
        return AgentConfig(**GLOBAL_DEFAULTS)
        
    # Create AgentConfig from the loaded data
    try:
        return AgentConfig(**config_data)
    except Exception as e:
        logger.error(f""Failed to create AgentConfig from data: {e}. Using defaults."")
        return AgentConfig(**GLOBAL_DEFAULTS)","Load or initialize global config as an AgentConfig, using defaults when loaded values are None.","???Load global configuration, defaulting to predefined settings on failure.???"
1543,generate_text,"def generate_text(self, prompt: str, system_prompt: str, model: str = None, **kwargs) -> str:
        
        try:
            client = self._get_client()
            
            # Use configured model if none provided
            if not model:
                model = self.config[""model""]

            message = client.messages.create(
                model=model,
                max_tokens=1000,
                temperature=0,
                system=system_prompt,
                messages=[
                    {
                        ""role"": ""user"",
                        ""content"": [
                            {
                                ""type"": ""text"",
                                ""text"": prompt
                            }
                        ]
                    }
                ]
            )
            return message.content[0].text
            
        except Exception as e:
            raise AnthropicAPIError(f""Text generation failed: {e}"")",Generate text using Anthropic models,???Generate text using a specified or default model with error handling.???
1544,handle_error,"def handle_error(error: Exception, context: str = None) -> None:
    
    error_msg = f""Error in {context}: {str(error)}"" if context else str(error)
    logger.error(error_msg, exc_info=True)
    st.error(error_msg)",Centralized error handling with logging,???Log and display error messages with optional context information.???
1545,_sentence_transformers_unavailable,"def _sentence_transformers_unavailable():
    
    try:
        # Actually try to import SentenceTransformer class to catch import chain issues
        # This will trigger all the dependent imports (transformers, etc.)
        from sentence_transformers import SentenceTransformer

        # Try to initialize a model name to further validate the import chain
        if not hasattr(SentenceTransformer, ""__init__""):
            print(""DEBUG: SentenceTransformer class doesn't have expected attributes"")
            return True

        print(""DEBUG: sentence_transformers and SentenceTransformer class imported OK"")
        return False
    except (ImportError, ModuleNotFoundError) as err:
        print(f""DEBUG: SentenceTransformer ImportError → skipping test: {err}"")
        return True
    except ValueError as err:
        # Catch specific ValueError from importlib.util about openai.__spec__ is None
        print(f""DEBUG: ValueError during SentenceTransformer import → skipping test: {err}"")
        return True
    except Exception as err:  # pragma: no cover - other unexpected issues
        print(f""DEBUG: Unexpected error during SentenceTransformer check → skipping: {err}"")
        return True",Return True if SentenceTransformer or model cannot be imported/loaded.,???Check if the SentenceTransformer library is available and correctly imported.???
1546,_validate_inputs,"def _validate_inputs(
        self,
        code_directory: Path,
        command: list[str],
        mem_limit: str,
        cpu_shares: int,
    ) -> None:
        

        if not code_directory.exists():
            raise SandboxExecutionError(
                f""Failed to run sandbox container: code directory not found: {code_directory}""
            )

        if not command or any(
            not isinstance(c, str) or any(x in c for x in ["";"", ""&"", ""|"", ""`"", ""\n""])
            for c in command
        ):
            raise ValueError(""Invalid command passed to sandbox"")

        if cpu_shares <= 0 or cpu_shares > MAX_CPU_SHARES:
            raise ValueError(""cpu_shares out of allowed range"")

        if mem_limit[-1].lower() not in {""m"", ""g""} or not mem_limit[:-1].isdigit():
            raise ValueError(""mem_limit must be like '256m' or '1g'"")",Validate inputs and resource limits for sandbox execution.,"???Validate sandbox execution parameters for directory, command, memory, and CPU constraints???"
1547,file_description,"def file_description(self) -> str:
        
        if self.files:
            file_description = ""You can work with the following files:\n""
            for file in self.files:
                name = getattr(file, ""name"", ""Unnamed file"")
                description = getattr(file, ""description"", ""No description"")
                file_description += f""<file>: {name} - {description} <\\file>\n""
            return file_description
        return """"",Returns a description of the files available to the agent.,??? Generate a formatted list of file names and descriptions if available ???
1548,run_publisher,"def run_publisher(context: dict, **kwargs) -> dict:
    

    report_date = context.get(""date"")
    introduction = context.get(""introduction"")
    table_of_contents = context.get(""table_of_contents"")
    conclusion = context.get(""conclusion"")

    headers = context.get(""headers"")
    title = headers.get(""title"")
    date_label = headers.get(""date"")

    references = ""\n"".join(context.get(""sources"", []))
    sections = ""\n\n"".join(context.get(""research_data"", []))

    # Construct the report layout
    report_layout = f

    return {""report"": report_layout, ""result"": ""success""}",Generates a formatted research report based on provided context data.,???Generate a structured report from context data and return success status.???
1549,prompt_for_git_provider,"def prompt_for_git_provider() -> str:
    
    providers = [""github""]  # Currently only GitHub is supported
    console.print(""\n🔄 Git Provider Selection"", style=""bold blue"")
    for i, provider in enumerate(providers, 1):
        console.print(f""{i}. {provider}"")

    while True:
        choice = click.prompt(
            ""\nSelect git provider"",
            type=click.Choice([""1""]),  # Only allow '1' since GitHub is the only option
            default=""1"",
        )
        return providers[int(choice) - 1]",Interactively prompt user for git provider selection.,"???  
Prompt user to select a Git provider, defaulting to GitHub.  
???"
1550,_import_submodules,"def _import_submodules():
    
    # Get the path of this package
    package_path = Path(__file__).parent
    
    # Find all modules in this package
    for _, module_name, is_pkg in pkgutil.iter_modules([str(package_path)]):
        if not module_name.startswith('_'):  # Skip private modules
            try:
                importlib.import_module(f""{__package__}.{module_name}"")
            except ImportError as e:
                # Log but don't crash
                print(f""Warning: Could not import command module {module_name}: {e}"")",Import all submodules to allow them to register their commands.,???Automatically import all non-private modules within a package directory.???
1551,is_configured,"def is_configured(self, verbose: bool = False) -> bool:
        
        try:
            load_dotenv()
            
            if not os.getenv('MONAD_PRIVATE_KEY'):
                if verbose:
                    logger.error(""Missing MONAD_PRIVATE_KEY in .env"")
                return False

            if not self._web3.is_connected():
                if verbose:
                    logger.error(""Not connected to Monad network"")
                return False
                
            # Test account access
            account = self._get_current_account()
            balance = self._web3.eth.get_balance(account.address)
                
            return True

        except Exception as e:
            if verbose:
                logger.error(f""Configuration check failed: {e}"")
            return False",Check if Monad connection is properly configured,???Check if environment and network are properly configured for Monad operations???
1552,_load_embedding_model,"def _load_embedding_model(self):
        
        self.logger.info(f""Loading embedding model: {self.embedding_model_name}"")
        try:
            self.embedding_tokenizer = AutoTokenizer.from_pretrained(
                self.embedding_model_name, token=self.hf_token
            )

            # For embedding models we typically use AutoModel instead of AutoModelForCausalLM
            self.embedding_model = AutoModel.from_pretrained(
                self.embedding_model_name, token=self.hf_token
            )

            # Move to appropriate device
            self.embedding_model.to(self.device)

            self.embedding_model.eval()
            self.logger.info(
                f""Successfully loaded embedding model: {self.embedding_model_name}""
            )
        except Exception as e:
            self.logger.error(f""Error loading embedding model: {e}"")
            raise",Load a separate model for generating embeddings.,???Load and initialize a pre-trained embedding model with error handling and logging.???
1553,decode,"def decode(self, text: str) -> Optional[Dict[str, Any]]:
        
        if self.enable_thinking:
            return self._parse_response(text)

        return {""content"": text}",Parse thinking content from model output,???The code conditionally parses text into a structured response or returns it as content.???
1554,list_result_files,"def list_result_files(s3_client, workspace_path):
    
    bucket, prefix = parse_s3_path(workspace_path)
    documents_prefix = os.path.join(prefix, ""documents"").rstrip(""/"") + ""/""

    all_files = []
    paginator = s3_client.get_paginator(""list_objects_v2"")

    for page in paginator.paginate(Bucket=bucket, Prefix=documents_prefix):
        if ""Contents"" in page:
            all_files.extend(
                [
                    for obj in page[""Contents""]
                    if (
                        obj[""Key""].endswith("".jsonl"")
                        or obj[""Key""].endswith("".json"")
                        or obj[""Key""].endswith("".jsonl.gz"")
                        or obj[""Key""].endswith("".jsonl.zst"")
                        or obj[""Key""].endswith("".jsonl.ztd"")
                        or obj[""Key""].endswith("".jsonl.zstd"")
                    )
                ]
            )

            if len(all_files) % 100 == 0:
                logger.info(f""Found {len(all_files)} files so far..."")

    logger.info(f""Total files found: {len(all_files)}"")
    return all_files",List all JSONL files in the workspace documents directory.,???List JSON-related files from an S3 bucket's documents directory using pagination.???
1555,_get_embeddings,"def _get_embeddings(self, text):
        
        if CosineScaledReward._model is None or CosineScaledReward._tokenizer is None:
            logger.error(""Model or tokenizer not available for embeddings"")
            return None

        try:
            # Tokenize and prepare for the model
            encoded_input = CosineScaledReward._tokenizer(
                text, padding=True, truncation=True, return_tensors=""pt""
            )

            # Get model output
            with torch.no_grad():
                model_output = CosineScaledReward._model(**encoded_input)

            # Perform mean pooling
            sentence_embeddings = self._mean_pooling(
                model_output, encoded_input[""attention_mask""]
            )

            return sentence_embeddings.numpy()
        except Exception as e:
            logger.error(f""Error getting embeddings: {e}"")
            logger.exception(e)
            return None",Get embeddings for text using the model,???Generate text embeddings using a pre-trained model and tokenizer with error handling.???
1556,expected_basename,"def expected_basename(query_sequence: str) -> str:
    
    seqhash = hash_sequence(query_sequence.upper())
    return f""{seqhash}.aligned.pqt""",Get the expected filename based on the uppercased query sequence.,???Generate a filename using a hashed sequence from the input string.???
1557,resource_prompt_file,"def resource_prompt_file(self):
        
        with tempfile.NamedTemporaryFile(mode=""w+"", suffix="".txt"", delete=False) as tf:
            tf.write()
            tf_path = Path(tf.name)

        # Create sample resource files
        sample_path = tf_path.parent / ""sample_python.txt""
        analysis_path = tf_path.parent / ""analysis_python.txt""
        with open(sample_path, ""w"", encoding=""utf-8"") as f:
            f.write(""def hello():\n    print('Hello, world!')"")
        with open(analysis_path, ""w"", encoding=""utf-8"") as f:
            f.write(""# Analysis\nYour function looks good but could use a docstring."")

        yield tf_path

        # Cleanup
        os.unlink(tf_path)
        if sample_path.exists():
            os.unlink(sample_path)
        if analysis_path.exists():
            os.unlink(analysis_path)",Create a prompt file with resources for testing,"???Generate temporary files for code samples and analysis, then clean up.???"
1558,report_bonds,"def report_bonds(self) -> None:
        
        for i, (atom_a, atom_b) in enumerate(zip(*self.atom_covalent_bond_indices)):
            tok_a = self.atom_token_index[atom_a]
            tok_b = self.atom_token_index[atom_b]
            asym_a = self.token_asym_id[tok_a]
            asym_b = self.token_asym_id[tok_b]
            res_idx_a = self.token_residue_index[tok_a]
            res_idx_b = self.token_residue_index[tok_b]
            resname_a = tensorcode_to_string(self.token_residue_name[tok_a])
            resname_b = tensorcode_to_string(self.token_residue_name[tok_b])
            logger.info(
                f""Bond {i} (asym res_idx resname): {asym_a} {res_idx_a} {resname_a} <> {asym_b} {res_idx_b} {resname_b}""
            )",Log information about covalent bonds.,???Log detailed information about covalent bonds between atoms in a molecular structure.???
1559,do_Tr,"def do_Tr(self, render: PDFStackT) -> None:
        
        render_i = safe_int(render)

        if render_i is None:
            log.warning(
                f""Could not set text rendering mode because {render!r} is an invalid int value""
            )
        else:
            self.textstate.render = render_i",Set the text rendering mode,???Set text rendering mode with validation and logging???
1560,register_tooltips,"def register_tooltips():
    

    # WhisperToPromptTravel tooltips
    TooltipManager.register_tooltips(""WhisperToPromptTravel"", {
        ""segments_alignment"": ""JSON string containing segment alignments from Whisper. Each segment should have 'start' (timestamp) and 'value' (text) fields."",
        ""fps"": ""Frame rate of the video to sync with (0.1 to 120.0 fps). Used to convert timestamps to frame numbers."",
    })",Register tooltips for misc nodes,???Register tooltips for video synchronization using segment alignments and frame rate.???
1561,_format_complex,"def _format_complex(self, z: complex) -> str:
        
        real, imag = z.real, z.imag
        if abs(imag) < 1e-10:
            return f""{real:.2f}""
        elif abs(real) < 1e-10:
            return f""{imag:.2f}i""
        else:
            sign = ""+"" if imag >= 0 else ""-""
            return f""{real} {sign} {abs(imag)}i""",Format complex number with 2 decimal places.,???Format complex numbers into a human-readable string representation.???
1562,_find_divergences,"def _find_divergences(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        
        divergences = {}
        
        try:
            # RSI Divergences
            divergences['rsi'] = self._find_indicator_divergences(df, df['rsi'], 'RSI')
            
            # MACD Divergences
            divergences['macd'] = self._find_indicator_divergences(df, df['macd'], 'MACD')
            
            # OBV Divergences
            divergences['obv'] = self._find_indicator_divergences(df, df['obv'], 'OBV')
            
            return divergences
            
        except Exception as e:
            logger.error(f""Error finding divergences: {e}"")
            raise",Identify regular and hidden divergences.,"???Identify and log divergences in financial indicators like RSI, MACD, and OBV.???"
1563,initialize_session_state,"def initialize_session_state() -> None:
    
    if ""user_chats"" not in st.session_state:
        st.session_state[""session_id""] = str(uuid.uuid4())
        st.session_state.uploader_key = 0
        st.session_state.run_id = None
        st.session_state.user_id = USER
        st.session_state[""gcs_uris_to_be_sent""] = """"
        st.session_state.modified_prompt = None
        st.session_state.session_db = LocalChatMessageHistory(
            session_id=st.session_state[""session_id""],
            user_id=st.session_state[""user_id""],
        )
        st.session_state.user_chats = (
            st.session_state.session_db.get_all_conversations()
        )
        st.session_state.user_chats[st.session_state[""session_id""]] = {
            ""title"": EMPTY_CHAT_NAME,
            ""messages"": [],
        }",Initialize the session state with default values.,???Initialize session state with unique identifiers and chat history for user interactions.???
1564,_format_ground_truth_annotations_for_detection,"def _format_ground_truth_annotations_for_detection(img_idx, image_path, batch, class_name_map=None):
    
    indices = batch[""batch_idx""] == img_idx
    bboxes = batch[""bboxes""][indices]
    if len(bboxes) == 0:
        LOGGER.debug(f""COMET WARNING: Image: {image_path} has no bounding boxes labels"")
        return None

    cls_labels = batch[""cls""][indices].squeeze(1).tolist()
    if class_name_map:
        cls_labels = [str(class_name_map[label]) for label in cls_labels]

    original_image_shape = batch[""ori_shape""][img_idx]
    resized_image_shape = batch[""resized_shape""][img_idx]
    ratio_pad = batch[""ratio_pad""][img_idx]

    data = []
    for box, label in zip(bboxes, cls_labels):
        box = _scale_bounding_box_to_original_image_shape(box, resized_image_shape, original_image_shape, ratio_pad)
        data.append(
            {
                ""boxes"": [box],
                ""label"": f""gt_{label}"",
                ""score"": _scale_confidence_score(1.0),
            }
        )

    return {""name"": ""ground_truth"", ""data"": data}",Format ground truth annotations for detection.,???Format and scale ground truth annotations for object detection with optional class mapping.???
1565,send_to_preview,"def send_to_preview(code):
    
    encoded_html = base64.b64encode(code.encode('utf-8')).decode('utf-8')
    data_uri = f""data:text/html;charset=utf-8;base64,{encoded_html}""
    return f'<iframe src=""{data_uri}"" width=""100%"" height=""920px""></iframe>'",Convert code to base64 encoded iframe source.,???Convert HTML code to a base64-encoded data URI for iframe preview display.???
1566,_log_llm_call_info,"def _log_llm_call_info(self, input_messages: list[BaseMessage], method: str) -> None:
		
		# Count messages and check for images
		message_count = len(input_messages)
		total_chars = sum(len(str(msg.content)) for msg in input_messages)
		has_images = any(
			hasattr(msg, 'content')
			and isinstance(msg.content, list)
			and any(isinstance(item, dict) and item.get('type') == 'image_url' for item in msg.content)
			for msg in input_messages
		)
		current_tokens = getattr(self._message_manager.state.history, 'current_tokens', 0)

		# Count available tools/actions from the current ActionModel
		# This gives us the actual number of tools exposed to the LLM for this specific call
		tool_count = len(self.ActionModel.model_fields) if hasattr(self, 'ActionModel') else 0

		# Format the log message parts
		image_status = ', 📷 img' if has_images else ''
		if method == 'raw':
			output_format = '=> raw text'
			tool_info = ''
		else:
			output_format = '=> JSON out'
			tool_info = f' + 🔨 {tool_count} tools ({method})'

		term_width = shutil.get_terminal_size((80, 20)).columns
		print('=' * term_width)
		self.logger.info(
			f'🧠 LLM call => {self.chat_model_library} [✉️ {message_count} msg, ~{current_tokens} tk, {total_chars} char{image_status}] {output_format}{tool_info}'
		)",Log comprehensive information about the LLM call being made,"???Log LLM call details including message count, character total, and tool availability.???"
1567,get_value,"def get_value(target: dict, path_parts: List[str]) -> Any:
    
    if not path_parts:
        return target

    current = target
    for i, part in enumerate(path_parts):
        if part.isdigit() and isinstance(current, list):
            part = int(part)
        if part not in current:
            raise ValueError(f""Path not found: {'/'.join(path_parts[:i+1])}"")
        current = current[part]

    return current",Get a value at a specific path.,???Navigate nested structures to retrieve a specified value by path.???
1568,_fetch_one,"def _fetch_one(self, sql: str, params: Optional[Union[tuple, Dict]] = None) -> Optional[Dict]:
        
        try:
            if self._is_sqlalchemy:
                 with self.conn.connect() as connection:
                      # No need for transaction for SELECT
                      result = connection.execute(text(sql), params or {})
                      row = result.fetchone()
                      return dict(row._mapping) if row else None
            else:
                 self.cursor.execute(sql, params or ())
                 row = self.cursor.fetchone()
                 return dict(row) if row else None
        except (sqlite3.Error, SQLAlchemyError) as e:
             error_type = ""SQLAlchemy"" if self._is_sqlalchemy else ""SQLite""
             print(f""{error_type} Error fetching one: {sql[:100]}... Error: {e}"")
             return None # Return None on error
        except Exception as e:
            print(f""Unexpected error in _fetch_one: {e}"")
            return None","Fetches a single row, adapting to connection type.","???Fetch a single database record using SQLAlchemy or SQLite, handling errors gracefully.???"
1569,_load_profile,"def _load_profile(self, profile_path: str) -> Dict[str, Any]:
        
        try:
            with open(profile_path, ""r"") as file:
                profile = json.load(file)
            return profile
        except Exception as e:
            raise ValueError(f""Failed to load profile from {profile_path}: {e}"")",Load student profile from JSON file.,"???Load and parse a JSON profile file, handling errors gracefully.???"
1570,parse_headers,"def parse_headers(self) -> bool:
        
        try:
            if b""\r\n\r\n"" not in self.buffer:
                return False

            headers_end = self.buffer.index(b""\r\n\r\n"")
            headers = self.buffer[:headers_end].split(b""\r\n"")

            request = headers[0].decode(""utf-8"")
            method, full_path, version = request.split("" "")

            self.request = HttpRequest(
                method=method,
                path=extract_path(full_path),
                version=version,
                headers=[header.decode(""utf-8"") for header in headers[1:]],
                original_path=full_path,
                target=full_path if method == ""CONNECT"" else None,
            )

            logger.debug(f""Request: {method} {full_path} {version}"")
            return True

        except Exception as e:
            logger.error(f""Error parsing headers: {e}"")
            return False",Parse HTTP headers from buffer,"???Parses HTTP headers from buffer, constructs request object, and logs errors.???"
1571,cleanup,"def cleanup(self):
        
        # Store current device before cleanup
        current_device = self.device
        
        # Clear rembg session
        if hasattr(self, 'rembg_session') and self.rembg_session is not None:
            del self.rembg_session
            self.rembg_session = None
        
        # Clear CPU models
        if hasattr(self, '_models_cpu'):
            for key in list(self._models_cpu.keys()):
                del self._models_cpu[key]
            self._models_cpu.clear()
        
        # Clear GPU models while preserving the models dict
        if hasattr(self, 'models'):
            for key in list(self.models.keys()):
                del self.models[key]
        
        # Force memory cleanup
        torch.cuda.empty_cache()
        gc.collect()
        
        # Restore essential properties
        self._device = current_device
        if not hasattr(self, 'models'):
            self.models = {}
        if not hasattr(self, '_models_cpu'):
            self._models_cpu = {}",Explicit cleanup method that preserves essential properties,???Release resources and reset device state for efficient memory management.???
1572,dump_image,"def dump_image(line, img_root):
    
    os.makedirs(img_root, exist_ok=True)
    
    if 'image' in line:
        if isinstance(line['image'], list):
            tgt_path = []
            assert 'image_path' in line
            for img, im_name in zip(line['image'], line['image_path']):
                path = os.path.join(img_root, im_name)
                if not os.path.exists(path):
                    decode_base64_to_image_file(img, path)
                tgt_path.append(path)
        else:
            tgt_path = os.path.join(img_root, f""{line['index']}.jpg"")
            if not os.path.exists(tgt_path):
                decode_base64_to_image_file(line['image'], tgt_path)
            tgt_path = [tgt_path]
    else:
        assert 'image_path' in line
        tgt_path = toliststr(line['image_path'])
    
    return tgt_path",Save image data to disk and return the path.,"???Convert base64 images to files, ensuring directory creation and path management.???"
1573,restore_terminal,"def restore_terminal(old_settings):
    
    if sys.platform != ""win32"" and termios and old_settings:
        try:
            termios.tcsetattr(sys.stdin.fileno(), termios.TCSADRAIN, old_settings)
        except (termios.error, AttributeError) as e:
            logger.warning(f""Failed to restore terminal settings: {e}"")",Restore terminal settings based on platform.,"???Restore terminal settings on non-Windows systems, handling potential errors.???"
1574,_save_checkpoint,"def _save_checkpoint(self):
        
        # path: given_path + `/global_step_{global_steps}` + `/actor`
        local_global_step_folder = os.path.join(self.config.trainer.default_local_dir, f""global_step_{self.global_steps}"")

        print(f""local_global_step_folder: {local_global_step_folder}"")
        actor_local_path = os.path.join(local_global_step_folder, ""actor"")

        actor_remote_path = None if self.config.trainer.default_hdfs_dir is None else os.path.join(self.config.trainer.default_hdfs_dir, f""global_step_{self.global_steps}"", ""actor"")

        remove_previous_ckpt_in_save = self.config.trainer.get(""remove_previous_ckpt_in_save"", False)
        if remove_previous_ckpt_in_save:
            print(""Warning: remove_previous_ckpt_in_save is deprecated,"" + "" set max_actor_ckpt_to_keep=1 and max_critic_ckpt_to_keep=1 instead"")
        max_actor_ckpt_to_keep = self.config.trainer.get(""max_actor_ckpt_to_keep"", None) if not remove_previous_ckpt_in_save else 1
        max_critic_ckpt_to_keep = self.config.trainer.get(""max_critic_ckpt_to_keep"", None) if not remove_previous_ckpt_in_save else 1

        self.actor_rollout_wg.save_checkpoint(actor_local_path, actor_remote_path, self.global_steps, max_ckpt_to_keep=max_actor_ckpt_to_keep)

        if self.use_critic:
            critic_local_path = os.path.join(local_global_step_folder, ""critic"")
            critic_remote_path = None if self.config.trainer.default_hdfs_dir is None else os.path.join(self.config.trainer.default_hdfs_dir, f""global_step_{self.global_steps}"", ""critic"")
            self.critic_wg.save_checkpoint(critic_local_path, critic_remote_path, self.global_steps, max_ckpt_to_keep=max_critic_ckpt_to_keep)

        # latest checkpointed iteration tracker (for atomic usage)
        local_latest_checkpointed_iteration = os.path.join(self.config.trainer.default_local_dir, ""latest_checkpointed_iteration.txt"")
        with open(local_latest_checkpointed_iteration, ""w"") as f:
            f.write(str(self.global_steps))","Different from VerlRayPPOTrainer, we have no dataloader so we won""t save it.","???Save model checkpoints locally and remotely, managing retention and logging progress.???"
1575,service_data,"def service_data() -> Dict[str, Any]:
    
    return {
        ""code"": ""amazon-elastic-compute-cloud-linux"",
        ""name"": ""Amazon Elastic Compute Cloud (Linux)"",
        ""categories"": [
            {""code"": ""using-aws"", ""name"": ""Using AWS""},
            {""code"": ""performance"", ""name"": ""Performance""},
        ],
    }",Return a dictionary with sample service data.,"???  
Return metadata for an Amazon Elastic Compute Cloud Linux service.  
???"
1576,react_to_message,"def react_to_message(
        self, channel_id: str, message_id: str, emoji_name: str, **kwargs
    ) -> None:
        
        logger.debug(""Reacting to a message"")

        request_path = (
            f""/channels/{channel_id}/messages/{message_id}/reactions/{emoji_name}/@me""
        )
        self._put_request(request_path)

        logger.info(""Reacted to message successfully"")
        return",React to a message,???Add emoji reaction to a specified message in a channel.???
1577,get_staged_files,"def get_staged_files() -> List[Path]:
    
    try:
        result = subprocess.run(
            [""git"", ""diff"", ""--cached"", ""--name-only"", ""--diff-filter=AMR""], capture_output=True, text=True, check=True
        )
        files = result.stdout.splitlines()
        return [Path(file) for file in files if file.endswith("".py"")]
    except subprocess.CalledProcessError as e:
        print(f""Error getting staged files: {e}"")
        return []",Get list of staged Python files using git command.,???Retrieve staged Python files from Git for further processing.???
1578,export_tflite,"def export_tflite(self, keras_model, nms, agnostic_nms, prefix=colorstr(""TensorFlow Lite:"")):
        
        import tensorflow as tf  # noqa

        LOGGER.info(f""\n{prefix} starting export with tensorflow {tf.__version__}..."")
        saved_model = Path(str(self.file).replace(self.file.suffix, ""_saved_model""))
        if self.args.int8:
            f = saved_model / f""{self.file.stem}_int8.tflite""  # fp32 in/out
        elif self.args.half:
            f = saved_model / f""{self.file.stem}_float16.tflite""  # fp32 in/out
        else:
            f = saved_model / f""{self.file.stem}_float32.tflite""
        return str(f), None",YOLO TensorFlow Lite export.,???Export a Keras model to TensorFlow Lite format with optional quantization.???
1579,read_pdf,"def read_pdf(file_path: str) -> List[dict]:
    
    try:
        reader = PyPDFReader()
        documents = reader.load(file_path)
        return [doc.model_dump() for doc in documents]
    except Exception as e:
        logger.error(f""Error reading document: {e}"")
        raise",Reads and extracts text from a PDF document.,???Extract structured data from a PDF file and handle errors.???
1580,_build_headers,"def _build_headers(self, device_time: str, sign: str) -> Dict[str, str]:
        
        return {
            'User-Agent': ""Cronet/TTNetVersion:d4572e53 2024-06-12 QuicVersion:4bf243e0 2023-04-17"",
            'appvr': ""6.6.0"",
            'device-time': str(device_time),
            'pf': ""4"",
            'sign': sign,
            'sign-ver': ""1"",
            'tdid': self.tdid,
        }",Build headers for requests,"???Constructs HTTP headers with device time, signature, and version info???"
1581,audio_generation,"def audio_generation(self, user_input):
        
        if self.tts_engine == ""bark"":
            # Bark Generation
            c_text = ctypes.c_char_p(user_input.encode('utf-8'))
            success = bark_cpp.bark_generate_audio(
                self.context, c_text, self.n_threads)

            if not success:
                raise RuntimeError(""Failed to generate audio with Bark"")

            audio_size = bark_cpp.bark_get_audio_data_size(self.context)
            audio_data = bark_cpp.bark_get_audio_data(self.context)
            return np.ctypeslib.as_array(audio_data, shape=(audio_size,))

        elif self.tts_engine == ""outetts"":
            # OuteTTS Generation
            output = self.interface.generate(
                text=user_input,
                temperature=0.1,
                repetition_penalty=1.1,
                max_length=4096,
                speaker=self.speaker
            )
            return output

        else:
            raise ValueError(f""Unknown TTS engine: {self.tts_engine}"")",Generate audio from the user input using the selected TTS engine.,"???Generate audio from text using specified TTS engine, Bark or OuteTTS.???"
1582,get_search_tool,"def get_search_tool(config: RunnableConfig):
    
    configurable = Configuration.from_runnable_config(config)
    search_api = get_config_value(configurable.search_api)

    # TODO: Configure other search functions as tools
    if search_api.lower() == ""tavily"":
        return tavily_search
    elif search_api.lower() == ""duckduckgo"":
        return duckduckgo_search
    else:
        raise NotImplementedError(
            f""The search API '{search_api}' is not yet supported in the multi-agent implementation. ""
            f""Currently, only Tavily/DuckDuckGo is supported. Please use the graph-based implementation in ""
            f""src/open_deep_research/graph.py for other search APIs, or set search_api to 'tavily' or 'duckduckgo'.""
        )",Get the appropriate search tool based on configuration,"???Selects a search function based on configuration, supporting Tavily and DuckDuckGo.???"
1583,search_artist,"def search_artist(api_url: str, api_key: str, api_timeout: int, artist_id: int) -> Optional[Dict]:
    
    payload = {
        ""name"": ""ArtistSearch"",
        ""artistIds"": [artist_id]
    }
    response = arr_request(api_url, api_key, api_timeout, ""command"", method=""POST"", data=payload)

    if response and isinstance(response, dict) and 'id' in response:
        command_id = response.get('id')
        lidarr_logger.info(f""Triggered Lidarr ArtistSearch for artist ID: {artist_id}. Command ID: {command_id}"")
        return response # Return the full command object
    else:
        lidarr_logger.error(f""Failed to trigger Lidarr ArtistSearch for artist ID {artist_id}. Response: {response}"")
        return None",Trigger a search for a specific artist in Lidarr.,"???Initiates artist search via API, logging success or failure based on response.???"
1584,web_scrape,"def web_scrape(url: str):
    
    scrape_result = app.scrape_url(url, params={'formats': ['markdown']})
    return scrape_result",Scrape a url and return markdown.,???Function scrapes a webpage URL and returns content in markdown format.???
1585,register_pattern,"def register_pattern(
        cls,
        pattern: str,
        embeddings_cls: Type[BaseEmbeddings]
    ) -> None:
        
        if not issubclass(embeddings_cls, BaseEmbeddings):
            raise ValueError(f""{embeddings_cls} must be a subclass of BaseEmbeddings"")

        compiled_pattern = re.compile(pattern)
        cls.pattern_registry[compiled_pattern] = embeddings_cls",Register a new pattern.,???Register regex pattern with associated embedding class in registry.???
1586,api_get_sleep_json,"def api_get_sleep_json():
    
    try:
        if os.path.exists(_SLEEP_DATA_PATH):
            # Add CORS headers to allow any origin to access this resource
            response = send_file(_SLEEP_DATA_PATH, mimetype='application/json')
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response
        else:
            # If file doesn't exist, create it and return empty object
            logger.info(f""[API] sleep.json not found at {_SLEEP_DATA_PATH}, creating it"")
            os.makedirs(os.path.dirname(_SLEEP_DATA_PATH), exist_ok=True)
            with open(_SLEEP_DATA_PATH, 'w') as f:
                json.dump({}, f, indent=2)
            return jsonify({}), 200
    except Exception as e:
        logger.error(f""Error serving sleep.json from {_SLEEP_DATA_PATH}: {e}"")
        # Return empty object instead of error to prevent UI breaking
        return jsonify({}), 200",API endpoint to directly serve the sleep.json file for frontend access,"???Serve or create sleep data JSON with CORS support, handling errors gracefully.???"
1587,start_flask_server_windows,"def start_flask_server_windows(port=5000):
    
    print(f""Starting Flask server on port {port} using Windows 'start' command..."")

    # Get the virtual environment Python executable
    venv_path = os.path.join("".venv"", ""Scripts"", ""python.exe"")
    if not os.path.exists(venv_path):
        print(f""Error: Could not find Python executable at {venv_path}"")
        return None

    try:
        # Use Windows 'start' command to launch in a new window
        # This is more reliable on Windows for Flask apps
        cmd = f'start ""Flask Server"" /MIN ""{venv_path}"" main.py --port {port}'
        subprocess.run(cmd, shell=True, check=True)

        print(f""Flask server starting on port {port}"")
        print(
            ""Note: The process is running in a minimized window. Close that window to stop the server.""
        )

        # Wait a moment to ensure the server has time to start
        time.sleep(3)

        # We can't get the PID easily with this method, but return True to indicate success
        return True

    except Exception as e:
        print(f""Error starting Flask server: {str(e)}"")
        return None",Start a Flask server using Windows 'start' command which is more reliable for Windows environments.,???Launch a Flask server on Windows using a minimized command window for reliability.???
1588,get_stateful_management_info,"def get_stateful_management_info() -> Dict[str, Any]:
    
    lock_info = get_lock_info()
    created_at_ts = lock_info.get(""created_at"")
    expires_at_ts = lock_info.get(""expires_at"")
    
    # Get the interval setting
    expiration_hours = get_advanced_setting(""stateful_management_hours"", DEFAULT_HOURS)

    return {
        ""created_at_ts"": created_at_ts,
        ""expires_at_ts"": expires_at_ts,
        ""interval_hours"": expiration_hours
    }",Get information about the stateful management system.,???Retrieve lock timestamps and expiration interval for stateful management settings.???
1589,get_eth_balance,"def get_eth_balance(agent, **kwargs):
    
    try:
        token_address = kwargs.get(""token_address"")
        
        load_dotenv()
        private_key = os.getenv('ETH_PRIVATE_KEY')
        web3 = agent.connection_manager.connections[""ethereum""]._web3
        account = web3.eth.account.from_key(private_key)
        address = account.address

        balance = agent.connection_manager.connections[""ethereum""].get_balance(
            address=address,
            token_address=token_address
        )
        
        if token_address:
            logger.info(f""Token Balance: {balance}"")
        else:
            logger.info(f""Native Token Balance: {balance}"")
            
        return balance

    except Exception as e:
        logger.error(f""Failed to get balance: {str(e)}"")
        return None",Get native or token balance,???Retrieve and log Ethereum balance using a specified token address or native currency.???
1590,create_bootcamp_instance,"def create_bootcamp_instance(self, name: str, **params) -> Any:
        
        bootcamp_class = self.get_bootcamp_class(name)

        # Get the __init__ signature to see what parameters are accepted
        try:
            sig = inspect.signature(bootcamp_class.__init__)
            valid_params = {}

            # Filter out parameters that the bootcamp doesn't accept
            for param_name, param_value in params.items():
                if param_name in sig.parameters:
                    valid_params[param_name] = param_value
                else:
                    logger.warning(
                        f""Parameter '{param_name}' not accepted by {name}, ignoring""
                    )

            return bootcamp_class(**valid_params)

        except Exception as e:
            logger.error(f""Failed to create instance of {name}: {e}"")
            # Try with no parameters as fallback
            try:
                return bootcamp_class()
            except Exception as e:
                raise e",Create an instance of a bootcamp with given parameters.,"???Instantiate bootcamp class with validated parameters, handling exceptions gracefully.???"
1591,update_page_title,"def update_page_title(file_path: Path, new_title: str):
    
    # Read the content of the file
    with open(file_path, encoding=""utf-8"") as file:
        content = file.read()

    # Replace the existing title with the new title
    updated_content = re.sub(r""<title>.*?</title>"", f""<title>{new_title}</title>"", content)

    # Write the updated content back to the file
    with open(file_path, ""w"", encoding=""utf-8"") as file:
        file.write(updated_content)",Update the title of an HTML file.,???Update HTML file's title tag with a new specified title???
1592,parse,"def parse(self, input: dict, response: OptimalProperties) -> dict:
        
        properties = [p.strata for p in response.selected_properties]
        probabilities = [p.probability for p in response.selected_properties]
        return {""question"": input[""question""], ""properties"": properties, ""probabilities"": probabilities}",Parse the model response into the desired output format.,"??? Extracts and returns question, properties, and probabilities from input and response. ???"
1593,_format_messages,"def _format_messages(self, messages: list):
        
        formatted_messages = []
        for message in messages:
            if message[""role""] == ""assistant"" and message.get(""tool_calls""):
                formatted_messages.append(
                    {
                        ""role"": message[""role""],
                        ""content"": message[""content""],
                        ""tool_calls"": [
                            {
                                ""id"": tool_call[""id""],
                                ""function"": {
                                    ""name"": tool_call[""tool""][""name""],
                                    ""arguments"": json.dumps(
                                        tool_call[""tool""][""arguments""]
                                    ),
                                },
                                ""type"": tool_call[""type""],
                            }
                            for tool_call in message[""tool_calls""]
                        ],
                    }
                )
            else:
                formatted_messages.append(message)
        return formatted_messages",Format the messages to the format that OpenAI expects.,"???  
Transforms messages by formatting assistant roles with tool call details.  
???"
1594,match,"def match(cls, responses, targets) -> float:
        
        logging.debug(f""{responses=}, {targets=}"")
        if not isinstance(responses, (tuple | list)):
            responses = str_to_bboxes(responses)
        if not isinstance(targets, (tuple | list)):
            targets = str_to_bboxes(targets)

        try:
            iou_scores = calculate_iou(responses, targets)
        except:
            return 0

        if not iou_scores:
            return 0

        # Take the mean IoU score for now.
        return sum(iou_scores) / len(iou_scores)",Exact match between targets and responses.,???Calculate average IoU score between predicted and target bounding boxes???
1595,get_max_output_tokens,"def get_max_output_tokens(model_name: str) -> int:
    
    validate_model_name(model_name)

    if model_name.startswith(""lm_studio/""):
        try:
            models = get_model_list()
            for model in models.data:
                if model.id == model_name[len(""lm_studio/"") :]:
                    return model.max_context_length
        except Exception:
            loguru.logger.warning(f""Could not fetch LM Studio model info for {model_name}, using default"")

    if model_name in model_info:
        return model_info[model_name].max_output_tokens

    try:
        return litellm_get_model_max_output_tokens(model_name)
    except Exception:
        loguru.logger.warning(f""Model {model_name} not found in LiteLLM registry, using default"")
        return DEFAULT_MAX_OUTPUT_TOKENS",Get max output tokens with safe fallback,"???Determine maximum output tokens for a given model name, with fallbacks and logging.???"
1596,_rotate_to_top,"def _rotate_to_top(self, cube: Cube, from_side: Side) -> None:
        
        rotation_map = {
            Side.FRONT: cube.rotate_front_to_top,
            Side.RIGHT: cube.rotate_right_to_top,
            Side.BACK: cube.rotate_back_to_top,
            Side.LEFT: cube.rotate_left_to_top,
            Side.BOTTOM: cube.rotate_bottom_to_top,
        }
        if from_side in rotation_map:
            rotation_map[from_side]()",Rotate cube so that given side becomes top,???Rotate cube face to top based on specified side???
1597,_generate_internal_response,"def _generate_internal_response(self, text, system_message, model_config, kwargs):
        
        messages = [{'role': 'user', 'content': system_message + text}]
        return internal_api_completion(
            messages=messages,
            model_config=model_config,
            kwargs=kwargs
        )",Generate response using internal API.,"???  
Constructs a user message and invokes an API for generating a response.  
???"
1598,track_cache_storage_for_storage_key,"def track_cache_storage_for_storage_key(
    storage_key: str,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""storageKey""] = storage_key
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Storage.trackCacheStorageForStorageKey"",
        ""params"": params,
    }
    json = yield cmd_dict",Registers storage key to be notified when an update occurs to its cache storage list.,???Generate command to monitor cache storage for a given storage key.???
1599,success_response,"def success_response(message, data=None):
    
    response = {'message': message, 'type': 'success'}
    if data:
        response['data'] = data
    return jsonify(response), 200",Create a successful response message.,???Generate a JSON success response with optional data for HTTP requests.???
1600,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Performance.disable"",
    }
    json = yield cmd_dict",Disable collecting and reporting metrics.,???Disables performance monitoring by sending a command dictionary.???
1601,assert_skipped_parameters_are_frozen,"def assert_skipped_parameters_are_frozen(model: nn.Module, patterns: List[str]):
    
    if not patterns:
        return

    frozen_state_dict = filter_params_matching_unix_pattern(
        patterns=patterns, state_dict=model.state_dict()
    )
    non_frozen_keys = {
        n
        for n, p in model.named_parameters()
        if n in frozen_state_dict and p.requires_grad
    }
    if non_frozen_keys:
        raise ValueError(
            f""Parameters excluded with `skip_saving_parameters` should be frozen: {non_frozen_keys}""
        )",Verifies that all the parameters matching the provided patterns are frozen - this acts as a safeguard when ignoring parameter when saving checkpoints - if the parameters are in fact trainable,???Ensure specified model parameters are frozen based on pattern matching???
1602,_process_multimodal_carryover,"def _process_multimodal_carryover(self, content: list[dict[str, Any]], kwargs: dict) -> list[dict[str, Any]]:
        
        # Makes sure there's a carryover
        if not kwargs.get(""carryover""):
            return content

        return [{""type"": ""text"", ""text"": self._process_carryover("""", kwargs)}] + content",Prepends the context to a multimodal message.,???Ensure carryover data is processed and prepended to content list???
1603,convert_jsonl_for_pyserini,"def convert_jsonl_for_pyserini(input_file, output_file):
    
    docs = []

    with open(input_file, ""r"", encoding=""utf-8"") as f:
        for line in f:
            data = json.loads(line.strip())
            
            # Create JSON document with a clear structure
            doc = {
                ""id"": data[""_id""],  # Unique identifier for search results
                ""contents"": data['title'] + '\n' + data['text'],  # Required field for Pyserini
            }
            
            docs.append(json.dumps(doc))

    with open(output_file, ""w"", encoding=""utf-8"") as f:
        for doc in docs:
            f.write(doc + ""\n"")

    print(f""✅ Converted JSONL saved to {output_file}"")",Convert JSONL data to Pyserini-compatible format with a structured 'contents' field,???Transform JSONL data into Pyserini-compatible format by restructuring and saving documents.???
1604,_build_metadata_filter,"def _build_metadata_filter(self, filters: Dict[str, Any]) -> str:
        
        if not filters:
            return """"

        filter_conditions = []
        for key, value in filters.items():
            # Handle list of values (IN operator)
            if isinstance(value, list):
                if not value:  # Skip empty lists
                    continue

                # New approach for lists: OR together multiple @> conditions
                # This allows each item in the list to be checked for containment.
                or_clauses_for_list = []
                for item_in_list in value:
                    json_filter_object = {key: item_in_list}
                    json_string_for_sql = json.dumps(json_filter_object)
                    sql_escaped_json_string = json_string_for_sql.replace(""'"", ""''"")
                    or_clauses_for_list.append(f""doc_metadata @> '{sql_escaped_json_string}'::jsonb"")
                if or_clauses_for_list:
                    filter_conditions.append(f""({' OR '.join(or_clauses_for_list)})"")

            else:
                # Handle single value (equality)
                # New approach for single value: Use JSONB containment operator @>
                json_filter_object = {key: value}
                json_string_for_sql = json.dumps(json_filter_object)
                sql_escaped_json_string = json_string_for_sql.replace(""'"", ""''"")
                filter_conditions.append(f""doc_metadata @> '{sql_escaped_json_string}'::jsonb"")

        return "" AND "".join(filter_conditions)",Build PostgreSQL filter for metadata.,???Constructs SQL filter conditions from metadata dictionary for querying JSONB data.???
1605,set_value,"def set_value(self, value: float):
        
        # Clamp the value to ensure it's within min and max range
        new_value = max(min(value, self.max_value), self.min_value)

        # Round the value to the nearest step size
        rounded_value = round(new_value / self.step_size) * self.step_size

        # Ensure the value is rounded to the specified number of decimals
        rounded_value = round(rounded_value, self.decimals)

        # Ensure the formatted value has exactly 'self.decimals' decimal places, even for negative numbers
        format_string = f""{{:.{self.decimals}f}}""

        formatted_value = format_string.format(rounded_value)

        # Set the text with the correct number of decimal places
        self.setText(formatted_value)",Set the line edit's value with proper handling for step size and rounding.,???Adjust and format a numeric input to fit specified constraints and precision.???
1606,display_tool_results,"def display_tool_results(results: List[ToolResult]) -> None:
    
    if not results:
        print(Fore.YELLOW + ""[no results returned]"")
        return
        
    print(Fore.CYAN + ""=== Tool Results ==="")
    
    for result in results:
        duration = (result.end_time - result.start_time).total_seconds()
        status = Fore.GREEN if not result.error else Fore.RED
        
        print(f""{status}{result.tool} ({duration:.3f}s){Style.RESET_ALL}"")
        
        # Format arguments if available
        if hasattr(result, 'call') and result.call and hasattr(result.call, 'arguments'):
            print(f""  {Fore.YELLOW}Arguments:{Style.RESET_ALL} {json.dumps(result.call.arguments, indent=2)}"")
        
        # Format result or error
        if result.error:
            print(f""  {Fore.RED}Error:{Style.RESET_ALL} {result.error}"")
        else:
            if isinstance(result.result, (dict, list)):
                print(f""  {Fore.CYAN}Result:{Style.RESET_ALL} {json.dumps(result.result, indent=2)}"")
            else:
                print(f""  {Fore.CYAN}Result:{Style.RESET_ALL} {result.result}"")",Display tool execution results with nice formatting.,"???Display formatted tool execution results with timing, status, and details.???"
1607,read_all_lights,"def read_all_lights() -> list[str]:
    
    if not (bridge := _get_bridge()):
        return [""Error: Bridge not connected""]
    try:
        light_dict = bridge.get_light_objects(""list"")
        return [light.name for light in light_dict]
    except (PhueException, Exception) as e:
        # Simplified error handling for list return type
        return [f""Error listing lights: {e}""]",Lists the names of all available Hue lights using phue2.,???Retrieve light names from a connected bridge or return error messages.???
1608,_open,"def _open(
        self,
        path: str,
        mode: str = ""rb"",
        block_size: Any = None,  # noqa: ARG002
        autocommit: Any = True,  # noqa: ARG002
        cache_options: Any = None,  # noqa: ARG002
        **kwargs: Any,
    ) -> BufferedFile:
        
        if mode not in (""wb"", ""rb""):
            err_msg = f""Only 'rb' and 'wb' modes supported, got: {mode}""
            raise ValueError(err_msg)

        return BufferedFile(self, path, mode, **kwargs)",Return raw bytes-mode file-like from the file-system.,"???Open a file with specified path and mode, returning a buffered file object.???"
1609,_path_matches_pattern,"def _path_matches_pattern(self, path: str, pattern: str) -> bool:
        
        import fnmatch

        match_result = fnmatch.fnmatch(path, pattern)
        logger.debug(
            f""Pattern match check: path '{path}' against pattern '{pattern}' -> {match_result}""
        )
        return match_result",Check if a path matches a pattern using glob-style matching.,"???  
Checks if a file path matches a specified pattern using wildcard matching.  
???"
1610,update_configmap,"def update_configmap(self, name, namespace, data):
        
        config_map = client.V1ConfigMap(
            api_version=""v1"",
            kind=""ConfigMap"",
            metadata=client.V1ObjectMeta(name=name),
            data=data,
        )
        try:
            return self.core_v1_api.replace_namespaced_config_map(
                name, namespace, config_map
            )
        except ApiException as e:
            print(f""Exception when updating configmap: {e}\n"")
            return",Update existing configmap with the provided data.,???Update a Kubernetes ConfigMap with new data in a specified namespace.???
1611,calibrate,"def calibrate(self, model_client, sample_tokens: int = 32, prompt: str = ""Hello""):
        
        print(""Calibrating inference estimator..."")
        # warm‑up (GPU driver lazy init)
        messages = [{""role"": ""user"", ""content"": prompt}]
        model_client.chat(messages)

        torch.cuda.synchronize() if self.hw.has_gpu else None
        t0 = time.time()
        model_client.chat(messages)
        torch.cuda.synchronize() if self.hw.has_gpu else None
        dt = time.time() - t0
        meas_tps = sample_tokens / dt

        theo_tps = self._theoretical_tok_s()
        factor = meas_tps / theo_tps
        self._calib = factor
        self._save_calib()
        return factor, meas_tps",Run a single timed generation to compute a correction factor and cache it.,???Calibrate model's inference speed by measuring token processing rate.???
1612,format_jsonl_logs,"def format_jsonl_logs(self, dict_logs):
        
        log = {}
        for key in dict_logs:
            # We don't want to log the accumulated values
            if ""_acc"" in key:
                continue
            elif isinstance(dict_logs[key], dict):
                for sub_key in dict_logs[key].keys():
                    prefix = f""{key}""
                    if sub_key != ""all"":
                        if LoggingTypes.JSONL not in self.hparams.train_logging_per_dataset_info:
                            continue
                        prefix += f""/{sub_key}""
                    log[prefix] = dict_logs[key][sub_key]
            else:
                log[key] = dict_logs[key]

        return log",Similar to format_print_logs but for jsonl logs,???Filter and format log data into a structured JSONL format???
1613,inject_misconfig_app,"def inject_misconfig_app(self, microservices: list[str]):
        
        for service in microservices:
            # Get the deployment associated with the service
            deployment = self.kubectl.get_deployment(service, self.namespace)
            if deployment:
                # Modify the image to use the buggy image
                for container in deployment.spec.template.spec.containers:
                    if container.name == f""hotel-reserv-{service}"":
                        container.image = ""yinfangchen/geo:app3""
                self.kubectl.update_deployment(service, self.namespace, deployment)
                time.sleep(10)",Inject a fault by pulling a buggy config of the application image.,???Injects faulty configurations into specified microservices by altering container images.???
1614,update_summary_chat,"def update_summary_chat(self, chat_display: tk.Text, sender: str, message: str):
        
        chat_display.config(state='normal')
        
        # Add the message with appropriate styling
        chat_display.insert(tk.END, ""\n"")  # Add spacing
        chat_display.insert(tk.END, sender, 
                        ""assistant_name"" if sender == self.summarizer.name else ""user_name"")
        chat_display.insert(tk.END, f"": {message}"")
        
        chat_display.see(tk.END)
        chat_display.config(state='disabled')",Update the summary chat display with new message,???Display formatted chat messages in a text widget with sender identification.???
1615,_execute_python_code,"def _execute_python_code(self, code: str, sandbox: Sandbox | None = None) -> str:
        
        if not sandbox:
            raise ValueError(""Sandbox instance is required for code execution."")
        code_hash = sha256(code.encode()).hexdigest()
        filename = f""/home/user/{code_hash}.py""
        sandbox.files.write(filename, code)
        try:
            process = sandbox.commands.run(f""python3 {filename}"")
        except Exception as e:
            raise ToolExecutionException(f""Error during Python code execution: {e}"", recoverable=True)

        if not (process.stdout or process.stderr):
            raise ToolExecutionException(
                ""Error: No output. Please use 'print()' to display the result of your Python code."",
                recoverable=True,
            )
        if process.exit_code != 0:
            raise ToolExecutionException(f""Error during Python code execution: {process.stderr}"", recoverable=True)
        return process.stdout",Executes Python code in the specified sandbox.,"???Execute Python code in a sandbox, handling errors and capturing output.???"
1616,init_process_group,"def init_process_group(
        self, master_address, master_port, rank_offset, world_size, group_name, backend=""nccl"", use_ray=False
    ):
        
        import torch
        from openrlhf.utils.distributed_util import init_process_group

        assert torch.distributed.is_initialized(), f""default torch process group must be initialized""
        assert group_name != """", f""group name must not be empty""

        rank = torch.distributed.get_rank() + rank_offset
        if use_ray:
            import ray.util.collective as collective

            collective.init_collective_group(world_size=world_size, rank=rank, backend=backend, group_name=group_name)
            self._model_update_group = group_name
        else:
            self._model_update_group = init_process_group(
                backend=backend,
                world_size=world_size,
                rank=rank,
                group_name=group_name,
            )
        self._model_update_with_ray = use_ray
        print(
            f""init_process_group: master_address={master_address}, master_port={master_port}, "",
            f""rank={rank}, world_size={world_size}, group_name={group_name}"",
        )",Init torch process group for model weights update,???Initialize a distributed process group with optional Ray support for model updates.???
1617,validate_and_clean_response,"def validate_and_clean_response(response_text: str) -> str | list[str] | None:
    
    try:
        # First try to extract a JSON array if present
        json_match = re.search(r""\[.*\]"", response_text, re.DOTALL)
        if json_match:
            cleaned_json = json_match.group(0)
            # Remove any markdown code block markers
            cleaned_json = re.sub(r""```json\s*|\s*```"", """", cleaned_json)
            return json.loads(cleaned_json)

        # If no JSON array found, fall back to extract_list
        topics = extract_list(response_text)
        if topics:
            return [topic.strip() for topic in topics if topic.strip()]
        return None  # noqa: TRY300
    except (json.JSONDecodeError, ValueError) as e:
        print(f""Error parsing response: {str(e)}"")
        return None",Clean and validate the response from the LLM.,"???Extract and clean JSON or list from text response, handling errors gracefully.???"
1618,mock_boto3,"def mock_boto3():
    
    with patch('boto3.client') as mock_client, patch('boto3.Session') as mock_session:
        mock_neptunedb = MagicMock()
        mock_neptuneanalytics = MagicMock()

        mock_client.side_effect = lambda service, region_name=None: {
            'neptunedata': mock_neptunedb,
            'neptune-graph': mock_neptuneanalytics,
        }[service]

        mock_session_instance = MagicMock()
        mock_session_instance.client.side_effect = lambda service, region_name=None: {
            'neptunedata': mock_neptunedb,
            'neptune-graph': mock_neptuneanalytics,
        }[service]
        mock_session.return_value = mock_session_instance

        yield {
            'client': mock_client,
            'Session': mock_session,
            'neptunedata': mock_neptunedb,
            'neptune-graph': mock_neptuneanalytics,
        }",Create a mock boto3 module.,???Simulate AWS Neptune services using mocked Boto3 client and session objects.???
1619,generate_ddp_file,"def generate_ddp_file(trainer):
    
    module, name = f""{trainer.__class__.__module__}.{trainer.__class__.__name__}"".rsplit(""."", 1)

    content = f
    (USER_CONFIG_DIR / ""DDP"").mkdir(exist_ok=True)
    with tempfile.NamedTemporaryFile(
        prefix=""_temp_"",
        suffix=f""{id(trainer)}.py"",
        mode=""w+"",
        encoding=""utf-8"",
        dir=USER_CONFIG_DIR / ""DDP"",
        delete=False,
    ) as file:
        file.write(content)
    return file.name",Generates a DDP file and returns its file name.,???Create a temporary Python file for a trainer object in a specific directory???
1620,process_base64_image,"def process_base64_image(base64_string: str) -> Image.Image:
    
    try:
        # Remove data URL prefix if present
        if 'base64,' in base64_string:
            base64_string = base64_string.split('base64,')[1]
        
        image_data = base64.b64decode(base64_string)
        image = Image.open(io.BytesIO(image_data))
        
        # Convert to RGB if necessary
        if image.mode not in ('RGB', 'L'):
            image = image.convert('RGB')
        
        return image
    except Exception as e:
        logger.error(f""Error processing base64 image: {str(e)}"")
        raise ValueError(f""Invalid base64 image data: {str(e)}"")",Process base64 image data and return PIL Image,"???Decode and convert a base64-encoded string into an RGB image, handling errors.???"
1621,load_database,"def load_database(self):
        
        logger = get_logger()
        if 'database_folder' not in self.config or not os.path.exists(self.config['database_folder']):
            logger.warning(
                f""{ConsoleColor.RED}Database folder not found. No database loaded.{ConsoleColor.RESET}"")
            return {}
        all_data_files = [file for file in os.listdir(self.config['database_folder'])
                          if file.endswith('.json') or file.endswith('.csv')]
        all_data = {}
        for file in all_data_files:
            if file.endswith('.json'):
                table = pd.read_json(os.path.join(self.config['database_folder'], file), orient='index')
            else:
                table = pd.read_csv(os.path.join(self.config['database_folder'], file))
            all_data[Path(file).stem] = table

        self.data_schema = {Path(file).stem: list(data.columns) for file, data in all_data.items()}
        self.data_examples = {table: all_data[table].iloc[0].to_json() for table in all_data}
        database_validators_path = self.config.get('database_validators', None)
        if database_validators_path is not None:
            self.database_validators = {table: get_validators_from_module(database_validators_path, table) for table in
                                        all_data}
        else:
            self.database_validators = {table: [] for table in all_data}",Load the database from the database folder.,"???Load and validate database files, generating schema and examples for each table.???"
1622,reject_all,"def reject_all():
    
    global PDF_TESTS, DATASET_DIR, DATASET_FILE

    data = request.json
    pdf_name = data.get(""pdf"")

    if pdf_name and pdf_name in PDF_TESTS:
        # Update all tests for this PDF to rejected
        for test in PDF_TESTS[pdf_name]:
            test[""checked""] = ""rejected""

        # Save the updated tests
        save_dataset(DATASET_FILE)

        return jsonify({""status"": ""success"", ""count"": len(PDF_TESTS[pdf_name])})

    return jsonify({""status"": ""error"", ""message"": ""PDF not found""})",API endpoint to reject all tests for a PDF.,???Rejects all tests for a specified PDF and updates the dataset.???
1623,copy_mjcf_with_assets,"def copy_mjcf_with_assets(mjcf_path, output_dir):
    
    try:
        # Create model name based on the file name
        model_name = os.path.splitext(os.path.basename(mjcf_path))[0]
        model_dir = os.path.join(output_dir, model_name)
        os.makedirs(model_dir, exist_ok=True)

        # Find all asset files
        asset_files = find_asset_files(mjcf_path)
        asset_files.append(mjcf_path)  # Add the MJCF file itself

        # Get the base directory of the MJCF file
        base_dir = os.path.dirname(mjcf_path)

        # Copy each asset file, maintaining directory structure
        for asset_path in asset_files:
            # Determine relative path (keep directory structure)
            rel_path = os.path.relpath(asset_path, base_dir)
            target_path = os.path.join(model_dir, rel_path)

            # Create directories if needed
            os.makedirs(os.path.dirname(target_path), exist_ok=True)

            # Copy the file
            shutil.copy2(asset_path, target_path)

        return model_dir, None
    except Exception as e:
        error_msg = f""Error copying assets: {e!s}""
        print(f""Error copying assets for {mjcf_path}: {e}"")
        return None, error_msg","Copy MJCF file and all its assets to output directory, maintaining relative paths.",???Copy MJCF file and associated assets to a structured output directory.???
1624,list_experiments,"def list_experiments():
    
    table = Table(title=""Registered Experiments"")
    table.add_column(""Name"", style=""cyan"")
    table.add_column(""Datasets"", style=""magenta"")
    table.add_column(""Size"", style=""blue"")
    table.add_column(""Seed"", style=""green"")

    try:
        experiments = client.list_experiments()
        for exp_name in experiments.experiments:
            try:
                config = client.get_experiment_config(exp_name)
                datasets = "", "".join(config.datasets.keys())
                table.add_row(exp_name, datasets, str(config.size), str(config.seed or """"))
            except Exception as e:
                console.print(f""[yellow]Warning: Could not get config for {exp_name}: {e}[/]"")
                table.add_row(exp_name, ""?"", ""?"", ""?"")
    except Exception as e:
        console.print(f""[red]Error listing experiments: {e}[/]"")
        raise typer.Exit(1)

    console.print(table)",List all registered experiments with their status.,"???Display a table of registered experiments with details on datasets, size, and seed.???"
1625,start_operation_backend,"def start_operation_backend(self, **kwargs):
        
        if not self._loop_switch:
            self._loop_switch = True
            self._backend_task = self.thread_pool.submit(self._loop_operation, **kwargs)
            self.logger.info(f""start operation={self.name}..."")",Initiates the background operation loop if it's not already running.,???Initialize and execute backend operation asynchronously if not already active.???
1626,do_Tz,"def do_Tz(self, scale: PDFStackT) -> None:
        
        scale_f = safe_float(scale)

        if scale_f is None:
            log.warning(
                f""Could not set horizontal scaling because {scale!r} is an invalid float value""
            )
        else:
            self.textstate.scaling = scale_f",Set the horizontal scaling.,???Set horizontal text scaling in PDF if valid float???
1627,write_script,"def write_script(self, script_path: str, script: str) -> None:
        
        if not script.strip():
            raise ValueError(""Script content cannot be empty"")

        # Always wrap in ElixirScript module
        wrapped_script =  % script.strip()  # noqa: UP031

        with open(script_path, ""w"", encoding=""utf-8"") as f:
            f.write(wrapped_script.strip())",Write Elixir script to file.,"??? 
Validates and writes non-empty script content to a file. 
???"
1628,_save_results,"def _save_results(self, results: List[EmbeddingMetrics]) -> None:
        
        results_file = self.results_dir / ""embedding_benchmark_results.json""

        results_data = []
        for result in results:
            results_data.append(
                {
                    ""model_name"": result.model_name,
                    ""indexing_time"": result.indexing_time,
                    ""avg_inference_time"": result.avg_inference_time,
                    ""cost_per_1k_tokens"": result.cost_per_1k_tokens,
                    ""recall_at_5"": result.recall_at_5,
                    ""mrr"": result.mrr,
                }
            )

        with open(results_file, ""w"", encoding=""utf-8"") as f:
            json.dump(
                {
                    ""timestamp"": self._get_timestamp(),
                    ""results"": results_data,
                },
                f,
                indent=2,
            )",Save benchmark results to file.,???Serialize embedding metrics to JSON for benchmarking results storage.???
1629,violates_moderation,"def violates_moderation(text):
    
    headers = {
        ""Content-Type"": ""application/json"",
        ""Authorization"": ""Bearer "" + os.environ[""OPENAI_API_KEY""],
    }
    text = text.replace(""\n"", """")
    data = ""{"" + '""input"": ' + f'""{text}""' + ""}""
    data = data.encode(""utf-8"")
    try:
        ret = requests.post(url, headers=headers, data=data, timeout=5)
        flagged = ret.json()[""results""][0][""flagged""]
    except requests.exceptions.RequestException as e:
        flagged = False
    except KeyError as e:
        flagged = False

    return flagged",Check whether the text violates OpenAI moderation API.,???Check if text violates moderation guidelines using an API request???
1630,_convert_row_number,"def _convert_row_number(self, value: Any, field_name: str) -> int:
        
        try:
            # Handle numeric strings and floats
            if isinstance(value, str):
                if ""."" in value:
                    num = float(value)
                else:
                    num = int(value)
            else:
                num = value

            converted = int(num)
            if converted != num:  # Check if float had decimal part
                raise ValueError(""Decimal values are not allowed for row numbers"")

            if converted <= 0:
                raise ValueError(f""{field_name} must be a positive integer"")

            return converted
        except (ValueError, TypeError) as e:
            raise ValueError(f""Invalid value for {field_name}: {repr(value)}"") from e",Convert and validate row number input.,???Validate and convert input to a positive integer for row numbers???
1631,_extract_line_numbers_for_file,"def _extract_line_numbers_for_file(self, text: str, filename: str) -> list[int]:
        
        # Look for patterns like ""user.py:123"" or ""in user.py line 123""
        patterns = [
            rf""{filename}:(\d+)"",
            rf""in {filename}.*?line\s+(\d+)"",
            rf""{filename}.*?#L(\d+)"",
        ]

        line_refs = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            line_refs.extend([int(m) for m in matches])

        return list(set(line_refs))",Extract line numbers specifically mentioned for a given file.,???Extract unique line numbers from text based on filename patterns.???
1632,get_session_id_by_thread_name,"def get_session_id_by_thread_name(thread_name: str) -> int | None:
    
    logger.info(f""Retrieving session_id for thread_id {thread_name}"")
    with _registry_lock:
        for session_id, entry in agent_thread_registry.items():
            # Check if the thread name matches
            logger.info(f""Comparing thread_name {entry['thread'].name}"")
            if entry[""thread""].name == thread_name:
                logger.info(f""Found session_id {session_id} for thread_id {thread_name}"")
                return session_id
    logger.warning(f""No session_id found for thread_id {thread_name}"")

    # none for not found
    return None",Retrieve the session_id associated with a given thread_id.,???Retrieve session ID by matching thread name in agent registry???
1633,patch_component,"def patch_component(self, updated_component):
        
        if ""id"" not in updated_component or updated_component[""id""] not in self.seen_ids:
            return False  # cannot patch if component is not existing

        component_id = updated_component[""id""]
        logger.debug(f""[PATCH] Patching existing component { component_id }"")

        for row in self.rows:
            for i, existing in enumerate(row):
                if existing.get(""id"") == component_id:
                    row[i] = updated_component
                    return True

        # Component id was seen, but not found in rows. This could be a layout bug
        logger.warning(f""[PATCH] Component id {component_id} was seen but not found in layout rows"")
        return False",Patch an existing component in the layout if it exists by ID.,"???Update component in layout if ID exists, logging actions and warnings.???"
1634,dbg_delete_breakpoint,"def dbg_delete_breakpoint(
    address: Annotated[str, ""del a breakpoint at the specified address""],
) -> str:
    
    ea = parse_address(address)
    if idaapi.del_bpt(ea):
        return f""Breakpoint deleted at {hex(ea)}""
    return f""Failed to delete breakpoint at address {hex(ea)}""",del a breakpoint at the specified address,??? Function removes a breakpoint at a given address and returns success status. ???
1635,rename_global_variable,"def rename_global_variable(
    old_name: Annotated[str, ""Current name of the global variable""],
    new_name: Annotated[str, ""New name for the global variable (empty for a default name)""],
):
    
    ea = idaapi.get_name_ea(idaapi.BADADDR, old_name)
    if not idaapi.set_name(ea, new_name):
        raise IDAError(f""Failed to rename global variable {old_name} to {new_name}"")
    refresh_decompiler_ctext(ea)",Rename a global variable,???Rename a global variable in IDA Pro and refresh decompiler context.???
1636,wait_for_callback,"def wait_for_callback(timeout: int = 300) -> bool:
    
    start_time = time.time()
    while not callback_received and (time.time() - start_time) < timeout:
        time.sleep(1)

    if not callback_received:
        logger.error(
            f""Timed out waiting for authorization callback after {timeout} seconds""
        )
        return False

    if callback_error:
        logger.error(f""Authorization error: {callback_error}"")
        return False

    return True",Wait for the callback to be received.,"???Waits for an authorization callback, logging errors if timeout or issues occur.???"
1637,add,"def add(self, message: Message) -> None:
        
        try:
            document = self._message_to_document(message)
            embedding_result = (
                self.embedder.execute(input_data=DocumentEmbedderInputSchema(documents=[document]))
                .get(""documents"")[0]
                .embedding
            )
            document.embedding = embedding_result

            self.vector_store.write_documents(documents=[document], policy=DuplicatePolicy.SKIP)
        except Exception as e:
            raise QdrantError(f""Failed to add message to Qdrant: {e}"") from e",Stores a message in Qdrant.,"???Convert message to document, embed it, and store in vector database, handling errors.???"
1638,run_background_tasks,"def run_background_tasks():
    
    bg_logger = get_logger(""HuntarrBackground"") # Use app's logger
    try:
        bg_logger.info(""Starting Huntarr background tasks..."")
        start_huntarr() # This function contains the main loop and shutdown logic
    except Exception as e:
        bg_logger.exception(f""Critical error in Huntarr background tasks: {e}"")
    finally:
        bg_logger.info(""Huntarr background tasks stopped."")",Runs the Huntarr background processing.,???Initialize and manage background tasks with error logging and graceful shutdown.???
1639,_extract_update_columns,"def _extract_update_columns(self, query: str) -> str:
        
        if not query:
            return """"

        import re

        # This is a simplified approach - a real implementation would use proper SQL parsing
        match = re.search(r""UPDATE\s+(?:\w+\.)?(?:\w+)\s+SET\s+([\w\s,=]+)\s+WHERE"", query, re.IGNORECASE)
        if match:
            # Extract column names from the SET clause
            set_clause = match.group(1)
            columns = re.findall(r""(\w+)\s*="", set_clause)
            if columns and len(columns) <= 3:  # Limit to 3 columns to keep name reasonable
                return ""_"".join(columns)
            elif columns:
                return f""{columns[0]}_and_others""

        return """"",Extract columns being updated in an UPDATE statement.,???Extracts and formats column names from SQL UPDATE query's SET clause.???
1640,get_credential_key,"def get_credential_key(self) -> str:
    
    auth_scheme = self.auth_config.auth_scheme
    auth_credential = self.auth_config.raw_auth_credential
    if auth_scheme.model_extra:
      auth_scheme = auth_scheme.model_copy(deep=True)
      auth_scheme.model_extra.clear()
    scheme_name = (
        f""{auth_scheme.type_.name}_{hash(auth_scheme.model_dump_json())}""
        if auth_scheme
        else """"
    )
    if auth_credential.model_extra:
      auth_credential = auth_credential.model_copy(deep=True)
      auth_credential.model_extra.clear()
    credential_name = (
        f""{auth_credential.auth_type.value}_{hash(auth_credential.model_dump_json())}""
        if auth_credential
        else """"
    )

    return f""temp:adk_{scheme_name}_{credential_name}""",Generates a unique key for the given auth scheme and credential.,???Generate a unique credential key based on authentication scheme and credential details.???
1641,_validate_file_path,"def _validate_file_path(self, file_path: str) -> str:
        
        abs_path = os.path.abspath(file_path)
        if not abs_path.startswith(self.TEMP_DIR):
            raise ValueError(f""Invalid file path. File must be in {self.TEMP_DIR}"")
        return abs_path",Validates that the file path is within the allowed temporary directory.,???Ensure file path is within a specified temporary directory.???
1642,calculate_cohere_cost,"def calculate_cohere_cost(input_tokens: int, output_tokens: int, model: str) -> float:
    
    total = 0.0

    if model in COHERE_PRICING_1K:
        input_cost_per_k, output_cost_per_k = COHERE_PRICING_1K[model]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        total = input_cost + output_cost
    else:
        warnings.warn(f""Cost calculation not available for {model} model"", UserWarning)

    return total",Calculate the cost of the completion using the Cohere pricing.,???Calculate total cost for token usage based on model-specific pricing???
1643,dummy_config_file,"def dummy_config_file(tmp_path):
    
    config_content = {
        ""mcpServers"": {
            ""Server1"": {""param"": ""value1""},
            ""Server2"": {""param"": ""value2""}
        }
    }
    config_file = tmp_path / ""server_config.json""
    config_file.write_text(json.dumps(config_content))
    return str(config_file)",Create a temporary config file that will be used by process_options.,???Create a temporary JSON configuration file with server parameters.???
1644,job_spec,"def job_spec(self, value: JobSpec) -> None:
        
        if self._state not in _PREFLIGHT_STATES:
            err_msg = f""Attempt to change job_spec after job submission: {self._state.name}""
            logger.error(err_msg)

            raise ValueError(err_msg)

        self._job_spec = value",Sets the job specification associated with the state.,???Ensure job specification is set only before job submission???
1645,metadata,"def metadata(self):
        
        # Some vector stores require the IDs to be ASCII.
        filename_ascii = self.filename.encode(""ascii"", ""ignore"").decode(""ascii"")
        chunk_metadata = {
            # Some vector stores require the IDs to be ASCII.
            ""id"": f""{filename_ascii}_{self.start_byte}_{self.end_byte}"",
            ""start_byte"": self.start_byte,
            ""end_byte"": self.end_byte,
            ""length"": self.end_byte - self.start_byte,
            # Note to developer: When choosing a large chunk size, you might exceed the vector store's metadata
            # size limit. In that case, you can simply store the start/end bytes above, and fetch the content
            # directly from the repository when needed.
            TEXT_FIELD: self.content,
        }
        chunk_metadata.update(self.file_metadata)
        return chunk_metadata",Converts the chunk to a dictionary that can be passed to a vector store.,"???Generate ASCII-compliant metadata for file chunks, including ID and byte range.???"
1646,_remove_extra_fields,"def _remove_extra_fields(model: Any, response: dict[str, object]) -> None:
    

    key_values = list(response.items())

    for key, value in key_values:
        # Need to convert to snake case to match model fields names
        # ex: UsageMetadata
        alias_map = {field_info.alias: key for key, field_info in model.model_fields.items()}

        if key not in model.model_fields and key not in alias_map:
            response.pop(key)
            continue

        key = alias_map.get(key, key)

        annotation = model.model_fields[key].annotation

        # Get the BaseModel if Optional
        if get_origin(annotation) is Union:
            annotation = get_args(annotation)[0]

        # if dict, assume BaseModel but also check that field type is not dict
        # example: FunctionCall.args
        if isinstance(value, dict) and get_origin(annotation) is not dict:
            _remove_extra_fields(annotation, value)
        elif isinstance(value, list):
            for item in value:
                # assume a list of dict is list of BaseModel
                if isinstance(item, dict):
                    _remove_extra_fields(get_args(annotation)[0], item)",Removes extra fields from the response that are not in the model.,???Refine response data by removing fields not defined in the model schema.???
1647,clear_history,"def clear_history(self):
        
        def clear():
            try:
                subprocess.run([""cliphist"", ""wipe""], check=True)
                self._pending_updates = True
                if not self._loading:
                    GLib.idle_add(self._load_clipboard_items_thread)
            except subprocess.CalledProcessError as e:
                print(f""Error clearing clipboard history: {e}"", file=sys.stderr)
            return False
        GLib.idle_add(clear)",Clear all clipboard history (GLib.idle_add),???Clear clipboard history and update state asynchronously.???
1648,list_to_quoted_csv_string,"def list_to_quoted_csv_string(data: List[List[Any]]) -> str:
    

    def enclose_string_with_quotes(content: Any) -> str:
        if isinstance(content, numbers.Number):
            return str(content)
        content = str(content).strip().strip(""'"").strip('""')
        return f'""{content}""'

    return ""\n"".join(
        [
            "",\t"".join([enclose_string_with_quotes(data_dd) for data_dd in data_d])
            for data_d in data
        ]
    )",Converts a list of lists into a CSV formatted string with quoted values.,???Convert nested lists into a CSV string with quoted elements.???
1649,module_name,"def module_name(self) -> str:
        
        # Check if this is a custom tool in the user's project
        custom_path = _get_custom_tool_path(self.name)
        if custom_path.exists():
            return f""src.tools.{self.name}""

        # Otherwise, it's a package tool
        return f""agentstack._tools.{self.name}""",Module name for the tool module.,???Determine tool path based on custom or package context???
1650,process_node,"def process_node(
        self, node, current_class, definitions, process_method, process_function, process_class, process_class_variable
    ):
        
        if node.type in (""function_definition"", ""async_function_definition""):
            if current_class:
                process_method(node, definitions[""classes""][current_class][""methods""])
            else:
                process_function(node, definitions[""functions""])
            return ""function""
        elif node.type == ""class_definition"":
            class_name, start_line, end_line = process_class(node)
            definitions[""classes""][class_name] = {""line"": (start_line, end_line), ""methods"": [], ""variables"": []}
            return ""class""",Processes a node in a Python syntax tree.,???Process nodes to categorize and update class or function definitions.???
1651,save_as_trec,"def save_as_trec(
    rank_result: Dict[str, Dict[str, Dict[str, Any]]], output_path: str, run_id: str = ""OpenMatch""
):
    
    pathlib.Path(""/"".join(output_path.split(""/"")[:-1])).mkdir(parents=True, exist_ok=True)
    with open(output_path, ""w"") as f:
        for qid in rank_result:
            # sort the results by score
            sorted_results = sorted(
                rank_result[qid].items(), key=lambda x: x[1], reverse=True
            )
            for i, (doc_id, score) in enumerate(sorted_results):
                f.write(""{}\tQ0\t{}\t{}\t{}\t{}\n"".format(qid, doc_id, i + 1, score, run_id))",Save the rank result as TREC format:  Q0,???Convert ranking results into TREC format and save to a specified file path.???
1652,add,"def add(self, message: Message) -> None:
        
        try:
            document = self._message_to_document(message)
            embedding_result = self.embedder.execute(input_data=DocumentEmbedderInputSchema(documents=[document]))
            document_embedding = embedding_result.get(""documents"")[0].embedding
            document.embedding = document_embedding
            self.vector_store.write_documents([document])

        except Exception as e:
            raise PineconeError(f""Error adding message to Pinecone: {e}"") from e",Stores a message in Pinecone.,"???Convert message to document, embed it, and store in vector database.???"
1653,initialize,"def initialize(self) -> None:
        
        try:
            self.conn = sqlite3.connect(self.config.db_path, check_same_thread=False)
            with self.conn:
                cur = self.conn.cursor()
                cur.execute(f)
            logger.info(f""Initialized SQLite storage at {self.config.db_path}"")
        except Exception as e:
            logger.error(f""Failed to initialize SQLite storage: {str(e)}"")
            raise",Initialize SQLite connection and create necessary tables,"???Establishes and verifies a SQLite database connection, logging success or failure.???"
1654,_sanitize_error_message,"def _sanitize_error_message(message: str) -> str:
    
    import re

    # Remove file paths but preserve filenames
    # Match paths with directories and capture the filename
    # Unix style: /path/to/file.py -> file.py
    message = re.sub(r""(/[^/\s]+)+/([^/\s]+)"", r""\2"", message)
    # Windows style: C:\path\to\file.py -> file.py
    message = re.sub(r""([A-Za-z]:\\[^\\]+\\)+([^\\]+)"", r""\2"", message)
    # Remaining absolute paths without filename
    message = re.sub(r""[/\\][^\s]*[/\\]"", ""[PATH]/"", message)

    # Remove potential API keys or tokens (common patterns)
    # Generic API keys (20+ alphanumeric with underscores/hyphens)
    message = re.sub(r""\b[a-zA-Z0-9_-]{32,}\b"", ""[REDACTED]"", message)
    # Bearer tokens
    message = re.sub(r""Bearer\s+[a-zA-Z0-9_.-]+"", ""Bearer [REDACTED]"", message)

    # Remove email addresses
    message = re.sub(
        r""\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"", ""[EMAIL]"", message
    )

    # Remove IP addresses
    message = re.sub(r""\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b"", ""[IP]"", message)

    # Remove port numbers in URLs
    message = re.sub(r"":[0-9]{2,5}(?=/|$|\s)"", "":[PORT]"", message)

    # Truncate to reasonable length
    if len(message) > 200:
        message = message[:197] + ""...""

    return message",Sanitize error messages to remove sensitive information.,"???Sanitize error messages by removing sensitive data like paths, keys, emails, and IPs.???"
1655,print_message_history_debug,"def print_message_history_debug(self):
        
        print(""\n=== MESSAGE HISTORY DEBUG ==="")
        for i, msg in enumerate(message_history):
            role = msg.get('role', 'unknown')
            content = str(msg.get('content', ''))[:100]
            tool_calls = msg.get('tool_calls', [])
            tool_call_id = msg.get('tool_call_id', '')
            
            print(f""[{i}] {role}: {content}"")
            if tool_calls:
                print(f""    Tool calls: {len(tool_calls)}"")
            if tool_call_id:
                print(f""    Tool call ID: {tool_call_id}"")
        print(""=== END MESSAGE HISTORY ===\n"")",Print the current message history for debugging.,"???Display a formatted debug log of message history with roles, content, and tool call details.???"
1656,_make_restype_pseudobeta_idx,"def _make_restype_pseudobeta_idx():
  
  restype_pseudobeta_index = np.zeros(
      (NUM_RESTYPES_WITH_UNK_AND_GAP,), dtype=np.int32
  )
  for restype, restype_letter in enumerate(
      residue_names.PROTEIN_TYPES_ONE_LETTER
  ):
    restype_name = residue_names.PROTEIN_COMMON_ONE_TO_THREE[restype_letter]
    atom_names = list(atom_types.ATOM14[restype_name])
    if restype_name in {'GLY'}:
      restype_pseudobeta_index[restype] = atom_names.index('CA')
    else:
      restype_pseudobeta_index[restype] = atom_names.index('CB')
  for nanum, resname in enumerate(residue_names.NUCLEIC_TYPES):
    atom_names = list(atom_types.DENSE_ATOM[resname])
    # 0: backbone frame only.
    # we have aa + unk , so we want to start after those
    restype = nanum + NUM_AA_WITH_UNK_AND_GAP
    if resname in {'A', 'G', 'DA', 'DG'}:
      restype_pseudobeta_index[restype] = atom_names.index('C4')
    else:
      restype_pseudobeta_index[restype] = atom_names.index('C2')
  return restype_pseudobeta_index",Returns indices of residue's pseudo-beta.,???Generate pseudobeta atom indices for protein and nucleic acid residues.???
1657,_generate_with_together,"def _generate_with_together(self, prompt: str, **kwargs) -> Dict[str, Any]:
        
        messages = [{""role"": ""user"", ""content"": prompt}]
        generation_config = {
            ""max_tokens"": kwargs.get(""max_new_tokens"", 10),
            ""temperature"": kwargs.get(""temperature"", 0.7),
            ""logprobs"": True,
            ""stream"": False,
        }

        response = self.together_client.chat.completions.create(
            model=self.together_model, messages=messages, **generation_config
        )

        logprobs_data = response.choices[0].logprobs
        return {
            ""text"": response.choices[0].message.content,
            ""tokens"": logprobs_data.tokens,
            ""token_logprobs"": logprobs_data.token_logprobs,
            ""token_ids"": logprobs_data.token_ids,
        }",Generate text using Together AI chat with logprobs,???Generate text response using AI model with configurable parameters.???
1658,_get_encoded_swap_data,"def _get_encoded_swap_data(self, route_summary: Dict, slippage: float = 0.5) -> str:
        
        try:
            private_key = os.getenv('SONIC_PRIVATE_KEY')
            account = self._web3.eth.account.from_key(private_key)
            
            url = f""{self.aggregator_api}/route/build""
            headers = {""x-client-id"": ""zerepy""}
            
            payload = {
                ""routeSummary"": route_summary,
                ""sender"": account.address,
                ""recipient"": account.address,
                ""slippageTolerance"": int(slippage * 100),  # Convert to bps
                ""deadline"": int(time.time() + 1200),  # 20 minutes
                ""source"": ""ZerePyBot""
            }
            
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            
            data = response.json()
            if data.get(""code"") != 0:
                raise SonicConnectionError(f""API error: {data.get('message')}"")
                
            return data[""data""][""data""]
                
        except Exception as e:
            logger.error(f""Failed to encode swap data: {e}"")
            raise",Get encoded swap data from Kyberswap API,???Encode swap transaction data using API with error handling and logging???
1659,split_into_sentences,"def split_into_sentences(text: str) -> List[str]:
    
    # Common sentence end markers for various languages
    end_markers = r'[.!?。！？︕︖]+'
    
    # Split on sentence end markers while preserving them
    parts = re.split(f'({end_markers}\\s*)', text)
    
    sentences = []
    current_sentence = """"
    
    for i in range(0, len(parts), 2):
        current_sentence = parts[i]
        
        # Add the sentence end marker if it exists
        if i + 1 < len(parts):
            current_sentence += parts[i + 1]
        
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
    
    return sentences",Split text into sentences using appropriate end markers.,???Function splits text into sentences using punctuation markers.???
1660,get_site_packages,"def get_site_packages(venv_dir: Path, python_path: Path) -> Path:
    
    # Construct site-packages path based on platform
    if (venv_dir / ""lib"").exists():  # Unix-like
        # Get Python version (e.g., ""3.10"")
        result = subprocess.run(
            [
                str(python_path),
                ""-c"",
                ""import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')"",
            ],
            capture_output=True,
            text=True,
            check=True,
        )
        python_version = result.stdout.strip()
        return venv_dir / ""lib"" / f""python{python_version}"" / ""site-packages""
    else:  # Windows
        return venv_dir / ""Lib"" / ""site-packages""",Get the site-packages directory for the virtual environment.,???Determine the site-packages directory path for a virtual environment based on the operating system.???
1661,validate_config,"def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        
        required_fields = [""model""]
        missing_fields = [field for field in required_fields if field not in config]

        if missing_fields:
            raise ValueError(f""Missing required configuration fields: {', '.join(missing_fields)}"")

        if not isinstance(config[""model""], str):
            raise ValueError(""model must be a string"")

        return config",Validate EternalAI configuration from JSON,???Ensure configuration contains required fields and validate model type???
1662,to_mcp_resource,"def to_mcp_resource(self, **overrides: Any) -> MCPResource:
        
        kwargs = {
            ""uri"": self.uri,
            ""name"": self.name,
            ""description"": self.description,
            ""mimeType"": self.mime_type,
        }
        return MCPResource(**kwargs | overrides)",Convert the resource to an MCPResource.,???Convert object attributes to an MCPResource with optional overrides.???
1663,print_trainable_parameters_visual,"def print_trainable_parameters_visual(self) -> None:
    
    trainable_blocks = []
    non_trainable_blocks = []

    # Check trainable status of vision attention blocks
    for block_idx, block in enumerate(self.blocks):
        is_trainable = all(param.requires_grad for param in block.parameters())
        if is_trainable:
            trainable_blocks.append(block_idx)
        else:
            non_trainable_blocks.append(block_idx)

    # Check trainable status of merger module
    is_merger_trainable = any(param.requires_grad for param in self.merger.parameters())

    # Print results
    print(""Vision Module - Attention Blocks:"")
    print(
        f""Trainable Block Indices: {trainable_blocks if trainable_blocks else 'None'}""
    )
    print(
        f""Non-Trainable Block Indices: {non_trainable_blocks if non_trainable_blocks else 'None'}""
    )
    print(f""Merger Module Trainable: {is_merger_trainable}"")",Prints the trainable status of all vision components including attention blocks and merger module.,???Identify and display trainable status of vision module components.???
1664,is_ollama_available,"def is_ollama_available(ollama_url: str) -> bool:
    
    try:
        response = requests.get(f""{ollama_url}/api/version"", timeout=5)
        if response.status_code == 200:
            return True
            
        print(f""{Fore.RED}Cannot connect to Ollama service at {ollama_url}.{Style.RESET_ALL}"")
        print(f""{Fore.YELLOW}Make sure the Ollama service is running in your Docker environment.{Style.RESET_ALL}"")
        return False
    except requests.RequestException as e:
        print(f""{Fore.RED}Error connecting to Ollama service: {e}{Style.RESET_ALL}"")
        return False",Check if Ollama service is available in Docker environment.,???Check if the Ollama service is accessible via a specified URL.???
1665,array_to_image_path,"def array_to_image_path(image_array):
    
    if image_array is None:
        raise ValueError(""No image provided. Please upload an image before submitting."")
    img = Image.fromarray(np.uint8(image_array))
    timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
    filename = f""image_{timestamp}.png""
    img.save(filename)
    return os.path.abspath(filename)",Save the uploaded image and return its path.,???Convert an image array to a timestamped file path and save it as a PNG.???
1666,get_xrefs_to,"def get_xrefs_to(
    address: Annotated[str, ""Address to get cross references to""],
) -> list[Xref]:
    
    xrefs = []
    xref: ida_xref.xrefblk_t
    for xref in idautils.XrefsTo(parse_address(address)):
        xrefs.append({
            ""address"": hex(xref.frm),
            ""type"": ""code"" if xref.iscode else ""data"",
            ""function"": get_function(xref.frm, raise_error=False),
        })
    return xrefs",Get all cross references to the given address,???Extract cross-references to a specified address and return them as a list of dictionaries.???
1667,update_student_learning,"def update_student_learning(self, topic: str, is_correct: bool, difficulty: str):
        
        current_prof = self.student_metrics[topic]

        # Learning rate based on correctness and difficulty
        if is_correct:
            base_improvement = 0.05
        else:
            # Student learns from mistakes with good explanations
            base_improvement = 0.03

        # Difficulty affects learning
        difficulty_multipliers = {""easy"": 0.7, ""medium"": 1.0, ""hard"": 1.3}
        multiplier = difficulty_multipliers.get(difficulty, 1.0)

        improvement = base_improvement * multiplier

        # Diminishing returns as proficiency increases
        diminishing_factor = (1.0 - current_prof) * 0.8 + 0.2
        improvement *= diminishing_factor

        # Update proficiency
        new_prof = min(1.0, current_prof + improvement)
        self.student_metrics[topic] = new_prof",Update student's proficiency based on learning.,"???Update student proficiency based on correctness, difficulty, and current skill level.???"
1668,_create_loss_mask,"def _create_loss_mask(self, batch, metrics):
        
        response_length = batch.batch['responses'].shape[-1]
        response_mask = batch.batch['attention_mask'][:, -response_length:]
        
        loss_mask = batch.batch['info_mask'][:, -response_length:]
        batch.batch['loss_mask'] = loss_mask

        metrics.update({
            'state_tokens/total': loss_mask.sum().item(),
            'state_tokens/coverage': (loss_mask.sum() / response_mask.sum()).item(),
        })
        
        return batch, metrics",Create loss mask for state tokens.,???Generate loss mask and update metrics for response data processing.???
1669,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Database.enable"",
    }
    json = yield cmd_dict","Enables database tracking, database events will now be delivered to the client.",???Enable database functionality by sending a command and yielding a JSON response.???
1670,populate_examples,"def populate_examples(session: CodegenSession, dest: Path, examples: list[SerializedExample], status: Status):
    
    status.update(""Populating example codemods..."")
    # Remove existing examples
    shutil.rmtree(dest, ignore_errors=True)
    dest.mkdir(parents=True, exist_ok=True)

    for example in examples:
        dest_file = dest / f""{example.name}.py""
        dest_file.parent.mkdir(parents=True, exist_ok=True)
        session.config.set(""repository.language"", str(example.language))
        formatted = format_example(example, session.config.repository.language)
        dest_file.write_text(formatted)",Populate the examples folder with examples for the current repository.,"???Clear destination, format, and save code examples to specified path.???"
1671,get_market_cap,"def get_market_cap(
    ticker: str,
    end_date: str,
) -> float | None:
    
    # Check if end_date is today
    if end_date == datetime.datetime.now().strftime(""%Y-%m-%d""):
        # Get the market cap from company facts API
        headers = {}
        if api_key := os.environ.get(""FINANCIAL_DATASETS_API_KEY""):
            headers[""X-API-KEY""] = api_key

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            print(f""Error fetching company facts: {ticker} - {response.status_code}"")
            return None

        data = response.json()
        response_model = CompanyFactsResponse(**data)
        return response_model.company_facts.market_cap

    financial_metrics = get_financial_metrics(ticker, end_date)
    if not financial_metrics:
        return None

    market_cap = financial_metrics[0].market_cap

    if not market_cap:
        return None

    return market_cap",Fetch market cap from the API.,???Determine a company's market capitalization using current or historical data.???
1672,preprocess_equation,"def preprocess_equation(expr_str: str, for_solving: bool = False) -> Tuple[str, bool]:
    
    expr_str = expr_str.strip()
    if not expr_str:
        raise ValueError(""Empty expression provided"")
    if '=' in expr_str:
        parts = [p.strip() for p in expr_str.split('=')]
        if len(parts) != 2:
            raise ValueError(""Invalid equation: multiple '=' signs detected"")
        if not all(parts):
            raise ValueError(""Invalid equation: empty side"")
        return f""Eq({parts[0]}, {parts[1]})"" if for_solving else f""({parts[0]}) - ({parts[1]})"", True
    return expr_str, False",Convert equation with '=' into appropriate form based on operation.,???Validate and format mathematical expressions for solving or analysis.???
1673,fetch_one,"def fetch_one(cls, query, params=None):
        
        with cls.get_connection() as conn:
            try:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    logger.info(f""Executing fetch_one query: {query}"")
                    logger.info(f""Parameters: {params}"")
                    cursor.execute(query, params or ())
                    row = cursor.fetchone()
                    if row:
                        logger.info(f""Query returned a row with id: {row.get('id')}"")
                        return dict(row)
                    else:
                        logger.warning(""Query returned no results"")
                        return None
            except Exception as e:
                logger.error(f""fetch_one error: {str(e)}"", exc_info=True)
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f""Database fetch failed: {str(e)}""
                )",Run a SELECT query and return one result.,???Retrieve a single database record with error handling and logging.???
1674,perform_human_validation,"def perform_human_validation(state: AgentState) -> AgentState:
        
        print(f""Selected agent: HUMAN_VALIDATION"")

        # Append validation request to the existing output
        validation_prompt = f""{state['output'].content}\n\n**Human Validation Required:**\n- If you're a healthcare professional: Please validate the output. Select **Yes** or **No**. If No, provide comments.\n- If you're a patient: Simply click Yes to confirm.""

        # Create an AI message with the validation prompt
        validation_message = AIMessage(content=validation_prompt)

        return {
            **state,
            ""output"": validation_message,
            ""agent_name"": f""{state['agent_name']}, HUMAN_VALIDATION""
        }",Handle human validation process.,???Enhance agent state with human validation prompt for healthcare context.???
1675,delete_model,"def delete_model(self, model_id: str) -> ModelDeletion:
        
        if not self.scanner.delete_model(model_id):
            raise ValueError(f""Model '{model_id}' not found in cache"")

        self.available_models = self._scan_models()
        return ModelDeletion(id=model_id, deleted=True)",Delete a model from local cache,???Delete a model by ID and update available models list.???
1676,format_tool_descriptions,"def format_tool_descriptions(schemas: List[Dict[str, Any]]) -> str:
    
    descriptions = []
    for schema in schemas:
        desc = [f""{schema['name']}: {schema['description']}""]
        
        desc.append(""\nArguments:"")
        for arg_name, arg_info in schema['args'].items():
            default = f"" (default: {arg_info['default']})"" if 'default' in arg_info else """"
            desc.append(f""  - {arg_name}: {arg_info['description']}{default}"")
        
        if schema['examples']:
            desc.append(""\nExamples:"")
            for example in schema['examples']:
                desc.append(f""  {example}"")
        
        if schema['returns']:
            desc.append(f""\nReturns: {schema['returns']}"")
        
        descriptions.append(""\n"".join(desc))
    
    return ""\n\n"".join(descriptions)",Formats tool schemas into a user-friendly description string.,???Generate formatted tool descriptions from schema data including arguments and examples.???
1677,sanity_check_inference,"def sanity_check_inference(inference_server: callable):
    
    start_time = time.time()
    lm_response = inference_server(""What does CUDA stand for?"")
    end_time = time.time()
    print(f""[Timing] Inference took {end_time - start_time:.2f} seconds"")
    print(lm_response) 
    return lm_response",Simple fucntion to intiiate call to server just to check we can call API endpoint,"???  
Measure and display inference server response time and output.  
???"
1678,invoke_agent,"def invoke_agent(
        self,
        data_raw: pd.DataFrame,
        user_instructions: str=None,
        target_variable: str=None,
        max_retries=3,
        retry_count=0,
        **kwargs
    ):
        
        response = self._compiled_graph.invoke({
            ""user_instructions"": user_instructions,
            ""data_raw"": data_raw.to_dict(),
            ""target_variable"": target_variable,
            ""max_retries"": max_retries,
            ""retry_count"": retry_count
        }, **kwargs)
        self.response = response
        return None","Synchronously trains an H2O AutoML model for the provided dataset, saving the best model to disk if model_directory or log_path is available.","???Invoke a compiled graph with user instructions and data, handling retries.???"
1679,_load_signatures,"def _load_signatures(cls) -> None:
        
        try:
            # Clear existing signatures before loading new ones
            cls._signature_groups = []
            cls._compiled_regexes = {}
            yaml_data = cls._load_yaml(cls._yaml_path)

            # Process patterns from YAML
            for item in yaml_data:
                for service_name, patterns in item.items():
                    service_patterns = {}
                    for pattern_dict in patterns:
                        for pattern_name, pattern in pattern_dict.items():
                            if not pattern or pattern.startswith(""#""):
                                continue
                            # Sanitize pattern before adding
                            service_patterns[pattern_name] = cls._sanitize_pattern(pattern)

                    if service_patterns:
                        cls._add_signature_group(service_name, service_patterns)

            logger.info(f""Loaded {len(cls._signature_groups)} signature groups"")
            logger.info(f""Compiled {len(cls._compiled_regexes)} regex patterns"")

        except Exception as e:
            logger.error(f""Error loading signatures: {e}"")
            raise",Load signature patterns from the YAML file.,"???Load and compile service signature patterns from a YAML file, handling errors.???"
1680,_convert_initial_actions,"def _convert_initial_actions(self, actions: list[dict[str, dict[str, Any]]]) -> list[ActionModel]:
		
		converted_actions = []
		action_model = self.ActionModel
		for action_dict in actions:
			# Each action_dict should have a single key-value pair
			action_name = next(iter(action_dict))
			params = action_dict[action_name]

			# Get the parameter model for this action from registry
			action_info = self.controller.registry.registry.actions[action_name]
			param_model = action_info.param_model

			# Create validated parameters using the appropriate param model
			validated_params = param_model(**params)

			# Create ActionModel instance with the validated parameters
			action_model = self.ActionModel(**{action_name: validated_params})
			converted_actions.append(action_model)

		return converted_actions",Convert dictionary-based actions to ActionModel instances,???Convert action dictionaries into validated action model instances.???
1681,context_aware_server,"def context_aware_server(basic_server_port: int) -> Generator[None, None, None]:
    
    proc = multiprocessing.Process(
        target=run_context_aware_server, args=(basic_server_port,), daemon=True
    )
    proc.start()

    # Wait for server to be running
    max_attempts = 20
    attempt = 0
    while attempt < max_attempts:
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.connect((""127.0.0.1"", basic_server_port))
                break
        except ConnectionRefusedError:
            time.sleep(0.1)
            attempt += 1
    else:
        raise RuntimeError(
            f""Context-aware server failed to start after {max_attempts} attempts""
        )

    yield

    proc.kill()
    proc.join(timeout=2)
    if proc.is_alive():
        print(""Context-aware server process failed to terminate"")",Start the context-aware server in a separate process.,???Launch and manage a context-aware server process with connection verification???
1682,to_chat_message,"def to_chat_message(self) -> ChatMessage:
        
        return ChatMessage(
            role=self.role,
            content=self.content,
            metadata={
                ""source"": self.source,
                ""plan_id"": self.plan_id,
                ""step_id"": self.step_id,
                ""session_id"": self.session_id,
                ""user_id"": self.user_id,
                ""message_id"": self.id,
                **self.metadata,
            },
        )",Convert to ChatMessage format.,???Convert object attributes into a structured chat message format.???
1683,fetch_tweets,"def fetch_tweets(self, cursor: str = """") -> Dict:
        
        # Join terms with OR for the search query
        query = "" OR "".join(self.search_terms)
        params = {""q"": query, ""cursor"": cursor}
        headers = {""apikey"": self.api_key}

        try:
            response = requests.get(self.base_url, headers=headers, params=params)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f""Request failed: {e}"")
            return {""tweets"": [], ""next_cursor_str"": None}
        except json.JSONDecodeError as e:
            logger.error(f""Failed to parse JSON response: {e}"")
            return {""tweets"": [], ""next_cursor_str"": None}",Fetch tweets matching configured search terms,???Fetch tweets using search terms and handle request errors???
1684,setUpClass,"def setUpClass(cls):
        
        if not cls.SERVICE_NAME:
            cls.skipTest(cls(), ""SERVICE_NAME not defined in test class"")
        
        if not APIKeyChecker.warn_if_missing(cls.SERVICE_NAME):
            cls.skipTest(cls(), f""API key for {cls.SERVICE_NAME} not available"")",Check API key availability before running tests,"???Initialize test class, skipping if service name or API key is missing.???"
1685,deep_to_dict,"def deep_to_dict(obj):
    
    if isinstance(obj, dict):
        return {key: deep_to_dict(val) for key, val in obj.items()}

    if isinstance(obj, list):
        return [deep_to_dict(item) for item in obj]

    if hasattr(obj, ""to_dict"") and callable(getattr(obj, ""to_dict"", None)):
        return deep_to_dict(obj.to_dict())

    if isinstance(obj, (int, float, str, bool, type(None))):
        return obj

    return None","Recursively convert objects that have a 'to_dict' method to dictionaries, otherwise drop them from the output.","???  
Recursively convert nested objects to dictionaries if possible.  
???"
1686,fetch_all,"def fetch_all(cls, query, params=None):
        
        with cls.get_connection() as conn:
            try:
                with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                    logger.info(f""Executing fetch_all query: {query}"")
                    logger.info(f""Parameters: {params}"")
                    cursor.execute(query, params or ())
                    results = cursor.fetchall()
                    logger.info(f""Query returned {len(results)} results"")
                    return [dict(row) for row in results]
            except Exception as e:
                logger.error(f""fetch_all error: {str(e)}"")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f""Database fetch failed: {str(e)}""
                )",Run a SELECT query and return all results.,???Retrieve all database records with error handling and logging.???
1687,record_vote,"def record_vote(vote_type, image_path, query, action_generated):
    
    vote_data = {
        ""vote_type"": vote_type,
        ""image_path"": image_path,
        ""query"": query,
        ""action_generated"": action_generated,
        ""timestamp"": datetime.now().isoformat()
    }
    with open(""votes.json"", ""a"") as f:
        f.write(json.dumps(vote_data) + ""\n"")
    return f""Your {vote_type} has been recorded. Thank you!""",Record a vote in a JSON file.,???Log user vote details with timestamp to a JSON file.???
1688,save,"def save(self, directory=None):
        
        if directory is None:
            directory = self.npc_directory
            
        ensure_dirs_exist(directory)
        npc_path = os.path.join(directory, f""{self.name}.npc"")
        
        return write_yaml_file(npc_path, self.to_dict())",Save NPC to file,??? Save NPC data to a specified or default directory in YAML format. ???
1689,get_board_state,"def get_board_state(self, raw_observation: Any, info: Dict[str, Any]) -> Optional[np.ndarray]:
        
        board_array_3d = None
        if isinstance(raw_observation, dict) and ""board"" in raw_observation:
            board_array_3d = raw_observation.get(""board"")
        # raw_observation could also be directly the board if not from _get_obs()
        elif isinstance(raw_observation, np.ndarray) and raw_observation.ndim == 3 and raw_observation.shape[0] == 2:
             board_array_3d = raw_observation

        if isinstance(board_array_3d, np.ndarray) and board_array_3d.ndim == 3 and board_array_3d.shape[0] >=1:
            # TileMatchEnv board obs is (2, rows, cols), first layer is color, second is type
            return board_array_3d[0] # Return the color layer
        elif isinstance(board_array_3d, np.ndarray) and board_array_3d.ndim == 2: # If it's already 2D
            return board_array_3d
        
        print(f""[CandyCrushEnvWrapper] Warning: Could not extract 2D color board from raw_observation: {type(raw_observation)}"")
        return None",Extracts the color board (2D numpy array) from raw gym observation.,"???Extracts a 2D color board from a 3D game observation if possible, otherwise returns None.???"
1690,_get_machine_model,"def _get_machine_model() -> str:
    
    try:
        result = subprocess.run(
            [""system_profiler"", ""-json"", ""SPHardwareDataType""], capture_output=True, text=True, check=True
        )
        data = json.loads(result.stdout)
        return data[""SPHardwareDataType""][0][""machine_model""]
    except (subprocess.CalledProcessError, json.JSONDecodeError, IndexError, KeyError) as e:
        logger.warning(f""Cannot determine machine model via 'system_profiler -json SPHardwareDataType': {e}"")
        return ""Unknown""",Get the Mac machine model using system_profiler.,"???Retrieve machine model using system profiler, handle errors gracefully???"
1691,format_line,"def format_line(self, include_line_numbers: bool = True) -> str:
        
        prefix = self.get_display_prefix()
        if include_line_numbers:
            line_num = str(self.line_number).rjust(4)
            prefix = f""{prefix}{line_num}""
        return f""{prefix}:{self.line_content}""","Format the line for display (e.g.,for logging or passing to an LLM).","??? 
Formats text line with optional line numbers and prefix. 
???"
1692,list_projects,"def list_projects() -> None:
    
    # Use API to list projects

    project_url = config.project_url

    try:
        response = asyncio.run(call_get(client, f""{project_url}/project/projects""))
        result = ProjectList.model_validate(response.json())

        table = Table(title=""Basic Memory Projects"")
        table.add_column(""Name"", style=""cyan"")
        table.add_column(""Path"", style=""green"")
        table.add_column(""Default"", style=""yellow"")
        table.add_column(""Active"", style=""magenta"")

        for project in result.projects:
            is_default = ""✓"" if project.is_default else """"
            is_active = ""✓"" if session.get_current_project() == project.name else """"
            table.add_row(project.name, format_path(project.path), is_default, is_active)

        console.print(table)
    except Exception as e:
        console.print(f""[red]Error listing projects: {str(e)}[/red]"")
        console.print(""[yellow]Note: Make sure the Basic Memory server is running.[/yellow]"")
        raise typer.Exit(1)",List all configured projects.,???Fetch and display project details from an API in a formatted table.???
1693,load_weights,"def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        
        params_dict = dict(self.named_parameters(remove_duplicate=False))
        loaded_names = set()
        for name, loaded_weight in weights:
            if name not in params_dict:
                continue

            param = params_dict[name]
            if ""c_attn"" in name or ""c_proj"" in name or ""c_fc"" in name:
                if name.endswith("".weight""):
                    loaded_weight = loaded_weight.t()

            weight_loader = getattr(param, ""weight_loader"", default_weight_loader)
            weight_loader(param, loaded_weight)
            loaded_names.add(name)
        # used to check if all weights were loaded
        assert set(params_dict.keys()) - loaded_names == set(), \
            (f""Missing weights: {set(params_dict.keys()) - loaded_names}, ""
             f""this probably means you are using an incompatible model, \n\nyour model has this weights: {set(params_dict.keys())}"")",Load weights following VLLM pattern.,"???Load and validate model weights, ensuring compatibility and completeness.???"
1694,bracket,"def bracket(self):
        
        if self.tag == ""td"":
            result = '""tag"": %s, ""colspan"": %d, ""rowspan"": %d, ""text"": %s' % (
                self.tag,
                self.colspan,
                self.rowspan,
                self.content,
            )
        else:
            result = '""tag"": %s' % self.tag
        for child in self.children:
            result += child.bracket()
        return ""{{{}}}"".format(result)",Show tree using brackets notation,???Format HTML element attributes and recursively process child elements.???
1695,_generate_dot_file,"def _generate_dot_file(self, graph: Dict[str, Dict[str, Any]]) -> str:
        
        dot_lines = [""digraph G {"", '  rankdir=""LR"";', ""  node [shape=box];""]

        # Add nodes
        for module, data in graph.items():
            node_color = ""lightblue"" if data[""type""] == ""internal"" else ""lightgreen""
            # Escape quotes in module name
            safe_module = module.replace('""', '\\""')
            dot_lines.append(f'  ""{safe_module}"" [style=filled, fillcolor={node_color}];')

        # Add edges
        for module, data in graph.items():
            safe_module = module.replace('""', '\\""')
            for dep in data[""dependencies""]:
                if dep in graph:  # Only add edges for modules in the graph
                    safe_dep = dep.replace('""', '\\""')
                    dot_lines.append(f'  ""{safe_module}"" -> ""{safe_dep}"";')

        dot_lines.append(""}"")
        return ""\n"".join(dot_lines)",Generate a DOT file for visualization with Graphviz.,???Generate a DOT format string representing a directed graph with nodes and edges.???
1696,remove_graph_edge,"def remove_graph_edge(self, edge: graph.Edge):
        

        def _get_node_name(node: ast.expr) -> str:
            if isinstance(node, ast.Str):
                return node.s
            if isinstance(node, ast.Name):
                return node.id
            raise ValidationError(f""Could not determine name of node `{node}` in {ENTRYPOINT}"")

        existing_edges: list[ast.Call] = self.get_graph_edge_nodes()
        for edge_node in existing_edges:
            source_node, target_node = edge_node.args
            source, target = _get_node_name(source_node), _get_node_name(target_node)
            if source == edge.source.name and target == edge.target.name:
                return self.remove_node(edge_node)

        raise ValidationError(
            f""Graph `add_edge({edge.source.name}, {edge.target.name})` not found for removal in {ENTRYPOINT}""
        )",Remove an edge from the graph configuration.,???Remove a specified edge from a graph by matching source and target node names.???
1697,setup_environment,"def setup_environment(env: str, openai_api_key: Optional[str] = None) -> Optional[str]:
    
    api_url = get_api_url(env)

    if env == ""local"":
        # Start local services (they should be healthy when this completes)
        if not start_local_services(openai_api_key):
            return None

        # Quick health check to verify (shorter timeout since services should already be ready)
        print(""Verifying backend is accessible..."")
        if not wait_for_health(api_url, timeout=30):
            print(""✗ Backend is not responding - may need more time to initialize"")
            return None

    else:
        # For dev/prod, just check if API is reachable
        print(f""Checking {env} API availability..."")
        if not wait_for_health(api_url, timeout=30):
            print(f""✗ {env.upper()} API is not reachable"")
            return None

    print(f""✓ Using API URL: {api_url}"")
    return api_url",Setup environment and return API URL if successful.,???Configure and verify environment setup for local or remote API access.???
1698,generate_random_config,"def generate_random_config(max_config: dict[str, Any]) -> dict[str, Any]:
        
        shuffle_moves = random.randint(1, max_config.get(""shuffle_moves""))
        if shuffle_moves % 2 == 0:
            shuffle_moves += 1
        return {
            ""size"": random.randint(2, max_config.get(""size"", 3)),
            ""shuffle_moves"": shuffle_moves,
        }",Generate a random config for the sliding puzzle game.,???Generate a random configuration with size and odd shuffle moves.???
1699,gen_user_csr,"def gen_user_csr(self, user_key: str, did: str) -> str:
        
        private_key = serialization.load_pem_private_key(
            data=user_key.encode('utf-8'), password=None)
        did_hash = self.__did_hash(did=did)
        builder = x509.CertificateSigningRequestBuilder().subject_name(
            x509.Name([
                # Central hub gateway service is only supported in China.
                x509.NameAttribute(NameOID.COUNTRY_NAME, 'CN'),
                x509.NameAttribute(NameOID.ORGANIZATION_NAME, 'Mijia Device'),
                x509.NameAttribute(
                    NameOID.COMMON_NAME, f'mips.{self._uid}.{did_hash}.2'),
            ]))
        csr = builder.sign(
            private_key, algorithm=None,  # type: ignore
            backend=default_backend())
        return csr.public_bytes(serialization.Encoding.PEM).decode('utf-8')",Generate CSR of user certificate.,???Generate a certificate signing request for a user using their key and identifier.???
1700,generate_answer,"def generate_answer(self, question):
        
        messages = [
            {
                ""role"": ""system"",
                ""content"": (
                    f""You are a student learning {self.profile['subject']} ""
                    f""at {self.profile['difficulty']} level. ""
                    f""Your goal is: {self.profile['goal']}.""
                ),
            },
            {""role"": ""user"", ""content"": f""Teacher asks: {question}""},
        ]

        response = openai.ChatCompletion.create(model=""gpt-4"", messages=messages)
        answer = response.choices[0].message.content.strip()
        self.log.append({""question"": question, ""student_answer"": answer})
        return answer",Generates a student-like answer to the teacher's question using the OpenAI LLM.,???Generate a student-tailored answer to a question using AI and log the interaction.???
1701,select_speaker_prompt,"def select_speaker_prompt(self, agents: Optional[list[Agent]] = None) -> str:
        
        if self.select_speaker_prompt_template is None:
            return None

        if agents is None:
            agents = self.agents

        agentlist = f""{[agent.name for agent in agents]}""

        return_prompt = f""{self.select_speaker_prompt_template}"".replace(""{agentlist}"", agentlist)
        return return_prompt",Return the floating system prompt selecting the next speaker.,??? Generate speaker prompt by replacing template placeholder with agent names. ???
1702,_reset_training_progress,"def _reset_training_progress() -> None:
        
        try:
            # Get all possible training progress file patterns
            base_dir = os.getenv('LOCAL_BASE_DIR', '.')
            progress_dir = os.path.join(base_dir, 'data', 'progress')
            if os.path.exists(progress_dir):
                for file in os.listdir(progress_dir):
                    if file.startswith('trainprocess_progress_'):
                        # Extract model name
                        model_name = file.replace('trainprocess_progress_', '').replace('.json', '')
                        # Create a new service instance for each model and reset progress
                        train_service = TrainProcessService(current_model_name=model_name)
                        train_service.progress.reset_progress()
                        logger.info(f""Reset training progress for model: {model_name}"")
            
            # Reset default training progress
            default_train_service = TrainProcessService.get_instance()
            if default_train_service is not None:
                default_train_service.progress.reset_progress()
            
            logger.info(""Reset default training progress"")
            
        except Exception as e:
            logger.error(f""Failed to reset training progress objects: {str(e)}"")",Reset training progress objects in memory,???Reset training progress for all models by clearing progress files and logging actions.???
1703,resolve_key_global_instance_to_ki,"def resolve_key_global_instance_to_ki( 
    key_hash_instance_str: str, 
    current_global_path_to_key_info: Dict[str, KeyInfo] 
) -> Optional[KeyInfo]:
    
    parts = key_hash_instance_str.split('#')
    base_key = parts[0]
    instance_num = 1 
    if len(parts) > 1:
        try: 
            instance_num = int(parts[1])
            if instance_num <= 0: 
                logger.warning(f""TrackerUtils.ResolveKI: Invalid instance num {instance_num} in '{key_hash_instance_str}'."")
                return None
        except ValueError: 
            logger.warning(f""TrackerUtils.ResolveKI: Invalid instance format in '{key_hash_instance_str}'."")
            return None
    
    matches = [ki for ki in current_global_path_to_key_info.values() if ki.key_string == base_key]
    if not matches:
        logger.warning(f""TrackerUtils.ResolveKI: Base key '{base_key}' (from '{key_hash_instance_str}') has no KeyInfo entries in global map."")
        return None
    
    matches.sort(key=lambda k_sort: k_sort.norm_path) 
            
    if 0 < instance_num <= len(matches):
        return matches[instance_num - 1]
    
    logger.warning(f""TrackerUtils.ResolveKI: Global instance {key_hash_instance_str} out of bounds (max {len(matches)} for key '{base_key}')."")
    return None",Resolves a KEY#global_instance string to a specific KeyInfo object from the provided current_global_path_to_key_info.,???Resolve key instance to key info from global map with validation and logging.???
1704,task_map,"def task_map(self):
        
        return {
            ""detect"": {
                ""model"": YOLOEModel,
                ""validator"": yolo.yoloe.YOLOEDetectValidator,
                ""predictor"": yolo.detect.DetectionPredictor,
                ""trainer"": yolo.yoloe.YOLOETrainer,
            },
            ""segment"": {
                ""model"": YOLOESegModel,
                ""validator"": yolo.yoloe.YOLOESegValidator,
                ""predictor"": yolo.segment.SegmentationPredictor,
                ""trainer"": yolo.yoloe.YOLOESegTrainer,
            },
        }","Map head to model, validator, and predictor classes.","???  
Map tasks to corresponding YOLO model components for detection and segmentation.  
???"
1705,load_update_data,"def load_update_data():
    
    if Path(LAST_CHECK_FILE_PATH).exists():
        try:
            with open(LAST_CHECK_FILE_PATH, 'r') as f:
                return json.load(f)
        except (json.JSONDecodeError, PermissionError):
            return {}
    return {}",Load existing update data or return empty dict if file doesn't exist,???Load and return update data from a file if it exists and is readable???
1706,deploy_prometheus,"def deploy_prometheus(username, private_key_file_1, public_ip_1):
    
    try:
        run_command(
            [
                ""ssh"",
                ""-o"",
                ""StrictHostKeyChecking=no"",
                ""-i"",
                private_key_file_1,
                f""{username}@{public_ip_1}"",
                f""bash {REPO}/scripts/setup.sh kubeworker1"",
            ]
        )
    except Exception as e:
        logger.error(f""Failed to deploy Prometheus: {str(e)}"")",Deploy Prometheus on the worker node.,???Deploy Prometheus on a remote server using SSH and a setup script.???
1707,mock_weaviate_vector_store,"def mock_weaviate_vector_store():
    
    mock_store = MockWeaviateVectorStore()

    # Configure the search results
    mock_search_result = [
        Document(id=""1"", content=""Document 1"", metadata={""source"": ""test""}, score=0.95),
        Document(id=""2"", content=""Document 2"", metadata={""source"": ""test""}, score=0.85),
    ]
    mock_store.search.return_value = mock_search_result
    mock_store.hybrid_search.return_value = mock_search_result

    return mock_store",Create a mock WeaviateVectorStore with all necessary attributes.,???Create a mock vector store with predefined search results for testing purposes.???
1708,load_checkpoint,"def load_checkpoint(self) -> None:
        
        # Load checkpoint file
        if self.checkpoint_path.exists():
            with open(self.checkpoint_path, ""r"") as f:
                checkpoint_data = json.load(f)
                self.completed_datasets = set(checkpoint_data.get(""completed_datasets"", []))

        # Load previous category results
        for category_dir in self.output_dir.iterdir():
            if category_dir.is_dir():
                category_name = category_dir.name
                self.previous_category_results[category_name] = []

                # Load each dataset result file in this category
                for dataset_file in category_dir.glob(""*.json""):
                    try:
                        with open(dataset_file, ""r"") as f:
                            dataset_result = json.load(f)
                            self.previous_category_results[category_name].append(dataset_result)
                    except Exception as e:
                        logging.warning(f""Error loading previous result {dataset_file}: {str(e)}"")",Load existing checkpoint and previous results if available.,???Load checkpoint and previous results from JSON files into memory???
1709,_queue_join,"def _queue_join(self):
        
        for i in range(MAX_JOINERS):
            joiner_id = self.global_store.get(f""joiner_{i}"").decode(""utf-8"")
            if joiner_id == ""null"":
                self.global_store.set(f""joiner_{i}"", self.world_info.global_unique_id)
                self.global_store.set(f""joiner_{i + 1}"", ""null"")
                break
        else:
            raise RuntimeError(""Too many joiners"")",Queue a node to join the mesh.,"???  
Assign unique identifiers to available slots in a queue until full.  
???"
1710,read_data,"def read_data(self) -> Dict:
        
        try:
            with self.file_path.open(""r"") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f""Error reading reply history: {str(e)}"")
            return {""processed_replies"": [], ""pending_replies"": []}",Read current data from file,"???Read JSON data from a file, handling errors with logging and default values.???"
1711,get_sonic_balance,"def get_sonic_balance(agent, **kwargs):
    
    try:
        address = kwargs.get(""address"")
        token_address = kwargs.get(""token_address"")
        
        if not address:
            load_dotenv()
            private_key = os.getenv('SONIC_PRIVATE_KEY')
            web3 = agent.connection_manager.connections[""sonic""]._web3
            account = web3.eth.account.from_key(private_key)
            address = account.address

        # Direct passthrough to connection method - add your logic before/after this call!
        agent.connection_manager.connections[""sonic""].get_balance(
            address=address,
            token_address=token_address
        )
        return

    except Exception as e:
        logger.error(f""Failed to get balance: {str(e)}"")
        return None",Get $S or token balance.,???Retrieve cryptocurrency balance using agent's connection and environment variables???
1712,get_address,"def get_address(self) -> str:
        
        try:
            account = self._get_current_account()
            return f""Your Monad address: {account.address}""
        except Exception as e:
            return f""Failed to get address: {str(e)}""",Get the wallet address,"???Retrieve and format the user's Monad account address, handling errors.???"
1713,update_db_version,"def update_db_version(self) -> None:
        
        logger.debug(f""Updating saved DB version to {package_version}."")

        self.delete_setting(""app.version"", commit=False)
        version = Setting(
            key=""app.version"",
            value=package_version,
            description=""Version of the app this database is associated with."",
            editable=False,
            name=""App Version"",
            type=SettingType.APP,
            ui_element=""text"",
            visible=False,
        )

        self.db_session.add(version)
        self.db_session.commit()",Updates the version saved in the DB based on the package version.,???Update database with the current application version information.???
1714,extract_server_names,"def extract_server_names(cfg: Optional[dict], specified: List[str] = None) -> Dict[int, str]:
    
    if not cfg or ""mcpServers"" not in cfg:
        return {}
    
    servers = cfg[""mcpServers""]
    
    if specified:
        return {i: name for i, name in enumerate(specified) if name in servers}
    else:
        return {i: name for i, name in enumerate(servers.keys())}","Extract server names from config, optionally filtered by specified list.",???Extract server names from configuration based on specified criteria???
1715,_resolve_value,"def _resolve_value(env_var: str, field_name: str, config_obj: Any) -> Any:
        
        value = os.environ.get(env_var)
        if value is not None:
            if (field_info := config_obj.__class__.model_fields.get(field_name)) is None:
                raise AttributeError(f""Config {config_obj} has no attribute {field_name}"")

            try:
                # Create a temporary model with just this field, then validate and rip it out.
                py_model = create_model('TempModel', __base__ = BaseConfig, **{field_name: (field_info.annotation, ...)}) # type: ignore
                validated = py_model.model_validate({field_name: value})
                return getattr(validated, field_name)
            except Exception as e:
                raise ValueError(f""Error setting {env_var}={value}: {e}"")
        return None",Resolve a single value from an environment variable env_var: full environment variable name (e.g. ZERO_BAND_TRAIN_MICRO_BS) field_name: actual field name in the config object (e.g. micro_bs),???Validate and assign environment variable to configuration field if present???
1716,mock_interrupted_command,"def mock_interrupted_command():
            
            try:
                # Simulate command starting
                output = ""Command started...\nProcessing files...""
                
                # Simulate interrupt during execution (like CTRL+C)
                raise KeyboardInterrupt(""User interrupted command"")
                
            except KeyboardInterrupt:
                # Simulate the real behavior - command returns partial output
                interrupted_output = f""{output}\nCommand interrupted by user""
                return interrupted_output",Simulate generic_linux_command being interrupted,???Simulate command execution interrupted by user input???
1717,extract_entities,"def extract_entities(self, text: str) -> List[str]:
        
        doc = self.nlp(text)

        # Define important entity types
        important_types = {
            ""PERSON"",
            ""ORG"",
            ""GPE"",
            ""NORP"",
            ""PRODUCT"",
            ""EVENT"",
            ""WORK_OF_ART"",
        }

        entities = [
            ent.text.strip()
            for ent in doc.ents
            if (
                ent.label_ in important_types
                and len(ent.text.strip()) > 1
                and not ent.text.isnumeric()
            )
        ]

        return list(set(entities))",Extract named entities with improved filtering,???Extracts and returns unique important entities from text using NLP processing.???
1718,_display_market_stats,"def _display_market_stats(self, df: pd.DataFrame) -> None:
        
        try:
            st.subheader(""📈 Market Statistics"")

            # Calculate momentum indicators
            df[""momentum""] = df[""Close""].diff(14)
            df[""roc""] = df[""Close""].pct_change(14)

            # Recent performance
            latest_close = df[""Close""].iloc[-1]
            prev_close = df[""Close""].iloc[-2]
            daily_change = (latest_close - prev_close) / prev_close

            # Volatility
            df[""returns""] = df[""Close""].pct_change()
            volatility = df[""returns""].std() * np.sqrt(252)  # Annualized volatility

            # Display stats
            col1, col2 = st.columns(2)
            with col1:
                st.metric(""Daily Change"", f""{daily_change:.2%}"")
                st.metric(""Momentum (14)"", f""{df['momentum'].iloc[-1]:.2f}"")
            with col2:
                st.metric(""Rate of Change"", f""{df['roc'].iloc[-1]:.2%}"")
                st.metric(""Volatility (Ann.)"", f""{volatility:.2%}"")

            logger.info(""Market statistics displayed successfully"")
        except Exception as e:
            logger.error(f""Failed to display market stats: {str(e)}"")",Display key market statistics and momentum indicators,"???Display market statistics with momentum, rate of change, and volatility metrics.???"
1719,mock_docker,"def mock_docker(self):
        
        with patch('meta_agent.sandbox.sandbox_manager.docker') as mock_docker:
            # Mock the Docker client
            mock_client = MagicMock()
            mock_client.ping.return_value = None
            mock_docker.from_env.return_value = mock_client
            
            # Mock container execution
            mock_container = MagicMock()
            mock_container.wait.return_value = {'StatusCode': 0}
            mock_container.logs.return_value = b""Test execution successful""
            mock_client.containers.run.return_value = mock_container
            
            yield mock_docker",Mock Docker to avoid requiring Docker daemon.,???Simulates Docker environment for testing by mocking client and container interactions.???
1720,_load_artifact_dir,"def _load_artifact_dir(self, artifact_dir: str) -> Dict[str, Union[str, bytes]]:
        
        artifacts = {}

        try:
            for filename in os.listdir(artifact_dir):
                file_path = os.path.join(artifact_dir, filename)
                if os.path.isfile(file_path):
                    try:
                        # Try to read as text first
                        with open(file_path, ""r"", encoding=""utf-8"") as f:
                            content = f.read()
                        artifacts[filename] = content
                    except UnicodeDecodeError:
                        # If text fails, read as binary
                        with open(file_path, ""rb"") as f:
                            content = f.read()
                        artifacts[filename] = content
                    except Exception as e:
                        logger.warning(f""Failed to read artifact file {file_path}: {e}"")
        except Exception as e:
            logger.warning(f""Failed to list artifact directory {artifact_dir}: {e}"")

        return artifacts",Load artifacts from a directory,???Load and return file contents from a directory as text or binary.???
1721,get_ideal_ctx,"def get_ideal_ctx(self, model_name: str) -> int | None:
        
        import re
        import math

        def extract_number_before_b(sentence: str) -> int:
            match = re.search(r'(\d+)b', sentence, re.IGNORECASE)
            return int(match.group(1)) if match else None

        model_size = extract_number_before_b(model_name)
        if not model_size:
            return None
        base_size = 7  # Base model size in billions
        base_context = 4096  # Base context size in tokens
        scaling_factor = 1.5  # Approximate scaling factor for context size growth
        context_size = int(base_context * (model_size / base_size) ** scaling_factor)
        context_size = 2 ** round(math.log2(context_size))
        self.logger.info(f""Estimated context size for {model_name}: {context_size} tokens."")
        return context_size",Estimate context size based on the model name.,???Determine optimal context size for a model based on its name.???
1722,minimal_support_case_data,"def minimal_support_case_data() -> Dict[str, Any]:
    
    return {
        ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
        ""subject"": ""EC2 instance not starting"",
        ""status"": ""opened"",
        ""serviceCode"": ""amazon-elastic-compute-cloud-linux"",
        ""categoryCode"": ""using-aws"",
        ""severityCode"": ""urgent"",
        ""submittedBy"": ""user@example.com"",
        ""timeCreated"": ""2023-01-01T12:00:00Z"",
    }",Return a dictionary with minimal support case data.,"???  
Generate a dictionary with predefined support case details for an EC2 issue.  
???"
1723,display_messages,"def display_messages() -> None:
    
    messages = st.session_state.user_chats[st.session_state[""session_id""]][""messages""]
    tool_calls_map = {}  # Map tool_call_id to tool call input

    for i, message in enumerate(messages):
        if message[""type""] in [""ai"", ""human""] and message[""content""]:
            display_chat_message(message, i)
        elif message.get(""tool_calls""):
            # Store each tool call input mapped by its ID
            for tool_call in message[""tool_calls""]:
                tool_calls_map[tool_call[""id""]] = tool_call
        elif message[""type""] == ""tool"":
            # Look up the corresponding tool call input by ID
            tool_call_id = message[""tool_call_id""]
            if tool_call_id in tool_calls_map:
                display_tool_output(tool_calls_map[tool_call_id], message)
            else:
                st.error(f""Could not find tool call input for ID: {tool_call_id}"")
        else:
            st.error(f""Unexpected message type: {message['type']}"")
            st.write(""Full messages list:"", messages)
            raise ValueError(f""Unexpected message type: {message['type']}"")",Display all messages in the current chat session.,"???Display chat and tool messages from session state, handling errors for unexpected types.???"
1724,request_api_wrapper,"def request_api_wrapper(url, data, score_key=""rewards"", try_max_times=5):
    
    headers = {
        ""Content-Type"": ""application/json"",
    }
    for _ in range(try_max_times):
        try:
            response = requests.post(url=url, json=data, headers=headers, timeout=180)
            response.raise_for_status()  # Raise an HTTPError for bad responses
            response = response.json()
            assert score_key in response, f""{score_key} not in {response}""
            return response.get(score_key)
        except requests.RequestException as e:
            logger.info(f""Request error, please check: {e}"")
        except Exception as e:
            logger.info(f""Unexpected error, please check: {e}"")
        time.sleep(1)

    raise Exception(f""Request error for {try_max_times} times, returning None. Please check the API server."")",Synchronous request API wrapper,???Retry API request to fetch specific data key with error handling and logging.???
1725,run_chat_loop,"def run_chat_loop(wf: Workflow):
    
    print(""\nWelcome to the AI Chat (DynamoDB Backend via YAML)! (Type 'exit' to end)"")

    user_id = f""user_{uuid.uuid4().hex[:6]}""
    session_id = f""session_{uuid.uuid4().hex[:8]}""

    print(""--- Starting New Chat ---"")
    print(f""   User ID: {user_id}"")
    print(f""   Session ID: {session_id}"")
    print(""-------------------------"")

    while True:
        try:
            user_input = input(f""{user_id} You: "")
            if user_input.lower() == ""exit"":
                break
            if not user_input.strip():
                continue

            agent_input = {""input"": user_input, ""user_id"": user_id, ""session_id"": session_id}

            result = wf.run(input_data=agent_input)

            response_content = result.output.get(""content"", ""..."")
            print(f""AI: {response_content}"")

        except KeyboardInterrupt:
            print(""\nExiting chat loop."")
            break
        except Exception as e:
            logger.error(f""An error occurred during chat: {e}"", exc_info=True)
            print(f""\nAn error occurred: {e}"", file=sys.stderr)
            break

    print(""\n--- Chat Session Ended ---"")",Runs the main chat loop using the loaded workflow.,"???Initiates an AI chat session with unique user and session IDs, processing inputs via a workflow.???"
1726,collect_images,"def collect_images(self):
        
        for slide_index, slide in enumerate(self.presentation.slides):
            for shape in slide.shape_filter(Picture):
                image_path = pbasename(shape.img_path)
                if image_path == ""pic_placeholder.png"":
                    continue
                if image_path not in self.image_stats:
                    size = PIL.Image.open(pjoin(self.config.IMAGE_DIR, image_path)).size
                    self.image_stats[image_path] = {
                        ""size"": size,
                        ""appear_times"": 0,
                        ""slide_numbers"": set(),
                        ""relative_area"": shape.area / self.slide_area * 100,
                    }
                self.image_stats[image_path][""appear_times""] += 1
                self.image_stats[image_path][""slide_numbers""].add(slide_index + 1)
        for image_path, stats in self.image_stats.items():
            stats[""slide_numbers""] = sorted(list(stats[""slide_numbers""]))
            ranges = self._find_ranges(stats[""slide_numbers""])
            top_ranges = sorted(ranges, key=lambda x: x[1] - x[0], reverse=True)[:3]
            top_ranges_str = "", "".join(
                [f""{r[0]}-{r[1]}"" if r[0] != r[1] else f""{r[0]}"" for r in top_ranges]
            )
            stats[""top_ranges_str""] = top_ranges_str",Collect images from the presentation and gather other information.,"???Extracts and analyzes image data from presentation slides, tracking occurrences and slide ranges.???"
1727,compute_score_multiple_choice,"def compute_score_multiple_choice(prediction: str, label: str) -> Dict[str, Any]:
    
    pred_match = re.search(r'([A-D])', prediction.upper())
    label_match = re.search(r'([A-D])', label.upper())
    
    is_valid = pred_match is not None
    
    if pred_match and label_match:
        pred_choice = pred_match.group(0)
        label_choice = label_match.group(0)
        is_correct = pred_choice == label_choice
    else:
        # Fallback to text comparison
        is_correct = normalize_text(prediction) == normalize_text(label)
    
    return {
        ""is_correct"": is_correct,
        ""is_valid"": is_valid,
        ""extracted_prediction"": pred_match.group(0) if pred_match else None,
        ""extracted_label"": label_match.group(0) if label_match else None
    }","Score multiple choice answers (A, B, C, D).",???Evaluate multiple-choice prediction accuracy and validity against a given label.???
1728,update_mint_json_with_api_nav,"def update_mint_json_with_api_nav(website_build_dir: Path, api_dir: Path) -> None:
    
    mint_json_path = website_build_dir / ""mint.json""
    if not mint_json_path.exists():
        print(f""File not found: {mint_json_path}"")
        sys.exit(1)

    # Get all MDX files in the API directory
    mdx_files = get_mdx_files(api_dir)

    # Create navigation structure
    nav_structure = create_nav_structure(mdx_files)

    # Update mint.json with new navigation
    update_nav(mint_json_path, nav_structure)",Update mint.json with MDX files in the API directory.,???Update website navigation in JSON using API directory structure???
1729,extract_category,"def extract_category(module_name):
    
    parts = module_name.split(""."")
    if len(parts) >= 3:
        return parts[1]  # reasoning_gym.{category}.dataset_name
    return ""other""",Extract category from module name.,"???Extracts the category from a module name or defaults to ""other"".???"
1730,get_trace_path,"def get_trace_path(self, stake_id: Optional[str] = None) -> PathsDict:
        
        base_path = self.get_project_source_root()
        test_id = stake_id if stake_id else self._default_test_id

        paths: PathsDict = {
            ""proofs"": os.path.join(base_path, ""proofs"", test_id, self.timestamp),
            ""junit_xml"": os.path.join(base_path, ""output"", self.timestamp),
            ""log_files"": os.path.join(base_path, ""log_files"", test_id, self.timestamp),
        }

        # Create directories
        for path in paths.values():
            os.makedirs(path, exist_ok=True)

        self.paths = paths
        return paths",Get all trace related paths for a test run.,???Generate and ensure directories for test-related paths based on a given or default identifier.???
1731,_handle_error,"def _handle_error(self, message: str, error: Exception) -> None:
        
        error_msg = f""{message}: {str(error)}""
        logger.error(error_msg)
        self.metrics['last_error'] = error_msg
        self._log_metrics()",Handle and log errors,???Log and record error details for monitoring purposes.???
1732,_run_event,"def _run_event(self):
        
        print(""Running event loop"")
        asyncio.set_event_loop(self.event_loop)

        async def setup():
            try:
                async with AsyncExitStack() as stack:
                    connections = [
                        await stack.enter_async_context(self._start_server(params))
                        for params in self.server_configs
                    ]
                    self.sessions, self.mcp_tools = [list(c) for c in zip(*connections)]
                    self.thread_running.set()  # Signal initialization is complete
                    
                    # Use asyncio.Future instead of Event for better cleanup
                    done_future = self.event_loop.create_future()
                    await done_future
            except Exception as e:
                logger.error(f""Error in MCP event loop: {str(e)}"")
                self.thread_running.set()  # Still set the event so we don't hang
                raise

        # Create and run the task directly in the event loop
        self.task = self.event_loop.create_task(setup())
        
        try:
            self.event_loop.run_until_complete(self.task)
        except asyncio.CancelledError:
            logger.info(""MCP client event loop was cancelled"")
        except Exception as e:
            logger.error(f""Error in MCP event loop: {str(e)}"")",Runs the event loop in a separate thread (for synchronous usage).,???Initialize and manage asynchronous server connections within an event loop???
1733,_get_backend,"def _get_backend(self) -> str:
        
        supported_backends = [
            ""chonkie"",
            ""transformers"",
            ""tokenizers"",
            ""tiktoken"",
        ]
        for backend in supported_backends:
            if backend in str(type(self.tokenizer)):
                return backend
        if (
            callable(self.tokenizer)
            or inspect.isfunction(self.tokenizer)
            or inspect.ismethod(self.tokenizer)
        ):
            return ""callable""
        raise ValueError(f""Unsupported tokenizer backend: {type(self.tokenizer)}"")",Get the tokenizer instance based on the identifier.,???Determine tokenizer backend type or raise error if unsupported???
1734,_endpoint_validator,"def _endpoint_validator(
        self,
        endpoint: str,
        telemetry_type: str,
    ) -> Union[str | Exception]:
        

        if endpoint == """":
            raise ValueError(
                ""OTLP endpoint must be set either in the environment variable OTEL_EXPORTER_OTLP_ENDPOINT or in the constructor.""
            )
            raise NotImplementedError(
                ""OTLP over HTTPS is not supported. Please use HTTP.""
            )

        endpoint = (
            endpoint
            if endpoint.endswith(f""/v1/{telemetry_type}"")
            else f""{endpoint}/v1/{telemetry_type}""
        )

        return endpoint",Validates the endpoint and method.,"???Validate and format telemetry endpoint URL, raising errors for invalid input.???"
1735,with_shape_cast,"def with_shape_cast(cls, shape_cast: dict[MSO_SHAPE_TYPE, type[ShapeElement]]):
        
        new_cls = type(f""{cls.__name__}_Isolated_{id(shape_cast)}"", (cls,), {})
        new_cls.shape_cast = MappingProxyType(shape_cast)
        return new_cls",Dynamically create a subclass of GroupShape with an isolated shape_cast.,???Create a new class with a shape type mapping as an immutable attribute.???
1736,delete_documents,"def delete_documents(self, document_ids: List[str]) -> bool:
        
        cursor = None
        try:
            cursor = self.conn.cursor()
            placeholders = "","".join([""?"" for _ in document_ids])
            cursor.execute(
                f""DELETE FROM documents WHERE id IN ({placeholders})"",
                document_ids,
            )
            self.conn.commit()
            return cursor.rowcount > 0
        except Exception as e:
            if self.conn:
                self.conn.rollback()
            logger.error(f""Failed to delete documents {document_ids}: {e}"")
            return False
        finally:
            if cursor:
                cursor.close()",Delete documents by IDs,???Delete specified documents from the database and handle errors.???
1737,mock_env_vars,"def mock_env_vars(self):
        
        with patch.dict(
            os.environ,
            {
                ""GOOGLE_IDENTITY_TOOL_KIT_KEY"": ""test_key"",
                ""isDevelopmentMode"": ""disabled"",
                ""defaultUsername"": ""test_user"",
            },
        ):
            yield",Mock environment variables for testing.,???Mock environment variables for testing with predefined values.???
1738,parse,"def parse(self, input: dict, response: str) -> dict:
        
        return {""question"": input[""question""], ""answer"": response}",Parse the model response into the desired output format.,???Extracts question from input and pairs it with response in a dictionary.???
1739,search_albums,"def search_albums(api_url: str, api_key: str, api_timeout: int, album_ids: List[int]) -> Optional[Dict]:
    
    if not album_ids:
        lidarr_logger.warning(""No album IDs provided for search."")
        return None
        
    payload = {
        ""name"": ""AlbumSearch"",
        ""albumIds"": album_ids
    }
    response = arr_request(api_url, api_key, api_timeout, ""command"", method=""POST"", data=payload)
    
    if response and isinstance(response, dict) and 'id' in response:
        command_id = response.get('id')
        lidarr_logger.info(f""Triggered Lidarr AlbumSearch for album IDs: {album_ids}. Command ID: {command_id}"")
        return response # Return the full command object including ID
    else:
        lidarr_logger.error(f""Failed to trigger Lidarr AlbumSearch for album IDs {album_ids}. Response: {response}"")
        return None",Trigger a search for specific albums in Lidarr.,???Search for albums using API with provided IDs and log the process.???
1740,get_conversation_history,"def get_conversation_history(conversation_id):
    
    if not conversation_id:
        return []

    conn = get_db_connection()
    cursor = conn.cursor()

    try:
        query = 
        cursor.execute(query, (conversation_id,))
        messages = cursor.fetchall()

        return [
            {
                ""role"": msg[""role""],
                ""content"": msg[""content""],
                ""timestamp"": msg[""timestamp""],
            }
            for msg in messages
        ]
    finally:
        conn.close()",Fetch all messages for a conversation in chronological order.,???Retrieve and format conversation messages from a database using a given ID.???
1741,sample_multiple_statements,"def sample_multiple_statements() -> dict[str, str]:
    
    return {
        ""multiple_ddl"": ""CREATE TABLE users (id SERIAL PRIMARY KEY); CREATE TABLE posts (id SERIAL PRIMARY KEY);"",
        ""mixed_with_migration"": ""SELECT * FROM users; CREATE TABLE logs (id SERIAL PRIMARY KEY);"",
        ""only_select"": ""SELECT * FROM users;"",
    }",Sample SQL with multiple statements for testing batch processing.,"??? 
Generate SQL statement examples for different database operations. 
???"
1742,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Accessibility.enable"",
    }
    json = yield cmd_dict",Enables the accessibility domain which causes ``AXNodeId``'s to remain consistent between method calls.,???Enable accessibility features via command dictionary generator.???
1743,state_dict,"def state_dict(self, *args, **kwargs):
        
        state_dict = self.layers.state_dict(*args, **kwargs)
        new_state_dict = {}
        for k, v in state_dict.items():
            k = k.lstrip(""module."").lstrip(""layers."")
            local_idx = int(k.split(""."")[0])
            name = k.split(""."", 1)[1]
            new_state_dict[f""{local_idx + self.layer_idx_start}.{name}""] = v
        return new_state_dict",Map layer indices to global layer indices.,"???  
Adjusts and returns a modified state dictionary with updated layer indices.  
???"
1744,log_final_cost,"def log_final_cost(self) -> None:
        
        # Skip displaying cost if already shown in the session summary
        if os.environ.get(""CAI_COST_DISPLAYED"", """").lower() == ""true"":
            return
        print(f""\nTotal CAI Session Cost: ${self.session_total_cost:.6f}"")",Display final cost information at exit,"???Check if session cost is displayed; if not, print total cost.???"
1745,_validate_indices,"def _validate_indices(self, indices):
        
        assert self.__video_reader is not None and self.__audio_reader is not None
        indices = np.array(indices, dtype=np.int64)
        # process negative indices
        indices[indices < 0] += len(self.__video_reader)
        if not (indices >= 0).all():
            raise IndexError(""Invalid negative indices: {}"".format(indices[indices < 0] + len(self.__video_reader)))
        if not (indices < len(self.__video_reader)).all():
            raise IndexError(""Out of bound indices: {}"".format(indices[indices >= len(self.__video_reader)]))
        return indices",Validate int64 integers and convert negative integers to positive by backward search,???Validate and adjust indices for video and audio data access???
1746,evaluate_answer_similarity,"def evaluate_answer_similarity(student_answer, ground_truth):
    
    try:
        response = client.chat.completions.create(
            model=""qwen2.5:7b"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""You are a evaluation expert. First, analyze the student's response to identify and extract their final answer. Then, compare the extracted answer with the correct solution. Output ONLY '1.0' if the extracted answer matches the correct solution in meaning, or '0.0' if the student's response does not contain a clear or correct answer. No other output is allowed.""
                },
                {
                    ""role"": ""user"",
                    ""content"": f""Student's response: {student_answer}\nCorrect solution: {ground_truth}\nOutput only 1.0 or 0.0:""
                }
            ],
            temperature=0
        )
        result = response.choices[0].message.content.strip()
        return float(result)
    
    except Exception as e:
        print(f""Error in GPT evaluation: {e}"")
        # If API call fails, fall back to simple text matching
        return 1.0 if student_answer ==ground_truth else 0.0",Use llm to evaluate answer similarity.,???Evaluate student answers against correct solutions using AI for semantic similarity.???
1747,extract_text_from_image,"def extract_text_from_image(self, image_path):
        
        return [
            {
                ""text"": ""LIFE IS LIKE"",
                ""bounding_box"": [41, 111, 963, 77, 966, 147, 41, 185],
                ""confidence"": 0.988,
            }
        ]",Mock implementation of extract_text_from_image.,???Extracts and returns text data with bounding boxes and confidence from an image.???
1748,_autocharge_amount,"def _autocharge_amount(cls, tenant: TenantData, min_amount: float) -> float:
        
        if (
            tenant.automatic_payment_threshold is None
            or tenant.automatic_payment_balance_to_maintain is None
            or tenant.current_credits_usd > tenant.automatic_payment_threshold
        ):
            return min_amount

        amount = tenant.automatic_payment_balance_to_maintain - tenant.current_credits_usd
        # This can happen if automatic_payment_threshold > automatic_payment_balance_to_maintain
        # For example: threshold = 100, maintain = 50, current = 75
        # This would be a stupid case.
        if amount <= min_amount:
            _logger.warning(
                ""Automatic payment would charge negative amount"",
                extra={""tenant"": tenant.model_dump(exclude_none=True, exclude={""providers""})},
            )
            # Returning the balance to maintain to avoid charging 0
            return min_amount or tenant.automatic_payment_balance_to_maintain

        return amount",Returns the amount to charge or `min_amount` if no amount is needed,??? Calculate tenant's autocharge amount based on payment thresholds and balances. ???
1749,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        result = {
            ""id"": self.id,
            ""body"": self.body,
            ""created"": self.format_timestamp(self.created),
            ""updated"": self.format_timestamp(self.updated),
        }

        if self.title:
            result[""title""] = self.title

        if self.author:
            result[""author""] = self.author.display_name

        return result",Convert to simplified dictionary for API response.,???Convert object attributes to a simplified dictionary format with optional fields.???
1750,reset_persistent_statistics,"def reset_persistent_statistics():
    
    try:
        success = reset_swaparr_stats()
        if success:
            swaparr_logger.info(""Reset Swaparr persistent statistics"")
            return jsonify({""success"": True, ""message"": ""Persistent statistics reset successfully""})
        else:
            return jsonify({""success"": False, ""message"": ""Failed to reset persistent statistics""}), 500
    except Exception as e:
        swaparr_logger.error(f""Error resetting persistent statistics: {str(e)}"")
        return jsonify({""success"": False, ""message"": f""Error resetting persistent statistics: {str(e)}""}), 500",Reset persistent statistics (the ones shown on homepage),"???  
Resets and logs the status of persistent statistics, handling errors.  
???"
1751,parse,"def parse(
        cls,
        response: str,
        answer_key: str,
        *,
        global_description: str = """",
        query_question: str = """",
        is_single_line_ans: bool = None,
    ) -> dict:
        
        response_parsed = cls._parse(
            response,
            is_ascii_art=True,
            should_remove_surrounding_whitespace=False,
            global_description=global_description,
            query_question=query_question,
            is_single_line_ans=is_single_line_ans,
        )
        results = {answer_key: response_parsed}
        return results",Try to parse a single answer.,???Parse response string into a dictionary using specified parameters.???
1752,validate_platform_str,"def validate_platform_str(cls, v):
        
        try:
            Platform(v)
            return v
        except ValueError:
            supported = ', '.join([p.value for p in Platform])
            raise ValueError(f""Invalid platform '{v}'. Must be one of: {supported}"")",Validate that the platform string is one of the supported platforms,"??? Validate input against supported platforms, raising error if invalid. ???"
1753,create_from_filepath,"def create_from_filepath(cls, filepath: str, ctx: CodebaseContext) -> Self | None:
        
        if filepath in ctx.filepath_idx:
            msg = f""File already exists in graph: {filepath}""
            raise ValueError(msg)

        ts_node = parse_file(filepath, """")
        if ts_node.has_error:
            logger.info(""Failed to parse file %s"", filepath)
            raise SyntaxError

        file = cls(ts_node, filepath, ctx)
        file.write("""", to_disk=True)
        return file",Makes a new empty file and adds it to the graph.,???Create a new file object from a filepath if it doesn't already exist in the context.???
1754,get_multiline_input,"def get_multiline_input(prompt_text: str) -> str:
    
    console.print(f""\n[bold blue]{prompt_text}[/]"")
    console.print(""[dim](Enter your text; press Ctrl+D (Unix) or Ctrl+Z+Enter (Windows) when done)[/dim]"")
    
    lines = []
    try:
        while True:
            line = input()
            lines.append(line)
    except EOFError:
        pass
    
    return ""\n"".join(lines)",Get multiline input from the user with a nice UX.,??? Collects multiline user input until EOF is reached. ???
1755,inject_required_but_invisible_defaults,"def inject_required_but_invisible_defaults(parameters_schema: dict, input_data: dict) -> dict:
    
    for prop, subschema in parameters_schema.get(""properties"", {}).items():
        # check if the property is not set by user and is required but invisible
        if (
            prop not in input_data
            and prop in parameters_schema.get(""required"", [])
            and prop not in parameters_schema.get(""visible"", [])
        ):
            # check if it has a default value, which should exist for non-object types
            if ""default"" in subschema:
                input_data[prop] = subschema[""default""]
            else:
                # If no default value, but it's an object, initialize it as an empty dict
                if subschema.get(""type"") == ""object"":
                    input_data[prop] = {}
                else:
                    raise Exception(
                        f""No default value found for property: {prop}, type: {subschema.get('type')}""
                    )
        # Recursively inject defaults for nested objects
        if isinstance(input_data.get(prop), dict):
            inject_required_but_invisible_defaults(subschema, input_data[prop])

    return input_data",Recursively injects required but invisible properties with their default values into the input data.,???Inject default values into input data for required but hidden schema properties???
1756,render_mdx_page_title,"def render_mdx_page_title(cls_doc: ClassDoc, icon: str | None = None) -> str:
    
    page_desc = cls_doc.description if hasattr(cls_doc, ""description"") else """"

    return f",Renders the MDX for the page title,"???  
Generates a formatted page title using class documentation and an optional icon.  
???"
1757,add_suffix,"def add_suffix(a: str, b: str) -> str:
    
    prot_a, path_a = _pathify(a)
    prot_b, path_b = _pathify(b)

    if prot_b:
        raise ValueError(f""{b} is not a relative path"")

    return join_path(prot_a, str(path_a / path_b))",Return the the path of a joined with b.,"???Combine two paths, ensuring the second is relative, and return the result.???"
1758,step,"def step(self, **kwargs) -> Union[SequentialWorkFlowGraph, ActionGraph]:
        
        graph = self._select_graph_with_highest_score(return_metrics=False)
        if isinstance(graph, SequentialWorkFlowGraph):
            new_graph = self._workflow_graph_step(graph)
        elif isinstance(graph, ActionGraph):
            new_graph = self._action_graph_step(graph)
        else:
            raise ValueError(f""Invalid graph type: {type(graph)}. The graph should be an instance of `WorkFlowGraph` or `ActionGraph`."")
        return new_graph",Take a step of optimization and return the optimized graph.,"???  
Determine and process the highest-scoring graph, returning an updated workflow or action graph.  
???"
1759,run_conversation,"def run_conversation(self):
        
        # Get initial thoughts from the human
        self.console.print(
            f""\n[bold green]Start the discussion about {self.topic}:[/bold green]""
        )
        self._get_user_input()

        for round_num in range(self.max_rounds):
            self.console.print(f""\n[bold green]Round {round_num + 1}[/bold green]"")

            # Let all AI models respond
            for model_name, session in self.MODEL_SESSIONS.items():
                for turn in range(self.turns_per_model):
                    conversation = self._create_conversation(session, model_name)

                    # Add the prompt (simplified since human always starts)
                    prompt = f""Continue the discussion about {self.topic}, responding to the previous points made.""
                    conversation.add_message(role=""user"", text=prompt)

                    # Get and print response
                    response = conversation.send()
                    self._print_response(model_name, response.text)

                    # Small delay to prevent rate limiting
                    time.sleep(1)

            # Then get user input at the end of the round
            self._get_user_input()

            # Optional: Add a separator between rounds
            self.console.print(""\n"" + ""-"" * 50)",Runs the multi-AI conversation.,???Facilitates a multi-round dialogue between a human and AI models on a specified topic.???
1760,validate_source,"def validate_source(self, source: str) -> bool:
        
        if os.path.isfile(source):
            return True
        try:
            result = requests.utils.urlparse(source)
            return all([result.scheme, result.netloc])
        except (requests.exceptions.RequestException, ValueError) as e:
            # Log the specific exception for debugging
            logger.debug(f""URL validation failed: {e}"")
            return False",Validate if source is a valid file path or URL.,???Determine if a given source is a valid file path or URL.???
1761,worker_init_handler,"def worker_init_handler(**kwargs):
    
    config = get_config_params()
    logger.info(
        f""Celery worker initialized with concurrency {config.get('maxConcurrentDownloads', 3)}""
    )
    logger.info(
        f""Worker config: spotifyQuality={config.get('spotifyQuality')}, deezerQuality={config.get('deezerQuality')}""
    )
    logger.debug(""Worker Redis connection: "" + REDIS_URL)",Log when a worker initializes with its configuration details,???Initialize Celery worker with configuration and log setup details.???
1762,register_agent,"def register_agent(cls, name: str, agent: Agent) -> None:
        
        if name in cls._agents:
            raise ValueError(f""Agent with name {name} already exists"")
        cls._agents[name] = agent",Register an agent instance with a name.,"???Register a new agent by name, ensuring no duplicates exist???"
1763,get_audio_info,"def get_audio_info(self, audio_latents, frame_rate):
        
        
        latent_tensor = self._extract_and_validate_latent(audio_latents)
        batch_size, channels, height, actual_length = latent_tensor.shape
        
        # Calculate duration using the default frame rate (10.77)
        default_frame_rate = 10.77
        duration = actual_length / default_frame_rate
        
        # Calculate length_frames based on the specified frame rate
        length_frames = int(duration * frame_rate)
        
        # Create info string
        info = f""Audio Latent Info:\n""
        info += f""Batch size: {batch_size}\n""
        info += f""Channels: {channels}\n""
        info += f""Height: {height}\n""
        info += f""Actual length: {actual_length} frames\n""
        info += f""Frame rate: {frame_rate:.2f} fps\n""
        info += f""Length: {length_frames} frames\n""
        info += f""Duration: {duration:.2f} seconds""
        
        return (batch_size, length_frames, duration, info)","Extract batch size, length, and duration from audio latents",???Extracts audio metadata and calculates duration and frame length.???
1764,tool_by_names,"def tool_by_names(self) -> dict[str, Node]:
        
        return {self.sanitize_tool_name(tool.name): tool for tool in self.tools}",Returns a dictionary mapping tool names to their corresponding Node objects.,???Create a dictionary mapping sanitized tool names to their corresponding nodes.???
1765,extract_path,"def extract_path(full_path: str) -> str:
    
    logger.debug(f""Extracting path from {full_path}"")
        parsed = urlparse(full_path)
        path = parsed.path
        if parsed.query:
            path = f""{path}?{parsed.query}""
        return path.lstrip(""/"")
    return full_path.lstrip(""/"")",Extract clean path from full URL or path string,???Extracts and returns the path and query from a URL string???
1766,set_extra_http_headers,"def set_extra_http_headers(
    headers: Headers,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""headers""] = headers.to_json()
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Network.setExtraHTTPHeaders"",
        ""params"": params,
    }
    json = yield cmd_dict",Specifies whether to always send extra HTTP headers with the requests from this page.,???Configure additional HTTP headers for network requests.???
1767,_handle_stdout,"def _handle_stdout(self, data: str) -> None:
        
        self.stdout_lines.append(data)
        logger.debug(f""[SANDBOX STDOUT] {data}"", end="""", flush=True)",Handle stdout data from the sandbox process.,???Appends data to stdout lines and logs it for debugging???
1768,_get_sample_prompt_embeddings,"def _get_sample_prompt_embeddings(self, dataset: Dataset, sample_size: int = 512) -> torch.FloatTensor:
        
        n_samples = min(len(dataset), sample_size)
        rand_indices = np.random.choice(len(dataset), size=(n_samples,))

        embedding_dataset = dataset.select(rand_indices)

        dataloader_params = {
            ""batch_size"": self.args.per_device_train_batch_size,
            ""collate_fn"": self.data_collator,
            ""num_workers"": self.args.dataloader_num_workers,
            ""pin_memory"": self.args.dataloader_pin_memory,
            ""shuffle"": False,
        }

        # prepare dataloader
        data_loader = self.accelerator.prepare(DataLoader(embedding_dataset, **dataloader_params))

        with torch.no_grad():
            all_embeddings = torch.empty(0)
            for padded_batch in tqdm(iterable=data_loader, desc=""Building sample prompt embeddings""):
                embeddings = self._vectorize_prompt(
                    input_ids=padded_batch[""embedding_input_ids""],
                    attention_mask=padded_batch[""embedding_attention_mask""],
                )
                embeddings = self.accelerator.gather_for_metrics(embeddings)
                all_embeddings = torch.cat((all_embeddings, embeddings.cpu()))

        return all_embeddings",Sample instances from dataset and get prompt embeddings.,???Generate prompt embeddings from a dataset sample using a dataloader and vectorization.???
1769,get_prices,"def get_prices(ticker: str, start_date: str, end_date: str) -> list[Price]:
    
    # Create a cache key that includes all parameters to ensure exact matches
    cache_key = f""{ticker}_{start_date}_{end_date}""
    
    # Check cache first - simple exact match
    if cached_data := _cache.get_prices(cache_key):
        return [Price(**price) for price in cached_data]

    # If not in cache, fetch from API
    headers = {}
    if api_key := os.environ.get(""FINANCIAL_DATASETS_API_KEY""):
        headers[""X-API-KEY""] = api_key

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f""Error fetching data: {ticker} - {response.status_code} - {response.text}"")

    # Parse response with Pydantic model
    price_response = PriceResponse(**response.json())
    prices = price_response.prices

    if not prices:
        return []

    # Cache the results using the comprehensive cache key
    _cache.set_prices(cache_key, [p.model_dump() for p in prices])
    return prices",Fetch price data from cache or API.,???Fetch and cache stock prices using API with error handling and caching mechanism.???
1770,collate,"def collate(self, out):
        
        df = out.to_pandas()
        grouped_df = df.groupby(""question"").agg({""property"": list, ""probability"": list, ""reasoning"": lambda x: "" | "".join(x)}).reset_index()
        return Dataset.from_pandas(grouped_df)",Collate the output into a single dataset.,???Transform output data into a grouped dataset by question with aggregated properties.???
1771,_log_processing_summary,"def _log_processing_summary(self, stats: ProcessingStats):
        
        summary_lines = [
            ""Processing Summary:"",
            f""Files Processed: {stats.processed_files}"",
            f""Total Documents: {stats.total_documents}"",
            f""Split Documents: {stats.split_documents}"",
            f""Failed Files: {stats.failed_files}"",
            f""Skipped Files: {stats.skipped_files}"",
            f""Blocklisted Files: {stats.blocklisted_files}"",
        ]

        if stats.total_file_size > 0:
            size_mb = stats.total_file_size / (1024 * 1024)
            summary_lines.append(f""Total File Size: {size_mb:.2f} MB"")

        for line in summary_lines:
            self.logger.info(line)",Log a summary of the processing results.,???Log a summary of file processing statistics including counts and sizes.???
1772,_prepare_session_state,"def _prepare_session_state(self, session_attributes: Dict[str, Any], function_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        
        session_state = {
            'promptSessionAttributes': session_attributes,
        }

        if function_result and self.invocation_id:
            session_state.update({
                'invocationId': self.invocation_id,
                'returnControlInvocationResults': [{
                    'functionResult': function_result
                }]
            })

        return session_state",Prepare the session state dictionary.,???Prepare and update session state with attributes and function results.???
1773,_append_to_log,"def _append_to_log(self, line: str) -> None:
        
        try:
            if os.path.exists(self.module_file):
                with open(self.module_file, ""r"") as f:
                    data = json.load(f)
            else:
                data = []

            data.append(line)
            with open(self.module_file, ""w"") as f:
                json.dump(data[-self.max_memory:], f, indent=2)
        except Exception as exc:
            print(f""[MemoryModule] failed to write log: {exc}"")",Persist *just the printable line* per update.,"???Append a log entry to a JSON file, handling file existence and exceptions.???"
1774,set_bypass_service_worker,"def set_bypass_service_worker(
    bypass: bool,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""bypass""] = bypass
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Network.setBypassServiceWorker"",
        ""params"": params,
    }
    json = yield cmd_dict",Toggles ignoring of service worker for each request.,???Configure network to bypass service worker based on input flag???
1775,create_service_container,"def create_service_container() -> dict[str, Any]:
    
    container = {""user_service"": UserService(), ""item_service"": ItemService()}
    return container",Create a container with all services,"??? 
Initialize and return a dictionary of service instances. 
???"
1776,_check_isomorphic,"def _check_isomorphic(self, s: str, t: str) -> bool:
        
        if len(s) != len(t):
            return False

        mapping, inverse_mapping = {}, {}  # s -> t, t -> s
        for i in range(len(s)):
            if (s[i] in mapping and mapping[s[i]] != t[i]) or (
                t[i] in inverse_mapping and s[i] != inverse_mapping[t[i]]
            ):
                return False
            mapping[s[i]] = t[i]
            inverse_mapping[t[i]] = s[i]

        return True",Check if two strings are isomorphic,"???  
Determine if two strings are isomorphic by checking consistent character mappings.  
???"
1777,run_everything_legacy_sse_http_server,"def run_everything_legacy_sse_http_server(server_port: int) -> None:
    
    _, app = make_everything_fastmcp_app()
    server = uvicorn.Server(
        config=uvicorn.Config(
            app=app, host=""127.0.0.1"", port=server_port, log_level=""error""
        )
    )
    print(f""Starting comprehensive server on port {server_port}"")
    server.run()",Run the comprehensive server with all features.,???Initialize and run a legacy SSE HTTP server on a specified port.???
1778,term_mp,"def term_mp(sig_num, frame):
    
    pid = os.getpid()
    pgid = os.getpgid(os.getpid())
    print(""main proc {} exit, kill process group "" ""{}"".format(pid, pgid))
    os.killpg(pgid, signal.SIGKILL)",kill all child processes,???Terminate process group upon receiving a specific signal.???
1779,find_server_files,"def find_server_files(servers_dir: Path) -> List[Path]:
    
    if not servers_dir.exists() or not servers_dir.is_dir():
        error_exit(f""Servers directory not found: {servers_dir}"")

    server_files = []
    for file_path in servers_dir.glob(""*.json""):
        if file_path.is_file():
            server_files.append(file_path)

    return server_files",Find all server JSON files in the servers directory,???Locate and return JSON files from a specified server directory.???
1780,chat_component,"def chat_component():
    
    st.subheader(f""{t('Welcome to ')}UltraRAG-KBAlign"")

    # Setup chat container
    his_container = st.container(height=500)
    
    with his_container:
        # Display chat history
        for (query, response) in st.session_state.kbalign_history:
            with st.chat_message(name=""user"", avatar=""user""):
                st.markdown(query, unsafe_allow_html=True)
            with st.chat_message(name=""assistant"", avatar=""assistant""):
                st.markdown(response, unsafe_allow_html=True)
        container = st.empty()
        container_a = st.empty()
        
    # Handle user input
    chat_input_container = st.container()
    with chat_input_container:
        if query := st.chat_input(t(""Please input your question"")):
            container.chat_message(""user"").markdown(query, unsafe_allow_html=True)
            response_with_extra_info, pure_response = asyncio.run(listen(query,container_a))
            
            # Update chat history
            st.session_state.kbalign_messages.append({""role"": ""user"", ""content"": query})
            st.session_state.kbalign_messages.append({""role"": ""assistant"", ""content"": pure_response})
            st.session_state.kbalign_history.append((query, response_with_extra_info))
            st.rerun()",Display the chat interface component.,???A chat interface for user queries and assistant responses with session history management.???
1781,save_index,"def save_index(self):
        
        try:
            faiss.write_index(self.index, self.index_file)
            with open(self.metadata_file, ""wb"") as f:
                pickle.dump(self.metadata, f)
            logger.debug(f""Saved FAISS index with {self.index.ntotal} entries"")
        except Exception as e:
            logger.error(f""Error saving FAISS index: {str(e)}"")",Save the FAISS index and metadata to disk.,"???Persist FAISS index and metadata to files, logging success or errors.???"
1782,tokenize_dataset,"def tokenize_dataset(self, dataset, text_column_name, sequence_length, num_proc):
        
        # Create a partial function with fixed arguments
        tokenizer_func = partial(
            self.tokenizer_group_text,
            tokenizer=self.tokenizer,
            sequence_length=sequence_length
        )

        tokenized_dataset = dataset.map(
            tokenizer_func,
            input_columns=text_column_name,
            remove_columns=dataset.column_names,
            features=Features({
                ""input_ids"": Sequence(feature=Value(dtype=""int64""), length=sequence_length + 1)
            }),
            batched=True,
            num_proc=num_proc,
            load_from_cache_file=True,
            desc=f""Grouping texts in chunks of {sequence_length+1}"",
        )

        return tokenized_dataset",Tokenize the dataset and group texts in chunks of sequence_length + 1,???Tokenize and map dataset text into sequences using a specified tokenizer.???
1783,__store_version,"def __store_version(
        session, new_version_number: int, description: str = None
) -> L1Version:
    
    version = L1Version(
        version=new_version_number,
        create_time=datetime.now(),
        status=""active"",
        description=description or f""L1 data version {new_version_number}"",
    )
    session.add(version)
    return version",Store L1 version information,???Store a new version entry in the database with metadata.???
1784,copy_images_from_notebooks_dir_to_target_dir,"def copy_images_from_notebooks_dir_to_target_dir(notebook_directory: Path, target_notebooks_dir: Path) -> None:
    
    # Define supported image extensions
    supported_img_extensions = {"".png"", "".jpg""}

    # Single loop through directory contents
    for image_path in notebook_directory.iterdir():
        if image_path.is_file() and image_path.suffix.lower() in supported_img_extensions:
            target_image = target_notebooks_dir / image_path.name
            shutil.copy(image_path, target_image)",Copy images from notebooks directory to the target directory.,???Copy images with specific extensions from source to target directory.???
1785,_create_agent_chunk,"def _create_agent_chunk(self, content: str) -> Dict[str, Any]:
        
        return {""agent"": {""messages"": [AIMessage(content=content)]}}",Create an agent chunk in the format expected by print_agent_output.,???Create a dictionary with AI message content for an agent.???
1786,get_upload_status,"def get_upload_status(self):
        
        if self.tracer_type == ""langchain"":
            if self._upload_task is None:
                return ""No upload task in progress.""
            if self._upload_task.done():
                try:
                    result = self._upload_task.result()
                    return f""Upload completed: {result}""
                except Exception as e:
                    return f""Upload failed: {str(e)}""
            return ""Upload in progress...""",Check the status of the trace upload.,???Determine upload status based on task completion and handle exceptions.???
1787,to_json,"def to_json(self):
        
        RESULTS_DIR.mkdir(parents=True, exist_ok=True)

        with open(RESULTS_DIR / f""{self.session_id}_{self.start_time}.json"", ""w"") as f:
            json.dump(self.to_dict(), f, indent=4)",Save the session to a JSON file.,"???  
Convert session data to JSON and save in results directory.  
???"
1788,transfer,"def transfer(
        self,
        to_address: str,
        amount: float,
        token_address: Optional[str] = None
    ) -> str:
        
        try:
            # Validate balance first
            current_balance = self.get_balance(token_address=token_address)
            if current_balance < amount:
                raise ValueError(
                    f""Insufficient balance. Required: {amount}, Available: {current_balance}""
                )

            # Prepare and send transaction
            tx = self._prepare_transfer_tx(to_address, amount, token_address)
            private_key = os.getenv('ETH_PRIVATE_KEY')
            account = self._web3.eth.account.from_key(private_key)
            
            signed = account.sign_transaction(tx)
            tx_hash = self._web3.eth.send_raw_transaction(signed.rawTransaction)
            
            # Return explorer link
            tx_url = self._get_explorer_link(tx_hash.hex())
            return tx_url

        except Exception as e:
            logger.error(f""Transfer failed: {str(e)}"")
            raise",Transfer ETH or tokens with balance validation,"???Facilitates cryptocurrency transfer by validating balance, preparing, signing, and sending a transaction.???"
1789,_single_check,"def _single_check(im):
        
        assert isinstance(im, (Image.Image, np.ndarray)), f""Expected PIL/np.ndarray image type, but got {type(im)}""
        if isinstance(im, Image.Image):
            if im.mode != ""RGB"":
                im = im.convert(""RGB"")
            im = np.asarray(im)[:, :, ::-1]
            im = np.ascontiguousarray(im)  # contiguous
        return im","Validate and format an image to numpy array, ensuring RGB order and contiguous memory.","???  
Convert and validate image input to a contiguous RGB NumPy array.  
???"
1790,_enforce_min_bucket_size,"def _enforce_min_bucket_size(self):
        
        logger.info(
            f""Enforcing minimum image size of {self.minimum_image_size}.""
            "" This could take a while for very-large datasets.""
        )
        for bucket in tqdm(
            list(self.aspect_ratio_bucket_indices.keys()),
            leave=False,
            desc=""Enforcing minimum bucket size"",
        ):  # Safe iteration over keys
            # Prune the smaller buckets so that we don't enforce resolution constraints on them unnecessarily.
            self._prune_small_buckets(bucket)
            if self.minimum_image_size is not None:
                self._enforce_resolution_constraints(bucket)
                # We do this twice in case there were any new contenders for being too small.
                self._prune_small_buckets(bucket)",Remove buckets that have fewer samples than batch_size and enforce minimum image size constraints.,"???Ensure image buckets meet minimum size constraints, pruning smaller ones as needed.???"
1791,_to_value_map,"def _to_value_map(value: Any) -> str:
        
        json_str = json.dumps(value, ensure_ascii=False, sort_keys=False)
        parsed_str = json_str.replace(""'"", r""\'"")

        # walk over the string and replace curly brackets with square brackets
        # outside of strings, as well as replace double quotes with single quotes
        # and ""deescape"" double quotes inside of strings
        outside_str = True
        escaped = False
        remove_indices = []
        for i, c in enumerate(parsed_str):
            if escaped:
                # previous character was an ""odd"" backslash
                escaped = False
                if c == '""':
                    # we want to ""deescape"" double quotes: store indices to delete
                    remove_indices.insert(0, i - 1)
            elif c == ""\\"":
                escaped = True
            elif c == '""':
                outside_str = not outside_str
                parsed_str = parsed_str[:i] + ""'"" + parsed_str[i + 1 :]
            elif c == ""{"" and outside_str:
                parsed_str = parsed_str[:i] + ""["" + parsed_str[i + 1 :]
            elif c == ""}"" and outside_str:
                parsed_str = parsed_str[:i] + ""]"" + parsed_str[i + 1 :]
        for idx in remove_indices:
            parsed_str = parsed_str[:idx] + parsed_str[idx + 1 :]
        return parsed_str",Dump supported Python object as Gremlin valueMap,???Convert JSON-like data to a custom string format with specific character replacements.???
1792,detach_from_model,"def detach_from_model(self, model: Model | None = None):
        
        if model is None:
            model = self.model
        model_num_layers = len(model.get_cache_layers())
        assert model_num_layers == self.num_layers, \
            f""Cannot detach cache with {self.num_layers} layers from model with {model_num_layers()} layers.""
        for layer, module in zip(self.layers, model.get_cache_layers()):
            module.cache_layers.remove(layer)",Detach cache from model.,???Detach cache layers from a model ensuring layer count consistency.???
1793,embed_batch,"def embed_batch(self, texts: List[str]) -> List[""np.ndarray""]:
        
        if not texts:
            return []

        all_embeddings = []

        # Process in batches
        for i in range(0, len(texts), self._batch_size):
            batch = texts[i : i + self._batch_size]
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=batch,
                )
                # Sort embeddings by index as OpenAI might return them in different order
                sorted_embeddings = sorted(response.data, key=lambda x: x.index)
                embeddings = [
                    np.array(e.embedding, dtype=np.float32) for e in sorted_embeddings
                ]
                all_embeddings.extend(embeddings)

            except Exception as e:
                # If the batch fails, try one by one
                if len(batch) > 1:
                    warnings.warn(
                        f""Batch embedding failed: {str(e)}. Trying one by one.""
                    )
                    individual_embeddings = [self.embed(text) for text in batch]
                    all_embeddings.extend(individual_embeddings)
                else:
                    raise e

        return all_embeddings",Get embeddings for multiple texts using batched API calls.,???Batch process text embeddings with error handling for individual retries.???
1794,check_imshow,"def check_imshow(warn=False):
    
    try:
        if LINUX:
            assert not IS_COLAB and not IS_KAGGLE
            assert ""DISPLAY"" in os.environ, ""The DISPLAY environment variable isn't set.""
        cv2.imshow(""test"", np.zeros((8, 8, 3), dtype=np.uint8))  # show a small 8-pixel image
        cv2.waitKey(1)
        cv2.destroyAllWindows()
        cv2.waitKey(1)
        return True
    except Exception as e:
        if warn:
            LOGGER.warning(f""WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n{e}"")
        return False",Check if environment supports image displays.,???Check if the environment supports displaying images using OpenCV's imshow function.???
1795,delete_iam_roles,"def delete_iam_roles(self):
        
        roles_to_delete = ['BedrockAgentRole', 'BedrockAgentLambdaRole']
        
        for role_name in roles_to_delete:
            try:
                # List and detach all policies attached to the role
                attached_policies = self.iam.list_attached_role_policies(RoleName=role_name)
                
                for policy in attached_policies.get('AttachedPolicies', []):
                    print(f""Detaching policy {policy['PolicyName']} from role {role_name}"")
                    self.iam.detach_role_policy(
                        RoleName=role_name,
                        PolicyArn=policy['PolicyArn']
                    )
                
                # Delete inline policies
                inline_policies = self.iam.list_role_policies(RoleName=role_name)
                
                for policy_name in inline_policies.get('PolicyNames', []):
                    print(f""Deleting inline policy {policy_name} from role {role_name}"")
                    self.iam.delete_role_policy(
                        RoleName=role_name,
                        PolicyName=policy_name
                    )
                
                # Delete the role
                print(f""Deleting IAM role: {role_name}"")
                self.iam.delete_role(RoleName=role_name)
                
            except Exception as e:
                print(f""Error deleting IAM role {role_name}: {str(e)}"")",Delete IAM roles created for the Bedrock agents and Lambda functions,???Remove specified IAM roles by detaching policies and deleting roles.???
1796,handle_validation_exception,"def handle_validation_exception(error):
    

    logger.exception(error)

    resp = Response()
    resp.response = json.dumps(
        {""status"": ""error"", ""message"": str(error), ""detail"": error.errors()}
    )
    resp.status = 400
    resp.mimetype = ""application/json""
    return resp",Handles all Pydantic validation errors.,"???Handle and log validation errors, returning a JSON error response.???"
1797,plot_predictions,"def plot_predictions(self, batch, preds, ni):
        
        plot_images(
            batch[""img""],
            *output_to_target(preds[0], max_det=15),  # not set to self.args.max_det due to slow plotting speed
            torch.cat(self.plot_masks, dim=0) if len(self.plot_masks) else self.plot_masks,
            paths=batch[""im_file""],
            fname=self.save_dir / f""val_batch{ni}_pred.jpg"",
            names=self.names,
            on_plot=self.on_plot,
        )  # pred
        self.plot_masks.clear()",Plots batch predictions with masks and bounding boxes.,???Visualize model predictions on image batch with optional masks and save output.???
1798,log_retry_attempt,"def log_retry_attempt(retry_state: RetryCallState) -> None:
    
    exception = retry_state.outcome.exception() if retry_state.outcome and retry_state.outcome.failed else None
    exception_str = str(exception) if exception else ""Unknown error""
    logger.warning(f""Network error, retrying ({retry_state.attempt_number}/3): {exception_str}"")",Log retry attempts with exception details if available.,???Log network retry attempts with exception details for debugging???
1799,init_app,"def init_app(app: FastAPI) -> None:
    
    logger = logging.getLogger(__name__)
    logger.info(""EE.ROUTERS.INIT_APP: Initializing enterprise routers..."")

    # Discover routers lazily – import sub-modules that register a global
    # ``router`` attribute.  Keep the list here explicit to avoid accidental
    # exposure of unfinished modules.
    for module_path in [
        ""ee.routers.cloud_uri"",
        ""ee.routers.apps"",
        ""ee.routers.connectors_router"",
    ]:
        try:
            mod = import_module(module_path)

            if hasattr(mod, ""router""):
                app.include_router(mod.router)
            else:
                logger.warning(f""EE.ROUTERS.INIT_APP: Module {module_path} does not have a 'router' attribute."")
        except ImportError as e:
            logger.error(f""EE.ROUTERS.INIT_APP: Failed to import {module_path}: {e}"", exc_info=True)
        except Exception as e:
            logger.error(f""EE.ROUTERS.INIT_APP: Unexpected error processing {module_path}: {e}"", exc_info=True)
            # Potentially re-raise or handle if a critical router fails, or decide to continue
            # For now, just log and continue to see if other routers load.
    logger.info(""EE.ROUTERS.INIT_APP: Finished initializing enterprise routers."")",Mount all enterprise routers onto the given *app* instance.,???Initialize and configure enterprise routers in a FastAPI application with error handling.???
1800,replay,"def replay():
    
    try:
        HowardsagentCrew().crew().replay(task_id=sys.argv[1])

    except Exception as e:
        raise Exception(f""An error occurred while replaying the crew: {e}"")",Replay the crew execution from a specific task.,"???  
Executes a task replay using a crew agent, handling exceptions.  
???"
1801,to_langchain,"def to_langchain(self) -> ChatOpenAI:
        

        kwargs = self.kwargs.copy()  # Make a copy to avoid modifying the original
        if self.json:
            kwargs[""response_format""] = {""type"": ""json_object""}

        return ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature or 0.5,
            streaming=self.streaming,
            max_tokens=self.max_tokens,
            top_p=self.top_p,
            model_kwargs=kwargs,
        )",Convert the language model to a LangChain chat model.,???Convert configuration to ChatOpenAI instance with optional JSON response.???
1802,sample_pricing_data_web,"def sample_pricing_data_web() -> Dict[str, Any]:
    
    return {
        'status': 'success',
        'service_name': 'lambda',
        'data': ,
        'message': 'Retrieved pricing for lambda from AWS Pricing url',
    }",Sample pricing data from web scraping.,???Function returns a dictionary with AWS Lambda pricing retrieval status.???
1803,_extract_boxed_answer,"def _extract_boxed_answer(self, text: str) -> Optional[str]:
        
        boxed_match = re.search(r""\\boxed{([^}]*)}"", text)
        if boxed_match:
            return boxed_match.group(1)
        return None",Extract answer from a LaTeX boxed expression.,???Extracts content within a boxed notation from a given text string.???
1804,_check_response,"def _check_response(
        self,
        response: httpx.Response,
        operation_name: str,
    ) -> dict[str, Any]:
        
        parsed = response.json()
        response.raise_for_status()
        if not parsed.get(""ok"", False):
            error_msg = f""Slack client failed to {operation_name}""
            error = parsed.get(""error"", ""Unknown error"")

            raise InternalError(error_msg, extra={""error_msg"": error})

        return parsed","Check if Slack API response has ok=True, log and possibly raise error if not",???Validate HTTP response and handle Slack API errors???
1805,run_skin_lesion_agent,"def run_skin_lesion_agent(state: AgentState) -> AgentState:
        

        current_input = state[""current_input""]
        image_path = current_input.get(""image"", None)

        print(f""Selected agent: SKIN_LESION_AGENT"")

        # classify chest x-ray into covid or normal
        predicted_mask = AgentConfig.image_analyzer.segment_skin_lesion(image_path)

        if predicted_mask:
            response = AIMessage(content=""Following is the analyzed **segmented** output of the uploaded skin lesion image:"")
        else:
            response = AIMessage(content=""The uploaded image is not clear enough to make a diagnosis / the image is not a medical image."")

        # response = AIMessage(content=""This would be handled by the skin lesion agent, analyzing the skin image."")

        return {
            **state,
            ""output"": response,
            ""needs_human_validation"": True,  # Medical diagnosis always needs validation
            ""agent_name"": ""SKIN_LESION_AGENT""
        }",Handle skin lesion image analysis.,"???Analyze skin lesion images, providing segmented results and requiring human validation.???"
1806,list_globals_filter,"def list_globals_filter(
    offset: Annotated[int, ""Offset to start listing from (start at 0)""],
    count: Annotated[int, ""Number of globals to list (100 is a good default, 0 means remainder)""],
    filter: Annotated[str, ""Filter to apply to the list (required parameter, empty string for no filter). Case-insensitive contains or /regex/ syntax""],
) -> Page[Global]:
    
    globals = []
    for addr, name in idautils.Names():
        # Skip functions
        if not idaapi.get_func(addr):
            globals.append({
                ""address"": hex(addr),
                ""name"": name,
            })
    globals = pattern_filter(globals, filter, ""name"")
    return paginate(globals, offset, count)","List matching globals in the database (paginated, filtered)",???Filter and paginate global variables based on user-defined criteria.???
1807,get_conversation_history,"def get_conversation_history(self) -> str:
        
        return ""\n"".join(f""{m['role']}: {m['content']}"" for m in st.session_state.messages)",Get the entire conversation history as a single string.,???Retrieve formatted conversation history from session state messages.???
1808,generate_random_email,"def generate_random_email():
    
    random_string = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))
    return f""cursor_{random_string}@cursor.ai""",Generate a random Cursor email address,???Generate a random email address with a unique identifier prefix.???
1809,calculate_cerebras_cost,"def calculate_cerebras_cost(input_tokens: int, output_tokens: int, model: str) -> float:
    
    total = 0.0

    if model in CEREBRAS_PRICING_1K:
        input_cost_per_k, output_cost_per_k = CEREBRAS_PRICING_1K[model]
        input_cost = math.ceil((input_tokens / 1000) * input_cost_per_k * 1e6) / 1e6
        output_cost = math.ceil((output_tokens / 1000) * output_cost_per_k * 1e6) / 1e6
        total = math.ceil((input_cost + output_cost) * 1e6) / 1e6
    else:
        warnings.warn(f""Cost calculation not available for model {model}"", UserWarning)

    return total",Calculate the cost of the completion using the Cerebras pricing.,???Calculate total cost for Cerebras model based on input and output tokens.???
1810,load_megatron_model_weights,"def load_megatron_model_weights(config, model_config, parallel_model, params_dtype, is_value_model=False, local_cache_path=""~/.cache/verl/rlhf""):
    
    architectures, model, state_dict, is_value_model = _load_hf_model(config, model_config, is_value_model, local_cache_path)

    from verl.models.weight_loader_registry import get_weight_loader

    print(f""before weight loader: architectures = {architectures}..."")
    for arch in architectures:
        print(f""call weight loader arch = {arch}, model config = {model.config}"")
        weight_loader = get_weight_loader(arch)
        weight_loader(
            state_dict=state_dict,
            wrapped_models=parallel_model,
            config=model.config,
            params_dtype=params_dtype,
            is_value_model=is_value_model,
            tie_word_embeddings=model_config.tie_word_embeddings,
        )
    return model.config",Load weights for verl customized model.,???Load and apply model weights to a parallel architecture using specified configurations.???
1811,print_attestation_analysis,"def print_attestation_analysis(analysis: dict) -> None:
    
    
    print(""\n"" + ""=""*60)
    print(""AZURE ATTESTATION VERIFICATION SUMMARY"")
    print(""=""*60)
    
    print(f""Platform Type: {analysis['platform_type']}"")
    print(f""Secure Boot: {'✓' if analysis['secure_boot_enabled'] else '✗'}"")
    print(f""Debug Disabled: {'✓' if analysis['debug_disabled'] else '✗'}"")
    print(f""VM ID: {analysis['vm_id']}"")
    print(f""Launch Measurement: {analysis['confidential_computing']['launch_measurement']}"")
    print(f""Report Data: {analysis['confidential_computing']['report_data']}"")

    if analysis['confidential_computing']['enabled']:
        cc = analysis['confidential_computing']
        print(f""\nCONFIDENTIAL COMPUTING:"")
        print(f""  Type: {cc['tee_type']}"")
        print(f""  Compliance: {cc['compliance_status']}"")
        print(f""  Guest SVN: {cc['guest_svn']}"")
        print(f""  Debuggable: {'✗' if not cc['is_debuggable'] else '✓'}"")
        
        if 'hardware_keys' in analysis:
            print(f""\nHARDWARE KEYS:"")
            for key in analysis['hardware_keys']:
                print(f""  - {key['kid']}: {key['key_type']} ({', '.join(key['operations'])})"")
    
    print(f""\nOS INFO:"")
    os_info = analysis['os_info']
    print(f""  {os_info['type']} - {os_info['distro']} {os_info['major_version']}.{os_info['minor_version']}"")
    
    print(""✅ CPU claim has been verified"")",Print the attestation analysis.,???Display Azure attestation analysis summary with security and compliance details.???
1812,add_transition,"def add_transition(
        self,
        from_node: str,
        to_node: Union[str, List[Union[str, BranchCondition]]],
        condition: Optional[str] = None,
        strict: bool = True,
    ) -> None:
        
        if strict:
            if from_node not in self.workflow.nodes:
                raise ValueError(f""Source node '{from_node}' does not exist"")
            if isinstance(to_node, str):
                if to_node not in self.workflow.nodes:
                    raise ValueError(f""Target node '{to_node}' does not exist"")
            else:
                for t in to_node:
                    target = t if isinstance(t, str) else t.to_node
                    if target not in self.workflow.nodes:
                        raise ValueError(f""Target node '{target}' does not exist"")
        transition = TransitionDefinition(
            from_node=from_node,
            to_node=to_node,
            condition=condition
        )
        self.workflow.workflow.transitions.append(transition)","Add a transition between nodes, supporting branching.","???Add a transition between nodes in a workflow, validating node existence if strict.???"
1813,mock_bedrock_runtime_client,"def mock_bedrock_runtime_client() -> MagicMock:
    
    mock_client = MagicMock()

    # Mock the invoke_model method
    mock_response = {'body': MagicMock()}
    mock_response['body'].read.return_value = json.dumps(
        {
            'images': [
                base64.b64encode(b'mock_image_data_1').decode('utf-8'),
                base64.b64encode(b'mock_image_data_2').decode('utf-8'),
            ]
        }
    ).encode('utf-8')

    mock_client.invoke_model.return_value = mock_response
    return mock_client",Create a mock Bedrock runtime client for testing.,"??? 
Create a mock client simulating image data response for testing a model invocation. 
???"
1814,_initial_inputs_to_variables,"def _initial_inputs_to_variables(self, initial_inputs: dict[str, str]) -> dict[str, Variable]:
        
        variables = {}
        initial_nodes = self.graph.find_initial_nodes()
        for initial_node in initial_nodes:
            for key, value in initial_inputs.items():
                for input in self.graph.get_node(initial_node).inputs:
                    if input.name == key:
                        initial_input_variable = Variable(
                            value,
                            requires_grad=False,
                            role_description=input.description,
                        )
                        variables[key] = initial_input_variable
                        if len(variables) == len(initial_inputs):
                            return variables
        missing_inputs = set(initial_inputs.keys()) - set(variables.keys())
        raise ValueError(f""Initial inputs do not match the inputs of the initial nodes. Missing inputs: {missing_inputs}"")",Converts inputs to the initial nodes to textgrad variables.,???Convert initial input strings to Variable objects for graph nodes???
1815,find_marker_genes,"def find_marker_genes(adata):
    
    print(""Finding marker genes..."")
    sc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon')
    
    # Extract top 10 marker genes for each cluster
    marker_genes = {}
    for i in range(len(adata.obs['leiden'].unique())):
        cluster_id = str(i)
        markers = [gene for gene in adata.uns['rank_genes_groups']['names'][cluster_id][:20]]
        marker_genes[cluster_id] = markers
        print(f""Cluster {cluster_id} markers: {', '.join(markers[:3])}..."")
    
    return marker_genes",Find marker genes for each cluster,???Identify top marker genes for each cluster in dataset.???
1816,delete_parent_sync_and_connection,"def delete_parent_sync_and_connection(mapper, connection, target):
    
    # Delete parent Sync if it exists
    if target.sync_id:
        # Get the session
        session = Session.object_session(target)
        if session:
            # If we're in a session, use the session to delete the Sync
            from airweave.models.sync import Sync

            sync = session.get(Sync, target.sync_id)
            if sync:
                session.delete(sync)
        else:
            # If we're not in a session, use the connection directly
            connection.execute(f""DELETE FROM sync WHERE id = '{target.sync_id}'"")

    sleep(0.2)

    # Delete related Connection if it exists
    if target.connection_id:
        session = Session.object_session(target)
        if session:
            from airweave.models.connection import Connection

            related_connection = session.get(Connection, target.connection_id)
            if related_connection:
                session.delete(related_connection)
        else:
            connection.execute(f""DELETE FROM connection WHERE id = '{target.connection_id}'"")","When a SourceConnection is deleted, also delete its parent Sync and Connection.",???Remove associated Sync and Connection records for a given target entity???
1817,task_copy_block_to_dots,"def task_copy_block_to_dots(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    block_size = 3 if rng.random() < 0.5 else 5
    if block_size >= size:
        return None

    color = rng.randint(1, 9)
    block = [color] * block_size

    # Generate dots with minimum distance to prevent overlap
    min_gap = block_size
    dot_positions = []

    while pos <= size - block_size:
        if rng.random() < 0.5:  # Control dot density
            dot_positions.append(pos)
            pos += min_gap
        pos += 1

    if not dot_positions:
        return None

    question = gen_field(size)
    question = write_block(0, block, question)
    for pos in dot_positions:
        question[pos] = color

    answer = gen_field(size)
    answer = write_block(0, block, answer)
    for pos in dot_positions:
        answer = write_block(block_start, block, answer)

    return {""input"": question, ""output"": answer}",Generate a task where a block pattern is copied to dot positions.,???Generate a block and dot pattern with random attributes and return input-output mapping.???
1818,type_with_custom_actions,"def type_with_custom_actions(custom_actions: type[ActionModel]) -> type[AgentOutput]:
		
		model_ = create_model(
			'AgentOutput',
			__base__=AgentOutput,
			action=(
				list[custom_actions],
				Field(..., description='List of actions to execute', json_schema_extra={'min_items': 1}),
			),
			__module__=AgentOutput.__module__,
		)
		model_.__doc__ = 'AgentOutput model with custom actions'
		return model_",Extend actions with custom actions,???Create a dynamic model for agent output incorporating custom action types.???
1819,_monitor_memory_usage,"def _monitor_memory_usage(self):
        
        while not self.stop_event.is_set():
            try:
                current_process = self.psutil.Process()
                # Get memory usage of current process and all children
                total_memory = current_process.memory_info().rss
                for child in current_process.children(recursive=True):
                    try:
                        total_memory += child.memory_info().rss
                    except (self.psutil.NoSuchProcess, self.psutil.AccessDenied):
                        pass

                # Convert to MB for better readability
                total_memory_mb = total_memory / (1024 * 1024)
                if total_memory_mb > self.peak_memory_usage:
                    self.peak_memory_usage = total_memory_mb
            except Exception as e:
                logger.warning(f""Error monitoring memory: {e}"")

            time.sleep(self.interval)",Background thread that periodically checks memory usage.,???Continuously track and update peak memory usage of a process and its children.???
1820,build_text_encoder,"def build_text_encoder(self):
        
        encoder_name = self.text_encoder_name
        if encoder_name != ""vit_l14"":
            raise ValueError(f""Not implemented: {encoder_name}"")
        text_encoder = clip_text_l14(
            pretrained=self.text_encoder_pretrained,
            embed_dim=self.text_encoder_d_model,
            context_length=self.max_txt_l,
            vocab_size=self.text_encoder_vocab_size,
            checkpoint_num=0,
        )

        return text_encoder",build text_encoder and possiblly video-to-text multimodal fusion encoder.,???Initialize a text encoder using a specific model configuration and parameters.???
1821,get_verifiers,"def get_verifiers(verifiers) -> dict:
    
    prefix = ""verify_""
    return {strip_prefix(n, prefix): getattr(verifiers, n) for n in dir(verifiers) if n.startswith(prefix)}",returns mapper from task identifiers (keys) to example verifier functions,"??? 
Extracts and maps verification methods from an object to a dictionary without prefixes. 
???"
1822,_run_ctf,"def _run_ctf(ctf, command, stdout=False, timeout=100, workspace_dir=None, stream=False):
    
    target_dir = workspace_dir or _get_workspace_dir()
    full_command = f""{command}""
    original_cmd_for_msg = command # For logging
    context_msg = f""(ctf:{target_dir})""
    try:
        output = ctf.get_shell(full_command, timeout=timeout)
        # In streaming mode, don't print to stdout to avoid duplication
        # The streaming system will handle the display
        if stdout and not stream:
            print(f""\033[32m{context_msg} $ {original_cmd_for_msg}\n{output}\033[0m"") # noqa E501
        return output
    except Exception as e:  # pylint: disable=broad-except
        error_msg = f""Error executing CTF command '{original_cmd_for_msg}' in '{target_dir}': {e}"" # noqa E501
        print(color(error_msg, fg=""red""))
        return error_msg","Runs command in CTF env, changing to workspace_dir first.",???Execute a command in a CTF environment with optional output handling and error reporting.???
1823,send_obj,"def send_obj(self, obj: Any, dst: int):
        
        self.expire_data()
        key = f""send_to/{dst}/{self.send_dst_counter[dst]}""
        self.store.set(key, pickle.dumps(obj))
        self.send_dst_counter[dst] += 1
        self.entries.append((key, time.perf_counter()))",Send an object to a destination rank.,??? Serialize and store an object for transmission to a specified destination. ???
1824,_build_system_prompt,"def _build_system_prompt(self) -> str:
        
        template_name = ""system_prompt.j2""
        try:
            tpl = self.jinja_env.get_template(template_name)
            context = {
                'name': self.name,
                'personality': self.personality
            }
            return tpl.render(**context)
        except TemplateNotFound:
            logger.error(f""System prompt template '{template_name}' not found."")
            return """"
        except Exception as e:
            logger.error(f""Error rendering system prompt: {e}"")
            return """"",Render system prompt via Jinja2 template only.,???Generate a system prompt using a Jinja2 template with error handling for missing templates.???
1825,_get_default_logging_level,"def _get_default_logging_level():
    
    env_level_str = os.getenv(""M4_VERBOSITY"", None)
    if env_level_str:
        if env_level_str in log_levels:
            return log_levels[env_level_str]
        else:
            logging.getLogger().warning(
                f""Unknown option M4_VERBOSITY={env_level_str}, has to be one of: { ', '.join(log_levels.keys()) }""
            )
    return _default_log_level",If M4_VERBOSITY env var is set to one of the valid choices return that as the new default level.,???Determine logging level from environment variable or use default???
1826,remove_blocks,"def remove_blocks(self, text: str) -> str:
        
        tag = f'```'
        lines = text.split('\n')
        post_lines = []
        in_block = False
        block_idx = 0
        for line in lines:
            if tag in line and not in_block:
                in_block = True
                continue
            if not in_block:
                post_lines.append(line)
            if tag in line:
                in_block = False
                post_lines.append(f""block:{block_idx}"")
                block_idx += 1
        return ""\n"".join(post_lines)",Remove all code/query blocks within a tag from the answer text.,"???Remove code blocks from text, replacing them with block identifiers.???"
1827,setup_git_provider,"def setup_git_provider(non_interactive: bool = False) -> str:
    
    if non_interactive:
        return ""github""  # Default to GitHub in non-interactive mode

    console.print(""\n> Git Provider Configuration"", style=""bold blue"")
    providers = [
        (""github"", ""GitHub""),
        # Add more providers here in the future
        # (""gitlab"", ""GitLab (Coming soon)""),
        # (""bitbucket"", ""Bitbucket (Coming soon)""),
    ]

    console.print(""Available Git providers:"")
    for i, (id, name) in enumerate(providers, 1):
        if id == ""github"":
            console.print(f""{i}. {name}"")
        else:
            console.print(f""{i}. {name}"", style=""dim"")

    choice = IntPrompt.ask(""Select your Git provider"", default=1)

    git_provider = providers[choice - 1][0]
    if git_provider != ""github"":
        console.print(""⚠️  Only GitHub is currently supported."", style=""bold yellow"")
        raise ValueError(""Unsupported git provider"")

    return git_provider",Interactive selection of git provider.,"???Configure and select a Git provider, defaulting to GitHub if non-interactive.???"
1828,shape,"def shape(self) -> dict[str, int]:
        
        return {name: size for name, size in zip(self._names, self._layout.shape)}",Returns the shape of the rank layout.,???Returns a dictionary mapping names to their corresponding sizes.???
1829,sample_entities,"def sample_entities():
    
    return [
        {
            ""type"": ""entity"",
            ""name"": ""test_entity"",
            ""entityType"": ""test"",
            ""observations"": [""Test observation 1"", ""Test observation 2""],
        },
        {
            ""type"": ""relation"",
            ""from"": ""test_entity"",
            ""to"": ""related_entity"",
            ""relationType"": ""test_relation"",
        },
    ]",Sample entities for testing.,???Generate sample data with entities and their relationships.???
1830,create_model_from_schema,"def create_model_from_schema(json_schema: Dict[str, Any]) -> Type[BaseModel]:
    
    model_name = json_schema.get(""title"", ""DynamicModel"")
    properties = json_schema.get(""properties"", {})
    required = json_schema.get(""required"", [])

    field_definitions = {}

    for field_name, field_schema in properties.items():
        # Get field type
        field_type = str  # Default to string
        schema_type = field_schema.get(""type"")

        if schema_type == ""integer"":
            field_type = int
        elif schema_type == ""number"":
            field_type = float
        elif schema_type == ""boolean"":
            field_type = bool
        elif schema_type == ""array"":
            field_type = List[Any]
        elif schema_type == ""object"":
            field_type = Dict[str, Any]

        # Handle optional fields
        if field_name not in required:
            field_type = Optional[field_type]

        # Create field with basic info
        field_info = {}
        if ""description"" in field_schema:
            field_info[""description""] = field_schema[""description""]

        field_definitions[field_name] = (field_type, Field(**field_info))

    return create_model(model_name, **field_definitions)",Create a Pydantic model from a JSON schema,???Generate a dynamic Pydantic model from a JSON schema definition.???
1831,get_project_config,"def get_project_config(project_name: Optional[str] = None) -> ProjectConfig:
    

    actual_project_name = None

    # load the config from file
    global app_config
    app_config = config_manager.load_config()

    # Get project name from environment variable
    os_project_name = os.environ.get(""BASIC_MEMORY_PROJECT"", None)
    if os_project_name:  # pragma: no cover
        logger.warning(
            f""BASIC_MEMORY_PROJECT is not supported anymore. Use the --project flag or set the default project in the config instead. Setting default project to {os_project_name}""
        )
        actual_project_name = project_name
    # if the project_name is passed in, use it
    elif not project_name:
        # use default
        actual_project_name = app_config.default_project
    else:  # pragma: no cover
        actual_project_name = project_name

    # the config contains a dict[str,str] of project names and absolute paths
    assert actual_project_name is not None, ""actual_project_name cannot be None""

    project_path = app_config.projects.get(actual_project_name)
    if not project_path:  # pragma: no cover
        raise ValueError(f""Project '{actual_project_name}' not found"")

    return ProjectConfig(name=actual_project_name, home=Path(project_path))",Get the project configuration for the current session.,???Retrieve project configuration using environment variables and default settings.???
1832,add_user_message,"def add_user_message(self, content: str, interrupt_after: bool = False):
        
        self.messages.append({
            'role': 'user',
            'content': content,
            'interrupt_after': interrupt_after
        })",Add a user message to the simulation.,???Add a user message to the message list with optional interruption flag???
1833,anthropic_fn,"def anthropic_fn(self, history, verbose=False):
        
        from anthropic import Anthropic

        client = Anthropic(api_key=self.api_key)
        system_message = None
        messages = []
        for message in history:
            clean_message = {'role': message['role'], 'content': message['content']}
            if message['role'] == 'system':
                system_message = message['content']
            else:
                messages.append(clean_message)

        try:
            response = client.messages.create(
                model=self.model,
                max_tokens=1024,
                messages=messages,
                system=system_message
            )
            if response is None:
                raise Exception(""Anthropic response is empty."")
            thought = response.content[0].text
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""Anthropic API error: {str(e)}"") from e",Use Anthropic to generate text.,???Process chat history to generate a response using the Anthropic API.???
1834,format_memories_for_prompt,"def format_memories_for_prompt(self, memories: List[str]) -> str:
        
        if not memories:
            return """"
        return ""\n"".join(f""- {memory}"" for memory in memories)",Format retrieved memories as bullet points.,???Formats a list of memories into a bullet-point string for prompts.???
1835,autocast_list,"def autocast_list(source):
    
    files = []
    for im in source:
        if isinstance(im, (str, Path)):  # filename or uri
            files.append(Image.open(requests.get(im, stream=True).raw if str(im).startswith(""http"") else im))
        elif isinstance(im, (Image.Image, np.ndarray)):  # PIL or np Image
            files.append(im)
        else:
            raise TypeError(
                f""type {type(im).__name__} is not a supported Ultralytics prediction source type. \n""
            )

    return files",Merges a list of sources into a list of numpy arrays or PIL images for Ultralytics prediction.,???Convert various image sources into a list of image objects for processing.???
1836,mkdir_p,"def mkdir_p(path: str, fs: Optional[AbstractFileSystem] = None) -> None:
    
    if is_glob(path):
        raise ValueError(f""Cannot create directory with glob pattern: {path}"")

    fs = fs or get_fs(path)
    fs.makedirs(path, exist_ok=True)",Create a directory if it does not exist.,"???Create directory path, handling glob patterns and filesystem abstraction.???"
1837,get_domain_cert,"def get_domain_cert(self, domain):
        
        cert_path = self.cert_dir / f""{domain}.crt""
        key_path = self.cert_dir / f""{domain}.key""
        
        if cert_path.exists() and key_path.exists():
            # Load existing certificate and key
            with open(key_path, 'rb') as f:
                private_key = serialization.load_pem_private_key(
                    f.read(),
                    password=None,
                    backend=default_backend()
                )
            
            with open(cert_path, 'rb') as f:
                cert = x509.load_pem_x509_certificate(
                    f.read(),
                    default_backend()
                )
            
            return private_key, cert
        
        # Generate new certificate
        return self._generate_domain_cert(domain)",Get or generate a certificate for the specified domain,???Retrieve or generate SSL certificates for a specified domain.???
1838,_setup_connector,"def _setup_connector(self):
        
        if not self.proxy_url:
            self.connector = TCPConnector()
            return

        # Parse the proxy URL
        parsed = urllib.parse.urlparse(self.proxy_url)
        proxy_type = parsed.scheme.lower()

        if proxy_type in ('http', 'https', 'socks4', 'socks5'):
            self.connector = ""SocksConnector""
        else:
            raise ValueError(f""Unsupported proxy type: {proxy_type}"")",Set up the appropriate connector based on the proxy URL,??? Initialize network connector based on proxy URL scheme or default to TCPConnector. ???
1839,add,"def add(a: int, b: int) -> int:  # Changed return type hint to int
    
    logger.info(f""Executing add tool with a={a}, b={b}"")
    return a + b",Add two numbers and return the result,"???  
Logs and returns the sum of two integers.  
???"
1840,add,"def add(self, name, seed):
        
        # Check seed is not already used.
        if seed in self.seeds_:
            raise Exception(""seed {} already exists"".format(seed))
        self.seeds_.add(seed)
        # Check that state is not already defined.
        if name in self.states_:
            raise Exception(""cuda rng state {} already exists"".format(name))
        # Get the current rng state.
        orig_rng_state = torch.cuda.get_rng_state()
        # Set the new state and store it.
        torch.cuda.manual_seed(seed)
        self.states_[name] = torch.cuda.get_rng_state()
        # Reset rng state to what it was.
        _set_cuda_rng_state(orig_rng_state)",Track the rng state.,???Ensure unique random seed and state for CUDA operations???
1841,item_sub_page,"def item_sub_page(self, session_id, **kwargs):
        
        session = self.user_sessions[session_id]
        clickable_name = kwargs['clickable_name']
        for k in ACTION_TO_TEMPLATE:
            if clickable_name.lower() == k.lower():
                clickable_name = k
                break
        
        # Set fields + url of page, then render page's HTML
        product_info = self.product_item_dict[session[""asin""]]
        session[""actions""][clickable_name] += 1
        keywords_url_string = '+'.join(session[""keywords""])
        url = (
            f'{self.base_url}/item_sub_page/{session_id}/'
            f'{session[""asin""]}/{keywords_url_string}/{session[""page""]}/'
            f'{clickable_name}/{session[""options""]}'
        )
        html = map_action_to_html(
            f'click[{clickable_name}]',
            session_id=session_id,
            product_info=product_info,
            keywords=session[""keywords""],
            page=session[""page""],
            asin=session[""asin""],
            options=session[""options""],
            instruction_text=session[""goal""][""instruction_text""],
        )
        return html, url","Render and return the HTML for a product's sub page (i.e. description, features)",???Generate a product subpage URL and HTML based on user session actions.???
1842,load_scaled_pixbuf,"def load_scaled_pixbuf(notification_box, width, height):
    
    notification = notification_box.notification
    if not hasattr(notification_box, 'notification') or notification is None:
        logger.error(""load_scaled_pixbuf: notification_box.notification is None or not set!"")
        return None

    pixbuf = None
    if hasattr(notification_box, ""cached_image_path"") and notification_box.cached_image_path and os.path.exists(notification_box.cached_image_path):
        try:
            logger.debug(f""Attempting to load cached image from: {notification_box.cached_image_path} for notification {notification.id}"")
            pixbuf = GdkPixbuf.Pixbuf.new_from_file(notification_box.cached_image_path)
            if pixbuf:
                pixbuf = pixbuf.scale_simple(width, height, GdkPixbuf.InterpType.BILINEAR)
                logger.info(f""Successfully loaded cached image from: {notification_box.cached_image_path} for notification {notification.id}"")
            return pixbuf
        except Exception as e:
            logger.error(f""Error loading cached image from {notification_box.cached_image_path} for notification {notification.id}: {e}"")
            logger.warning(f""Falling back to notification.image_pixbuf for notification {notification.id}"")

    if notification.image_pixbuf:
        logger.debug(f""Loading image directly from notification.image_pixbuf for notification {notification.id}"")
        pixbuf = notification.image_pixbuf.scale_simple(width, height, GdkPixbuf.InterpType.BILINEAR)
        return pixbuf

    logger.debug(f""No image_pixbuf or cached image found, trying app icon for notification {notification.id}"")
    return get_app_icon_pixbuf(notification.app_icon, width, height)","Loads and scales a pixbuf for a notification_box, prioritizing cached images.",???Load and scale notification image or fallback to app icon if unavailable???
1843,generate_two_between,"def generate_two_between(puzzle: Puzzle, solution: dict[Literal, int]) -> set[Clue]:
    

    clues: set[Clue] = set()
    for left, right in zip(puzzle.houses, puzzle.houses[3:]):
        items_left = {item: loc for item, loc in solution.items() if loc == left}
        items_right = {item: loc for item, loc in solution.items() if loc == right}
        pairs: set[tuple[Literal, Literal]] = {(item1, item2) for item1, item2 in product(items_left, items_right)}
        for pair in pairs:
            clues.add(two_between(pair[0], pair[1], puzzle.houses))

    return clues",Generate the `two_between` Clue instances,???Generate clues for pairs of items with two houses between them in a puzzle.???
1844,get_libero_env,"def get_libero_env(task, resolution=256):
    
    task_description = task.language
    task_bddl_file = os.path.join(get_libero_path(""bddl_files""), task.problem_folder, task.bddl_file)
    env_args = {""bddl_file_name"": task_bddl_file, ""camera_heights"": resolution, ""camera_widths"": resolution}
    env = OffScreenRenderEnv(**env_args)
    env.seed(0)  # IMPORTANT: seed seems to affect object positions even when using fixed initial state
    return env, task_description","Initializes and returns the LIBERO environment, along with the task description.",???Create a rendering environment for a task with specified resolution.???
1845,generate_sample_table,"def generate_sample_table(md: List[str], samples: list) -> None:
    
    if not samples:
        md.append(""No records found.\n\n"")
        return

    headers = samples[0].keys()
    md.append(""| "" + "" | "".join(headers) + "" |\n"")
    md.append(""|"" + ""|"".join([""---""] * len(headers)) + ""|\n"")

    for row in samples:
        values = []
        for val in row.values():
            if isinstance(val, str) and len(val) > 50:
                values.append(f""{val[:47]}..."")
            else:
                values.append(str(val))
        md.append(""| "" + "" | "".join(values) + "" |\n"")
    md.append(""\n"")",Generate sample data table section.,???Generate a markdown table from sample data with truncated long strings.???
1846,ensure_divisibility,"def ensure_divisibility(numerator, denominator) -> None:
    
    assert numerator % denominator == 0, ""{} is not divisible by {}"".format(
        numerator, denominator)",Ensure that numerator is divisible by the denominator.,"???Ensure the numerator is divisible by the denominator, raising an error if not.???"
1847,rgba_to_hex,"def rgba_to_hex(self, rgba: Gdk.RGBA) -> str:
        
        r = int(rgba.red * 255)
        g = int(rgba.green * 255)
        b = int(rgba.blue * 255)
        return f""#{r:02X}{g:02X}{b:02X}""",Converts Gdk.RGBA to a HEX color string.,??? Convert RGBA color to hexadecimal string format ???
1848,update_flask_imports_and_init,"def update_flask_imports_and_init(file):
    
    print(f""🔍 Processing file: {file.filepath}"")

    # Update imports
    for imp in file.imports:
        if imp.name == ""Flask"":
            print(""  📦 Updating import: Flask -> FastAPI"")
            imp.set_name(""FastAPI"")
        elif imp.symbol_name == ""flask"":
            print(""  📦 Updating import module: flask -> fastapi"")
            imp.set_import_module(""fastapi"")

    # Update Flask initialization and remove __name__
    for call in file.function_calls:
        if call.name == ""Flask"":
            print(""  🔧 Updating function call: Flask -> FastAPI"")
            call.set_name(""FastAPI"")
            if len(call.args) > 0 and call.args[0].value == ""__name__"":
                print(""  🗑️ Removing __name__ argument from FastAPI initialization"")
                call.args[0].remove()",Update Flask imports and initialization to FastAPI,???Refactor Flask imports and initialization to FastAPI in a given file.???
1849,handle_user_input,"def handle_user_input(side_bar: SideBar) -> None:
    
    prompt = st.chat_input() or st.session_state.modified_prompt
    if prompt:
        st.session_state.modified_prompt = None
        parts = get_parts_from_files(
            upload_gcs_checkbox=st.session_state.checkbox_state,
            uploaded_files=side_bar.uploaded_files,
            gcs_uris=side_bar.gcs_uris,
        )
        st.session_state[""gcs_uris_to_be_sent""] = """"
        parts.append({""type"": ""text"", ""text"": prompt})
        st.session_state.user_chats[st.session_state[""session_id""]][""messages""].append(
            HumanMessage(content=parts).model_dump()
        )

        display_user_input(parts)
        generate_ai_response(
            remote_agent_engine_id=side_bar.remote_agent_engine_id,
            agent_callable_path=side_bar.agent_callable_path,
            url=side_bar.url_input_field,
            authenticate_request=side_bar.should_authenticate_request,
        )
        update_chat_title()
        if len(parts) > 1:
            st.session_state.uploader_key += 1
        st.rerun()","Process user input, generate AI response, and update chat history.","???Process user input, update session state, and generate AI response in a chat application.???"
1850,get_log_summary,"def get_log_summary(self, markdown=False):
        
        if self.response:
            if self.response.get('data_wrangler_function_path'):
                log_details = f
                if markdown:
                    return Markdown(log_details) 
                else:
                    return log_details","Logs a summary of the agent's operations, if logging is enabled.","???  
Generate a log summary with optional Markdown formatting based on response data.  
???"
1851,get_document_l0,"def get_document_l0(document_id: int):
    
    try:
        l0_data = document_service.get_document_l0(document_id)
        return jsonify(APIResponse.success(data=l0_data))
    except Exception as e:
        logger.error(f""Error getting document L0 data: {str(e)}"", exc_info=True)
        return jsonify(
            APIResponse.error(message=f""Error getting document L0 data: {str(e)}"")
        )",Get document L0 data including chunks and embeddings,"???Retrieve and return document data, handling errors gracefully.???"
1852,update_layout,"def update_layout(self):
        
        pos = nx.spring_layout(
            self.graph,
            dim=3,
            pos={
                node_id: list(node.position)
                for node_id, node in self.id_node_map.items()
            },
            k=2.0,
            iterations=100,
            weight=None,
        )

        # Update node positions
        for node_id, position in pos.items():
            self.id_node_map[node_id].position = glm.vec3(position)
        self.update_buffers()",Update the graph layout,???Update 3D graph node positions using a spring layout algorithm.???
1853,update_favicon,"def update_favicon(self, container, icon_widget, favicon_path):
        
        if not favicon_path or not os.path.exists(favicon_path):

            return
        
        try:

            self.favicon_temp_path = favicon_path
            

            if data.PANEL_THEME == ""Panel"" and data.BAR_POSITION in [""Left"", ""Right""]:

                pixbuf = GdkPixbuf.Pixbuf.new_from_file_at_scale(
                    favicon_path, width=36, height=36, preserve_aspect_ratio=True)
            else:
                pixbuf = GdkPixbuf.Pixbuf.new_from_file_at_scale(
                    favicon_path, width=48, height=48, preserve_aspect_ratio=True)
            

            container.remove(icon_widget)
            

            img = Gtk.Image.new_from_pixbuf(pixbuf)
            img.set_name(""pin-favicon"")
            container.pack_start(img, True, True, 0)
            

            container.show_all()
        except Exception as e:
            print(f""Error setting favicon: {e}"")",Update the icon with the downloaded favicon or keep the default.,???Update UI container with a scaled favicon image based on theme settings.???
1854,get_relevant_memories,"def get_relevant_memories(self, context: str) -> List[str]:
        
        memories = self.vector_store.search_memories(context, k=settings.MEMORY_TOP_K)
        if memories:
            for memory in memories:
                self.logger.debug(f""Memory: '{memory.text}' (score: {memory.score:.2f})"")
        return [memory.text for memory in memories]",Retrieve relevant memories based on the current context.,???Retrieve and log top relevant memories based on context input.???
1855,event_server,"def event_server(
    event_server_port: int, event_store: SimpleEventStore
) -> Generator[tuple[SimpleEventStore, str], None, None]:
    
    proc = multiprocessing.Process(
        target=run_server,
        kwargs={""port"": event_server_port, ""event_store"": event_store},
        daemon=True,
    )
    proc.start()

    # Wait for server to be running
    max_attempts = 20
    attempt = 0
    while attempt < max_attempts:
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.connect((""127.0.0.1"", event_server_port))
                break
        except ConnectionRefusedError:
            time.sleep(0.1)
            attempt += 1
    else:
        raise RuntimeError(f""Server failed to start after {max_attempts} attempts"")


    # Clean up
    proc.kill()
    proc.join(timeout=2)",Start a server with event store enabled.,"???Launches and monitors a background event server process, ensuring successful startup.???"
1856,extract_query,"def extract_query(response_text: str) -> str:
    
    try:
        # Find the last occurrence of <answer>...</answer>
        if ""<answer>"" not in response_text:
            response_text = ""<answer>"" + response_text
        if ""</answer>"" not in response_text:
            response_text = response_text + ""</answer>""
        answer_pattern = r'<answer>(.*?)</answer>'
        matches = re.findall(answer_pattern, response_text, re.DOTALL)
        
        if matches:
            # Get the last matched answer and parse it as JSON
            answer_json = json.loads(matches[-1].strip())
            return answer_json['query']
        else:
            raise ValueError(""No answer tags found in response"")
    except Exception as e:
        raise ValueError(f""Failed to extract query from response: {e}"")",Extract the rewritten query from the model's response.,???Extracts a query from a JSON-formatted answer within a text response.???
1857,_get_or_create_source,"def _get_or_create_source(self, source_name: str) -> DataSource:
        
        if source_name not in self.sources:
            # check if source_name is a valid file path
            if os.path.exists(source_name):
                if source_name.endswith("".csv""):
                    cfg = CSVConfig(path=source_name)
                    self.sources[source_name] = CSVSource(
                        source_name, cfg, self.duckdb_conn
                    )
                elif source_name.endswith("".json""):
                    cfg = JSONConfig(path=source_name)
                    self.sources[source_name] = JSONSource(
                        source_name, cfg, self.duckdb_conn
                    )
                elif source_name.endswith("".parquet""):
                    cfg = ParquetConfig(path=source_name)
                    self.sources[source_name] = ParquetSource(
                        source_name, cfg, self.duckdb_conn
                    )
                else:
                    raise ValueError(f""Unsupported file type: {source_name}"")
            else:
                raise ValueError(f""Unknown source: {source_name}"")

        return self.sources[source_name]",Get an existing source or create a new one from a file path.,???Create or retrieve data source object based on file type and path validity???
1858,patch_vlm_for_ulysses_input_slicing,"def patch_vlm_for_ulysses_input_slicing(model_class: type):
    

    def _create_ulysses_wrapped_decoder_forward(original_forward):
        def ulysses_wrapped_decoder_forward(self, *args, **kwargs):
            inputs_embeds = kwargs.get(""inputs_embeds"")
            call_kwargs = kwargs.copy()

            current_ulysses_sp_size = get_ulysses_sequence_parallel_world_size()

            slice_now = inputs_embeds is not None and current_ulysses_sp_size > 1 and getattr(self, ""_needs_initial_slice"", True)
            if slice_now:
                call_kwargs[""inputs_embeds""] = slice_input_tensor(inputs_embeds, dim=1, padding=False)
                self._needs_initial_slice = False
            try:
                return original_forward(self, *args, **call_kwargs)
            finally:
                if slice_now:
                    self._needs_initial_slice = True

        return ulysses_wrapped_decoder_forward

    original_forward = model_class.forward
    wrapped_forward = _create_ulysses_wrapped_decoder_forward(original_forward)
    model_class.forward = wrapped_forward
    print(f""Monkey patch {model_class.__name__}.forward for Ulysses SP input slicing."")",Applies a monkey patch to the forward method of a given model class to enable Ulysses sequence parallelism input slicing.,???Enhances model class to support Ulysses input slicing for parallel processing.???
1859,compose_prompt_params,"def compose_prompt_params(self,
                            config_conversation: Dict[str, Any],
                            image_file_paths: List[str] = [],
                            image_path_keys: List[str] = [],
                            input_texts: str = """") -> Dict[str, Any]:
        
        prompt_params = {
            ""input_text"": input_texts,
            ""conversation_style"": "", "".join(
                config_conversation.get(""conversation_style"", [])
            ),
            ""roles_person1"": config_conversation.get(""roles_person1""),
            ""roles_person2"": config_conversation.get(""roles_person2""),
            ""dialogue_structure"": "", "".join(
                config_conversation.get(""dialogue_structure"", [])
            ),
            ""podcast_name"": config_conversation.get(""podcast_name""),
            ""podcast_tagline"": config_conversation.get(""podcast_tagline""),
            ""output_language"": config_conversation.get(""output_language""),
            ""engagement_techniques"": "", "".join(
                config_conversation.get(""engagement_techniques"", [])
            ),
        }

        # Add image paths to parameters if any
        for key, path in zip(image_path_keys, image_file_paths):
            prompt_params[key] = path

        return prompt_params",Compose prompt parameters for standard content generation.,???Generate structured prompt parameters for conversation configuration and image paths.???
1860,_prepare_retrieve_docs_request,"def _prepare_retrieve_docs_request(
        self,
        query: str,
        filters: Optional[Dict[str, Any]],
        k: int,
        min_score: float,
        use_colpali: bool,
        folder_name: Optional[Union[str, List[str]]],
        end_user_id: Optional[str],
    ) -> Dict[str, Any]:
        
        request = {
            ""query"": query,
            ""filters"": filters,
            ""k"": k,
            ""min_score"": min_score,
            ""use_colpali"": use_colpali,
        }
        if folder_name:
            request[""folder_name""] = folder_name
        if end_user_id:
            request[""end_user_id""] = end_user_id
        return request",Prepare request for retrieve_docs endpoint,"???Constructs a document retrieval request with query, filters, and optional parameters.???"
1861,__delitem__,"def __delitem__(self, key: str) -> None:
        
        try:
            del self.data[key]
        except KeyError:
            raise KeyError(f""Cannot delete non-existent context variable '{key}'"")",Delete a key using dictionary syntax: del context[key],"???  
Safely remove a key from a dictionary, raising an error if it doesn't exist.  
???"
1862,agent_action,"def agent_action(self, input_list: List[str]) -> None:
        
        if self.agent is None:
            logger.info(""No agent is currently loaded. Use 'load-agent' to load an agent."")
            return

        if len(input_list) < 3:
            logger.info(""Please specify both a connection and an action."")
            logger.info(""Format: agent-action {connection} {action}"")
            return

        try:
            result = self.agent.perform_action(
                connection=input_list[1],
                action=input_list[2],
                params=input_list[3:]
            )
            logger.info(f""Result: {result}"")
        except Exception as e:
            logger.error(f""Error running action: {e}"")",Handle agent action command,???Check agent readiness and execute specified action with parameters???
1863,write_obj_file,"def write_obj_file(
    vertices: np.ndarray,
    faces: np.ndarray,
    normals: np.ndarray,
    texcoords: np.ndarray,
    scale: list,
    filepath: str,
    material_name: str,
) -> None:
    
    mtl_filename = os.path.splitext(os.path.basename(filepath))[0] + "".mtl""
    mtl_filepath = os.path.join(os.path.dirname(filepath), mtl_filename)
    texture_filename = ""texture_map.png""
    write_mtl_file(material_name, texture_filename, mtl_filepath)
    with open(filepath, ""w"") as f:
        f.write(f""mtllib {mtl_filename}\n"")
        for v in vertices:
            scaled_v = [v[0] * scale[0], v[1] * scale[1], v[2] * scale[2]]
            f.write(f""v {scaled_v[0]} {scaled_v[1]} {scaled_v[2]}\n"")
        for vt in texcoords:
            f.write(f""vt {vt[0]} {1.0 - vt[1]}\n"")
        for vn in normals:
            f.write(f""vn {vn[0]} {vn[1]} {vn[2]}\n"")
        f.write(f""usemtl {material_name}\n"")
        for face in faces:
            v1, v2, v3 = face + 1
            f.write(f""f {v1}/{v1}/{v1} {v2}/{v2}/{v2} {v3}/{v3}/{v3}\n"")",Write mesh data to OBJ file with proper UV mapping and material reference.,"???Generate a 3D model file with vertices, faces, and material properties.???"
1864,add_agent_to_automation,"def add_agent_to_automation(self, automation_name: str, agent_prompt: str, position: int = -1) -> list:
        
        if automation_name not in self.automations:
            raise ValueError(f""Automation '{automation_name}' does not exist"")
        
        new_agent = {
            ""prompt"": agent_prompt,
            ""max_steps"": 25,  # Default values
            ""max_actions"": 1
        }
        
        if position == -1 or position >= len(self.automations[automation_name][""agents""]):
            self.automations[automation_name][""agents""].append(new_agent)
        else:
            self.automations[automation_name][""agents""].insert(position, new_agent)
            
        return self.automations[automation_name][""agents""]",Add a new agent to an automation flow,???Add a new agent to a specified automation sequence at a given position.???
1865,track_indexed_db_for_storage_key,"def track_indexed_db_for_storage_key(
    storage_key: str,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""storageKey""] = storage_key
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Storage.trackIndexedDBForStorageKey"",
        ""params"": params,
    }
    json = yield cmd_dict",Registers storage key to be notified when an update occurs to its IndexedDB.,???Generate command to monitor IndexedDB activity for a specific storage key.???
1866,validate_region,"def validate_region(cls, v: str, info: ValidationInfo) -> str:
        
        # Get the project_ref from the values
        values = info.data
        project_ref = values.get(""supabase_project_ref"", """")

        # If this is a remote project and region is the default
        if not project_ref.startswith(""127.0.0.1"") and v == ""us-east-1"" and ""SUPABASE_REGION"" not in os.environ:
            logger.warning(
                ""You're connecting to a remote Supabase project but haven't specified a region. ""
                ""Using default 'us-east-1', which may cause 'Tenant or user not found' errors if incorrect. ""
                ""Please set the correct SUPABASE_REGION in your configuration.""
            )

        # Validate that the region is supported
        if v not in SUPPORTED_REGIONS.__args__:
            supported = ""\n  - "".join([""""] + list(SUPPORTED_REGIONS.__args__))
            raise ValueError(f""Region '{v}' is not supported. Supported regions are:{supported}"")
        return v",Validate that the region is supported by Supabase.,???Validate and warn about Supabase project region configuration.???
1867,super_simple_chunker,"def super_simple_chunker(text, chunk_size=1000):
    
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    return [{""text"": chunk} for chunk in chunks]",Chunks text into smaller segments of specified size.,???Splits text into specified-size chunks and returns them as dictionaries.???
1868,full_config_dict,"def full_config_dict(self) -> dict[str, dict[str, Any]]:
		
		return {
			'embedder': self.embedder_config_dict,
			'llm': self.llm_config_dict,
			'vector_store': self.vector_store_config_dict,
		}",Returns the complete configuration dictionary for Mem0.,"???  
Constructs a nested configuration dictionary for embedder, language model, and vector store.  
???"
1869,rename_session,"def rename_session(self, old_name, new_name):
        
        try:
            # Clean the session name (replace spaces with underscores)
            clean_name = new_name.strip().replace("" "", ""_"")
            
            # Rename session
            subprocess.run(
                [""tmux"", ""rename-session"", ""-t"", old_name, clean_name],
                check=True
            )
            
            # Refresh the session list
            self.refresh_sessions()
            
        except Exception as e:
            print(f""Error renaming tmux session: {e}"")",Rename a tmux session,"???Renames a tmux session and updates the session list, handling errors.???"
1870,is_track_in_playlist_db,"def is_track_in_playlist_db(playlist_spotify_id: str, track_spotify_id: str) -> bool:
    
    table_name = f""playlist_{playlist_spotify_id.replace('-', '_')}""
    try:
        with _get_playlists_db_connection() as conn:
            cursor = conn.cursor()
            # First, check if the table exists to prevent errors on non-watched or new playlists
            cursor.execute(
                f""SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}';""
            )
            if cursor.fetchone() is None:
                return False  # Table doesn't exist, so track cannot be in it

            cursor.execute(
                f""SELECT 1 FROM {table_name} WHERE spotify_track_id = ?"",
                (track_spotify_id,),
            )
            return cursor.fetchone() is not None
    except sqlite3.Error as e:
        logger.error(
            f""Error checking if track {track_spotify_id} is in playlist {playlist_spotify_id} DB: {e}"",
            exc_info=True,
        )
        return False",Checks if a specific track Spotify ID exists in the given playlist's tracks table.,???Check if a specific track exists in a playlist database table by querying its ID.???
1871,embed,"def embed(self, text: str) -> List[float]:
        
        text = text.replace(""\n"", "" "")
        response = requests.post(
            f""{self.base_url}/api/embed"",
            json={""model"": self.model_name, ""input"": [text]},
        )
        return response.json()[""embeddings""][0]",Embeds the content using Open AI embedding,??? Convert text to vector using API embedding service. ???
1872,tell,"def tell(self) -> int:
        
        if self.mode == ""rb"":
            return self._reader.tell()

        if self.mode == ""wb"":
            # There's no way to get the stream position from the underlying writer
            # because it's async. Here we happen to be using the async writer in a
            # synchronous way, so we keep our own stream position.
            assert self._writer_loc is not None
            return self._writer_loc

        raise ValueError(f""Unexpected mode {self.mode}"")",Get current file location.,???Determine the current position in a file stream based on the mode.???
1873,delete_bot_pod,"def delete_bot_pod(self, user_id, meeting_id=None):
        
        try:
            if meeting_id:
                pod_name = f""bot-{user_id}-{meeting_id}""
                self.core_v1.delete_namespaced_pod(
                    name=pod_name,
                    namespace=self.namespace
                )
                logger.info(f""Deleted pod {pod_name}"")
                return {""status"": ""deleted"", ""pod_name"": pod_name}
            else:
                # Delete all pods for user
                pod_list = self.core_v1.list_namespaced_pod(
                    namespace=self.namespace,
                    label_selector=f""app=bot,user-id={user_id}""
                )
                for pod in pod_list.items:
                    self.core_v1.delete_namespaced_pod(
                        name=pod.metadata.name,
                        namespace=self.namespace
                    )
                    logger.info(f""Deleted pod {pod.metadata.name}"")
                return {""status"": ""deleted"", ""count"": len(pod_list.items)}
        except client.rest.ApiException as e:
            logger.error(f""Error deleting pod: {e}"")
            raise",Delete a bot pod by user_id and optionally meeting_id,"???Delete user-specific bot pods in a namespace, optionally by meeting ID.???"
1874,save_to_cache,"def save_to_cache(self, filename, embeddings):
        
        if not self.batch_write_thread.is_alive():
            logger.debug(""Restarting background write thread."")
            # Start the thread again.
            self.process_write_batches = True
            self.batch_write_thread = Thread(target=self.batch_write_embeddings)
            self.batch_write_thread.start()
        self.write_queue.put((embeddings, filename))
        logger.debug(
            f""save_to_cache called for {filename}, write queue has {self.write_queue.qsize()} items, and the write thread's status: {self.batch_write_thread.is_alive()}""
        )",Add write requests to the queue instead of writing directly.,???Initiates background thread to cache embeddings if not active???
1875,generate_table_row,"def generate_table_row(self, model_name, t_onnx, t_engine, model_info):
        
        layers, params, gradients, flops = model_info
        return (
            f""| {model_name:18s} | {self.imgsz} | - | {t_onnx[0]:.1f}±{t_onnx[1]:.1f} ms | {t_engine[0]:.1f}±""
            f""{t_engine[1]:.1f} ms | {params / 1e6:.1f} | {flops:.1f} |""
        )",Generates a table row string with model performance metrics including inference times and model details.,??? Format model performance data into a structured table row string. ???
1876,get_nation_skill,"def get_nation_skill(
    name: str,
    store: SkillStoreABC,
) -> Optional[NationBaseTool]:
    
    if name == ""nft_check"":
        if name not in _cache:
            _cache[name] = NftCheck(
                skill_store=store,
            )
        return _cache[name]
    else:
        logger.error(f""Unknown Nation skill: {name}"")
        return None",Get a nation skill by name.,??? Retrieve or log error for nation skill by name from cache ???
1877,review_plan,"def review_plan(context: dict, **kwargs) -> dict:
    
    include_human_feedback = context.get(""task"").get(""include_human_feedback"")

    if not include_human_feedback:
        return {""human_feedback"": None, ""result"": ""ok""}

    layout = context.get(""sections"")
    human_feedback = input(
        f""Any feedback on this plan of topics to research? {layout}? If not, please reply with 'no'.""
    )

    return {""human_feedback"": human_feedback, ""result"": ""ok""}",Gathers human feedback on the research plan if required.,???Determine if human feedback is needed for a research plan???
1878,generate_text,"def generate_text(self, prompt: str, system_prompt: str, model: str = None, **kwargs) -> str:
        
        try:
            client = self._get_client()
            
            # Use configured model if none provided
            if not model:
                model = self.config[""model""]

            messages = [{""role"": ""user"", ""content"": prompt},{""role"": ""system"", ""content"": system_prompt},] 

            completion = client.chat.completions.create(
                model=model,
                messages=messages,
            )

            return completion.choices[0].message.content
            
        except Exception as e:
            raise TogetherAIAPIError(f""Text generation failed: {e}"")",Generate text using Together AI models,???Generate text response using AI model based on user and system prompts???
1879,run_chest_xray_agent,"def run_chest_xray_agent(state: AgentState) -> AgentState:
        

        current_input = state[""current_input""]
        image_path = current_input.get(""image"", None)

        print(f""Selected agent: CHEST_XRAY_AGENT"")

        # classify chest x-ray into covid or normal
        predicted_class = AgentConfig.image_analyzer.classify_chest_xray(image_path)

        if predicted_class == ""covid19"":
            response = AIMessage(content=""The analysis of the uploaded chest X-ray image indicates a **POSITIVE** result for **COVID-19**."")
        elif predicted_class == ""normal"":
            response = AIMessage(content=""The analysis of the uploaded chest X-ray image indicates a **NEGATIVE** result for **COVID-19**, i.e., **NORMAL**."")
        else:
            response = AIMessage(content=""The uploaded image is not clear enough to make a diagnosis / the image is not a medical image."")

        # response = AIMessage(content=""This would be handled by the chest X-ray agent, analyzing the image."")

        return {
            **state,
            ""output"": response,
            ""needs_human_validation"": True,  # Medical diagnosis always needs validation
            ""agent_name"": ""CHEST_XRAY_AGENT""
        }",Handle chest X-ray image analysis.,???Analyze chest X-ray images for COVID-19 diagnosis and generate a response.???
1880,_create_textgrad_agent,"def _create_textgrad_agent(self, node: Union[str, WorkFlowNode]) -> TextGradAgent:
        
        if isinstance(node, str):
            node = self.graph.get_node(node)

        if isinstance(node.agents[0], dict):
            agent_llm = node.agents[0].get(""llm"")
            agent_llm_config = node.agents[0].get(""llm_config"")
            if agent_llm is None and agent_llm_config is None:
                node.agents[0][""llm""] = self.executor_llm
            # CustomizeAgent.from_dict creates a CustomizeAgent if dict follows CustomizeAgent format
            # creates an Agent if dict follows Agent format
            agent: Union[CustomizeAgent, Agent] = CustomizeAgent.from_dict(node.agents[0])
        else:
            raise ValueError(f""Unsupported agent type {type(node.agents[0])}. Expected 'dict'."")
            
        textgrad_agent = TextGradAgent(agent, self.optimize_mode)
        return textgrad_agent",Creates a textgrad agent for a given node in a WorkFlowGraph.,???Create a TextGradAgent by configuring or validating agent details from a node.???
1881,extract_context,"def extract_context(
    path: str = typer.Argument(..., help=""Path to the local repository.""),
    file_path: str = typer.Argument(..., help=""Relative path to the file within the repository.""),
    line: int = typer.Argument(..., help=""Line number to extract context around.""),
    output: Optional[str] = typer.Option(None, ""--output"", ""-o"", help=""Output to JSON file instead of stdout.""),
):
    
    from kit import Repository

    try:
        repo = Repository(path)
        context = repo.extract_context_around_line(file_path, line)

        if output:
            Path(output).write_text(json.dumps(context, indent=2) if context else ""null"")
            typer.echo(f""Context written to {output}"")
        else:
            if context:
                typer.echo(f""Context for {file_path}:{line}"")
                typer.echo(f""Symbol: {context.get('name', 'N/A')} ({context.get('type', 'N/A')})"")
                typer.echo(f""Lines: {context.get('start_line', 'N/A')}-{context.get('end_line', 'N/A')}"")
                typer.echo(""Code:"")
                typer.echo(context.get(""code"", """"))
            else:
                typer.echo(f""No context found for {file_path}:{line}"")
    except Exception as e:
        typer.secho(f""Error: {e}"", fg=typer.colors.RED)
        raise typer.Exit(code=1)",Extract surrounding code context for a specific line.,???Extracts and outputs code context from a specified line in a repository file.???
1882,generate_text,"def generate_text(
    prompt: str,
    *,
    llm_model: str | None = None,
    llm_provider: str | None = None,
    stream: bool = False,
    **kwargs,
) -> str:
    

    # Find the provider.
    provider = find_provider(llm_provider or settings.DEFAULT_LLM_PROVIDER)

    # Generate the text.
    if stream:
        if not provider.supports_streaming:
            raise ValueError(f""{provider} does not support streaming."")

        return provider.generate_stream_text(
            prompt=prompt, llm_model=llm_model, **kwargs
        )
    else:
        return provider.generate_text(prompt=prompt, llm_model=llm_model, **kwargs)",Generate text from a given prompt.,???Generate text using specified or default language model provider with optional streaming???
1883,prompt_description,"def prompt_description(self) -> str:
		
		skip_keys = ['title']
		s = f'{self.description}: \n'
		s += '{' + str(self.name) + ': '
		s += str(
			{
				k: {sub_k: sub_v for sub_k, sub_v in v.items() if sub_k not in skip_keys}
				for k, v in self.param_model.model_json_schema()['properties'].items()
			}
		)
		s += '}'
		return s",Get a description of the action for the prompt,???Generate a formatted string excluding specific keys from a JSON schema.???
1884,api_get_all_cycle_status,"def api_get_all_cycle_status():
    
    try:
        from src.primary.cycle_tracker import get_cycle_status
        status = get_cycle_status()
        return jsonify(status), 200
    except Exception as e:
        web_logger = get_logger(""web_server"")
        web_logger.error(f""Error getting cycle status: {e}"")
        return jsonify({""error"": ""Failed to retrieve cycle status information.""}), 500",API endpoint to get cycle status for all apps.,"???Retrieve and return cycle status data, handling errors with logging.???"
1885,get_navigable,"def get_navigable(self) -> List[str]:
        
        try:
            links = []
            elements = self.driver.find_elements(By.TAG_NAME, ""a"")
            
            for element in elements:
                href = element.get_attribute(""href"")
                if href and href.startswith((""http"", ""https"")):
                    links.append({
                        ""url"": href,
                        ""text"": element.text.strip(),
                        ""is_displayed"": element.is_displayed()
                    })
            
            self.logger.info(f""Found {len(links)} navigable links"")
            return [self.clean_url(link['url']) for link in links if (link['is_displayed'] == True and self.is_link_valid(link['url']))]
        except Exception as e:
            self.logger.error(f""Error getting navigable links: {str(e)}"")
            return []",Get all navigable links on the current page.,??? Extracts and validates visible hyperlinks from a webpage using a web driver. ???
1886,init_device,"def init_device(self) -> None:
        

        # torch.distributed.all_reduce does not free the input tensor until
        # the synchronization point. This causes the memory usage to grow
        # as the number of all_reduce calls increases. This env var disables
        # this behavior.
        # Related issue:
        os.environ[""TORCH_NCCL_AVOID_RECORD_STREAMS""] = ""1""

        # This env var set by Ray causes exceptions with graph building.
        os.environ.pop(""NCCL_ASYNC_ERROR_HANDLING"", None)
        self.device = torch.device(f""cuda:{self.local_rank}"")
        torch.cuda.set_device(self.device)

        # _check_if_gpu_supports_dtype(self.model_config.dtype)
        gc.collect()
        torch.cuda.empty_cache()
        self.init_gpu_memory = torch.cuda.mem_get_info()[0]

        os.environ[""MASTER_ADDR""] = ""localhost""
        os.environ[""MASTER_PORT""] = str(self.master_port)
        os.environ[""LOCAL_RANK""] = str(self.local_rank)
        os.environ[""RANK""] = str(self.rank)
        os.environ[""WORLD_SIZE""] = str(self.fastvideo_args.num_gpus)

        # Initialize the distributed environment.
        maybe_init_distributed_environment_and_model_parallel(
            self.fastvideo_args.tp_size, self.fastvideo_args.sp_size)

        self.pipeline = build_pipeline(self.fastvideo_args)",Initialize the device for the worker.,???Initialize GPU device and distributed environment for parallel processing???
1887,__context_kwargs,"def __context_kwargs(self):
        
        context_kwargs = {
            ""proxy"": self.proxy,
            ""locale"": self.locale,
            ""color_scheme"": 'dark',  # Bypasses the 'prefersLightColor' check in creepjs
            ""device_scale_factor"": 2,
            ""extra_http_headers"": self.extra_headers if self.extra_headers else {},
            ""user_agent"": self.useragent if self.useragent else generate_headers(browser_mode=True).get('User-Agent'),
        }
        if self.stealth:
            context_kwargs.update({
                'is_mobile': False,
                'has_touch': False,
                # I'm thinking about disabling it to rest from all Service Workers headache but let's keep it as it is for now
                'service_workers': 'allow',
                'ignore_https_errors': True,
                'screen': {'width': 1920, 'height': 1080},
                'viewport': {'width': 1920, 'height': 1080},
                'permissions': ['geolocation', 'notifications']
            })

        return context_kwargs",Creates the arguments for the browser context,"??? Configure browser context settings with proxy, locale, and stealth options ???"
1888,freeze_tool_schema,"def freeze_tool_schema(tool: dict, fixed_args: dict) -> ChatCompletionToolParam:
    
    fields = {k: (Literal[v], ...) for k, v in fixed_args.items()}
    FrozenModel = create_model(
        f""{tool['function']['name'].title()}FrozenArgs"", **fields
    )

    locked = deepcopy(tool)
    locked[""function""][""parameters""] = FrozenModel.model_json_schema()
    return locked",Return a clone of *tool* whose parameters schema permits *only* `fixed_args`.,???Create a frozen schema for a tool by locking its parameters with fixed arguments.???
1889,_setup_steering,"def _setup_steering(self):
        
        try:
            # Initialize steering vector manager
            self.steering_manager = SteeringVectorManager(
                dataset_name=self.config[""steering_dataset""],
                target_layer=self.config[""target_layer""]
            )
            
            # Set pattern strengths
            if ""pattern_strengths"" in self.config:
                for pattern, strength in self.config[""pattern_strengths""].items():
                    self.steering_manager.set_steering_strength(pattern, strength)
            
            # Create tokenized contexts for efficient matching
            self.steering_manager.create_tokenized_contexts(self.tokenizer)
            
            # Install hooks on the model
            self.steering_hooks = install_steering_hooks(
                self.model, 
                self.steering_manager,
                self.tokenizer
            )
            
            logger.info(f""STEERING: Set up steering with {len(self.steering_hooks)} hooks"")
        
        except Exception as e:
            logger.error(f""STEERING: Error setting up steering: {e}"")
            self.steering_manager = None
            self.steering_hooks = []",Set up steering vector management.,???Initialize and configure steering mechanisms with pattern strengths and model hooks???
1890,stream_messages,"def stream_messages(
        self, data: dict[str, Any]
    ) -> Generator[dict[str, Any], None, None]:
        
        if self.url:
            headers = {
                ""Content-Type"": ""application/json"",
                ""Accept"": ""text/event-stream"",
            }
            if self.authenticate_request:
                headers[""Authorization""] = f""Bearer {self.id_token}""
            with requests.post(
                self.url, json=data, headers=headers, stream=True, timeout=60
            ) as response:
                for line in response.iter_lines():
                    if line:
                        try:
                            event = json.loads(line.decode(""utf-8""))
                            yield event
                        except json.JSONDecodeError:
                            print(f""Failed to parse event: {line.decode('utf-8')}"")
        elif self.agent is not None:
            yield from self.agent.stream_query(**data)","Stream events from the server, yielding parsed event data.","???Stream events from a URL or agent, handling authentication and JSON parsing.???"
1891,add_graph_node,"def add_graph_node(self, node_config: Union[AgentConfig, TaskConfig]):
        
        # this adds the node to the graph and relies on an existing edge
        existing_nodes: list[ast.Call] = self.get_graph_nodes()
        if len(existing_nodes):  # add the new node after the last existing node
            _, end = self.get_node_range(existing_nodes[-1])
        else:  # find the instantiation of `StateGraph`
            graph_instance = asttools.find_method_calls(self.get_run_method(), 'StateGraph')[0]
            _, end = self.get_node_range(graph_instance)

        # node is always either an Agent or a Task so we can make this assumption
        code = f
        self.edit_node_range(end, end, code)",Add a new node to the graph configuration.,???Add a new node to a graph based on existing nodes or graph instantiation???
1892,cleanup_processes,"def cleanup_processes():
    
    global worker_process
    if worker_process and worker_process.poll() is None:  # Check if process is still running
        logging.info(f""Stopping ARQ worker (PID: {worker_process.pid})..."")

        # Log the worker termination
        try:
            log_dir = os.path.join(os.getcwd(), ""logs"")
            worker_log_path = os.path.join(log_dir, ""worker.log"")

            with open(worker_log_path, ""a"") as worker_log:
                timestamp = subprocess.check_output([""date""]).decode().strip()
                worker_log.write(f""\n\n--- Worker stopping at {timestamp} ---\n\n"")
        except Exception as e:
            logging.warning(f""Could not write worker stop message to log: {e}"")

        # Send SIGTERM first for graceful shutdown
        worker_process.terminate()
        try:
            # Wait a bit for graceful shutdown
            worker_process.wait(timeout=5)
            logging.info(""ARQ worker stopped gracefully."")
        except subprocess.TimeoutExpired:
            logging.warning(""ARQ worker did not terminate gracefully, sending SIGKILL."")
            worker_process.kill()  # Force kill if it doesn't stop
            logging.info(""ARQ worker killed."")

        # Close any open file descriptors for the process
        if hasattr(worker_process, ""stdout"") and worker_process.stdout:
            worker_process.stdout.close()
        if hasattr(worker_process, ""stderr"") and worker_process.stderr:
            worker_process.stderr.close()",Stop the ARQ worker process on exit.,???Terminate and log the shutdown of a running worker process gracefully or forcefully.???
1893,execute_code,"def execute_code(code: str) -> Tuple[Any, str]:
    
    logger.info(""Attempting to execute code"")
    logger.info(f""Code:\n{code}"")
    
    try:
        # Create a clean environment
        execution_env = {}
        
        # Execute the code as-is
        exec(code, execution_env)
        
        # Look for answer variable
        if 'answer' in execution_env:
            answer = execution_env['answer']
            logger.info(f""Execution successful. Answer: {answer}"")
            return answer, None
        else:
            error = ""Code executed but did not produce an answer variable""
            logger.warning(error)
            return None, error
            
    except Exception as e:
        error = str(e)
        logger.error(f""Execution failed: {error}"")
        return None, error",Attempt to execute the code and return result or error.,"???  
Executes given code string, returning 'answer' variable or error message.  
???"
1894,message_param_str,"def message_param_str(self, message: MessageParam) -> str:
        
        if message.content:
            if isinstance(message.content, str):
                return message.content

            content: list[str] = []
            for c in message.content:
                if isinstance(c, TextContentItem):
                    content.append(c.text)
                elif isinstance(c, ImageContentItem):
                    content.append(f""Image url: {c.image_url.url}"")
                elif isinstance(c, AudioContentItem):
                    content.append(f""{c.input_audio.format}: {c.input_audio.data}"")
                else:
                    content.append(str(c))
            return ""\n"".join(content)
        else:
            return str(message)",Convert an input message to a string representation.,???Convert message content into a formatted string based on content type.???
1895,_format_tool_descriptions,"def _format_tool_descriptions(self, tools: List[Any]) -> str:
        
        descriptions = []
        for tool in tools:
            desc = [f""{tool.name}: {tool.description}""]
            
            desc.append(""\nArguments:"")
            for arg_name, arg_info in tool.inputs.items():
                desc.append(f""  - {arg_name}: {arg_info['description']}"")
            
            desc.append(f""\nReturns: {tool.output_type}"")
            
            descriptions.append(""\n"".join(desc))
        
        return ""\n\n"".join(descriptions)",Formats tool schemas into a user-friendly description string.,???Generate formatted descriptions for a list of tools with their arguments and return types.???
1896,mesh_to_colored_voxels,"def mesh_to_colored_voxels(obj_file=""voxel_mesh.obj"", 
                          mapping_file=""voxel_mapping.json"",
                          texture_file=""texture.png"",
                          output_file=""colored_voxels.png""):
    
    # Load the mesh
    mesh = trimesh.load(obj_file)
    print(f""Loaded mesh with {len(mesh.vertices)} vertices and {len(mesh.faces)} faces"")
    
    # Load the mapping
    mapping = load_mapping(mapping_file)
    print(f""Loaded mapping with {len(mapping)} voxel faces"")
    
    # Load the texture
    texture = plt.imread(texture_file)
    if texture.dtype == np.float32:
        texture = (texture * 255).astype(np.uint8)
    print(f""Loaded texture with shape {texture.shape}"")
    
    # Reconstruct voxel grid
    voxel_grid = reconstruct_voxel_grid(mapping, dimension=[20, 20, 20])
    print(f""Reconstructed voxel grid with shape {voxel_grid.shape}"")
    
    # Sample colors
    sampled_colors = {}
    for voxel_key in mapping.keys():
        coords = voxel_key[:3]
        face_idx = voxel_key[3]
        color = get_voxel_average_color(texture, coords, face_idx, mapping)
        sampled_colors[voxel_key] = color
    
    # Visualize the result
    colored_voxels = visualize_colored_voxel_grid(voxel_grid, sampled_colors, output_file=output_file)
    
    return voxel_grid, sampled_colors, colored_voxels",Convert a textured mesh back to colored voxels,???Convert 3D mesh to colored voxel representation using texture mapping.???
1897,_process_mermaid_diagrams,"def _process_mermaid_diagrams(self, content: str) -> str:
        
        def replace_mermaid(match):
            try:
                diagram = mermaid.generate_diagram(match.group(1))
                return f""```html\n<div class='mermaid-diagram'>\n{diagram}\n</div>\n```""
            except Exception as e:
                logger.error(f""Error processing Mermaid diagram: {e}"")
                return match.group(0)
        
        pattern = r'```mermaid\n(.*?)\n```'
        return re.sub(pattern, replace_mermaid, content, flags=re.DOTALL)",Convert Mermaid diagram code blocks to embedded SVG.,"???Transforms Mermaid code blocks in text into HTML-rendered diagrams, handling errors gracefully.???"
1898,_prepare_request_headers,"def _prepare_request_headers(self) -> bytes:
        
        new_headers = []
        has_host = False

        for header in self.request.headers:
            if header.lower().startswith(""host:""):
                has_host = True
                new_headers.append(f""Host: {self.target_host}"")
            else:
                new_headers.append(header)

        if not has_host:
            new_headers.append(f""Host: {self.target_host}"")

        request_line = f""{self.request.method} /{self.request.path} {self.request.version}\r\n""
        header_block = ""\r\n"".join(new_headers)
        return f""{request_line}{header_block}\r\n\r\n"".encode()",Prepare modified request headers,"???Constructs HTTP request headers, ensuring the host is correctly set.???"
1899,get_summary,"def get_summary(self) -> str:  # pragma: no cover
        
        if self._global_status == SyncStatus.IDLE:
            return ""✅ System ready""
        elif self._global_status == SyncStatus.COMPLETED:
            return ""✅ All projects synced successfully""
        elif self._global_status == SyncStatus.FAILED:
            failed_projects = [
                p.project_name
                for p in self._project_statuses.values()
                if p.status == SyncStatus.FAILED
            ]
            return f""❌ Sync failed for: {', '.join(failed_projects)}""
        else:
            active_projects = [
                p.project_name
                for p in self._project_statuses.values()
                if p.status in (SyncStatus.SCANNING, SyncStatus.SYNCING)
            ]
            total_files = sum(p.files_total for p in self._project_statuses.values())
            processed_files = sum(p.files_processed for p in self._project_statuses.values())

            if total_files > 0:
                progress_pct = (processed_files / total_files) * 100
                return f""🔄 Syncing {len(active_projects)} projects ({processed_files}/{total_files} files, {progress_pct:.0f}%)""
            else:
                return f""🔄 Syncing {len(active_projects)} projects""",Get a user-friendly summary of sync status.,???Generate a status message summarizing system synchronization progress and outcomes.???
1900,task_gravity_one_step,"def task_gravity_one_step(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    question = [rng.randint(1, 9) if rng.random() < 0.5 else 0 for _ in range(size)]
    answer = question.copy()

    # Move each non-zero pixel one step left if possible
    for i in range(1, size):
        if answer[i] != 0 and answer[i - 1] == 0:
            answer[i - 1] = answer[i]
            answer[i] = 0

    return {""input"": question, ""output"": answer}",Generate a task where non-zero elements move one step left if possible.,???Simulate gravity effect on a list by shifting non-zero elements left???
1901,get_url_markdown,"def get_url_markdown(url: str) -> str:
    
    import requests
    from markdownify import markdownify as md
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return md(response.text)
    except Exception as e:
        return f""Error: {str(e)}""",Get contents of URL as nicely formatted markdown.,"???  
Converts webpage content from a URL into markdown format, handling errors.  
???"
1902,convert_to_scheme,"def convert_to_scheme(self, scheme: str) -> str:
        
        if scheme not in VALID_SCHEMES:
            raise ValueError(f""Invalid scheme: {scheme}. The scheme should be one of {VALID_SCHEMES}."") 
        if scheme == ""python"":
            repr = self.get_workflow_python_repr()
        elif scheme == ""yaml"":
            repr = self.get_workflow_yaml_repr()
        elif scheme == ""code"":
            repr = self.get_workflow_code_repr()
        elif scheme == ""core"":
            repr = self.get_workflow_core_repr()
        elif scheme == ""bpmn"":
            repr = self.get_workflow_bpmn_repr()
        return repr",Transform the WorkflowGraph to the desired scheme.,???Convert workflow representation based on specified scheme type???
1903,_extract_basic_info,"def _extract_basic_info(self, repo_url: str, readme_content: str) -> Dict:
        
        schema = {
            ""name"": ""extract_basic_info"",
            ""description"": ""Extract basic manifest information"",
            ""parameters"": {
                ""type"": ""object"",
                ""required"": [""display_name"", ""tags""],
                ""properties"": {
                    ""display_name"": {
                        ""type"": ""string"",
                        ""description"": ""Human-readable server name""
                    },
                    ""license"": {""type"": ""string""},
                    ""tags"": {
                        ""type"": ""array"",
                        ""items"": {""type"": ""string""}
                    }
                },
                ""additionalProperties"": False
            },
        }

        return self._call_llm(
            repo_url=repo_url,
            readme_content=readme_content,
            schema=schema,
            prompt=(""Extract the display_name, license, and tags from the README file. ""
                    ""The display_name should be a human-readable server name close to the name of the repository. ""
                    ""The tags should be a list of tags that describe the server."")
        )","Extract basic information (display_name, license, tags) using LLM.","???Extracts repository metadata like name, license, and tags from README content.???"
1904,run_streamable_http_server,"def run_streamable_http_server(server_port: int) -> None:
    
    _, app = make_fastmcp_streamable_http_app()
    server = uvicorn.Server(
        config=uvicorn.Config(
            app=app, host=""127.0.0.1"", port=server_port, log_level=""error""
        )
    )
    print(f""Starting StreamableHTTP server on port {server_port}"")
    server.run()",Run the StreamableHTTP server.,???Initialize and run a streamable HTTP server on a specified port.???
1905,_load_api_key,"def _load_api_key(self, api_key_path: str) -> None:
        
        try:
            config_path = Path(api_key_path)
            if config_path.exists():
                with open(config_path) as f:
                    config = json.load(f)
                self.api_key = config.get('api_key')
            if not self.api_key:
                raise ValueError(""API key not found in config file"")
        except Exception as e:
            logger.error(f""Error loading API key: {e}"")
            raise",Load Alpha Vantage API key from config file.,???Load API key from a configuration file and handle errors.???
1906,set_whitelist,"def set_whitelist(self, wxid: str, stat: bool) -> bool:
        
        session = self.DBSession()
        try:
            user = session.query(User).filter_by(wxid=wxid).first()
            if not user:
                user = User(wxid=wxid)
                session.add(user)
            user.whitelist = stat
            session.commit()
            logger.info(f""数据库: 用户{wxid}白名单状态设置为{stat}"")
            return True
        except Exception as e:
            session.rollback()
            logger.error(f""数据库: 用户{wxid}白名单状态设置失败, 错误: {e}"")
            return False
        finally:
            session.close()",Set user's whitelist status,"???Update user whitelist status in database, handling exceptions and logging results.???"
1907,_compute_in_batch_neg_loss,"def _compute_in_batch_neg_loss(self, q_reps, p_reps, teacher_targets=None, compute_score_func=None, **kwargs):
        

        if compute_score_func is None:
            scores = self.compute_score(q_reps, p_reps) # (batch_size, batch_size * group_size)
        else:
            scores = compute_score_func(q_reps, p_reps, **kwargs)   # (batch_size, batch_size * group_size)

        if teacher_targets is not None:
            # compute kd loss
            if self.kd_loss_type == ""kl_div"":
                student_scores = self.get_local_score(q_reps, p_reps, scores) # (batch_size, group_size)

                loss = self.distill_loss(self.kd_loss_type, teacher_targets, student_scores, group_size)

                idxs = torch.arange(q_reps.size(0), device=q_reps.device, dtype=torch.long)
                loss += self.compute_loss(scores, targets)
            elif self.kd_loss_type == ""m3_kd_loss"":
                loss = self.distill_loss(self.kd_loss_type, teacher_targets, scores, group_size)
            else:
                raise ValueError(f""Invalid kd_loss_type: {self.kd_loss_type}"")
        else:
            idxs = torch.arange(q_reps.size(0), device=q_reps.device, dtype=torch.long)
            targets = idxs * group_size # (batch_size)
            loss = self.compute_loss(scores, targets)

        return scores, loss",Compute loss when only using in-batch negatives,???Calculate negative loss using batch representations and optional teacher targets.???
1908,get_settings_file_path,"def get_settings_file_path(app_name: str) -> pathlib.Path:
    
    if app_name not in KNOWN_APP_TYPES:
        # Log a warning but allow for potential future app types
        settings_logger.warning(f""Requested settings file for unknown app type: {app_name}"")
    return SETTINGS_DIR / f""{app_name}.json""",Get the path to the settings file for a specific app.,???Determine settings file path for given application name???
1909,_get_recommendation_reason,"def _get_recommendation_reason(self, similarity: float) -> str:
        
        if similarity >= 0.8:
            return f""The component is highly similar (score: {similarity:.2f}) to the request. Reuse as-is for efficiency.""
        elif similarity >= 0.4:
            return f""The component is moderately similar (score: {similarity:.2f}) to the request. Evolve it to better match the requirements.""
        return f""No sufficiently similar component found (best score: {similarity:.2f}). Create a new component.""",Get a reason for the recommendation.,???Determine recommendation rationale based on component similarity score.???
1910,validate_log_level,"def validate_log_level(cls, v):
        
        valid_levels = [""DEBUG"", ""INFO"", ""WARNING"", ""ERROR"", ""CRITICAL""]
        if v.upper() not in valid_levels:
            raise ValueError(f""log_level must be one of {', '.join(valid_levels)}"")
        return v.upper()",Ensure log_level is valid.,???Ensure log level is valid by checking against predefined levels???
1911,_render_template,"def _render_template(self, template_str, context):
        
        if not template_str:
            return """"
            
        try:
            template = Template(template_str)
            return template.render(**context)
        except Exception as e:
            print(f""Error rendering template: {e}"")
            return template_str",Render a template with the given context,"???Renders a template string with context, handling errors gracefully.???"
1912,json_schema_from_json,"def json_schema_from_json(data: dict[str, Any]) -> dict[str, Any]:
    

    builder: SchemaBuilder = SchemaBuilder()
    builder.add_object(data)  # pyright: ignore[reportUnknownMemberType]
    schema = builder.to_schema()  # pyright: ignore[reportUnknownVariableType]
    schema.pop(""$schema"", None)  # pyright: ignore[reportUnknownMemberType]
    schema = strip_metadata(schema, keys={""required""})
    return streamline_schema(schema)",Build a JSON schema from a JSON object.,???Convert JSON data into a streamlined JSON schema representation.???
1913,extract_sound,"def extract_sound(
    vid_input: str,
    output_temp_dir: str = TEMP_VIDEO_FRAMES_DIR,
):
    
    if Path(vid_input).suffix == "".gif"":
        print(""Sound extracting process has passed because gif has no sound"")
        return None

    os.makedirs(output_temp_dir, exist_ok=True)
    output_path = os.path.join(output_temp_dir, ""sound.mp3"")

    command = [
        'ffmpeg',
        '-loglevel', 'error',
        '-y',  # Enable overwriting
        '-i', vid_input,
        '-vn',
        output_path
    ]

    try:
        subprocess.run(command, check=True)
    except subprocess.CalledProcessError as e:
        print(f""Warning: Failed to extract sound from the video: {e}"")

    return output_path",Extract audio from a video file and save it as a separate sound file.,"???Extracts audio from video files, excluding GIFs, using ffmpeg.???"
1914,_get_data_file_preprocessing_code,"def _get_data_file_preprocessing_code(file: File) -> Optional[str]:
  

  def _get_normalized_file_name(file_name: str) -> str:
    var_name, _ = os.path.splitext(file_name)
    # Replace non-alphanumeric characters with underscores
    var_name = re.sub(r'[^a-zA-Z0-9_]', '_', var_name)

    # If the filename starts with a digit, prepend an underscore
    if var_name[0].isdigit():
      var_name = '_' + var_name
    return var_name

  if file.mime_type not in _DATA_FILE_UTIL_MAP:
    return

  var_name = _get_normalized_file_name(file.name)
  loader_code = _DATA_FILE_UTIL_MAP[file.mime_type].loader_code_template.format(
      filename=file.name
  )
  return f",Returns the code to explore the data file.,???Generate preprocessing code for data files based on their type and name.???
1915,_get_name_node,"def _get_name_node(self) -> TSNode:
        
        for child in self.ts_node.children:
            # =====[ Identifier ]=====
            # Just `@dataclass` etc.
            if child.type == ""identifier"":
                return child

            # =====[ Attribute ]=====
            # e.g. `@a.b`
            elif child.type == ""member_expression"":
                return child

            # =====[ Call ]=====
            # e.g. `@a.b()`
            elif child.type == ""call_expression"":
                func = child.child_by_field_name(""function"")
                return func

        msg = f""Could not find decorator name within {self.source}""
        raise ValueError(msg)",Returns the name of the decorator.,"???Extracts decorator name node from syntax tree, raising error if not found.???"
1916,convert_to_pytest_fixtures,"def convert_to_pytest_fixtures(file):
    
    print(f""🔍 Processing file: {file.filepath}"")

    if not any(imp.name == ""pytest"" for imp in file.imports):
        file.add_import(""import pytest"")
        print(f""➕ Added pytest import to {file.filepath}"")

    for cls in file.classes:
        setup_method = cls.get_method(""setUp"")
        if setup_method:
            fixture_name = f""setup_{cls.name.lower()}""
            fixture_body = ""\n"".join([line.replace(""self."", """") for line in setup_method.body.split(""\n"")])
            fixture_code = f

            model_class = ""Character"" if ""Character"" in cls.name else ""Castle""

            for method in cls.methods:
                if method.name == ""setUp"":
                    method.insert_before(fixture_code)
                    print(f""🔧 Created fixture {fixture_name} for class {cls.name}"")
                elif method.name.startswith(""test_""):
                    new_signature = f""def {method.name}({fixture_name}, {model_class}):""
                    method_body = ""\n"".join([line.replace(""self."", """") for line in method.source.split(""\n"")[1:]])
                    method.edit(f""{new_signature}\n{method_body}"")
                    print(f""🔄 Updated test method {method.name} signature and removed self references"")
            setup_method.remove()
            print(f""🗑️ Removed setUp method from class {cls.name}"")",Converts unittest setUp methods to pytest fixtures and updates test methods,???Transform unittest setup methods into pytest fixtures and update test method signatures accordingly.???
1917,_format_messages,"def _format_messages(system_prompt: str = None, user_prompt: str = None, messages: List[Dict] = None) -> List[Dict]:
    
    if messages is not None:
        return messages
    if system_prompt is not None and user_prompt is not None:
        return [{""role"": ""system"", ""content"": system_prompt}, {""role"": ""user"", ""content"": user_prompt}]
    raise ValueError(""Either (system_prompt, user_prompt) or messages must be provided"")",Convert between different message formats while maintaining backward compatibility,???Format message data based on provided prompts or existing message list.???
1918,load_chat,"def load_chat(self, chat_id: str):
        
        filename = f""{self.chats_dir}/chat_{chat_id}.json""
        try:
            with open(filename, 'r') as f:
                data = json.load(f)
                self.current_chat_id = data['id']
                self.chat_history = [ChatMessage.from_dict(msg) for msg in data['messages']]
                # Store original state for modification tracking
                self._original_chat_state = [msg.to_dict() for msg in self.chat_history]
        except FileNotFoundError:
            print(f""Chat {chat_id} not found"")",Load a specific chat from disk,???Load chat history from a JSON file and handle missing file errors.???
1919,create_or_get_corpus,"def create_or_get_corpus():
  
  embedding_model_config = rag.EmbeddingModelConfig(
      publisher_model=""publishers/google/models/text-embedding-004""
  )
  existing_corpora = rag.list_corpora()
  corpus = None
  for existing_corpus in existing_corpora:
    if existing_corpus.display_name == CORPUS_DISPLAY_NAME:
      corpus = existing_corpus
      print(f""Found existing corpus with display name '{CORPUS_DISPLAY_NAME}'"")
      break
  if corpus is None:
    corpus = rag.create_corpus(
        display_name=CORPUS_DISPLAY_NAME,
        description=CORPUS_DESCRIPTION,
        embedding_model_config=embedding_model_config,
    )
    print(f""Created new corpus with display name '{CORPUS_DISPLAY_NAME}'"")
  return corpus",Creates a new corpus or retrieves an existing one.,"???Check for existing corpus by name, create if absent, and return it???"
1920,_update_description_with_files,"def _update_description_with_files(self, upload_details: list[dict]) -> None:
        
        if upload_details:
            self.description = self.description.strip().replace(""</tool_description>"", """")
            self.description += ""\n\n**Uploaded Files Details:**""
            for file_info in upload_details:
                self.description += (
                    f""\n- **Original File Name**: {file_info['original_name']}\n""
                    f""  **Description**: {file_info['description']}\n""
                    f""  **Uploaded Path**: {file_info['uploaded_path']}\n""
                )
            self.description += ""\n</tool_description>""",Updates the tool description with detailed information about the uploaded files.,???Enhances description with formatted details of uploaded files.???
1921,hash_password,"def hash_password(password: str) -> str:
    
    # Use SHA-256 with a salt
    salt = secrets.token_hex(16)
    pw_hash = hashlib.sha256((password + salt).encode()).hexdigest()
    return f""{salt}:{pw_hash}""",Hash a password for storage,"???  
Generate a salted SHA-256 hash for secure password storage.  
???"
1922,_load_eval_set_from_file,"def _load_eval_set_from_file(
      eval_set_file: str,
      criteria: dict[str, float],
      initial_session: dict[str, Any],
  ) -> EvalSet:
    
    if os.path.isfile(eval_set_file):
      with open(eval_set_file, ""r"", encoding=""utf-8"") as f:
        content = f.read()

      try:
        eval_set = EvalSet.model_validate_json(content)
        assert len(initial_session) == 0, (
            ""Intial session should be specified as a part of EvalSet file.""
            "" Explicit initial session is only needed, when specifying data in""
            "" the older schema.""
        )
        return eval_set
      except ValidationError:
        # We assume that the eval data was specified in the old format
        logger.warning(
            f""Contents of {eval_set_file} appear to be in older format.To avoid""
            "" this warning, please update your test files to contain data in""
            "" EvalSet schema. You can use `migrate_eval_data_to_new_schema`""
            "" for migrating your old test files.""
        )

    # If we are here, the data must be specified in the older format.
    return AgentEvaluator._get_eval_set_from_old_format(
        eval_set_file, criteria, initial_session
    )",Loads an EvalSet from the given file.,"???Load and validate evaluation data from a file, handling both new and old formats.???"
1923,check_server_configs,"def check_server_configs():
    
    print(""\n3. Server Configuration Files"")
    print(""-"" * 31)
    
    config_files = [
        'server_config.json',
        'server_config_minimal.json', 
        'server_config_working.json'
    ]
    
    valid_configs = []
    
    for config_file in config_files:
        if Path(config_file).exists():
            try:
                with open(config_file, 'r') as f:
                    config = json.load(f)
                
                servers = list(config.get('mcpServers', {}).keys())
                print(f""   ✅ {config_file}: {len(servers)} servers"")
                print(f""      Servers: {', '.join(servers)}"")
                
                # Check timeout settings
                tool_settings = config.get('toolSettings', {})
                if 'defaultTimeout' in tool_settings:
                    print(f""      Timeout: {tool_settings['defaultTimeout']}s"")
                
                valid_configs.append(config_file)
                
            except json.JSONDecodeError as e:
                print(f""   ❌ {config_file}: Invalid JSON - {e}"")
            except Exception as e:
                print(f""   ❌ {config_file}: Error - {e}"")
        else:
            print(f""   ⚠️ {config_file}: Not found"")
    
    return len(valid_configs) > 0",Check server configuration files.,???Check server configuration files for validity and report server details.???
1924,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Animation.disable"",
    }
    json = yield cmd_dict",Disables animation domain notifications.,???Disables animation by sending a command and yielding a JSON response.???
1925,analyze_management_actions,"def analyze_management_actions(financial_line_items: list) -> dict[str, any]:
    
    if not financial_line_items:
        return {""score"": 0, ""details"": ""No management action data""}

    latest = financial_line_items[0]
    score = 0
    reasoning = []

    issuance = getattr(latest, ""issuance_or_purchase_of_equity_shares"", None)
    if issuance is not None:
        if issuance < 0:  # Negative indicates share buybacks
            score += 2
            reasoning.append(f""Company buying back shares: {abs(issuance)}"")
        elif issuance > 0:
            reasoning.append(f""Share issuance detected (potential dilution): {issuance}"")
        else:
            score += 1
            reasoning.append(""No recent share issuance or buyback"")
    else:
        reasoning.append(""No data on share issuance or buybacks"")

    return {""score"": score, ""details"": ""; "".join(reasoning)}",Look at share issuance or buybacks to assess shareholder friendliness.,???Evaluate management actions based on share issuance or buyback data to generate a score and reasoning.???
1926,_generate_agentic_comment,"def _generate_agentic_comment(self, pr_details: Dict[str, Any], files: list[Dict[str, Any]], analysis: str) -> str:
        
        comment = f
        return comment",Generate an agentic review comment.,"???Generate a comment based on pull request details, file changes, and analysis.???"
1927,_optimize_pipeline,"def _optimize_pipeline(self, pipeline, use_fp16: bool = True):
        
        if self.device.type == ""cuda"":
            try:
                if hasattr(pipeline, 'cuda'):
                    pipeline.cuda()

                if use_fp16:
                    if hasattr(pipeline, 'enable_attention_slicing'):
                        pipeline.enable_attention_slicing(slice_size=""auto"")
                    if hasattr(pipeline, 'half'):
                        pipeline.half()
            except Exception as e:
                logger.warning(f""Some pipeline optimizations failed: {str(e)}"")

        return pipeline","Apply typical optimizations, half-precision, etc.",???Optimize machine learning pipeline for CUDA with optional FP16 precision???
1928,handle_connect,"def handle_connect(self) -> None:
        
        try:
            path = unquote(self.request.target)
            if "":"" not in path:
                raise ValueError(f""Invalid CONNECT path: {path}"")

            self.target_host, port = path.split("":"")
            self.target_port = int(port)

            # Get SSL context through the TLS handler
            self.ssl_context = self.cert_manager.get_domain_context(self.target_host)

            self.is_connect = True
            asyncio.create_task(self.connect_to_target())
            self.handshake_done = True

        except Exception as e:
            logger.error(f""Error handling CONNECT: {e}"")
            self.send_error_response(502, str(e).encode())",Handle CONNECT request for SSL/TLS tunneling,"???Handle secure connection requests by parsing target, setting SSL, and initiating connection???"
1929,_init_metrics,"def _init_metrics(self) -> None:
        
        # Response time history with fixed size per API
        self._response_times: Dict[str, deque] = {}

        # Request counts by API
        self._request_counts: Dict[str, Dict[str, int]] = {}

        # Status code distribution by API
        self._status_codes: Dict[str, Dict[int, int]] = {}

        # Rate limit hits by API
        self._rate_limit_hits: Dict[str, int] = {}

        # Queue hits by API
        self._queue_hits: Dict[str, int] = {}

        # Key usage statistics by API
        self._key_usage: Dict[str, Dict[str, Dict[str, Any]]] = {}

        # Reset request history
        self.request_history.clear()",Initialize or reset all metrics data structures.,???Initialize tracking structures for API performance and usage metrics???
1930,create_session,"def create_session(self, metadata: t.Dict, session_id: str | None = None) -> str | None:
        
        if not self.hosted:
            return str(uuid.uuid4().hex)

        if session_id:
            self._session = session_id
            return session_id
        if self.session:
            return self.session
        metadata.update({""status"": _SessionStatus.STARTED})

        response = requests.post(f""{BASE_CLIENT_URL}/sessions"", json=metadata, headers=self._headers)

        if response.status_code == 200:
            self._session = response.json().get(""session_id"")
            self._state = _SessionStatus.STARTED
            return self.session
        else:
            logger.warning(f""Failed to create session: {response.status_code}, {response.text}"")
            return str(uuid.uuid4().hex)",Sends a POST request to the server to create a session.,"???Create or retrieve a session ID, handling both hosted and non-hosted scenarios.???"
1931,create_group,"def create_group(conn, name: str, metadata: str = """"):
    
    if conn is None:
        print(""Cannot create group: database connection is None"")
        return False

    try:
        # Properly escape quotes in strings
        escaped_name = name.replace('""', '\\""')
        escaped_metadata = metadata.replace('""', '\\""')

        query = f

        result, error = safe_kuzu_execute(
            conn, query, f""Failed to create group: {name}""
        )
        if error:
            return False

        print(f""Created group: {name}"")
        return True
    except Exception as e:
        print(f""Error creating group {name}: {str(e)}"")
        traceback.print_exc()
        return False",Create a new group in the database with robust error handling,???Create a database group with error handling and input sanitization.???
1932,reply_to_tweet,"def reply_to_tweet(self, tweet_id: str, message: str, **kwargs) -> dict:
        
        logger.debug(f""Replying to tweet {tweet_id}"")
        self._validate_tweet_text(message, ""Reply"")

        response = self._make_request('post',
                                      'tweets',
                                      json={
                                          'text': message,
                                          'reply': {
                                              'in_reply_to_tweet_id': tweet_id
                                          }
                                      })

        logger.info(""Reply posted successfully"")
        return response",Reply to an existing tweet,???Function posts a reply to a specified tweet with validation and logging.???
1933,format_plan_result,"def format_plan_result(plan_result: PlanResult) -> str:
    
    from mcp_agent.llm.prompt_utils import format_fastagent_tag

    # Format objective
    objective_tag = format_fastagent_tag(""objective"", plan_result.objective)

    # Format step results
    step_results = []
    for step in plan_result.step_results:
        step_results.append(format_step_result_xml(step))

    # Build progress section
    if step_results:
        steps_content = ""\n"".join(step_results)
        progress_content = (
            f""{objective_tag}\n""
            f""<fastagent:steps>\n{steps_content}\n</fastagent:steps>\n""
            f""<fastagent:status>{plan_result.result if plan_result.is_complete else 'In Progress'}</fastagent:status>\n""
        )
    else:
        # No steps executed yet
        progress_content = (
            f""{objective_tag}\n""
            f""<fastagent:steps>No steps executed yet</fastagent:steps>\n""
            f""<fastagent:status>Not Started</fastagent:status>\n""
        )

    return format_fastagent_tag(""progress"", progress_content)",Format the full plan execution state with XML for better semantic understanding,???Format plan results into structured XML-like progress report???
1934,bindEntityToQuery,"def bindEntityToQuery(self, entity: Entity):
        
        for field in self.fields:
            value = entity[field]
            self.query.bindValue(f':{field}', value)",bind the value of entity to query object,???Bind entity fields to query parameters for database operations.???
1935,synthesize_findings,"def synthesize_findings(state: ResearchState) -> ResearchState:
    
    prompt = PromptTemplate(
        input_variables=[""topic"", ""answers""],
        template=""Synthesize a 200-word report on '{topic}' using these findings:\n{answers}""
    )
    synthesis = llm.invoke(prompt.format(
        topic=state[""topic""],
        answers=""\n"".join([f""Q: {a['question']}\nA: {a['answer']}"" for a in state[""answers""]])
    ))
    return {""synthesis"": synthesis.content, ""status"": ""synthesized""}",Synthesize answers into a cohesive report.,???Generate a concise report by synthesizing research findings into a structured summary.???
1936,fill_holes_in_mask_scores,"def fill_holes_in_mask_scores(mask, max_area):
    
    # Holes are those connected components in background with area <= self.max_area
    # (background regions are those with mask scores <= 0)
    assert max_area > 0, ""max_area must be positive""

    input_mask = mask
    try:
        labels, areas = get_connected_components(mask <= 0)
        is_hole = (labels > 0) & (areas <= max_area)
        # We fill holes with a small positive mask score (0.1) to change them to foreground.
        mask = torch.where(is_hole, 0.1, mask)
    except Exception as e:
        # Skip the post-processing step on removing small holes if the CUDA kernel fails
        warnings.warn(
            f""{e}\n\nSkipping the post-processing step due to the error above. You can ""
            ""still use SAM 2 and it's OK to ignore the error above, although some post-processing ""
            ""functionality may be limited (which doesn't affect the results in most cases; see ""
            category=UserWarning,
            stacklevel=2,
        )
        mask = input_mask

    return mask",A post processor to fill small holes in mask scores with area under `max_area`.,???Fill small background holes in mask with positive scores to convert them to foreground.???
1937,broadcast_object,"def broadcast_object(self, obj: Optional[Any] = None, src: int = 0):
        
        assert src < self.world_size, f""Invalid src rank ({src})""

        # Bypass the function if we are using only 1 GPU.
        if self.world_size == 1:
            return obj
        if self.mq_broadcaster is not None:
            assert src == 0, ""Message queue broadcaster only supports src=0""
            return self.mq_broadcaster.broadcast_object(obj)
        if self.rank_in_group == src:
            torch.distributed.broadcast_object_list([obj],
                                                    src=self.ranks[src],
                                                    group=self.cpu_group)
            return obj
        else:
            recv = [None]
            torch.distributed.broadcast_object_list(recv,
                                                    src=self.ranks[src],
                                                    group=self.cpu_group)
            return recv[0]",Broadcast the input object.,"???Distribute an object across multiple GPUs, handling different source ranks and configurations.???"
1938,extract_parameters,"def extract_parameters(kwargs):
    
    parameters = {k: v for k, v in kwargs.items() if v is not None}

    # Remove contents key in parameters (Google LLM Response)
    if 'contents' in parameters:
        del parameters['contents']

    # Remove messages key in parameters (OpenAI message)
    if 'messages' in parameters:
        del parameters['messages']
        
    if 'run_manager' in parameters:
        del parameters['run_manager']

    if 'generation_config' in parameters:
        generation_config = parameters['generation_config']
        # If generation_config is already a dict, use it directly
        if isinstance(generation_config, dict):
            config_dict = generation_config
        else:
            # Convert GenerationConfig to dictionary if it has a to_dict method, otherwise try to get its __dict__
            config_dict = getattr(generation_config, 'to_dict', lambda: generation_config.__dict__)()
        parameters.update(config_dict)
        del parameters['generation_config']
        
    return parameters",Extract all non-null parameters from kwargs,"???  
Filter and refine input parameters by removing specific keys and expanding configuration details.  
???"
1939,to_df,"def to_df(self) -> pd.DataFrame:
        
        return self._duckdb.execute(f""SELECT * FROM {self._table_name}"").df()",Get entire CSV as a DataFrame,???Convert database table to a pandas DataFrame using DuckDB query execution.???
1940,read_image_batch,"def read_image_batch(
        self, filepaths: list, delete_problematic_images: bool = False
    ) -> list:
        
        if type(filepaths) != list:
            raise ValueError(
                f""read_image_batch must be given a list of image filepaths. we received: {filepaths}""
            )
        output_images = []
        available_keys = []
        for filepath in filepaths:
            try:
                image_data = self.read_image(filepath, delete_problematic_images)
                if image_data is None:
                    logger.warning(f""Unable to load image '{filepath}', skipping."")
                    continue
                output_images.append(image_data)
                available_keys.append(filepath)
            except Exception as e:
                if delete_problematic_images:
                    logger.error(
                        f""Deleting image '{filepath}', because --delete_problematic_images is provided. Error: {e}""
                    )
                else:
                    logger.warning(
                        f""A problematic image {filepath} is detected, but we are not allowed to remove it, because --delete_problematic_image is not provided.""
                        f"" Please correct this manually. Error: {e}""
                    )
        return (available_keys, output_images)",Read a batch of images from the specified filepaths.,"???Batch processes image files, optionally deleting problematic ones, and returns successfully loaded images.???"
1941,replay,"def replay():
    
    try:
        instance.replay(task_id=sys.argv[1])
    except Exception as e:
        raise Exception(f""An error occurred while replaying the crew: {e}"")",Replay the crew execution from a specific task.,"???Attempts to replay a task using a given task identifier, handling errors.???"
1942,log_proxy_forward,"def log_proxy_forward(target_url: str, method: str, status_code: int) -> None:
    
    logger = logging.getLogger(""proxy_pilot"")
    log_data = {
        ""timestamp"": datetime.now().isoformat(),
        ""type"": ""proxy_forward"",
        ""target_url"": target_url,
        ""method"": method,
        ""status_code"": status_code,
    }
    logger.info(f""Proxy Forward: {json.dumps(log_data, indent=2)}"")",Log proxy forwarding details,"???Log proxy forwarding details including URL, method, and status.???"
1943,_prepare_images,"def _prepare_images(self, images: list):
        
        files = {}
        if images:
            for index, img in enumerate(images):
                img_byte_array = BytesIO()
                img.save(img_byte_array, format=""PNG"")
                img_byte_array.seek(0)
                files[f""file{index}""] = (
                    f""image{index}.png"",
                    img_byte_array,
                    ""image/png"",
                )
        return files",Convert images to file objects for Discord uploads.,???Convert images to PNG format and store in a dictionary.???
1944,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        result = {""name"": self.name}
        if self.id != JIRA_DEFAULT_ID:
            result[""id""] = self.id
        return result",Convert to simplified dictionary for API response.,"???Convert object attributes to a simplified dictionary format, excluding default IDs.???"
1945,_format_observation,"def _format_observation(self, obs: Tuple[int, int, int]) -> str:
        
        player_sum, dealer_card, usable_ace = obs
        return (
            f""Your current hand sum is {player_sum}. ""
            f""The dealer is showing a {dealer_card}. ""
            f""You have a usable ace: {'yes' if usable_ace else 'no'}.""
        )",Converts a Blackjack observation to a human-readable string.,"???  
Formats game state into a descriptive string for player observation.  
???"
1946,enable_gradient_checkpointing,"def enable_gradient_checkpointing(self) -> None:
        
        if not self._supports_gradient_checkpointing:
            raise ValueError(f""{self.__class__.__name__} does not support gradient checkpointing."")
        self.apply(partial(self._set_gradient_checkpointing, value=True))",Activates gradient checkpointing for the current model (may be referred to as *activation checkpointing* or checkpoint activations* in other frameworks).,???Enable gradient checkpointing if supported by the model class.???
1947,save_cache,"def save_cache(self) -> Path:
        
        if self.kv_cache is None:
            raise ValueError(""No cache to save"")

        cache_dir = self.cache_path / ""kv_cache""
        cache_dir.mkdir(parents=True, exist_ok=True)

        # Save key and value caches
        cache_data = {
            ""key_cache"": self.kv_cache.key_cache,
            ""value_cache"": self.kv_cache.value_cache,
            ""origin_len"": self.origin_len,
        }
        cache_path = cache_dir / ""cache.pt""
        torch.save(cache_data, cache_path)
        return cache_path",Save the KV cache to disk,???Save key-value cache to a specified directory path???
1948,inject_wrong_bin_usage,"def inject_wrong_bin_usage(self, microservices: list[str]):
        
        for service in microservices:
            deployment_yaml = self._get_deployment_yaml(service)

            # Modify the deployment YAML to use the 'geo' binary instead of the 'profile' binary
            containers = deployment_yaml[""spec""][""template""][""spec""][""containers""]
            for container in containers:
                if ""command"" in container and ""profile"" in container[""command""]:
                    print(
                        f""Changing binary for container {container['name']} from 'profile' to 'geo'.""
                    )
                    container[""command""] = [""geo""]  # Replace 'profile' with 'geo'

            modified_yaml_path = self._write_yaml_to_file(service, deployment_yaml)

            # Delete the deployment and re-apply
            delete_command = f""kubectl delete deployment {service} -n {self.namespace}""
            apply_command = f""kubectl apply -f {modified_yaml_path} -n {self.namespace}""
            self.kubectl.exec_command(delete_command)
            self.kubectl.exec_command(apply_command)

            print(f""Injected wrong binary usage fault for service: {service}"")",Inject a fault to use the wrong binary of a service.,???Injects a fault by replacing a binary in microservice deployments and redeploying them.???
1949,parse,"def parse(self, input: dict, response: str) -> dict:
        
        instruction = input[""instruction""]
        return {""instruction"": instruction, ""new_response"": response}",Parse the model response along with the input to the model into the desired output format..,???Extracts and returns instruction with updated response from input data.???
1950,run_file_sync,"def run_file_sync():
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(initialize_file_sync(app_config))
        except Exception as e:
            logger.error(f""File sync error: {e}"", err=True)
        finally:
            loop.close()",Run file sync in a separate thread with its own event loop.,???Initialize and execute asynchronous file synchronization with error handling???
1951,validate_json_schema,"def validate_json_schema(data: Dict[str, Any], schema: Dict[str, Any]) -> Dict[str, Any]:
    
    try:
        validate(instance=data, schema=schema)
        return data
    except ValidationError as e:
        logger.warning(f""JSON schema validation failed: {e}"")
        
        # Attempt to fix schema issues (this is a simplified example)
        for error in e.context:
            if error.validator == 'required':
                for missing_property in error.validator_value:
                    data[missing_property] = None
        
        # Validate again after fixes
        try:
            validate(instance=data, schema=schema)
            return data
        except ValidationError as e:
            logger.error(f""JSON schema validation failed after attempted fixes: {e}"")
            raise","Validate JSON data against a schema, attempting to fix issues.",???Validate and attempt to auto-correct JSON data against a given schema.???
1952,format_step_result_text,"def format_step_result_text(step_result: StepResult) -> str:
    
    tasks_str = ""\n"".join(
        f""  - {format_task_result_text(task)}"" for task in step_result.task_results
    )
    return STEP_RESULT_TEMPLATE.format(
        step_description=step_result.step.description,
        step_result=step_result.result,
        tasks_str=tasks_str,
    )",Format a step result as plain text for display,???Format step results into a structured text using a predefined template.???
1953,get_model_outputs,"def get_model_outputs(pdf_path):
    
    try:
        print(f""Attempting to process PDF: {pdf_path}"")
        print(f""File exists: {os.path.exists(pdf_path)}"")

        chatgpt_output = run_chatgpt(pdf_path)
        gemini_output = run_gemini(pdf_path)
        return chatgpt_output, gemini_output
    except Exception as e:
        print(f""Error getting model outputs for {pdf_path}: {str(e)}"")
        return f""Error: {str(e)}"", f""Error: {str(e)}""",Get outputs from both models for a given PDF.,???Process PDF to obtain outputs from ChatGPT and Gemini models???
1954,mock_sessions,"def mock_sessions():
    
    return [
        SessionModel(
            id=1,
            created_at=datetime.datetime(2025, 1, 1, 0, 0, 0),
            updated_at=datetime.datetime(2025, 1, 1, 0, 0, 0),
            start_time=datetime.datetime(2025, 1, 1, 0, 0, 0),
            command_line=""ra-aid test1"",
            program_version=""1.0.0"",
            machine_info={""os"": ""test""},
            status='test_status'
        ),
        SessionModel(
            id=2,
            created_at=datetime.datetime(2025, 1, 2, 0, 0, 0),
            updated_at=datetime.datetime(2025, 1, 2, 0, 0, 0),
            start_time=datetime.datetime(2025, 1, 2, 0, 0, 0),
            command_line=""ra-aid test2"",
            program_version=""1.0.0"",
            machine_info={""os"": ""test""},
            status='test_status'
        )
    ]",Return a list of mock sessions for testing.,???Generate mock session data with predefined attributes for testing purposes.???
1955,run_pytest_with_coverage,"def run_pytest_with_coverage():
    
    try:
        # Run pytest with coverage
        result = subprocess.run(
            [""pytest"", ""--cov=api"", ""-v""], capture_output=True, text=True, check=True
        )

        # Extract test results
        test_output = result.stdout
        passed_tests = len(re.findall(r""PASSED"", test_output))

        # Extract coverage from .coverage file
        coverage_output = subprocess.run(
            [""coverage"", ""report""], capture_output=True, text=True, check=True
        ).stdout

        # Extract total coverage percentage
        coverage_match = re.search(r""TOTAL\s+\d+\s+\d+\s+(\d+)%"", coverage_output)
        coverage_percentage = coverage_match.group(1) if coverage_match else ""0""

        return passed_tests, coverage_percentage
    except subprocess.CalledProcessError as e:
        print(f""Error running tests: {e}"")
        print(f""Output: {e.output}"")
        return 0, ""0""",Run pytest with coverage and return the results,"???Execute tests with coverage, returning passed count and coverage percentage.???"
1956,makedir,"def makedir(dir_path):
    
    is_success = False
    try:
        if not g_pathmgr.exists(dir_path):
            g_pathmgr.mkdirs(dir_path)
        is_success = True
    except BaseException:
        logging.info(f""Error creating directory: {dir_path}"")
    return is_success",Create the directory if it does not exist.,"???  
Create directory if it doesn't exist, logging errors.  
???"
1957,_calculate_momentum_indicators,"def _calculate_momentum_indicators(self, df: pd.DataFrame) -> Dict[str, TechnicalSignal]:
        
        signals = {}
        
        try:
            # RSI
            df['rsi'] = ta.momentum.rsi(df['close'])
            
            # Stochastic
            df['stoch_k'] = ta.momentum.stoch(df['high'], df['low'], df['close'])
            df['stoch_d'] = ta.momentum.stoch_signal(df['high'], df['low'], df['close'])
            
            # ROC
            df['roc'] = ta.momentum.roc(df['close'])
            
            # Ultimate Oscillator
            df['uo'] = ta.momentum.ultimate_oscillator(df['high'], df['low'], df['close'])
            
            # TSI
            df['tsi'] = ta.momentum.tsi(df['close'])
            
            # Momentum Signals
            signals['rsi'] = self._analyze_rsi(df)
            signals['stochastic'] = self._analyze_stochastic(df)
            signals['ultimate_oscillator'] = self._analyze_ultimate_oscillator(df)
            signals['tsi'] = self._analyze_tsi(df)
            
            return signals
            
        except Exception as e:
            logger.error(f""Error calculating momentum indicators: {e}"")
            raise",Calculate comprehensive momentum indicators.,???Compute and analyze various momentum indicators for financial data to generate trading signals.???
1958,initialize_sequence_parallel_group,"def initialize_sequence_parallel_group(sequence_parallel_size):
    
    rank = int(os.getenv(""RANK"", ""0""))
    world_size = int(os.getenv(""WORLD_SIZE"", ""1""))
    assert (
        world_size % sequence_parallel_size == 0
    ), ""world_size must be divisible by sequence_parallel_size, but got world_size: {}, sequence_parallel_size: {}"".format(
        world_size, sequence_parallel_size)
    nccl_info.sp_size = sequence_parallel_size
    nccl_info.global_rank = rank
    for i in range(num_sequence_parallel_groups):
        ranks = range(i * sequence_parallel_size, (i + 1) * sequence_parallel_size)
        group = dist.new_group(ranks)
        if rank in ranks:
            nccl_info.group = group
            nccl_info.rank_within_group = rank - i * sequence_parallel_size
            nccl_info.group_id = i",Initialize the sequence parallel group.,???Initialize parallel processing groups based on sequence size and rank environment variables.???
1959,embedder_config_dict,"def embedder_config_dict(self) -> dict[str, Any]:
		
		return {
			'provider': self.embedder_provider,
			'config': {'model': self.embedder_model, 'embedding_dims': self.embedder_dims},
		}",Returns the embedder configuration dictionary.,"???  
Generate a configuration dictionary for an embedding provider and model.  
???"
1960,msa_search,"def msa_search(seqs: Sequence[str], msa_res_dir: str) -> Sequence[str]:
    
    os.makedirs(msa_res_dir, exist_ok=True)
    tmp_fasta_fpath = os.path.join(msa_res_dir, f""tmp_{uuid.uuid4().hex}.fasta"")
    RequestParser.msa_search(
        seqs_pending_msa=seqs,
        tmp_fasta_fpath=tmp_fasta_fpath,
        msa_res_dir=msa_res_dir,
    )
    msa_res_subdirs = RequestParser.msa_postprocess(
        seqs_pending_msa=seqs,
        msa_res_dir=msa_res_dir,
    )
    return msa_res_subdirs",do msa search with mmseqs and return result subdirs.,???Perform multiple sequence alignment and post-process results in a specified directory.???
1961,prompt_contents,"def prompt_contents() -> dict[str, str]:
    
    current_dir: Path = Path(__file__).parent
    project_root: Path = current_dir.parent.parent

    raw_path: Path = project_root / ""packages/notte-core/src/notte_core/llms/prompts/data-extraction/all_data/user.md""
    relevant_path: Path = (
        project_root / ""packages/notte-core/src/notte_core/llms/prompts/data-extraction/only_main_content/user.md""
    )

    return {
        ""raw"": read_file_content(raw_path),
        ""relevant"": read_file_content(relevant_path),
    }",Fixture to load both prompt files,"???  
Retrieve and return file contents from specified markdown paths as a dictionary.  
???"
1962,register_routes,"def register_routes(app): # pylint: disable=redefined-outer-name
    

    @app.route('/', defaults={'path': ''})
    @app.route('/<path:path>')
    def serve_frontend(path):
        
        app.static_folder = '../static'
        if path == """" or not os.path.exists(os.path.join(app.static_folder, path)):
            return send_from_directory(app.static_folder, 'index.html')
        else:
            # Serve the requested file (static assets like JS, CSS, images, etc.)
            return send_from_directory(app.static_folder, path)",Register the application routes.,???Configure app routes to serve frontend files and default to index.html.???
1963,_log_images,"def _log_images(imgs_dict, group=""""):
    
    if run:
        for k, v in imgs_dict.items():
            run[f""{group}/{k}""].upload(File(v))",Log scalars to the NeptuneAI experiment logger.,???Uploads images from a dictionary to a specified group if a run is active.???
1964,add,"def add(profile, force=False):
    
    if profile_config_manager.get_profile(profile) is not None and not force:
        console.print(f""[bold red]Error:[/] Profile '{profile}' already exists."")
        console.print(""Use '--force' to overwrite the existing profile."")
        return

    profile_config_manager.new_profile(profile)

    console.print(f""\n[green]Profile '{profile}' added successfully.[/]\n"")
    console.print(f""You can now add servers to this profile with 'mcpm add --target %{profile} <server_name>'\n"")
    console.print(
        f""Or apply existing config to this profile with 'mcpm profile apply {profile} --server <server_name>'\n""
    )",Add a new MCPM profile.,???Add or overwrite a user profile with optional force flag.???
1965,get_default_config_path,"def get_default_config_path(app_name: str) -> pathlib.Path:
    
    return pathlib.Path(DEFAULT_CONFIGS_DIR) / f""{app_name}.json""",Get the path to the default config file for a specific app.,???Constructs a file path for an application's default configuration file.???
1966,validate_pdf_path,"def validate_pdf_path(pdf_path: str) -> bool:
    
    if not pdf_path:
        logger.error(""PDF path is required"")
        return False
    if not os.path.exists(pdf_path):
        logger.error(f""PDF file not found: {pdf_path}"")
        return False
    if not pdf_path.lower().endswith("".pdf""):
        logger.error(f""File must be a PDF: {pdf_path}"")
        return False
    return True",Validate the PDF file path.,???Check if a given file path is a valid and existing PDF.???
1967,read_pid_file,"def read_pid_file():
    
    if not PID_FILE.exists():
        return None

    try:
        pid = int(PID_FILE.read_text().strip())
        if is_process_running(pid):
            return pid
        else:
            # if the process is not running, delete the pid file
            remove_pid_file()
            return None
    except (ValueError, IOError) as e:
        logger.error(f""Error reading PID file: {e}"")
        return None","read the pid file and return the process id, if the file does not exist or the process is not running, return None",???Check if a process is running using a PID file and handle errors.???
1968,start_sampling,"def start_sampling(
    sampling_interval: typing.Optional[float] = None,
    include_objects_collected_by_major_gc: typing.Optional[bool] = None,
    include_objects_collected_by_minor_gc: typing.Optional[bool] = None,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    if sampling_interval is not None:
        params[""samplingInterval""] = sampling_interval
    if include_objects_collected_by_major_gc is not None:
        params[""includeObjectsCollectedByMajorGC""] = (
            include_objects_collected_by_major_gc
        )
    if include_objects_collected_by_minor_gc is not None:
        params[""includeObjectsCollectedByMinorGC""] = (
            include_objects_collected_by_minor_gc
        )
    cmd_dict: T_JSON_DICT = {
        ""method"": ""HeapProfiler.startSampling"",
        ""params"": params,
    }
    json = yield cmd_dict",:param sampling_interval: *(Optional)* Average sample interval in bytes.,???Initialize heap sampling with configurable interval and garbage collection options.???
1969,open_log_dashboard,"    
    # Try to refresh existing tabs first
    if refresh_dashboard():
        try:
            send_log(""Refreshed existing dashboard tab."", ""🔄"", log_type='status')
        except Exception:
            pass
        return
    
    # No active tabs, open a new one
    try:
        # Use open_new_tab for better control
        webbrowser.open_new_tab(url)
        try:
            send_log(f""Opened new dashboard in browser at {url}."", ""🌐"", log_type='status')
        except Exception:
            pass
    except Exception as e:
        try:
            send_log(f""Could not open browser automatically: {e}"", ""⚠️"", log_type='status')
        except Exception:
            pass",Opens or refreshes the dashboard in the browser.,"???Attempt to refresh dashboard; if unsuccessful, open a new browser tab and log actions.???"
1970,_set_active_plan,"def _set_active_plan(self, plan_id: Optional[str]) -> ToolResult:
        
        if not plan_id:
            raise ToolError(""Parameter `plan_id` is required for command: set_active"")

        if plan_id not in self.plans:
            raise ToolError(f""No plan found with ID: {plan_id}"")

        self._current_plan_id = plan_id
        return ToolResult(
            output=f""Plan '{plan_id}' is now the active plan.\n\n{self._format_plan(self.plans[plan_id])}""
        )",Set a plan as the active plan.,???Set the active plan by validating and updating the plan identifier.???
1971,delete_video,"def delete_video(collection_id, video_id):
    
    try:
        if not video_id:
            return {""message"": ""Video ID is required""}, 400
        videodb = VideoDBHandler(collection_id)
        result = videodb.delete_video(video_id)
        return result, 200
    except Exception as e:
        return {""message"": str(e)}, 500",Delete a video by ID from a specific collection.,"???Attempt to remove a video from a collection, handling errors gracefully.???"
1972,_join_url,"def _join_url(base: str, path: str) -> str:
    
    parsed = urlparse(base)
    base_path = parsed.path.strip(""/"")

    b_ver, p_ver = _extract_version(base_path), _extract_version(path)
    if b_ver and b_ver == p_ver:
        path = path[len(f""/{p_ver}"") :]

    pieces = [p for p in (base_path, path.lstrip(""/"")) if p]",Join *base* and *path* while avoiding duplicated version segments and double slashes.,???Constructs a URL by intelligently merging base and path components.???
1973,search_with_query,"def search_with_query(self, query, topk=-1):
        
        err_msg = """"

        if ""pageSize="" not in query:
            query += ""&pageSize=1000"" # max of api is 1000

        nctid_list = []
        for i in range(self.retry):
            if i > 0:
                print(f""Retry {i} times"")
            try:
                response = requests.get(query)
                if response.status_code != 200:
                    raise ConnectionError(f""CTGov connection error occurred - {response.text}"")

                # parse the response to format a list of trials to display
                output_df = self._parse_response(response.text, query, topk)
                if len(output_df) == 0:
                    nctid_list = []
                else:
                    nctid_list = output_df['NCT Number'].tolist()[:topk]

                break
            except:
                err_msg = traceback.format_exc()
                print(err_msg)

        if err_msg != """":
            raise RuntimeError(""A CTGOV API error occurred"")

        search_results = nctid_list
        return search_results",Search with query input to get the response of nctids.,???Fetch and parse clinical trial data from an API with retry logic and error handling.???
1974,_prompt_runner,"def _prompt_runner(self, prompts):
        
        matches = []
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future_to_index = {
                executor.submit(
                    self.llm.chat_completions,
                    [ContextMessage(content=prompt, role=RoleTypes.user).to_llm_msg()],
                    response_format={""type"": ""json_object""},
                ): i
                for i, prompt in enumerate(prompts)
            }
            for future in concurrent.futures.as_completed(future_to_index):
                try:
                    llm_response = future.result()
                    if not llm_response.status:
                        logger.error(f""LLM failed with {llm_response.content}"")
                        continue
                    output = json.loads(llm_response.content)
                    matches.extend(output[""sentences""])
                except Exception as e:
                    logger.exception(f""Error in getting matches: {e}"")
                    continue
        return matches",Run the prompts in parallel.,???Execute parallel chat completions and aggregate sentence matches???
1975,fetch_ref_arch_from_problem_id,"def fetch_ref_arch_from_problem_id(problem_id, problems, with_name=False) -> str:
    
    if isinstance(problem_id, str):
        problem_id = int(problem_id)

    problem_path = problems[problem_id]

    # problem_path = os.path.join(REPO_ROOT_PATH, problem)
    if not os.path.exists(problem_path):
        raise FileNotFoundError(f""Problem file at {problem_path} does not exist."")

    ref_arch = utils.read_file(problem_path)
    if not with_name:
        return ref_arch
    else:
        return (problem_path, ref_arch)",Fetches the reference architecture in string for a given problem_id,"???Retrieve reference architecture from problem ID, optionally including file path.???"
1976,_setup_routes,"def _setup_routes(self):
        

        @self.app.get(""/"", response_class=HTMLResponse)
        async def _root():
            return await self.root()

        # @self.app.post(""/{org}/{repo}/slack/events"")
        @self.app.post(""/slack/events"")
        async def _handle_slack_event(request: Request):
            return await self.handle_slack_event(request)

        # @self.app.post(""/{org}/{repo}/github/events"")
        @self.app.post(""/github/events"")
        async def _handle_github_event(request: Request):
            return await self.handle_github_event(request)

        # @self.app.post(""/{org}/{repo}/linear/events"")
        @self.app.post(""/linear/events"")
        async def handle_linear_event(request: Request):
            return await self.handle_linear_event(request)",Set up the FastAPI routes for different event types.,"???Define asynchronous HTTP routes for root, Slack, GitHub, and Linear event handling.???"
1977,start_tab_mirroring,"def start_tab_mirroring(
    sink_name: str,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""sinkName""] = sink_name
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Cast.startTabMirroring"",
        ""params"": params,
    }
    json = yield cmd_dict",Starts mirroring the tab to the sink.,???Initiate tab mirroring to specified sink using command dictionary???
1978,mock_group_chat_target_cls,"def mock_group_chat_target_cls(self) -> Generator[MagicMock, None, None]:
        
        with patch(""autogen.agentchat.group.targets.group_chat_target.GroupChatTarget"") as mock_cls:
            mock_instance = MagicMock()
            mock_cls.return_value = mock_instance

            # Mock the methods using configure_mock
            mock_instance.configure_mock(**{
                ""can_resolve_for_speaker_selection.return_value"": False,
                ""display_name.return_value"": ""a group chat"",
                ""normalized_name.return_value"": ""group_chat"",
                ""__str__.return_value"": ""Transfer to group chat"",
                ""needs_agent_wrapper.return_value"": True,
            })

            yield mock_cls",Create a mock GroupChatTarget class.,???Mock a group chat target class for testing with predefined method behaviors.???
1979,parse_mjcf,"def parse_mjcf(path: str) -> mujoco.MjModel:
    
    try:
        mj = mujoco.MjModel.from_xml_path(path)
    except Exception as e:
        raise Exception(
            f""There is an error in the MJCF file. Please fix the MJCF file error first before doing the conversion: {e}""
        ) from e
    return mj",Wraps MuJoCo's XML parsing to return an MjModel.,"???Parse MJCF file to create a MuJoCo model, handling errors.???"
1980,reset_processed_state,"def reset_processed_state():
    
    try:
        # Reset the state files for missing and upgrades
        reset_state_file(""eros"", ""processed_missing"")
        reset_state_file(""eros"", ""processed_upgrades"")
        
        eros_logger.info(""Successfully reset Eros processed state files"")
        return jsonify({""success"": True, ""message"": ""Successfully reset processed state""})
    except Exception as e:
        error_msg = f""Error resetting Eros state: {str(e)}""
        eros_logger.error(error_msg)
        return jsonify({""success"": False, ""message"": error_msg}), 500",Reset the processed state files for Eros,"???Reset and log the state of processed files, handling errors gracefully.???"
1981,get_media_queries,"def get_media_queries() -> typing.Generator[
    T_JSON_DICT, T_JSON_DICT, typing.List[CSSMedia]
]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""CSS.getMediaQueries"",
    }
    json = yield cmd_dict
    return [CSSMedia.from_json(i) for i in json[""medias""]]",Returns all media queries parsed by the rendering engine.,???Retrieve and process CSS media queries into structured CSSMedia objects.???
1982,_is_tool_call_complete,"def _is_tool_call_complete(self, tool_call: Dict[str, Any]) -> bool:
        
        try:
            if not tool_call.get(""function""):
                return False
            
            func = tool_call[""function""]
            name = func.get(""name"", """")
            args = func.get(""arguments"", """")
            
            # Tool call is complete if:
            # 1. Has a name
            # 2. Arguments appear to be valid JSON or empty
            if not name:
                return False
            
            # Try to parse arguments as JSON if not empty
            if args.strip():
                try:
                    json.loads(args)
                except json.JSONDecodeError:
                    # Still accumulating arguments
                    return False
            
            return True
            
        except Exception as e:
            logger.debug(f""Error checking tool call completeness: {e}"")
            return False",Check if a tool call has all required fields and appears complete.,???Determine if a tool call is complete by validating its function name and JSON arguments.???
1983,get_kubeadm_join_remote,"def get_kubeadm_join_remote(username, private_key, public_ip):
    
    generate_join_command = [
        ""ssh"",
        ""-i"",
        private_key,
        f""{username}@{public_ip}"",
        ""sudo kubeadm token create --print-join-command"",
    ]
    try:
        print(generate_join_command)
        result = run_command(generate_join_command, capture_output=True)
        return result
    except Exception as e:
        logger.error(
            f""Failed to retrieve kubeadm join command from {public_ip}: {str(e)}""
        )
        return None",SSH into the remote machine and generate the kubeadm join command.,???Execute SSH command to obtain Kubernetes join token from remote server???
1984,_execute_query,"def _execute_query(self, query: str, variables: Dict = None) -> Dict:
        
        response = requests.post(
            self.url,
            headers=self.headers,
            json={'query': query, 'variables': variables}
        )
        response.raise_for_status()
        return response.json()",Execute a GraphQL query using requests,"???  
Send a GraphQL query with variables and return the JSON response.  
???"
1985,_parse_flags_and_subocomand,"def _parse_flags_and_subocomand(self, args: List[str]) -> Tuple[Dict[str, str], List[str], str]:
        
        i = 0
        read_flags = {}
        # Parse all recognized flags at the start
        while i < len(args):
            if args[i] in self.flags:
                flag_name = args[i]
                if i + 1 >= len(args):
                    raise NoFlagValueError(f""Flag {flag_name} needs a value, but none provided."")
                read_flags[flag_name] = args[i + 1]
                i += 2
            else:
                # Once we encounter something that's not a recognized flag,
                # we assume it's the subcommand
                break

        if i >= len(args):
            raise NoSubcommandError(""No subcommand found after optional flags."")

        subcommand = args[i]
        i += 1

        # The rest of the arguments after the subcommand
        rest = args[i:]
        return read_flags, rest, subcommand",Reads the flags and subcommand from the args,"???Parse command-line arguments into flags, subcommand, and remaining inputs.???"
1986,mock_api_response,"def mock_api_response():
    
    def _mock_response(text_input, chunk_count=1):
        if isinstance(text_input, str):
            if not text_input.strip():
                return []
            # Single text input - SDPM might not chunk very short text
            if len(text_input.split()) < 3:
                return []  # SDPM typically requires more text
            return [{
                ""text"": text_input,
                ""token_count"": max(1, len(text_input.split())),
                ""start_index"": 0,
                ""end_index"": len(text_input)
            }]
        else:
            # Batch input
            results = []
            for text in text_input:
                if not text.strip() or len(text.split()) < 3:
                    results.append([])
                else:
                    results.append([{
                        ""text"": text,
                        ""token_count"": max(1, len(text.split())),
                        ""start_index"": 0,
                        ""end_index"": len(text)
                    }])
            return results
    return _mock_response",Mock successful API response.,???Simulate API response by processing text input into structured data with token counts.???
1987,get_api_value,"def get_api_value(cls, bot_platform_name: str) -> Optional[str]:
        
        reverse_mapping = {
            ""google_meet"": Platform.GOOGLE_MEET.value,
            ""zoom"": Platform.ZOOM.value,
            ""teams"": Platform.TEAMS.value
        }
        return reverse_mapping.get(bot_platform_name)",Gets the external API enum value from the internal bot platform name.,???Map platform names to their corresponding API values.???
1988,generate_ticket_number,"def generate_ticket_number():
    
    # Generate 8 random letters
    letters = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase, k=8))
    # Generate 3 random numbers
    numbers = ''.join(random.choices(string.digits, k=3))
    return f""{letters}{numbers}""",Generate a random ticket number in the format: 8 random characters + 3 numbers,???Create a unique identifier by combining random letters and numbers.???
1989,generate_and_upload_clip,"def generate_and_upload_clip(
    stream_name: str,
    start_time: datetime,
    end_time: datetime,
    bucket_name: str,
) -> str:
    
    s3_client = boto3.client(""s3"")

    # Generate unique filename
    filename = f""{stream_name}_{start_time.strftime('%Y%m%d_%H%M%S')}.mp4""

    response = get_clip(stream_name, start_time, end_time)

    # Upload to S3
    s3_key = f""clips/{filename}""
    s3_client.upload_fileobj(
        response[""Payload""],
        bucket_name,
        s3_key,
        ExtraArgs={""ContentType"": ""video/mp4""},
    )

    # Generate presigned URL (valid for 1 hour)
    presigned_url = s3_client.generate_presigned_url(
        ""get_object"", Params={""Bucket"": bucket_name, ""Key"": s3_key}, ExpiresIn=3600
    )

    return presigned_url",Generate a video clip and upload it to S3.,"???Generate video clip from stream, upload to S3, and return presigned URL.???"
1990,generate_query,"def generate_query(state: SummaryState):
    

    # Format the prompt
    query_writer_instructions_formatted = query_writer_instructions.format(research_topic=state.research_topic)

    # Generate a query
    llm_json_mode = ChatOpenAI(
        model=config.llm_model,
        openai_api_key=config.llm_api_key,
        openai_api_base=config.llm_api_base,
        temperature=0,
        model_kwargs={""response_format"": {""type"": ""json_object""}},
    )
    result = llm_json_mode.invoke(
        [
            SystemMessage(content=query_writer_instructions_formatted),
            HumanMessage(content=""Generate a query for web search:""),
        ]
    )
    query = json.loads(result.content)

    return {""search_query"": query[""query""]}",Generate a query for web search,???Generate a web search query using AI based on a given research topic.???
1991,temp_delimited_file,"def temp_delimited_file(self):
        
        with tempfile.NamedTemporaryFile(mode=""w+"", suffix="".txt"", delete=False) as tf:
            tf.write()
            tf_path = Path(tf.name)

        # Create the resource files in the same directory
        resource_path1 = tf_path.parent / ""some_resource.txt""
        resource_path2 = tf_path.parent / ""another_resource.txt""
        with open(resource_path1, ""w"", encoding=""utf-8"") as rf:
            rf.write(""This is some resource content"")
        with open(resource_path2, ""w"", encoding=""utf-8"") as rf:
            rf.write(""This is another resource content"")

        yield tf_path

        # Cleanup
        os.unlink(tf_path)
        if resource_path1.exists():
            os.unlink(resource_path1)
        if resource_path2.exists():
            os.unlink(resource_path2)",Create a temporary delimited template file for testing,"???Create temporary text file and associated resources, then clean up???"
1992,export_torchscript,"def export_torchscript(self, prefix=colorstr(""TorchScript:"")):
        
        LOGGER.info(f""\n{prefix} starting export with torch {torch.__version__}..."")
        f = self.file.with_suffix("".torchscript"")

        ts = torch.jit.trace(self.model, self.im, strict=False)
        extra_files = {""config.txt"": json.dumps(self.metadata)}  # torch._C.ExtraFilesMap()
            LOGGER.info(f""{prefix} optimizing for mobile..."")
            from torch.utils.mobile_optimizer import optimize_for_mobile

            optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)
        else:
            ts.save(str(f), _extra_files=extra_files)
        return f, None",YOLO TorchScript model export.,"???Export a PyTorch model to TorchScript format, optimizing for mobile deployment.???"
1993,save_bytes_to_video,"def save_bytes_to_video(video_bytes):
    
    if not isinstance(video_bytes, bytes):
        raise ValueError(f""Expected bytes input, got {type(video_bytes)}"")
        
    # Create a temporary file with .mp4 extension
    temp_dir = tempfile.gettempdir()
    temp_path = os.path.join(temp_dir, f""temp_{int(time.time())}_{os.urandom(4).hex()}.mp4"")
    
    try:
        # Write the bytes to the temporary file
        with open(temp_path, ""wb"") as f:
            f.write(video_bytes)
        
        # Ensure the file exists and has content
        if not os.path.exists(temp_path) or os.path.getsize(temp_path) == 0:
            raise ValueError(""Failed to save video file or file is empty"")
        
        return str(temp_path)  # Return string path as expected by Gradio
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.remove(temp_path)",Save video bytes to a temporary file and return the path,???Convert video byte data into a temporary MP4 file and return its path.???
1994,text_to_speech,"def text_to_speech(self, **kwargs) -> Optional[TextToSpeechModel]:
        
        model_id = self.defaults.default_text_to_speech_model
        if not model_id:
            return None
        model = self.get_model(model_id, **kwargs)
        assert model is None or isinstance(
            model, TextToSpeechModel
        ), f""Expected TextToSpeechModel but got {type(model)}""
        return model",Get the default text-to-speech model,???Retrieve and validate a text-to-speech model based on default settings.???
1995,reset_to_defaults,"def reset_to_defaults():
    
    db_session = get_db_session()

    # Import default settings from files
    try:
        # Create settings manager for the temporary config
        settings_mgr = get_settings_manager(db_session)
        # Import settings from default files
        settings_mgr.load_from_defaults_file()

        logger.info(""Successfully imported settings from default files"")

    except Exception:
        logger.exception(""Error importing default settings"")

        # Fallback to predefined settings if file import fails
        logger.info(""Falling back to predefined settings"")
        # Import here to avoid circular imports
        from ..database.migrations import setup_predefined_settings as setup_settings

        setup_settings(db_session)

    # Return success
    return jsonify(
        {
            ""status"": ""success"",
            ""message"": ""All settings have been reset to default values"",
        }
    )",Reset all settings to their default values,"???Reset system settings to defaults, handling errors with predefined fallbacks???"
1996,get_table_schema_query,"def get_table_schema_query(cls, schema_name: str, table: str) -> str:
        
        query = cls.load_sql(""get_table_schema"")
        return query.replace(""{schema_name}"", schema_name).replace(""{table}"", table)",Get a query to get the schema of a table.,"???  
Generates a SQL query to retrieve a table's schema by replacing placeholders with actual schema and table names.  
???"
1997,download_and_cache_file,"def download_and_cache_file(url, cache_dir=None):
    
    if cache_dir is None:
        cache_dir = Path.home() / "".cache"" / ""elo_plot""

    cache_dir = Path(cache_dir)
    cache_dir.mkdir(parents=True, exist_ok=True)

    # Create filename from URL hash
    url_hash = hashlib.sha256(url.encode()).hexdigest()[:12]
    file_name = url.split(""/"")[-1]
    cached_path = cache_dir / f""{url_hash}_{file_name}""

    if not cached_path.exists():
        response = requests.get(url, stream=True)
        response.raise_for_status()

        with open(cached_path, ""wb"") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

    return str(cached_path)",Download a file and cache it locally.,???Download file from URL and store in cache directory if not already cached.???
1998,update_subdir_edit_links,"def update_subdir_edit_links(subdir="""", docs_url=""""):
    
    if str(subdir[0]) == ""/"":
        subdir = str(subdir[0])[1:]
    html_files = (SITE / subdir).rglob(""*.html"")
    for html_file in tqdm(html_files, desc=""Processing subdir files""):
        with html_file.open(""r"", encoding=""utf-8"") as file:
            soup = BeautifulSoup(file, ""html.parser"")

        # Find the anchor tag and update its href attribute
        a_tag = soup.find(""a"", {""class"": ""md-content__button md-icon""})
        if a_tag and a_tag[""title""] == ""Edit this page"":
            a_tag[""href""] = f""{docs_url}{a_tag['href'].split(subdir)[-1]}""

        # Write the updated HTML back to the file
        with open(html_file, ""w"", encoding=""utf-8"") as file:
            file.write(str(soup))",Update the HTML head section of each file.,???Update HTML edit links in a subdirectory to point to a new documentation URL.???
1999,_load_clipboard_items_thread,"def _load_clipboard_items_thread(self):
        
        try:
            result = subprocess.run(
                [""cliphist"", ""list""], 
                capture_output=True, 
                check=True
            )
            # Decode stdout with error handling
            stdout_str = result.stdout.decode('utf-8', errors='replace')
            lines = stdout_str.strip().split('\n')
            new_items = []
            for line in lines:
                if not line or ""<meta http-equiv"" in line:
                    continue
                new_items.append(line)
            self._update_items(new_items)
        except subprocess.CalledProcessError as e:
            print(f""Error loading clipboard history: {e}"", file=sys.stderr)
        except Exception as e:
            print(f""Unexpected error: {e}"", file=sys.stderr)
        finally:
            self._loading = False
            if self._pending_updates:
                self._pending_updates = False
                GLib.idle_add(self._load_clipboard_items_thread)
        return False","Worker for loading clipboard items, now runs in main loop via idle_add","???Fetch and update clipboard history items, handling errors and scheduling updates.???"
2000,sanitize_dict,"def sanitize_dict(x):
    
    return {k.replace(""("", """").replace("")"", """"): float(v) for k, v in x.items()}",Sanitize dictionary keys by removing parentheses and converting values to floats.,???Transforms dictionary keys by removing parentheses and converts values to floats.???
2001,_get_tool_info,"def _get_tool_info(self) -> Dict[str, Any]:
        
        return {
            ""name"": self.name,
            ""action"": self.action,
            ""need_validation"": self.need_validation,
            ""schema_loaded"": self.action_schema is not None,
            ""toolset_initialized"": self.toolset is not None,
            ""execution_count"": self._execution_count,
            ""error_count"": self._error_count,
            ""last_execution"": self._last_execution_time.isoformat() if self._last_execution_time else None
        }",Get tool configuration for logging purposes.,"???Collects and returns tool metadata including name, action, and execution details.???"
2002,_display_analysis,"def _display_analysis(self, df: pd.DataFrame, symbol: str, indicator: str, period: int, timeframe: str) -> None:
        
        # Add pattern recognition
        patterns = self._detect_patterns(df)

        col1, col2 = st.columns([3, 1])

        with col1:
            if indicator == ""fibonacci"":
                self._plot_fibonacci(df, symbol)
            elif indicator == ""bollinger"":
                self._plot_bollinger(df, symbol, period)
            elif indicator == ""macd"":
                self._plot_macd(df, symbol)
            elif indicator == ""stoch"":
                self._plot_stochastic(df, symbol)
            else:
                self._plot_standard(df, symbol, indicator, period)

            if patterns:
                st.subheader(""📊 Pattern Analysis"")
                for pattern, confidence in patterns.items():
                    st.write(f""{pattern}: {confidence:.1%} confidence"")

        with col2:
            st.dataframe(df.tail(10).style.format(precision=2), use_container_width=True, height=400)

            # Add market statistics
            self._display_market_stats(df)",Display enhanced technical analysis with pattern recognition,???Visualize financial data with technical indicators and pattern analysis in a dashboard???
2003,filter_dataclass_fields,"def filter_dataclass_fields(data_dict, dataclass_type):
    
    valid_fields = {f.name for f in dataclass_type.__dataclass_fields__.values()}
    return {k: v for k, v in data_dict.items() if k in valid_fields}",Filter a dictionary to only include keys that are fields in the dataclass.,???Filter dictionary to match dataclass field names for validation.???
2004,_initialize_web3,"def _initialize_web3(self) -> None:
        
        if not self._web3:
            for attempt in range(3):
                try:
                    self._web3 = Web3(Web3.HTTPProvider(self.rpc_url))
                    self._web3.middleware_onion.inject(geth_poa_middleware, layer=0)
                    
                    if not self._web3.is_connected():
                        raise EthereumConnectionError(""Failed to connect to Ethereum network"")
                    
                    chain_id = self._web3.eth.chain_id
                    if chain_id != self.chain_id:
                        raise EthereumConnectionError(f""Connected to wrong chain. Expected {self.chain_id}, got {chain_id}"")
                        
                    logger.info(f""Connected to {self.network} network with chain ID: {chain_id}"")
                    break
                    
                except Exception as e:
                    if attempt == 2:
                        raise EthereumConnectionError(f""Failed to initialize Web3 after 3 attempts: {str(e)}"")
                    logger.warning(f""Web3 initialization attempt {attempt + 1} failed: {str(e)}"")
                    time.sleep(1)",Initialize Web3 connection with retry logic,???Initialize and verify a Web3 connection to the specified Ethereum network with retries???
2005,create_nav_menu_yaml,"def create_nav_menu_yaml(nav_items: list, save: bool = False):
    
    nav_tree = nested_dict()

    for item_str in nav_items:
        item = Path(item_str)
        parts = item.parts
        current_level = nav_tree[""reference""]
        for part in parts[2:-1]:  # skip the first two parts (docs and reference) and the last part (filename)
            current_level = current_level[part]

        md_file_name = parts[-1].replace("".md"", """")
        current_level[md_file_name] = item

    nav_tree_sorted = sort_nested_dict(nav_tree)

    def _dict_to_yaml(d, level=0):
        
        yaml_str = """"
        indent = ""  "" * level
        for k, v in d.items():
            if isinstance(v, dict):
                yaml_str += f""{indent}- {k}:\n{_dict_to_yaml(v, level + 1)}""
            else:
                yaml_str += f""{indent}- {k}: {str(v).replace('docs/en/', '')}\n""
        return yaml_str

    # Print updated YAML reference section
    print(""Scan complete, new mkdocs.yaml reference section is:\n\n"", _dict_to_yaml(nav_tree_sorted))

    # Save new YAML reference section
    if save:
        (PACKAGE_DIR.parent / ""nav_menu_updated.yml"").write_text(_dict_to_yaml(nav_tree_sorted))",Creates a YAML file for the navigation menu based on the provided list of items.,???Generate and optionally save a YAML navigation menu from a list of file paths.???
2006,display,"def display(self) -> Optional[Audio]:
        
        try:
            # Convert to bytes
            audio_bytes = self.to_bytes(format='wav')

            # Create and display audio widget
            audio_widget = Audio(audio_bytes, rate=self.sample_rate, autoplay=False)
            display(audio_widget)
            return audio_widget
        except Exception as e:
            print(f""Could not display audio widget: {str(e)}"")
            print(""Try using .play() method instead"")
            return None",Display audio player in Jupyter notebook.,"???Convert audio to bytes and display as widget, handling exceptions.???"
2007,to_txt,"def to_txt(self, save_path=None, layout: str = ""原文在上"") -> str:
        
        result = []
        for seg in self.segments:
            # 检查是否有换行符
            original = seg.text
            translated = seg.translated_text

            # 根据字幕类型组织文本
            if layout == ""原文在上"":
                text = f""{original}\n{translated}"" if translated else original
            elif layout == ""译文在上"":
                text = f""{translated}\n{original}"" if translated else original
            elif layout == ""仅原文"":
                text = original
            elif layout == ""仅译文"":
                text = translated if translated else original
            else:
                text = seg.transcript
            result.append(text)
        text = ""\n"".join(result)
        if save_path:
            # 处理Windows长路径问题
            save_path = handle_long_path(save_path)

            with open(save_path, ""w"", encoding=""utf-8"") as f:
                f.write(""\n"".join(result))
        return text",Convert to plain text subtitle format (without timestamps),???Convert and save text segments with translation layout options to a file.???
2008,on_train_end,"def on_train_end(trainer):
    
    if live:
        # At the end log the best metrics. It runs validator on the best model internally.
        all_metrics = {**trainer.label_loss_items(trainer.tloss, prefix=""train""), **trainer.metrics, **trainer.lr}
        for metric, value in all_metrics.items():
            live.log_metric(metric, value, plot=False)

        _log_plots(trainer.plots, ""val"")
        _log_plots(trainer.validator.plots, ""val"")
        _log_confusion_matrix(trainer.validator)

        if trainer.best.exists():
            live.log_artifact(trainer.best, copy=True, type=""model"")

        live.end()","Logs the best metrics, plots, and confusion matrix at the end of training if DVCLive is active.",???Log and visualize training metrics and artifacts upon completion???
2009,initialize_and_check_models,"def initialize_and_check_models(self) -> Generator[dict, None, None]:
        
        try:
            if self.config.provider == ModelProvider.OLLAMA:
                if not self.model_manager:
                    raise ValueError(
                        ""Ollama model manager not initialized but provider is Ollama""
                    )

                # Check Ollama server health
                self.model_manager.check_server_health()

                # Verify and potentially pull required models
                required_models = [self.config.model_name, self.config.embedding_model]
                for model_name in required_models:
                    yield from self.model_manager.verify_and_pull_model(model_name)
            else:
                yield {
                    ""type"": ""model_status"",
                    ""status"": ""success"",
                    ""message"": ""Using HuggingFace provider"",
                }
        except Exception as e:
            self.logger.error(f""Model initialization failed: {str(e)}"", exc_info=True)
            yield {""type"": ""model_status"", ""status"": ""error"", ""error"": str(e)}
            raise","Verify model availability and health, pulling models if needed.","???Initialize and verify models based on provider configuration, handling errors gracefully.???"
2010,transfer_money,"def transfer_money(from_account_id: str, to_account_id: str, amount: int, **kwargs) -> None:
            

            logger.info(f""Transferring {amount} from {from_account_id} account {to_account_id}"")

            context[""current_task""] = """"
            context[""task_result""] = f""{amount}$ was successfully transferred""

            return f""Transferred {amount} to account {to_account_id}""",Useful for transferring money between accounts.,???Log and confirm monetary transfer between accounts???
2011,update_selection,"def update_selection():
    
    selected = st.session_state['multiselect']
    kb_df = st.session_state.get('kb_df')
    
    # Update current knowledge base config ID
    if selected:
        selected_row = kb_df.apply(
            lambda row: f""{row['knowledge_base_name']}({row['knowledge_base_id']})"", 
            axis=1
        ) == selected[0]
        st.session_state['current_kb_config_id'] = kb_df[selected_row].iloc[0]['kb_config_id']
    else:
        st.session_state['current_kb_config_id'] = None
    
    st.session_state['selected_options'] = selected
    
    # Update filtered options if knowledge base dataframe exists
    if kb_df is not None:
        st.session_state['filtered_options'] = kb_df.apply(
            lambda row: f""{row['knowledge_base_name']}({row['knowledge_base_id']})"", 
            axis=1
        ).values
        st.session_state['selected_options'] = selected
    close_existing_searcher()",Update the selected knowledge base configuration and options in session state.,???Update session state with selected knowledge base configuration and options???
2012,visualize_action,"def visualize_action(self, x: int, y: int, img_base64: str) -> None:
        
        if (
            not self.agent.save_trajectory
            or not hasattr(self.agent, ""experiment_manager"")
            or not self.agent.experiment_manager
        ):
            return

        try:
            # Use the visualization utility
            img = visualize_click(x, y, img_base64)

            # Save the visualization
            self.agent.experiment_manager.save_action_visualization(img, ""click"", f""x{x}_y{y}"")
        except Exception as e:
            logger.error(f""Error visualizing action: {str(e)}"")",Visualize a click action by drawing on the screenshot.,???Visualize and save user interaction on image if conditions met???
2013,_get_complexity_emoji,"def _get_complexity_emoji(self, complexity: TemplateComplexity) -> str:
        
        emoji_map = {
            TemplateComplexity.BASIC: ""🟢"",
            TemplateComplexity.INTERMEDIATE: ""🟡"",
            TemplateComplexity.ADVANCED: ""🔴"",
        }
        return emoji_map.get(complexity, ""⚪"")",Get emoji for complexity level.,"???  
Map template complexity levels to corresponding emoji symbols.  
???"
2014,_remove_pv_finalizers,"def _remove_pv_finalizers(self, pv_name: str):
        
        # Patch the PersistentVolume to remove finalizers if it is stuck
        patch_command = (
            f'kubectl patch pv {pv_name} -p \'{{""metadata"":{{""finalizers"":null}}}}\''
        )
        _ = self.kubectl.exec_command(patch_command)",Remove finalizers from the PersistentVolume to prevent it from being stuck in a 'Terminating' state.,???Remove stuck finalizers from a PersistentVolume using a kubectl patch command.???
2015,schema_with_invalid_reference,"def schema_with_invalid_reference() -> dict[str, Any]:
    
    return {
        ""openapi"": ""3.1.0"",
        ""info"": {""title"": ""Invalid Reference API"", ""version"": ""1.0.0""},
        ""paths"": {
            ""/broken-ref"": {
                ""get"": {
                    ""summary"": ""Endpoint with broken reference"",
                    ""operationId"": ""brokenRef"",
                    ""parameters"": [
                        {""$ref"": ""#/components/parameters/NonExistentParam""}
                    ],
                    ""responses"": {""200"": {""description"": ""Something""}},
                }
            }
        },
        ""components"": {
            ""parameters"": {}  # Empty parameters object to ensure the reference is broken
        },
    }",Fixture that returns a schema with an invalid reference.,???Generate an OpenAPI schema with a deliberately broken parameter reference.???
2016,is_empty,"def is_empty(self) -> bool:
        
        try:
            return self.vector_store.count_documents() == 0
        except UnexpectedResponse as e:
            if e.status_code == 404:  # Collection doesn't exist
                return True
            raise QdrantError(f""Failed to check if Qdrant collection is empty: {e}"") from e",Checks if the Qdrant collection is empty.,"???Determine if a document collection is empty, handling errors.???"
2017,tokenizer,"def tokenizer() -> Tokenizer:
    
    try:
        return Tokenizer.from_pretrained(""gpt2"")
    except (OSError, ValueError) as e:
        pytest.skip(f""Could not load tokenizers tokenizer: {e}"")",Fixture that returns a GPT-2 tokenizer from the tokenizers library.,"???  
Load a pretrained GPT-2 tokenizer, skipping tests if loading fails.  
???"
2018,ensure_project_selected,"def ensure_project_selected() -> str:
    
    try:
        project_id = subprocess.check_output(
            [""gcloud"", ""config"", ""get-value"", ""project""], text=True
        ).strip()

        if not project_id:
            print(""\nNo Google Cloud project selected."")
            print(""Available projects:"")

            # List available projects
            subprocess.run([""gcloud"", ""projects"", ""list""], check=True)

            # Prompt for project ID
            project_id = input(""\nEnter the project ID you want to use: "").strip()

            # Set the project
            subprocess.run(
                [""gcloud"", ""config"", ""set"", ""project"", project_id], check=True
            )

        return project_id

    except subprocess.CalledProcessError as e:
        raise Exception(f""Failed to get or set project: {e!s}"") from e",Ensure a Google Cloud project is selected.,???Ensure a Google Cloud project is selected or prompt user to choose one???
2019,set_show_grid_overlays,"def set_show_grid_overlays(
    grid_node_highlight_configs: typing.List[GridNodeHighlightConfig],
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""gridNodeHighlightConfigs""] = [
        i.to_json() for i in grid_node_highlight_configs
    ]
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Overlay.setShowGridOverlays"",
        ""params"": params,
    }
    json = yield cmd_dict",Highlight multiple elements with the CSS Grid overlay.,???Configure grid overlay highlights for visualization using JSON commands???
2020,disassociate_collaborators,"def disassociate_collaborators(self, orchestrator_id, collaborations):
        
        for collab in collaborations:
            try:
                print(f""Disassociating collaborator {collab['name']} (ID: {collab['id']}) from version {collab['version']}"")
                self.bedrock_agent.disassociate_agent_collaborator(
                    agentId=orchestrator_id,
                    agentVersion=collab['version'],
                    collaboratorId=collab['id']
                )
            except Exception as e:
                print(f""Error disassociating collaborator: {str(e)}"")",Disassociate all collaborators from all versions of an orchestrator,"??? Disassociate specified collaborators from an orchestrator version, handling errors ???"
2021,init_random,"def init_random(self):
        
        self._copy_non_transformer_params()

        # copy pretrained layers
        for i in range(self.pretrained_layers):
            for key in self.pretrained_state:
                if f""blocks.{i}."" in key:
                    new_key = key.replace(f""blocks.{i}."", f""blocks.{i}."")
                    self.target_state[new_key] = self.pretrained_state[key]

        # keep the remaining layers random init (do not process, use the model original init)
        self.target_model.load_state_dict(self.target_state)
        return self.target_model","partial random init strategy: keep the first N layers, random init the remaining layers",???Initialize model by copying pretrained layers and randomizing others???
2022,get_extra_params,"def get_extra_params(self) -> Dict[str, Any]:
        
        standard_fields: Set[str] = {
            ""model"",
            ""messages"",
            ""temperature"",
            ""top_p"",
            ""max_tokens"",
            ""max_completion_tokens"",
            ""stream"",
            ""seed"",
            ""stop"",
            ""presence_penalty"",
            ""frequency_penalty"",
            ""logit_bias"",
            ""logprobs"",
            ""top_logprobs"",
            ""n"",
            ""tools"",
            ""tool_choice"",
            ""parallel_tool_calls"",
            ""stream_options"",
            ""response_format"",
            ""user"",
            ""metadata"",
            ""modalities"",
            ""store"",
            ""draft-model"",
        }
        return {k: v for k, v in self.model_dump().items() if k not in standard_fields}",Get all extra parameters that aren't part of the standard OpenAI API.,???Extracts non-standard parameters from a model's configuration dictionary.???
2023,show,"def show():
    
    path = config_manager.GLOBAL_CONFIG_PATH.expanduser().resolve()
    console.print(f""[bold]Config file path:[/bold] {path}"")
    if path.exists():
        with open(path) as f:
            config = yaml.safe_load(f) or {""installed_toolboxes"": []}
        console.print(""[bold cyan]Current Configuration:[/bold cyan]"")
        console.print(yaml.safe_dump(config, default_flow_style=False))
    else:
        console.print(f""[yellow]No config file found at {path}[/yellow]"")",Show the quantalogic-config.yaml file.,"???Display configuration file path and contents if it exists, otherwise notify absence???"
2024,remove_small_regions,"def remove_small_regions(mask: np.ndarray, area_thresh: float, mode: str) -> Tuple[np.ndarray, bool]:
    
    import cv2  # type: ignore

    assert mode in {""holes"", ""islands""}, f""Provided mode {mode} is invalid""
    correct_holes = mode == ""holes""
    working_mask = (correct_holes ^ mask).astype(np.uint8)
    n_labels, regions, stats, _ = cv2.connectedComponentsWithStats(working_mask, 8)
    sizes = stats[:, -1][1:]  # Row 0 is background label
    small_regions = [i + 1 for i, s in enumerate(sizes) if s < area_thresh]
    if not small_regions:
        return mask, False
    fill_labels = [0] + small_regions
    if not correct_holes:
        # If every region is below threshold, keep largest
        fill_labels = [i for i in range(n_labels) if i not in fill_labels] or [int(np.argmax(sizes)) + 1]
    mask = np.isin(regions, fill_labels)
    return mask, True",Removes small disconnected regions or holes in a mask based on area threshold and mode.,???Remove small regions from a mask based on area threshold and mode.???
2025,extract_text_from_pdf,"def extract_text_from_pdf(pdf_bytes):
    
    try:
        doc = fitz.open(stream=pdf_bytes, filetype=""pdf"")
        text = """"
        for page in doc:
            text += page.get_text()
        doc.close()
        return text
    except Exception as e:
        st.error(f""Error processing PDF: {str(e)}"")
        return None",Extract text from a PDF file using PyMuPDF.,"???Extracts and returns text content from a PDF byte stream, handling errors.???"
2026,get_sorting,"def get_sorting(request: HttpRequest):
        

        profile = request.user.profile

        sorting_dict = {
            'Newest': '-creation_date',
            'Oldest': 'creation_date',
            'Name_asc': Lower('name'),
            'Name_desc': Lower('name').desc(),
        }

        return sorting_dict[profile.shared_pdf_sorting]",Get the sorting of the overview page.,???Determine sorting order for user profile's PDF list based on preferences.???
2027,_initialize_hub_repo,"def _initialize_hub_repo(repo_id: str, token: bool | str | None, private: bool | None) -> str:
        
        repo_url = create_repo(
            repo_id=repo_id,
            token=token,
            private=private,
            exist_ok=True,
            repo_type=""space"",
            space_sdk=""gradio"",
        )
        metadata_update(repo_url.repo_id, {""tags"": [""smolagents"", ""tool""]}, repo_type=""space"", token=token)
        return repo_url.repo_id",Initialize repository on Hugging Face Hub.,???Initialize and configure a repository with metadata and access settings.???
2028,_extract_classification_attributes_for_tracing,"def _extract_classification_attributes_for_tracing(
        self, classification: LLMIntentClassificationResult, prefix: str = """"
    ) -> dict:
        
        if not self.context.tracing_enabled:
            return {}

        attr_prefix = f""{prefix}."" if prefix else """"
        attributes = {
            f""{attr_prefix}intent"": classification.intent,
            f""{attr_prefix}confidence"": classification.confidence,
        }

        if classification.reasoning:
            attributes[f""{attr_prefix}reasoning""] = classification.reasoning
        if classification.p_score is not None:
            attributes[f""{attr_prefix}p_score""] = classification.p_score

        if classification.extracted_entities:
            for (
                entity_name,
                entity_value,
            ) in classification.extracted_entities.items():
                attributes[f""{attr_prefix}extracted_entities.{entity_name}""] = (
                    entity_value
                )

        return attributes",Extract attributes from the classification result for tracing.,???Extracts and returns classification attributes for tracing if enabled.???
2029,get_token_from_session,"def get_token_from_session(session_path):
    
    if not os.path.exists(session_path):
        return None
        
    try:
        # try to find all possible session files
        for file in os.listdir(session_path):
            if file.endswith('.log'):
                file_path = os.path.join(session_path, file)
                try:
                    with open(file_path, 'rb') as f:
                        content = f.read().decode('utf-8', errors='ignore')
                        # find token pattern
                        token_match = re.search(r'""token"":""([^""]+)""', content)
                        if token_match:
                            return token_match.group(1)
                except:
                    continue
    except Exception as e:
        logger.error(f""get token from session failed: {str(e)}"")
    
    return None",get token from session,???Extracts a token from log files within a specified session directory.???
2030,enable,"def enable(
    disable_rejection_delay: typing.Optional[bool] = None,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    if disable_rejection_delay is not None:
        params[""disableRejectionDelay""] = disable_rejection_delay
    cmd_dict: T_JSON_DICT = {
        ""method"": ""FedCm.enable"",
        ""params"": params,
    }
    json = yield cmd_dict",":param disable_rejection_delay: *(Optional)* Allows callers to disable the promise rejection delay that would normally happen, if this is unimportant to what's being tested.",???Enable a feature with optional delay setting using a command generator.???
2031,get_token_by_ticker,"def get_token_by_ticker(self, ticker: str) -> Optional[str]:
        
        try:
            if ticker.lower() in [""s"", ""S""]:
                return ""0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE""
                
            response = requests.get(
            )
            response.raise_for_status()

            data = response.json()
            if not data.get('pairs'):
                return None

            sonic_pairs = [
                pair for pair in data[""pairs""] if pair.get(""chainId"") == ""sonic""
            ]
            sonic_pairs.sort(key=lambda x: x.get(""fdv"", 0), reverse=True)

            sonic_pairs = [
                pair
                for pair in sonic_pairs
                if pair.get(""baseToken"", {}).get(""symbol"", """").lower() == ticker.lower()
            ]

            if sonic_pairs:
                return sonic_pairs[0].get(""baseToken"", {}).get(""address"")
            return None

        except Exception as error:
            logger.error(f""Error fetching token address: {str(error)}"")
            return None",Get token address by ticker symbol,"???Fetches and returns the token address for a given ticker symbol, handling errors gracefully.???"
2032,get_instruction_args,"def get_instruction_args(self):
        
        return {
            ""capital_frequency"": self._frequency,
            ""capital_relation"": self._comparison_relation,
        }",Returns the keyword args of build description.,???Return a dictionary mapping frequency and relation attributes to keys.???
2033,generate_email,"def generate_email(self, length=4):
        
        length = random.randint(0, length)  # Generate a random int between 0 and length
        timestamp = str(int(time.time()))[-length:]  # Use the last length digits of timestamp
        return f""{self.default_first_name}{timestamp}@{self.domain}""",Generate a random email address,???Generate a unique email using a random timestamp suffix and default domain.???
2034,data_analysis,"def data_analysis(
    directory: Path = typer.Argument(
        Path("".""),
        help=""Directory where data analysis examples will be created (creates 'data-analysis' subdirectory with mount-point)"",
    ),
    force: bool = typer.Option(False, ""--force"", ""-f"", help=""Force overwrite existing files""),
) -> None:
    
    target_dir = directory.resolve()
    if not target_dir.exists():
        target_dir.mkdir(parents=True)
        console.print(f""Created directory: {target_dir}"")

    created = copy_example_files(""data-analysis"", target_dir, force)
    _show_completion_message(""data-analysis"", created)",Create data analysis examples with sample dataset.,"???Create a data analysis directory and copy example files, optionally overwriting existing ones.???"
2035,renew_resnet_paths,"def renew_resnet_paths(old_list, n_shave_prefix_segments=0):
    
    mapping = []
    for old_item in old_list:
        new_item = old_item.replace(""in_layers.0"", ""norm1"")
        new_item = new_item.replace(""in_layers.2"", ""conv1"")

        new_item = new_item.replace(""out_layers.0"", ""norm2"")
        new_item = new_item.replace(""out_layers.3"", ""conv2"")

        new_item = new_item.replace(""emb_layers.1"", ""time_emb_proj"")
        new_item = new_item.replace(""skip_connection"", ""conv_shortcut"")

        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)

        mapping.append({""old"": old_item, ""new"": new_item})

    return mapping",Updates paths inside resnets to the new naming scheme (local renaming),???Transform ResNet path strings by updating segment names and optionally trimming prefixes.???
2036,start_scheduler,"def start_scheduler():
    
    global scheduler_thread
    
    if scheduler_thread and scheduler_thread.is_alive():
        scheduler_logger.info(""Scheduler already running"")
        return
    
    # Reset the stop event
    stop_event.clear()
    
    # Create and start the scheduler thread
    scheduler_thread = threading.Thread(target=scheduler_loop, name=""SchedulerEngine"", daemon=True)
    scheduler_thread.start()
    
    # Add a startup entry to the history
    startup_entry = {
        ""id"": ""system"",
        ""action"": ""startup"",
        ""app"": ""scheduler""
    }
    add_to_history(startup_entry, ""info"", ""Scheduler engine started"")
    
    scheduler_logger.info(f""Scheduler engine started. Thread is alive: {scheduler_thread.is_alive()}"")
    return True",Start the scheduler engine,"???Initialize and start a scheduler engine, logging its startup status.???"
2037,_on_chat_future_done,"def _on_chat_future_done(chat_future: asyncio.Future, chat_id: int):
    
    logger.debug(f""Update chat {chat_id} result on task completion."" + __system_now_str())
    chat_result = chat_future.result()
    chat_result.chat_id = chat_id",Update ChatResult when async Task for Chat is completed.,???Log and update chat result upon task completion.???
2038,rearrange_micro_batches,"def rearrange_micro_batches(batch: TensorDict, max_token_len, dp_group=None):
    
    # this is per local micro_bsz
    max_seq_len = batch['attention_mask'].shape[-1]
    assert max_token_len >= max_seq_len, \
        f'max_token_len must be greater than the sequence length. Got {max_token_len=} and {max_seq_len=}'

    seq_len_effective: torch.Tensor = batch['attention_mask'].sum(dim=1)
    total_seqlen = seq_len_effective.sum().item()
    num_micro_batches = ceildiv(total_seqlen, max_token_len)
    if dist.is_initialized():
        num_micro_batches = torch.tensor([num_micro_batches], device='cuda')
        dist.all_reduce(num_micro_batches, op=dist.ReduceOp.MAX, group=dp_group)
        num_micro_batches = num_micro_batches.cpu().item()

    seq_len_effective = seq_len_effective.tolist()
    assert num_micro_batches <= len(seq_len_effective)

    micro_bsz_idx = get_seqlen_balanced_partitions(seq_len_effective, num_micro_batches, equal_size=False)

    micro_batches = []

    for partition in micro_bsz_idx:
        curr_micro_batch = []
        for idx in partition:
            curr_micro_batch.append(batch[idx:idx + 1])
        curr_micro_batch = torch.cat(curr_micro_batch)

        micro_batches.append(curr_micro_batch)

    return micro_batches, micro_bsz_idx","Split the batch into a list of micro_batches, where the max_token_len is smaller than max_token_len and the number of valid tokens in each micro batch is well balanced.",???Reorganize tensor batches into balanced micro-batches based on token length.???
2039,get_all_documents,"def get_all_documents(self, user_id: str) -> List[Document]:
        
        session = None
        try:
            session = self._Session()
            query = session.query(ChunkEmbedding).filter(ChunkEmbedding.user_id == user_id)
                
            chunks = query.all()
            # Modify this to ensure document_id is in metadata
            return [
                Document(
                    page_content=chunk.data[""page_content""],
                    metadata={
                        **chunk.data.get(""metadata"", {}),
                        ""document_id"": chunk.document_id  # Explicitly add document_id
                    }
                ) 
                for chunk in chunks
            ]
        finally:
            if session:
                session.close()","Get all documents from the store, optionally filtered by user_id.","???Retrieve all documents for a user, ensuring document IDs are included in metadata.???"
2040,together_fn,"def together_fn(self, history, verbose=False):
        
        from together import Together
        client = Together(api_key=self.api_key)
        if self.is_local:
            raise Exception(""Together AI is not available for local use. Change config.ini"")

        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=history,
            )
            if response is None:
                raise Exception(""Together AI response is empty."")
            thought = response.choices[0].message.content
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""Together AI API error: {str(e)}"") from e",Use together AI for completion,???Invoke Together AI API for chat completion and handle exceptions???
2041,dashboard,"def dashboard(db_path: Path) -> None:
    
    db = TelemetryDB(db_path)
    records = db.fetch_all()
    if not records:
        click.echo(""No telemetry data found."")
        db.close()
        return

    click.echo(""Telemetry Dashboard:"")
    header = (
        f""{'Timestamp':<20} {'Tokens':>6} {'Cost':>7} {'Latency':>8} {'Guardrails':>10}""
    )
    click.echo(header)
    for row in records:
        ts = row[""timestamp""][:19]
        line = (
            f""{ts:<20} ""
            f""{row['tokens']:>6} ""
            f""${row['cost']:.2f} ""
            f""{row['latency']:>8.2f} ""
            f""{row['guardrail_hits']:>10}""
        )
    click.echo(line)
    db.close()",Display a simple telemetry dashboard.,???Display telemetry data in a formatted dashboard if records exist???
2042,on_train_end,"def on_train_end(self, args, state, control, **kwargs):
        
        if state.is_world_process_zero and self.use_peft:
            # Wait for all pending pushes to complete
            logger.info(f""\nCleaned up for lora models."")
            for future, dir_path in self.pending_futures:
                future.result()  # Wait for completion
                try:
                    shutil.rmtree(dir_path)
                    logger.info(f""\nCleaned up merged model directory: {dir_path}\n"")
                except Exception as e:
                    logger.error(f""\nFailed to clean up directory {dir_path}: {e}\n"")

            self.pending_futures = []",Make sure to clean up any remaining directories at the end of training.,???Clean up resources and directories after training with pending tasks completion.???
2043,_log_step_context,"def _log_step_context(self, current_page, browser_state_summary) -> None:
		
		url_short = current_page.url[:50] + '...' if len(current_page.url) > 50 else current_page.url
		interactive_count = len(browser_state_summary.selector_map) if browser_state_summary else 0
		self.logger.info(
			f'📍 Step {self.state.n_steps}: Evaluating page with {interactive_count} interactive elements on: {url_short}'
		)",Log step context information,???Log browser step details with page URL and interactive element count.???
2044,memory_id,"def memory_id(self) -> str:
        
        return compute_obj_hash(
            self.model_dump(exclude={""previous_task_inputs"", ""current_datetime"", ""base_input""}, exclude_none=True),
        )",A label identifiying the input by ignoring the previous task inputs and number of examples.,???Generate a unique identifier by hashing a filtered model representation.???
2045,solve_with_structure,"def solve_with_structure(self, problem: str, reasoning_structure: Dict[str, Any]) -> str:
        
        
        structure_text = json.dumps(reasoning_structure, indent=2)
        
        solve_prompt = f

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{""role"": ""user"", ""content"": solve_prompt}],
            max_tokens=self.max_tokens,
            temperature=0.7
        )
        
        self.completion_tokens += response.usage.completion_tokens
        
        return response.choices[0].message.content.strip()",Stage 2: Use the discovered reasoning structure to solve a specific problem.,??? Generate solution using structured reasoning and AI model interaction ???
2046,notte_stop_session,"def notte_stop_session() -> str:
    
    _session = get_session()
    _session.stop()
    global session
    session = None
    return f""Session {_session.session_id} stopped""",Stop the current Notte session,???This function stops the current session and resets the session variable.???
2047,plot_training_samples,"def plot_training_samples(self, batch, ni):
        
        images = batch[""img""]
        kpts = batch[""keypoints""]
        cls = batch[""cls""].squeeze(-1)
        bboxes = batch[""bboxes""]
        paths = batch[""im_file""]
        batch_idx = batch[""batch_idx""]
        plot_images(
            images,
            batch_idx,
            cls,
            bboxes,
            kpts=kpts,
            paths=paths,
            fname=self.save_dir / f""train_batch{ni}.jpg"",
            on_plot=self.on_plot,
        )","Plot a batch of training samples with annotated class labels, bounding boxes, and keypoints.","???Visualize training data samples with images, keypoints, and bounding boxes???"
2048,_save_report_to_md,"def _save_report_to_md(report: str, output_dir: Path):
    
    report_file = os.path.join(output_dir, REPORT_FILENAME)
    try:
        with open(report_file, ""w"", encoding=""utf-8"") as f:
            f.write(report)
        logger.info(f""Final report saved to {report_file}"")
    except Exception as e:
        logger.error(f""Failed to save final report to {report_file}: {e}"")",Saves the final report to a markdown file.,"???Save a report as a markdown file in a specified directory, logging success or failure.???"
2049,list_shell_sessions,"def list_shell_sessions():
    
    result = []
    for session_id, session in list(ACTIVE_SESSIONS.items()):
        # Clean up terminated sessions
        if not session.is_running:
            del ACTIVE_SESSIONS[session_id]
            continue

        result.append({
            ""session_id"": session_id,
            ""command"": session.command,
            ""running"": session.is_running,
            ""last_activity"": time.strftime(
                ""%H:%M:%S"",
                time.localtime(session.last_activity))
        })
    return result",List all active shell sessions,"???Function retrieves and cleans up active shell sessions, returning their details.???"
2050,form,"def form(self, fields: Sequence[str]) -> dict[str, str]:
        
        responses: dict[str, str] = {}
        for field in fields:
            responses[field] = self.ask(f""{field}:"")
        return responses",Prompt for multiple fields and return a mapping of answers.,???Collect user input for specified fields into a dictionary of responses.???
2051,checkpoint_filter_fn,"def checkpoint_filter_fn(state_dict, model):
    
    if 'stages.0.blocks.0.norm1.weight' in state_dict:
        return state_dict  # already translated checkpoint
    if 'model' in state_dict:
        state_dict = state_dict['model']

    import re
    out_dict = {}
    for k, v in state_dict.items():
        k = re.sub(r'blocks.([0-9]+).([0-9]+).down', lambda x: f'stages.{int(x.group(1)) + 1}.downsample.down', k)
        k = re.sub(r'blocks.([0-9]+).([0-9]+)', r'stages.\1.blocks.\2', k)
        k = k.replace('head.', 'head.fc.')
        out_dict[k] = v

    return out_dict",Remap original checkpoints -> timm,???Transforms model checkpoint keys for compatibility with a new architecture???
2052,mock_agent,"def mock_agent(self) -> MagicMock:
        
        agent = MagicMock(spec=ConversableAgent)
        agent.name = ""mock_agent""
        agent._function_map = {}
        agent.handoffs = MagicMock()
        agent.handoffs.llm_conditions = []
        agent.handoffs.context_conditions = []
        agent.handoffs.after_work = None
        agent._group_is_established = False
        agent.description = ""Mock agent description""
        agent.llm_config = {""config_list"": [{""model"": ""test-model""}]}
        return agent",Create a mock ConversableAgent for testing.,???Create a mock conversational agent with predefined attributes and configurations.???
2053,context_with_results,"def context_with_results(entity_summary):
    
    from basic_memory.schemas.memory import ObservationSummary, ContextResult

    # Create an observation for the entity
    observation = ObservationSummary(
        title=""Test Observation"",
        permalink=""test/entity/observations/1"",
        category=""test"",
        content=""This is a test observation."",
        file_path=""/path/to/test/entity.md"",
        created_at=datetime.datetime(2023, 1, 1, 12, 0),
    )

    # Create a context result with primary_result, observations, and related_results
    context_item = ContextResult(
        primary_result=entity_summary,
        observations=[observation],
        related_results=[entity_summary],
    )

    return {
        ""topic"": ""Test Topic"",
        ""timeframe"": ""7d"",
        ""has_results"": True,
        ""hierarchical_results"": [context_item],
    }",Create a sample context with results for testing.,???Generate a structured context summary with observations and related results.???
2054,image_url_to_mime_and_base64,"def image_url_to_mime_and_base64(image_url: ImageUrl) -> tuple[str, str]:
    
    import re

    url = image_url.url

    match = re.match(r""data:(image/\w+);base64,(.*)"", url)
    if not match:
        raise ValueError(f""Invalid image data URI: {url[:30]}..."")
    mime_type, base64_data = match.groups()
    return mime_type, base64_data",Extract mime type and base64 data from ImageUrl,???Extract MIME type and base64 data from image URL string.???
2055,get_example_data,"def get_example_data() -> dict:
    
    return {""name"": ""Test"", ""value"": 123, ""status"": True}",Returns some example data.,???Returns a dictionary with predefined example data values.???
2056,get_aixbt_skill,"def get_aixbt_skill(
    name: str,
    store: SkillStoreABC,
    api_key: str = """",
) -> AIXBTBaseTool:
    
    cache_key = f""{name}:{api_key}""

    if name == ""aixbt_projects"":
        if cache_key not in _cache:
            _cache[cache_key] = AIXBTProjects(
                skill_store=store,
                api_key=api_key,
            )
        return _cache[cache_key]
    else:
        raise ValueError(f""Unknown AIXBT skill: {name}"")",Get an AIXBT API skill by name.,???Retrieve or cache AIXBT skill instance by name and API key from skill store???
2057,backup_and_replace,"def backup_and_replace(src: str, dest: str, config_name: str):
    
    try:
        if os.path.exists(dest):
            backup_path = dest + "".bak""
            # Asegurarse que el directorio de backup existe si es diferente
            # os.makedirs(os.path.dirname(backup_path), exist_ok=True)
            shutil.copy(dest, backup_path)
            print(f""{config_name} config backed up to {backup_path}"")
        os.makedirs(os.path.dirname(dest), exist_ok=True) # Ensure dest directory exists
        shutil.copy(src, dest)
        print(f""{config_name} config replaced from {src}"")
    except Exception as e:
        print(f""Error backing up/replacing {config_name} config: {e}"")",Backup the existing configuration file and replace it with a new one.,"???Backup and replace configuration file, ensuring directory existence and error handling.???"
2058,_generate_tools,"def _generate_tools(self) -> None:
        
        tools_dir = self.output_dir / ""components"" / ""tools""

        for tool in self.components.get(ComponentType.TOOL, []):
            # Get the tool directory structure
            rel_path = Path(tool.file_path).relative_to(self.project_path)
            if not rel_path.is_relative_to(Path(self.settings.tools_dir)):
                console.print(
                    f""[yellow]Warning: Tool {tool.name} is not in the tools directory[/yellow]""
                )
                continue

            try:
                rel_to_tools = rel_path.relative_to(self.settings.tools_dir)
                tool_dir = tools_dir / rel_to_tools.parent
            except ValueError:
                # Fall back to just using the filename
                tool_dir = tools_dir

            tool_dir.mkdir(parents=True, exist_ok=True)

            # Create the tool file
            output_file = tool_dir / rel_path.name
            transform_component(tool, output_file, self.project_path, self.import_map)",Generate code for all tools.,???Generate and organize tool components into a specified directory structure.???
2059,sample_tool_spec,"def sample_tool_spec():
    
    return {
        ""name"": ""TestGreeter"",
        ""purpose"": ""A simple tool to greet a user."",
        ""input_parameters"": [
            {
                ""name"": ""user_name"",
                ""type"": ""string"",
                ""description"": ""Name to greet"",
                ""required"": True
            }
        ],
        ""output_format"": ""string""
    }","Sample tool specification for testing, conforming to ToolSpecification model.",???Defines a specification for a user-greeting tool with input and output details.???
2060,analyze_sentiment,"def analyze_sentiment(news_items: list) -> dict:
    
    if not news_items:
        return {""score"": 5, ""details"": ""No news data; default to neutral sentiment""}

    negative_keywords = [""lawsuit"", ""fraud"", ""negative"", ""downturn"", ""decline"", ""investigation"", ""recall""]
    negative_count = 0
    for news in news_items:
        title_lower = (news.title or """").lower()
        if any(word in title_lower for word in negative_keywords):
            negative_count += 1

    details = []
    if negative_count > len(news_items) * 0.3:
        # More than 30% negative => somewhat bearish => 3/10
        score = 3
        details.append(f""High proportion of negative headlines: {negative_count}/{len(news_items)}"")
    elif negative_count > 0:
        # Some negativity => 6/10
        score = 6
        details.append(f""Some negative headlines: {negative_count}/{len(news_items)}"")
    else:
        # Mostly positive => 8/10
        score = 8
        details.append(""Mostly positive or neutral headlines"")

    return {""score"": score, ""details"": ""; "".join(details)}",Basic news sentiment check.,???Evaluate news sentiment by counting negative keywords to assign a sentiment score.???
2061,get_sortable_parts_for_key,"def get_sortable_parts_for_key(key_str: str) -> list:
    
    if not key_str or not isinstance(key_str, str): return []
    parts = re.findall(KEY_PATTERN, key_str)
    try:
        converted_parts = [(int(p) if p.isdigit() else p) for p in parts]
    except (ValueError, TypeError):
        logger.warning(f""Could not convert parts for sorting key string '{key_str}', using original string parts."")
        converted_parts = parts # Fallback to string parts
    return converted_parts",Splits a key string into parts and converts numeric parts to integers for hierarchical/natural sorting.,???Extracts and converts sortable components from a string for sorting purposes.???
2062,create_tasks,"def create_tasks(self, data, **kwargs):
        
        tasks = []
        for i, row in enumerate(data):
            tasks.append({
                ""id"": str(i),
                ""data"": row
            })
        return tasks",Minimal mock implementation: data is expected to be a list of dicts.,"??? 
Generate task objects from input data with unique identifiers. 
???"
2063,token_counter_with_history,"def token_counter_with_history(self, messages_history: list[Message], prompt: str) -> int:
        
        litellm_messages = [{""role"": msg.role, ""content"": str(msg.content)} for msg in messages_history]
        litellm_messages.append({""role"": ""user"", ""content"": str(prompt)})
        return token_counter(model=self.model, messages=litellm_messages)",Count the number of tokens in a list of messages and a prompt.,???Count tokens in message history and prompt for a model.???
2064,get_repo_map,"def get_repo_map(self) -> Dict[str, Any]:
        
        self.scan_repo()
        self._file_tree = None
        return {""file_tree"": self.get_file_tree(), ""symbols"": {k: v[""symbols""] for k, v in self._symbol_map.items()}}",Returns a dict with file tree and a mapping of files to their symbols.,???Generate a repository map with file structure and symbols.???
2065,revise_answer_based_on_explanation,"def revise_answer_based_on_explanation(self, question, explanation):
        
        messages = [
            {
                ""role"": ""system"",
                ""content"": (
                    f""You are a student learning {self.profile['subject']} ""
                    f""at {self.profile['difficulty']} level. ""
                    f""Your goal is: {self.profile['goal']}.""
                ),
            },
            {
                ""role"": ""user"",
                ""content"": f,
            },
        ]
        response = openai.ChatCompletion.create(model=""gpt-4"", messages=messages)
        revised = response.choices[0].message.content.strip()
        self.log.append({""question"": question, ""revised_answer"": revised})
        return revised",Generates a revised answer based on the teacher's explanation.,???Refine student answers using AI based on provided explanations.???
2066,batch_process,"def batch_process(self, items: list[str | dict[str, Any] | tuple[Any, ...]]) -> dict[str, list[Any]]:
        
        results: dict[str, list[Any]] = {""success"": [], ""error"": []}

        for item in items:
            try:
                result = self.process(item)
                results[""success""].append(result)
            except Exception as e:
                results[""error""].append((item, str(e)))

        return results",Process multiple items in a batch.,"???Batch process items, categorizing results into success or error.???"
2067,clear_cache,"def clear_cache(args):
    
    cache = LLMCache(cache_dir=args.cache_dir)
    older_than = args.older_than * 86400 if args.older_than else None
    count = cache.clear_cache(older_than)
    
    logger.info(f""Cleared {count} cache entries"")",Clear the LLM cache.,???Clear outdated cache entries based on specified age criteria???
2068,_apply_template,"def _apply_template(self, template: str, row: pd.Series) -> str:
        
        result = template
        for match in re.finditer(r'\$(\w+)\$', template):
            var_name = match.group(1)
            if var_name in row.index:
                value = str(row[var_name])
                result = result.replace(f""${var_name}$"", value)
        return result",Apply template replacing $variable$ with values from row.,???Replace placeholders in a template with corresponding data from a row.???
2069,listByFields,"def listByFields(self, field: str, values: list):
        
        if field not in self.fields:
            raise ValueError(f""field name `{field}` is illegal"")

        if not values:
            return []

        placeHolders = ','.join(['?']*len(values))
        sql = f""SELECT * FROM {self.table} WHERE {field} IN ({placeHolders})""
        self.query.prepare(sql)

        for value in values:
            self.query.addBindValue(value)

        if not self.query.exec():
            return []

        return self.iterRecords()",query the records of field values in the list,???Retrieve records from a database table based on specified field values.???
2070,convert_urdf_to_usd,"def convert_urdf_to_usd(urdf_path, output_dir):
    
    rel_path = os.path.relpath(urdf_path, args.input_dir)
    output_path = os.path.join(output_dir, rel_path)
    output_path = os.path.splitext(output_path)[0] + "".usd""

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    print(f""\nProcessing: {urdf_path}"")
    print(f""Output to: {output_path}"")

    try:
        subprocess.run(
            [sys.executable, ""scripts/urdf2usd.py"", urdf_path, output_path, ""--merge-joints"", ""--fix-base""], check=True
        )
        print(f""Successfully converted {urdf_path}"")
        return True
    except subprocess.CalledProcessError as e:
        print(f""Error converting {urdf_path}: {e}"")
        return False",Convert a single URDF file to USD.,???Convert URDF files to USD format with error handling and directory management.???
2071,track_indexed_db_for_origin,"def track_indexed_db_for_origin(
    origin: str,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""origin""] = origin
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Storage.trackIndexedDBForOrigin"",
        ""params"": params,
    }
    json = yield cmd_dict",Registers origin to be notified when an update occurs to its IndexedDB.,???Generate a command to monitor IndexedDB activity for a specific origin.???
2072,render_current_user_message,"def render_current_user_message(
        self, 
        values: Optional[dict], 
        inputs_format: Optional[Type[LLMOutputParser]], 
        outputs_format: Optional[Type[LLMOutputParser]], 
        parse_mode: str, 
        title_format: str, 
        custom_output_format: Optional[str] = None
    ) -> str:
        
        
        input_pieces = []
        if inputs_format or values:
            input_pieces.append(self.render_inputs(inputs_format, values))
        
        if custom_output_format:
            input_pieces.append(f""### Outputs Format\n{custom_output_format}"")
        else:
            input_pieces.append(self.render_outputs(outputs_format, parse_mode, title_format))

        input_pieces = [piece for piece in input_pieces if piece]
        user_message = ""\n"".join(input_pieces)
        return user_message.strip()",Render the current user input message.,???Generate a formatted user message by rendering inputs and outputs based on provided formats and values.???
2073,_BrowserUseonTabVisibilityChange,"def _BrowserUseonTabVisibilityChange(source: dict[str, str]):
			
			new_page = source['page']

			# Update human foreground tab state
			old_foreground = self.human_current_page
			assert self.browser_context is not None, 'BrowserContext object is not set'
			assert old_foreground is not None, 'Old foreground page is not set'
			old_tab_idx = self.browser_context.pages.index(old_foreground)
			self.human_current_page = new_page
			new_tab_idx = self.browser_context.pages.index(new_page)

			# Log before and after for debugging
			old_url = old_foreground and old_foreground.url or 'about:blank'
			new_url = new_page and new_page.url or 'about:blank'
			agent_url = self.agent_current_page and self.agent_current_page.url or 'about:blank'
			agent_tab_idx = self.browser_context.pages.index(self.agent_current_page)
			if old_url != new_url:
				self.logger.info(
					f'👁️ Foregound tab changed by human from [{old_tab_idx}]{_log_pretty_url(old_url)} '
					f'➡️ [{new_tab_idx}]{_log_pretty_url(new_url)} '
					f'(agent will stay on [{agent_tab_idx}]{_log_pretty_url(agent_url)})'
				)",hook callback fired when init script injected into a page detects a focus event,???Log and update browser tab visibility changes for user interaction tracking.???
2074,get_closed_set_messages_embeddings,"def get_closed_set_messages_embeddings(self):
        
        save_path = f'../model/model_saved/adpsemevent/{self.dataset_name}/closed_set/'
        
        SBERT_embedding_path = f'{save_path}/SBERT_embeddings.pkl'
        if not exists(SBERT_embedding_path):
            test_set_df_np_path = save_path + 'test_set.npy'
            test_df_np = np.load(test_set_df_np_path, allow_pickle=True)
            
            test_df = pd.DataFrame(data=test_df_np, columns=self.columns)
            print(""Dataframe loaded."")

            processed_text = [preprocess_sentence(s) for s in test_df['text'].values]
            print('message text contents preprocessed.')

            embeddings = SBERT_embed(processed_text, language=self.language)

            with open(SBERT_embedding_path, 'wb') as fp:
                pickle.dump(embeddings, fp)
            print('SBERT embeddings stored.')
        return",Get SBERT embeddings for closed set messages,???Generate and store SBERT embeddings for preprocessed message texts if not already saved.???
2075,enable,"def enable(
    enable_file_chooser_opened_event: typing.Optional[bool] = None,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    if enable_file_chooser_opened_event is not None:
        params[""enableFileChooserOpenedEvent""] = enable_file_chooser_opened_event
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Page.enable"",
        ""params"": params,
    }
    json = yield cmd_dict",Enables page domain notifications.,???Enable file chooser event in page interaction generator???
2076,check_all_documents_embeding_status,"def check_all_documents_embeding_status(self) -> bool:
        
        try:
            unembedding_docs = self._repository.find_unembedding()
            return len(unembedding_docs) > 0
        except Exception as e:
            logger.error(f""Error checking documents embedding status: {str(e)}"", exc_info=True)
            raise",Check if there are any documents that need embedding,???Check if any documents lack embedding and log errors if encountered.???
2077,speak_text,"def speak_text(text: str) -> Optional[Path]:
    
    try:
        client = _get_client()
        response = client.audio.speech.create(
            model=""tts-1"",
            voice=""alloy"",
            input=text,
        )

        project_root = Path(__file__).parent.parent
        audio_dir = project_root / ""audio""
        audio_dir.mkdir(exist_ok=True)

        random_string = str(random.randint(1, 1000000))
        hash_object = hashlib.sha1(random_string.encode())
        filename = hash_object.hexdigest()[:8]
        file_path = audio_dir / f""{filename}.mp3""

        response.stream_to_file(file_path)
        logger.info(f""Audio content saved as '{file_path}'"")
        return file_path
    except Exception as e:
        logger.error(f""Error generating speech: {str(e)}"")
        raise",Convert text to speech using OpenAI's TTS model,"???Convert text to speech, save as MP3, and return file path.???"
2078,save_chat,"def save_chat(st: Any) -> None:
    
    Path(SAVED_CHAT_PATH).mkdir(parents=True, exist_ok=True)
    session_id = st.session_state[""session_id""]
    session = st.session_state.user_chats[session_id]
    messages = session.get(""messages"", [])
    if len(messages) > 0:
        session[""messages""] = sanitize_messages(session[""messages""])
        filename = f""{session_id}.yaml""
        with open(Path(SAVED_CHAT_PATH) / filename, ""w"") as file:
            yaml.dump(
                [session],
                file,
                allow_unicode=True,
                default_flow_style=False,
                encoding=""utf-8"",
            )
        st.toast(f""Chat saved to path: ↓ {Path(SAVED_CHAT_PATH) / filename}"")",Save the current chat session to a YAML file.,???Save sanitized chat messages to a YAML file in a specified directory.???
2079,get_frames_and_annotations,"def get_frames_and_annotations(
        self, video_id: str
    ) -> Tuple[List | None, Dict | None, Dict | None]:
        
        # load the video
        mp4_path = os.path.join(self.sav_dir, video_id + "".mp4"")
        frames = self.read_frames(mp4_path)
        if frames is None:
            return None, None, None

        # load the manual annotations
        manual_annot_path = os.path.join(self.sav_dir, video_id + ""_manual.json"")
        if not os.path.exists(manual_annot_path):
            print(f""{manual_annot_path} doesn't exist. Something might be wrong."")
            manual_annot = None
        else:
            manual_annot = json.load(open(manual_annot_path))

        # load the manual annotations
        auto_annot_path = os.path.join(self.sav_dir, video_id + ""_auto.json"")
        if not os.path.exists(auto_annot_path):
            print(f""{auto_annot_path} doesn't exist."")
            auto_annot = None
        else:
            auto_annot = json.load(open(auto_annot_path))

        return frames, manual_annot, auto_annot",Get the frames and annotations for video.,???Retrieve video frames and annotations from specified file paths.???
2080,temp_config_file,"def temp_config_file(tmp_path: Path) -> Iterator[Path]:
    
    config_data = {
        ""port"": 8989,
        ""host"": ""localhost"",
        ""log_level"": ""DEBUG"",
        ""log_format"": ""JSON"",
    }
    config_file = tmp_path / ""config.yaml""

    with open(config_file, ""w"") as f:
        yaml.dump(config_data, f)

    yield config_file",Create a temporary config file.,???Create a temporary YAML configuration file with predefined settings.???
2081,update_changelog,"def update_changelog(changelog_path: Path, release_tag: str, release_body: str):
    

    with open(changelog_path, 'r') as f:
        lines = f.readlines()

    now = datetime.now()
    release_date = now.strftime('%b %d, %Y')

    # construct new changelog entry
    changelog_str = '# Changelog\n\n'
    release_header = f'##  {release_tag} ({release_date})\n'
    release_body = f'{release_body}\n'

    # construct changelog
    lines = [changelog_str, release_header, release_body] + lines[1:]

    with open(changelog_path, 'w') as f:
        f.writelines(lines)",Update the changelog based on the release tag and the release_body.,???Append new release details to a changelog file with date and version.???
2082,mjcf_to_urdf,"def mjcf_to_urdf(mjcf_path: str, out_urdf_path: str, scale: float = 1.0) -> None:
    
    mj = parse_mjcf(mjcf_path)
    mesh_dir = os.path.dirname(os.path.abspath(mjcf_path))
    os.makedirs(os.path.dirname(os.path.abspath(out_urdf_path)), exist_ok=True)
    robot_links = []
    robot_joints = []
    world_link_dict = {
        ""name"": ""world_link"",
        ""inertial"": None,
        ""visuals"": [],
        ""collisions"": [],
    }
    robot_links.append(world_link_dict)
    process_body_recursive(mj, 1, robot_links, robot_joints, scale, mesh_dir, out_urdf_path, mjcf_path)
    robot_name = os.path.basename(mjcf_path).split(""."")[0]
    robot_el = build_urdf_xml(robot_name, robot_links, robot_joints)
    write_urdf_xml(robot_el, out_urdf_path)
    print(f""URDF successfully written to: {out_urdf_path}"")",Convert an MJCF XML file to a URDF XML file without using urdfpy.,???Convert MJCF file to URDF format with optional scaling and save output.???
2083,wrap_value,"def wrap_value(val):
    
    if isinstance(val, str):
        return {'stringValue': val}
    elif isinstance(val, bool):
        return {'booleanValue': val}
    elif isinstance(val, int):
        return {'longValue': val}
    elif isinstance(val, float):
        return {'doubleValue': val}
    elif isinstance(val, decimal.Decimal):
        return {'stringValue': str(val)}
    elif isinstance(val, uuid.UUID):
        return {'stringValue': str(val)}
    elif isinstance(val, datetime.datetime):
        return {'stringValue': val.isoformat()}
    elif isinstance(val, datetime.date):
        return {'stringValue': val.isoformat()}
    elif isinstance(val, datetime.time):
        return {'stringValue': val.isoformat()}
    elif isinstance(val, list):
        return {'arrayValue': {'stringValues': [str(v) for v in val]}}
    elif isinstance(val, dict):
        return {'stringValue': json.dumps(val)}
    elif val is None:
        return {'isNull': True}
    else:
        raise TypeError(f'Unsupported value type: {type(val)}')",Convert a Python value into an AWS RDS Data API-compatible field dict.,"???  
Convert various data types into a standardized dictionary format for serialization.  
???"
2084,_process_code_snippet,"def _process_code_snippet(self, code_snippet: str) -> str:
        
        # Log the original code for debugging
        logger.debug(f""Original code snippet:\n{code_snippet}"")
        
        # Remove markdown code blocks
        # First, try to extract Python code between ```python and ``` tags
        python_blocks = re.findall(r""```python(.*?)```"", code_snippet, re.DOTALL)
        if python_blocks:
            # Use the first Python code block found
            code_snippet = python_blocks[0]
        else:
            # Remove any generic code blocks
            code_blocks = re.findall(r""```(.*?)```"", code_snippet, re.DOTALL)
            if code_blocks:
                # Use the first code block found
                code_snippet = code_blocks[0]
                
        # Clean up the code - remove leading/trailing whitespace
        code_snippet = code_snippet.strip()
        
        # Ensure the code has proper input handling
        if ""input"" not in code_snippet:
            logger.warning(f""Code snippet doesn't reference input parameter. Adding fallback input handling."")
            code_snippet = ""# Input is available as 'input' variable\n"" + code_snippet
        
        # Ensure the code sets a 'result' variable
        if ""result ="" not in code_snippet and not code_snippet.strip().endswith(""result = ""):
            logger.warning(f""Code snippet doesn't set 'result' variable. Adding fallback result assignment."")
            code_snippet += ""\n\n# Ensure result is set\nif 'result' not in locals():\n    result = 'Tool executed but did not set a result'""
        
        # Log the processed code
        logger.debug(f""Processed code snippet:\n{code_snippet}"")
        
        return code_snippet",Process the code snippet to ensure it's executable.,"???Extracts and processes code snippets, ensuring input handling and result assignment.???"
2085,validate_config,"def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        
        if ""rpc"" not in config and ""network"" not in config:
            raise ValueError(""Config must contain either 'rpc' or 'network'"")
        if ""network"" in config and config[""network""] not in EVM_NETWORKS:
            raise ValueError(f""Invalid network '{config['network']}'. Must be one of: {', '.join(EVM_NETWORKS.keys())}"")
        return config",Validate Ethereum configuration from JSON,???Ensure configuration includes valid 'rpc' or 'network' settings???
2086,algebraic_to_pixels,"def algebraic_to_pixels(square: str, square_size: float) -> tuple[str, str]:
	
	file_char = square[0].lower()
	rank_char = square[1]

	if file_char not in FILES or rank_char not in RANKS:
		raise ValueError(f'Invalid square: {square}')

	x_index = FILES.index(file_char)
	y_index = RANKS.index(rank_char)

	x_px = x_index * square_size
	y_px = y_index * square_size
	return to_px(x_px), to_px(y_px)",Converts algebraic notation to Lichess pixel coordinates using dynamic size.,???Convert chessboard coordinates to pixel positions based on square size.???
2087,_generate_mcp_token,"def _generate_mcp_token(
        self,
        client_id: str,
        user_id: str,
        email: str,
        scopes: list[str],
        supabase_access_token: str,
    ) -> str:
        
        payload = {
            ""iss"": self.issuer_url,
            ""sub"": user_id,
            ""client_id"": client_id,
            ""email"": email,
            ""scopes"": scopes,
            ""supabase_token"": supabase_access_token[:10] + ""..."",  # Reference only
            ""exp"": datetime.utcnow() + timedelta(hours=1),
            ""iat"": datetime.utcnow(),
        }

        # Use Supabase JWT secret if available
        secret = os.getenv(""SUPABASE_JWT_SECRET"", secrets.token_urlsafe(32))

        return jwt.encode(payload, secret, algorithm=""HS256"")",Generate an MCP token that wraps Supabase authentication.,???Generate a JWT token with user and client details for authentication???
2088,fixtures_stream_hex,"def fixtures_stream_hex(category: str, name: str) -> list[bytes]:
    
    with open(fixture_path(category, f""{name}""), ""r"") as f:
        return [
            eval(line.strip())  # This will evaluate the full b'...' string
            for line in f
            if line.strip()
        ]",Load stream fixture data from a text file with one hex-encoded string per line.,???Read and evaluate hex data from a file into a byte list.???
2089,reset_plugins,"def reset_plugins(cls) -> None:
        
        cls._custom_languages.clear()
        cls._language_extensions.clear()
        cls._queries.clear()
        cls._parsers.clear()

        # Reset LANGUAGES to original state
        global LANGUAGES
        original_languages = {
            "".py"": ""python"",
            "".js"": ""javascript"",
            "".go"": ""go"",
            "".rs"": ""rust"",
            "".hcl"": ""hcl"",
            "".tf"": ""hcl"",
            "".ts"": ""typescript"",
            "".tsx"": ""tsx"",
            "".c"": ""c"",
            "".rb"": ""ruby"",
            "".java"": ""java"",
        }
        LANGUAGES.clear()
        LANGUAGES.update(original_languages)
        cls.LANGUAGES = set(LANGUAGES.keys())",Reset all custom languages and extensions.,???Reset language configurations to their default state.???
2090,display_update_message,"def display_update_message() -> None:
    
    try:
        needs_update, current, latest = check_for_updates()

        if needs_update:
            console.print(
                f""\n[yellow]⚠️  Update available: {current} → {latest}[/]"",
                highlight=False,
            )
            console.print(
                f""[yellow]Run `pip install --upgrade {PACKAGE_NAME}` to update."",
                highlight=False,
            )
            console.print(
                f""[yellow]Or, if you used pipx: `pipx upgrade {PACKAGE_NAME}`"",
                highlight=False,
            )
            console.print(
                f""[yellow]Or, if you used uv: `uv pip install --upgrade {PACKAGE_NAME}`"",
                highlight=False,
            )
    except Exception as e:
        # Don't let version checking errors affect the CLI
        logging.debug(f""Error checking for updates: {e}"")",Check for updates and display a message if an update is available.,???Check for software updates and notify users with upgrade instructions if available.???
2091,exec,"def exec(self, inputs):
        
        action, action_input = inputs
        
        print(f""🚀 Executing action: {action}, input: {action_input}"")
        
        # Execute different operations based on action type
        if action == ""search"":
            # Simulate search operation
            result = self.search_web(action_input)
        elif action == ""calculate"":
            # Simulate calculation operation
            result = self.calculate(action_input)
        elif action == ""answer"":
            # Direct return answer
            result = action_input
        else:
            # Unknown action type
            result = f""Unknown action type: {action}""
        
        return result",Execute action and return result,"???  
Execute specified action with input and return result or error message.  
???"
2092,untrack_indexed_db_for_storage_key,"def untrack_indexed_db_for_storage_key(
    storage_key: str,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""storageKey""] = storage_key
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Storage.untrackIndexedDBForStorageKey"",
        ""params"": params,
    }
    json = yield cmd_dict",Unregisters storage key from receiving notifications for IndexedDB.,???Generate command to stop tracking IndexedDB for a given storage key.???
2093,apply_substitutions,"def apply_substitutions(self, context: Dict[str, Any]) -> ""PromptContent"":
        

        # Define placeholder pattern once to avoid repetition
        def make_placeholder(key: str) -> str:
            return f""{{{{{key}}}}}""

        # Apply substitutions to text
        result = self.text
        for key, value in context.items():
            result = result.replace(make_placeholder(key), str(value))

        # Apply substitutions to resource paths
        substituted_resources = []
        for resource in self.resources:
            substituted = resource
            for key, value in context.items():
                substituted = substituted.replace(make_placeholder(key), str(value))
            substituted_resources.append(substituted)

        return PromptContent(text=result, role=self.role, resources=substituted_resources)",Apply variable substitutions to the text and resources,???Replace placeholders in text and resources with context values.???
2094,index,"def index(self, session_id, **kwargs):
        
        html = map_action_to_html(
            'start',
            session_id=session_id,
            instruction_text=kwargs['instruction_text'],
        )
        url = f'{self.base_url}/{session_id}'
        return html, url",Redirect to the search page with the given session ID,???Generate HTML and URL for session initiation with instructions.???
2095,mock_failed_login_response,"def mock_failed_login_response(self):
        
        mock_response = MagicMock()
        mock_response.json.return_value = {""error"": {""message"": ""INVALID_PASSWORD""}}
        mock_response.raise_for_status.side_effect = Exception(mock_response.json())
        return mock_response",Mock failed login response.,???Simulate a failed login response with an invalid password error.???
2096,get_document,"def get_document(self, document_id: str) -> Optional[Document]:
        
        try:
            docstore = self.store.docstore
            document = docstore.search(document_id)
            if isinstance(document, Document):
                return document
            return None
        except Exception as e:
            logger.error(f""Error retrieving document {document_id}: {str(e)}"")
            return None",Get a document by its ID,"???Retrieve a document by ID from storage, handling errors gracefully.???"
2097,_register_tasks,"def _register_tasks(self, tasks: Dict[str, Callable]) -> None:
        
        for task_name, method in tasks.items():
            llm = self._choose_llm_for(method)
            logger.debug(
                f""Registering task '{task_name}' with llm={getattr(llm, '__class__', None)}""
            )
            kwargs = getattr(method, ""_task_kwargs"", {})
            task_instance = WorkflowTask(
                func=method,
                description=getattr(method, ""_task_description"", None),
                agent=getattr(method, ""_task_agent"", None),
                llm=llm,
                include_chat_history=getattr(
                    method, ""_task_include_chat_history"", False
                ),
                workflow_app=self,
                **kwargs,
            )
            # Wrap for Dapr invocation
            wrapped = self._make_task_wrapper(task_name, method, task_instance)
            activity_decorator = self.wf_runtime.activity(name=task_name)
            self.tasks[task_name] = activity_decorator(wrapped)",Register each discovered task with the Dapr runtime.,???Registers and wraps tasks with specific configurations for workflow execution.???
2098,chunk,"def chunk(self, text: Union[str, List[str]]) -> List[Dict]:
        
        # Make the payload
        payload = {
            ""text"": text,
            ""tokenizer_or_token_counter"": self.tokenizer_or_token_counter,
            ""chunk_size"": self.chunk_size,
            ""min_characters_per_chunk"": self.min_characters_per_chunk,
            ""rules"": self.rules.to_dict(),
            ""return_type"": self.return_type,
        }
        # Make the request to the Chonkie API
        response = requests.post(
            f""{self.BASE_URL}/{self.VERSION}/chunk/recursive"",
            json=payload,
            headers={""Authorization"": f""Bearer {self.api_key}""},
        )

        # Try to parse the response
        try:
            result: List[Dict] = cast(List[Dict], response.json())
        except Exception as error:
            raise ValueError(
                ""Oh no! The Chonkie API returned an invalid response.""
                + ""Please try again in a short while.""
                + ""If the issue persists, please contact support at support@chonkie.ai.""
            ) from error

        return result",Chunk the text into a list of chunks.,???Send text to Chonkie API for chunking and return structured response.???
2099,speed_value,"def speed_value(speed: int):
        
        speed = max(0, int(speed))
        speed = min(10, int(speed))
        return f""<|speed_value_{speed}|>""",Turn special token of speed value.,???Converts speed to a bounded string representation between 0 and 10.???
2100,_plan_research_prompt,"def _plan_research_prompt(
    initial_research: str, include_human_feedback: bool, human_feedback: str, max_sections: int
) -> str:
    
    feedback_instruction = (
        f""Human feedback: {human_feedback}. You must plan the sections based on the human feedback.""
        if include_human_feedback and human_feedback and human_feedback.lower() != ""no""
        else """"
    )

    return f",Generate a research prompt with optional human feedback.,???Generate research plan sections incorporating optional human feedback.???
2101,__from_env,"def __from_env():
    
    #global ATTN
    global ATTN
    global DEBUG
    
    # Get current settings from central config
    #ATTN = 
    ATTN = get_attention_backend()
    DEBUG = get_debug_mode()
    
    print(f""[ATTENTION] sparse backend: {ATTN}"")",Read current backend configuration,???Initialize global settings from configuration and print attention backend???
2102,setup_terminal,"def setup_terminal():
    
    if sys.platform == ""win32"":
        if msvcrt:
            return None  # Windows terminal is already configured
        logger.warning(""msvcrt module not available on Windows"")
        return None
    else:
        if termios and tty:
            try:
                fd = sys.stdin.fileno()
                old_settings = termios.tcgetattr(fd)
                tty.setraw(fd)
                return old_settings
            except (termios.error, AttributeError) as e:
                logger.warning(f""Failed to configure terminal: {e}"")
                return None
        return None",Configure terminal settings based on platform.,???Configure terminal settings based on operating system and handle errors???
2103,read_python_files_from_specific_folders,"def read_python_files_from_specific_folders(top_level_folder, target_folders):
    
    code_collection = []

    # For each target folder, construct its full path, then walk only inside it.
    for folder_name in target_folders:
        folder_path = os.path.join(top_level_folder, folder_name)

        # Make sure the child folder actually exists before walking it.
        if os.path.isdir(folder_path):
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    if file.endswith('.py'):  # Only .py files
                        file_path = os.path.join(root, file)
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                file_content = f.read()
                            # Get the path relative to top_level_folder for clarity
                            relative_path = os.path.relpath(file_path, top_level_folder)
                            # Append the file name and content
                            code_collection.append(f""File: {relative_path}\n{file_content}\n"")
                        except (IOError, OSError) as e:
                            print(f""Could not read file {file_path}: {e}"")
        else:
            print(f""Warning: Folder '{folder_name}' does not exist at '{folder_path}'."")

    # Combine all collected code into a single text
    return ""\n"".join(code_collection)","Gathers all .py files from each of the specified folders (and subfolders), returning a single string with the path and source code.",???Aggregate Python code from specified directories into a single text output.???
2104,encode_audio,"def encode_audio(data: np.ndarray) -> dict:
    
    return {
        ""mime_type"": ""audio/pcm"",
        ""data"": base64.b64encode(data.tobytes()).decode(""UTF-8"")
    }",Encode Audio data to send to the server,???Converts audio data to a base64-encoded PCM format dictionary.???
2105,stop_sampling,"def stop_sampling() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, SamplingHeapProfile]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""HeapProfiler.stopSampling"",
    }
    json = yield cmd_dict
    return SamplingHeapProfile.from_json(json[""profile""])",:returns: Recorded sampling heap profile.,???Stop heap sampling and return the profile as a JSON object.???
2106,preprocess_text,"def preprocess_text(self, text: str, lang: str) -> str:
        
        base_lang = lang.split(""-"")[0]  # remove region
        if base_lang in {""ar"", ""cs"", ""de"", ""en"", ""es"", ""fr"", ""hu"", ""it"",
                         ""nl"", ""pl"", ""pt"", ""ru"", ""tr"", ""zh"", ""ko""}:
            text = multilingual_cleaners(text, base_lang)
            if base_lang == ""zh"":
                text = chinese_transliterate(text)
            if base_lang == ""ko"":
                text = korean_transliterate(text, self._korean_transliter)
        elif base_lang == ""ja"":
            text = japanese_cleaners(text, self.katsu)
        else:
            text = basic_cleaners(text)
        return text",Apply text preprocessing for language,"???  
Normalize and clean text based on language-specific rules.  
???"
2107,_filter_tools,"def _filter_tools(self, tools: Sequence[T]) -> Sequence[T]:
        
        requested_tools = list(self.mcp_tool.tools or [])

        if not requested_tools:
            return tools

        name_to_tool = {
            tool.name if isinstance(tool, HasName) else tool: tool for tool in tools
        }

        missing_tools = [name for name in requested_tools if name not in name_to_tool]
        if missing_tools:
            error_message = dedent(
                f
            )
            raise ValueError(error_message)

        return [name_to_tool[name] for name in requested_tools]",Filter the tools to only include the ones listed in mcp_tool['tools'].,"???Filter and validate requested tools against available tools, raising error if any are missing.???"
2108,create_venv,"def create_venv(example_dir: Path) -> Path:
    
    venv_dir = example_dir / "".venv""
    if not venv_dir.exists():
        console.print(f""Creating virtual environment in [cyan]{venv_dir}[/]"")
        subprocess.run([""uv"", ""venv"", str(venv_dir)], check=True)
        # venv.create(venv_dir, with_pip=True)
    return venv_dir",Create a virtual environment if it doesn't exist.,???Create a virtual environment in a specified directory if it doesn't exist.???
2109,_create_default_functions,"def _create_default_functions(self) -> Dict[str, Callable]:
        

        def add(a: float, b: float) -> float:
            return a + b

        def subtract(a: float, b: float) -> float:
            return a - b

        def multiply(a: float, b: float) -> float:
            return a * b

        def divide(a: float, b: float) -> Optional[float]:
            return a / b if b != 0 else None

        def sqrt(a: float) -> Optional[float]:
            return a**0.5 if a >= 0 else None

        base_functions = {
            ""add"": add,
            ""subtract"": subtract,
            ""multiply"": multiply,
            ""divide"": divide,
            ""sqrt"": sqrt,
            ""print_answer"": self._create_print_answer(),
        }

        for name, func in base_functions.items():
            func.__name__ = name
            func.__doc__ = f""Execute {name} operation: {func.__name__}({inspect.signature(func)})""

        logger.info(""Initialized default math functions with type annotations"")
        return base_functions",Create default math functions with metadata and type annotations.,???Initialize and document basic math operations with type annotations???
2110,get_gcp_location,"def get_gcp_location() -> str:
    
    # Check if endpoint in config contains location information
    provider_config = CONFIG.get_embedding_provider(""gemini"")
    if provider_config and provider_config.endpoint:
        # The endpoint might contain location information
        # Try to extract it if it follows a pattern like ""us-central1-aiplatform.googleapis.com""
        parts = provider_config.endpoint.split('-')
        if len(parts) >= 2:
            location = f""{parts[0]}-{parts[1]}""
            return location
    
    # Fallback to environment or default
    return os.getenv(""GCP_LOCATION"", ""us-central1"")",Retrieve the GCP location from configuration or environment.,???Extract GCP location from config endpoint or environment variable???
2111,show_cost_comparison,"def show_cost_comparison():
    
    print(""\n💰 Cost Comparison"")
    print(""="" * 50)

    scenarios = [
        (""Single PR review"", ""1 review""),
        (""Daily reviews"", ""30 reviews/month""),
        (""Enterprise usage"", ""1000 reviews/month""),
        (""Continuous integration"", ""10,000 reviews/month""),
    ]

    print(f""{'Scenario':<25} {'Ollama':<15} {'OpenAI GPT-4o':<15} {'Claude Sonnet'}"")
    print(""-"" * 70)

    for scenario, usage in scenarios:
        reviews = int(usage.split()[0].replace("","", """"))
        openai_cost = reviews * 0.10  # $0.10 per review
        claude_cost = reviews * 0.08  # $0.08 per review

        print(f""{scenario:<25} {'$0.00':<15} ${openai_cost:<14.2f} ${claude_cost:<14.2f}"")

    print(""\n🎯 Ollama Benefits:"")
    print(""   - Zero cost for unlimited usage"")
    print(""   - Complete privacy (code never leaves your machine)"")
    print(""   - No rate limits (only hardware constraints)"")
    print(""   - Works offline"")
    print(""   - Latest open source models"")",Show cost comparison between Ollama and cloud providers.,???Display a cost comparison of review scenarios highlighting Ollama's benefits over competitors.???
2112,health_check,"def health_check():
    
    logger = get_logger(""system"")
    logger.debug(""Health check endpoint accessed"")
    return jsonify({""status"": ""OK""})",Simple health check endpoint for Docker health checks.,"???  
Defines a function to log and return a system health status as ""OK"".  
???"
2113,exec,"def exec(self, prep_res):
        
        query = prep_res[""query""]
        observations_text = prep_res[""observations_text""]
        current_thought_number = prep_res[""current_thought_number""]
        
        # Build the prompt
        prompt = f
        
        # Call LLM to get thinking result
        response = call_llm(prompt)
        
        # Parse YAML response
        yaml_str = response.split(""```yaml"")[1].split(""```"")[0].strip()
        thought_data = yaml.safe_load(yaml_str)
        
        # Add thought number
        thought_data[""thought_number""] = current_thought_number
        
        return thought_data","Execute the thinking process, decide the next action","???Process a query with LLM, parse YAML response, and return structured thought data.???"
2114,checkout_commit,"def checkout_commit(self, commit_hash: str | GitCommit, remote_name: str = ""origin"") -> CheckoutResult:
        
        logger.info(f""Checking out commit: {commit_hash}"")
        if not self.git_cli.is_valid_object(commit_hash, ""commit""):
            self.fetch_remote(remote_name=remote_name, refspec=commit_hash)
            if not self.git_cli.is_valid_object(commit_hash, ""commit""):
                return CheckoutResult.NOT_FOUND

        if self.git_cli.is_dirty():
            logger.info(f""Environment is dirty, discarding changes before checking out commit: {commit_hash}"")
            self.discard_changes()

        self.git_cli.git.checkout(commit_hash)
        return CheckoutResult.SUCCESS",Checks out the relevant commit,"???Ensure valid commit checkout, handling remote fetch and local changes???"
2115,process_image_content,"def process_image_content(content_item: dict[str, Any]) -> dict[str, Any]:
    
    if content_item[""type""] != ""image_url"":
        return content_item

    url = content_item[""image_url""][""url""]
    try:
        # Handle data URLs
        if url.startswith(""data:""):
            data_url_pattern = r""data:image/([a-zA-Z]+);base64,(.+)""
            match = re.match(data_url_pattern, url)
            if match:
                media_type, base64_data = match.groups()
                return {
                    ""type"": ""image"",
                    ""source"": {""type"": ""base64"", ""media_type"": f""image/{media_type}"", ""data"": base64_data},
                }

        else:
            print(""Error processing image."")
            # Return original content if image processing fails
            return content_item

    except Exception as e:
        print(f""Error processing image image: {e}"")
        # Return original content if image processing fails
        return content_item",Process an OpenAI image content item into Claude format.,"???Transforms image URL content into base64 format if applicable, otherwise returns original content.???"
2116,config_load,"def config_load(filename: str = typer.Argument(..., help=""Path to the configuration file to load"")):
    
    try:
        with open(filename) as f:
            new_config = yaml.safe_load(f) or {}
        config_file_path = GLOBAL_CONFIG_PATH
        config_file_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_file_path, ""w"") as f:
            yaml.safe_dump(new_config, f, default_flow_style=False)
        logger.info(f""Configuration loaded from {filename} and saved to {config_file_path}"")
        typer.echo(f""Configuration loaded from {filename} and saved to {config_file_path}"")
    except Exception as e:
        logger.error(f""Error loading configuration from {filename}: {e}"")
        typer.echo(f""Error loading configuration: {e}"")
        raise typer.Exit(code=1)",Load a configuration from a file into the default config location.,"???Load and save configuration file, handling errors and logging actions.???"
2117,get_mark_size,"def get_mark_size(self, text_str, image_height, image_width):
        
        key = f""{image_height}_{image_width}""
        if key not in self.markSize_dict:
            self._setup_new_font(image_height, image_width)

        largest_size = self.markSize_dict[key].get(3, None)
        text_h, text_w = self.markSize_dict[key].get(len(text_str), largest_size) # default to the largest size if the text is too long
        return text_h, text_w",Get the font size for the given image dimensions.,???Determine text dimensions based on image size and cache results.???
2118,_extract_json_strategy_2,"def _extract_json_strategy_2(self, text: str) -> Dict[str, Any]:
        
        # Look for JSON object with non-greedy matching
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text)
        if not json_match:
            raise ValueError(""No JSON object found with regex"")
        
        json_str = json_match.group(0)
        return json.loads(json_str)",Strategy 2: Use regex with non-greedy matching.,???Extract JSON object from text using regex and parse it???
2119,parse_datetime,"def parse_datetime(datetime_string: str, time_zone: str) -> datetime:
    
    try:
        # Try to parse as ISO format
        dt = datetime.fromisoformat(datetime_string.replace('Z', '+00:00'))
        # Convert to specified timezone if not already timezone-aware
        if dt.tzinfo is None:
            tz = ZoneInfo(time_zone)
            dt = dt.replace(tzinfo=tz)
        return dt
    except ValueError:
        raise ValueError(f""Invalid datetime format: {datetime_string}"")",Parse datetime string to datetime object with timezone.,"???Convert string to timezone-aware datetime object, handling ISO format and errors.???"
2120,save_memory,"def save_memory(self, agent_type: str = ""casual_agent"") -> None:
        
        if not os.path.exists(self.conversation_folder):
            self.logger.info(f""Created folder {self.conversation_folder}."")
            os.makedirs(self.conversation_folder)
        save_path = os.path.join(self.conversation_folder, agent_type)
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        filename = self.get_filename()
        path = os.path.join(save_path, filename)
        json_memory = json.dumps(self.memory)
        with open(path, 'w') as f:
            self.logger.info(f""Saved memory json at {path}"")
            f.write(json_memory)",Save the session memory to a file.,???Save conversation memory to a JSON file in a structured directory.???
2121,format_team_result,"def format_team_result(team_result: TeamResult) -> dict:
    
    formatted_result = {
        ""task_result"": format_task_result(team_result.task_result),
        ""usage"": team_result.usage,
        ""duration"": team_result.duration,
    }
    return formatted_result",Format the result from TeamResult to a dictionary.,???Transforms a team's performance data into a structured dictionary format.???
2122,apply,"def apply(self, project: str) -> str:
        
        active_project, new_project_generated, new_project_config_generated = self.agent.activate_project_from_path_or_name(project)
        if new_project_generated:
            result_str = (
                f""Created and activated a new project with name {active_project.project_name} at {active_project.project_root}, language: {active_project.project_config.language.value}. ""
                + ""You can activate this project later by name.""
            )
        else:
            result_str = f""Activated existing project with name {active_project.project_name} at {active_project.project_root}, language: {active_project.project_config.language.value}""
        if new_project_config_generated:
            result_str += (
                f""\nNote: A new project configuration was autogenerated because the given path did not contain a {ProjectConfig.SERENA_DEFAULT_PROJECT_FILE} file.""
                + f""You can now edit the project configuration in the file {active_project.path_to_project_yml()}. In particular, you may want to edit the project name and the initial prompt.""
            )

        if active_project.project_config.initial_prompt:
            result_str += f""\nAdditional project information:\n {active_project.project_config.initial_prompt}""
        result_str += (
            f""\nAvailable memories:\n {json.dumps(list(self.memories_manager.list_memories()))}""
            + ""You should not read these memories directly, but rather use the `read_memory` tool to read them later if needed for the task.""
        )
        result_str += f""\nAvailable tools:\n {json.dumps(self.agent.get_active_tool_names())}""
        return result_str",Activates the project with the given name.,"???Activate or create a project, returning status and configuration details???"
2123,setup_page_config,"def setup_page_config() -> None:
    
    st.set_page_config(
        page_title=""Finance Suite Pro"",
        layout=""wide"",
        initial_sidebar_state=""expanded"",
        menu_items={
            'About': ""# Finance Suite Pro\nYour AI-Powered Financial Analysis Platform""
        }
    )",Configure Streamlit page settings and styling,???Configure a financial analysis app's page settings with a wide layout and expanded sidebar.???
2124,get_format_str,"def get_format_str(self) -> str:
        
        format_str = """"
        for field in self._fields:
            if len(field[1]) > 1:
                options = "" | "".join(field[1])
                format_str += f""<[ {options} ]>\n...\n</[ {options} ]>\n""
            else:
                format_str += f""<{field[0]}>\n...\n</{field[0]}>\n""
        return format_str.strip()",Return a string that describes the format of the XML.,???Generate a formatted string based on field options for structured output.???
2125,mock_api_response,"def mock_api_response():
    
    def _mock_response(text_input, chunk_count=1):
        if isinstance(text_input, str):
            if not text_input.strip():
                return []
            # Single text input
            return [{
                ""text"": text_input,
                ""token_count"": max(1, len(text_input.split())),
                ""start_index"": 0,
                ""end_index"": len(text_input),
                ""embedding"": [0.1] * 384  # Mock embedding
            }]
        else:
            # Batch input
            results = []
            for text in text_input:
                if not text.strip():
                    results.append([])
                else:
                    results.append([{
                        ""text"": text,
                        ""token_count"": max(1, len(text.split())),
                        ""start_index"": 0,
                        ""end_index"": len(text),
                        ""embedding"": [0.1] * 384
                    }])
            return results  # Return the list of lists directly for batch response
    return _mock_response",Mock successful API response.,"???  
Generate mock API responses with text analysis and embeddings.  
???"
2126,validate_version,"def validate_version(self, version: str) -> None:
        
        valid_versions = [""1.14"", ""1.15"", ""1.16""]
        if version not in valid_versions:
            raise ValueError(f""Unsupported Elixir version '{version}'. Valid versions: {', '.join(valid_versions)}"")",Validate Elixir version is supported.,"???Check if the given version is supported, raising an error if not.???"
2127,to_dict_exclude_params,"def to_dict_exclude_params(self) -> dict[str, bool]:
        
        return super().to_dict_exclude_params | {
            ""embedder"": True,
            ""_vector_store"": True,
        }",Define parameters to exclude during serialization.,???Override method to exclude specific parameters in dictionary output???
2128,generate_l1,"def generate_l1():
    
    try:
        # 1. Generate L1 data
        result = generate_l1_from_l0()

        if result is None:
            return jsonify(APIResponse.error(""No valid L1 data generated""))

        # 2. Store L1 data
        with DatabaseSession.session() as session:
            version_number = store_l1_data(session, result)

        # 3. Convert result to serializable format
        serializable_result = serialize_value(result.to_dict())

        # 4. return the result
        response_data = {
            ""version"": version_number,
            ""message"": f""L1 data generated and stored successfully with version {version_number}"",
            ""data"": serializable_result,
        }

        return jsonify(APIResponse.success(data=response_data))

    except Exception as e:
        logger.error(f""Error generating L1: {str(e)}"", exc_info=True)
        return jsonify(APIResponse.error(str(e)))",Generate L1 data from L0 data and store,"???Generate and store L1 data, returning success or error response.???"
2129,sample_marker_genes_list,"def sample_marker_genes_list():
    
    return [
        {""1"": [""CD3D"", ""CD3E"", ""CD2""], ""2"": [""CD19"", ""MS4A1"", ""CD79A""]},
        {""3"": [""NCAM1"", ""KLRB1"", ""KLRD1""], ""4"": [""CD14"", ""CD68"", ""FCGR3A""]},
    ]",Create a list of sample marker genes dictionaries for testing.,"???  
Return a list of dictionaries mapping cell types to marker genes.  
???"
2130,load_config,"def load_config():
    
    config_path = os.path.expanduser(f""~/.config/{APP_NAME_CAP}/config/config.json"")
    config = {}
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
        except Exception as e:
            print(f""Error loading config: {e}"")
    
    return config",Load the configuration from config.json,???Load application configuration from a JSON file in the user's config directory.???
2131,load_agent,"def load_agent(self, input_list: List[str]) -> None:
        
        if len(input_list) < 2:
            logger.info(""Please specify an agent name."")
            logger.info(""Format: load-agent {agent_name}"")
            logger.info(""Use 'list-agents' to see available agents."")
            return

        self._load_agent_from_file(agent_name=input_list[1])",Handle load agent command,???Load an agent by name from a file if specified in the input list.???
2132,_get_tool,"def _get_tool(self, action: str) -> Node:
        
        tool = self.tool_by_names.get(self.sanitize_tool_name(action))
        if not tool:
            raise AgentUnknownToolException(
                f""Unknown tool: {action}.""
                ""Use only available tools and provide only the tool's name in the action field. ""
                ""Do not include any additional reasoning. ""
                ""Please correct the action field or state that you cannot answer the question.""
            )
        return tool",Retrieves the tool corresponding to the given action.,"???Retrieve tool node by sanitized action name, raise exception if unknown???"
2133,load_math500_dataset,"def load_math500_dataset() -> list[dict]:
    
    dataset = load_dataset(""HuggingFaceH4/MATH-500"")
    dataset = dataset[""test""]
    logging.debug(f""Dataset size: {len(dataset)}."")
    return dataset",Load the MATH-500 dataset.,???Load and return the test portion of the MATH-500 dataset.???
2134,create_and_queue_audio,"def create_and_queue_audio(text, state):
    
    # Set TTS speaking flag
    state[""tts_is_speaking""] = True

    if not text.strip():
        print(""Empty text, skipping TTS"")
        state[""tts_is_speaking""] = False
        return

    try:
        unique_id = uuid.uuid4()
        with tempfile.TemporaryDirectory() as temp_dir:
            mp3_file = os.path.join(temp_dir, f""temp_{unique_id}.mp3"")
            wav_file = os.path.join(temp_dir, f""temp_{unique_id}.wav"")

            tts = gTTS(text=text, lang=""en"", slow=False)
            tts.save(mp3_file)

            convert_mp3_to_wav(mp3_file, wav_file)

            # Play audio and wait for completion
            play_audio(wav_file, state)
    except Exception as e:
        print(f""Error in TTS process: {e}"")
    finally:
        # Ensure flag is reset even if there's an error
        state[""tts_is_speaking""] = False
        state[""tts_just_finished""] = True

        for file in [mp3_file, wav_file]:
            try:
                if os.path.exists(file):
                    os.remove(file)
            except Exception as e:
                print(f""Error removing temporary file {file}: {e}"")",Create and queue audio with state awareness for TTS/recording coordination,"???Generate and play audio from text, managing state and cleanup.???"
2135,_load_models,"def _load_models(self):
        
        # Load the main model
        self.model, self.tokenizer = load(
            self.model_id_obj.model_id,
            tokenizer_config={""trust_remote_code"": True},
            adapter_path=self.model_id_obj.adapter_path,
        )
        logger.info(f""Loaded new model: {self.model_id_obj.model_id}"")

        # If needed, load the draft model
        if self.model_id_obj.draft_model:
            self.draft_model, self.draft_tokenizer = load(
                self.model_id_obj.draft_model,
                tokenizer_config={""trust_remote_code"": True},
            )

            # Check if vocabulary sizes match
            if self.draft_tokenizer.vocab_size != self.tokenizer.vocab_size:
                logger.warn(
                    f""Draft model({self.model_id_obj.draft_model}) tokenizer does not match model tokenizer.""
                )

            logger.info(f""Loaded new draft model: {self.model_id_obj.draft_model}"")",Load the main model and draft model (if needed).,???Load and configure primary and optional draft models with tokenizer validation.???
2136,msh_to_obj,"def msh_to_obj(msh_file: pathlib.Path) -> str:
    
    msh = Msh.create(msh_file)

    out = io.StringIO()
    for x, y, z in msh.vertex_positions:
        out.write(f""v {x} {y} {z}\n"")
    for x, y, z in msh.vertex_normals:
        out.write(f""vn {x} {y} {z}\n"")
    for u, v in msh.vertex_texcoords:
        # Write texture coordinates directly since we already swapped them
        out.write(f""vt {u} {v}\n"")
    for i, j, k in msh.face_vertex_indices:
        out.write(f""f {i + 1}/{i + 1}/{i + 1} {j + 1}/{j + 1}/{j + 1} {k + 1}/{k + 1}/{k + 1}\n"")

    return out.getvalue()",Convert a legacy .msh file to the .obj format.,???Convert 3D mesh data to OBJ format string representation.???
2137,translate_response,"def translate_response(self, response: dict, model: str) -> dict:
        
        choices = [
            {
                ""finish_reason"": ""stop"",
                ""index"": i,
                ""message"": {""content"": output[""result""], ""role"": ""assistant""},
                ""logprobs"": None,
            }
            for i, output in enumerate(response.get(""outputs"", []))
        ]

        return {
            ""choices"": choices,
            ""created"": int(time.time()),
            ""model"": model,
            ""object"": ""chat.completion"",
            ""usage"": {""total_tokens"": ""-1""},
        }",Converts a Dapr response dict into a structure compatible with Choice and ChatCompletion.,???Transforms API response into structured chat completion format.???
2138,error_suggestion,"def error_suggestion(self, error_message: str) -> str | None:
        
        suggestions = {
            ""failed to load"": ""Check that the file path exists and is readable."",
            ""network"": ""Ensure your internet connection is available."",
        }
        error_lower = error_message.lower()
        for token, suggestion in suggestions.items():
            if token in error_lower:
                self.cli_output.info(f""Suggestion: {suggestion}"")
                return suggestion
        return None",Return a suggestion string for the given error message and output it.,???Provide user-friendly suggestions based on error message content.???
2139,_upload_documents_sync,"def _upload_documents_sync(self, documents: List[Dict[str, Any]], 
                             collection_name: str, embedding_size: str) -> int:
        
        client = self._get_milvus_client(embedding_size)
        
        # Convert documents to Milvus format
        milvus_docs = []
        for doc in documents:
            # Skip documents without embeddings
            if ""embedding"" not in doc or not doc[""embedding""]:
                continue
                
            milvus_docs.append({
                ""id"": int(doc[""id""]) if isinstance(doc[""id""], (int, str)) else doc[""id""],
                ""vector"": doc[""embedding""],
                ""text"": doc[""schema_json""],
                ""url"": doc[""url""],
                ""name"": doc[""name""],
                ""site"": doc[""site""]
            })
        
        if milvus_docs:
            client.insert(collection_name=collection_name, data=milvus_docs)
            logger.info(f""Uploaded {len(milvus_docs)} entities to Milvus collection '{collection_name}'"")
            return len(milvus_docs)
        
        return 0",Synchronous implementation of upload_documents for thread execution,"???Synchronize document upload to Milvus, filtering by embedding presence.???"
2140,execution_failure_check,"def execution_failure_check(self, feedback:str) -> bool:
        
        error_patterns = [
            r""expected"", 
            r""errno"", 
            r""failed"", 
            r""traceback"", 
            r""invalid"", 
            r""unrecognized"", 
            r""exception"", 
            r""syntax"", 
            r""crash"", 
            r""segmentation fault"", 
            r""core dumped""
        ]
        combined_pattern = ""|"".join(error_patterns)
        if re.search(combined_pattern, feedback, re.IGNORECASE):
            self.logger.error(f""Execution failure detected: {feedback}"")
            return True
        self.logger.info(""No execution success detected."")
        return False",Check if the code execution failed.,???Detects execution errors in feedback using predefined error patterns.???
2141,verify_document_embeddings,"def verify_document_embeddings():
    
    try:
        verbose = request.args.get(""verbose"", """").lower() == ""true""
        results = document_service.verify_document_embeddings(verbose=verbose)
        return jsonify(APIResponse.success(data=results))

    except Exception as e:
        logger.error(f""Error verifying document embeddings: {str(e)}"", exc_info=True)
        return jsonify(APIResponse.error(message=f""Error verifying document embeddings: {str(e)}""))",Verify all document embeddings and return statistics,???Verify document embeddings and handle errors with logging???
2142,like_cast,"def like_cast(self, cast_hash: str) -> ReactionsPutResult:
        
        logger.debug(f""Liking cast: {cast_hash}"")
        return self._client.like_cast(cast_hash)",Like a specific cast,???The function logs and sends a like request for a specific cast identified by a hash.???
2143,process_streaming_response,"def process_streaming_response(stream):
    
    full_response = """"
    try:
        for event in stream:
            # Convert event to dictionary if it's a botocore Event object
            event_dict = (
                event.to_response_dict()
                if hasattr(event, ""to_response_dict"")
                else event
            )
            if ""chunk"" in event_dict:
                chunk_data = event_dict[""chunk""]
                if ""bytes"" in chunk_data:
                    output_bytes = chunk_data[""bytes""]
                    # Convert bytes to string if needed
                    if isinstance(output_bytes, bytes):
                        output_text = output_bytes.decode(""utf-8"")
                    else:
                        output_text = str(output_bytes)
                    full_response += output_text
    except Exception as e:
        print(f""\nError processing stream: {e}"")
    return full_response",Process a streaming response from Bedrock Agent.,???Process and concatenate streaming data into a complete response string.???
2144,load_analysis_history,"def load_analysis_history() -> List[Dict]:
    
    try:
        analyses_dir = Path(""analyses"")
        if not analyses_dir.exists():
            return []
            
        history = []
        for file in sorted(analyses_dir.glob(""analysis_*.md""), reverse=True):
            with open(file) as f:
                content = f.read()
                query_section = content.split(""# Results"")[0].replace(""# Analysis Query\n"", """").strip()
                result_section = content.split(""# Results"")[1].strip()
                
                timestamp = file.stem.replace(""analysis_"", """")
                formatted_time = datetime.strptime(timestamp, ""%Y%m%d_%H%M%S"").strftime(""%Y-%m-%d %H:%M:%S"")
                
                history.append({
                    ""timestamp"": timestamp,
                    ""formatted_time"": formatted_time,
                    ""query"": query_section,
                    ""result"": result_section,
                    ""filename"": file.name
                })
        return history
    except Exception as e:
        logger.error(f""Error loading analysis history: {str(e)}"")
        return []",Load all saved analyses,???Load and parse analysis history files into structured data records.???
2145,capture_event,"def capture_event(event_name: str, properties: dict[str, Any] | None = None) -> None:
    
    if not TELEMETRY_ENABLED or posthog_client is None:
        return

    try:
        event_properties = properties or {}
        event_properties.update(get_system_info())
        event_properties.update(POSTHOG_EVENT_SETTINGS)

        if DEBUG_LOGGING:
            logger.debug(f""Telemetry event: {event_name} {event_properties}"")

        posthog_client.capture(distinct_id=INSTALLATION_ID, event=event_name, properties=event_properties)
    except Exception as e:
        logger.debug(f""Failed to send telemetry event {event_name}: {e}"")",Capture an event if telemetry is enabled.,???Capture and log telemetry events with system info and error handling.???
2146,health_check,"def health_check(self) -> bool:
        
        try:
            query = 
            self.cursor.execute(query)
            table_count = self.cursor.fetchone()[0]
            if table_count < 3:
                logger.info(""Tables not found. Initializing SQLite DB..."")
                initialize_sqlite(self.db_path)
            return True

        except Exception as e:
            logger.exception(f""SQLite health check failed: {e}"")
            return False",Check if the SQLite database is healthy and the necessary tables exist.,???Check database health and initialize if tables are missing???
2147,print_available_tools,"def print_available_tools(self):
        
        if not self.available_tools:
            print(""No tools available. Connect to a server first."")
            return
        print(""Available tools:"")
        print(self.available_tools)

        print(f""\nAvailable Tools ({len(self.available_tools)}):"")
        for i, tool in enumerate(self.available_tools):
            print(f""{i + 1}. {tool.name}: {tool.description}"")
            print(f""   Input schema: {tool.inputSchema}"")
            print()",Print all available tools with their descriptions and input schemas,??? Display available tools with details if connected to a server. ???
2148,_interpolate_chunk,"def _interpolate_chunk(self, curr_qpos, qpos_action_chunk):
        
        last_action = qpos_action_chunk[-1]
        assert curr_qpos.shape == last_action.shape, (
            f""Expected {curr_qpos.shape} and {last_action.shape} to be the same, got {curr_qpos.shape} and {last_action.shape} instead""
        )

        return [
            curr_qpos + (last_action - curr_qpos) * (i + 1) / self.policy_cfg.action_config.action_chunk_steps
            for i in range(self.policy_cfg.action_config.action_chunk_steps)
        ]",Smoothly interpolates between the current state and final predicted action of the chunk,???Generate interpolated positions between current and target states over defined steps.???
2149,offload_before_refit,"def offload_before_refit(self) -> None:
        
        torch.randn(1).cuda()  # wake up torch allocator
        if not self.cfg[""fsdp_offload_enabled""]:
            if hasattr(self, ""optimizer"") and self.optimizer is not None:
                for state in self.optimizer.state.values():
                    for k, v in state.items():
                        if torch.is_tensor(v):
                            state[k] = v.to(""cpu"")

        gc.collect()
        torch.cuda.empty_cache()

        # Print memory stats after offloading
        allocated = torch.cuda.memory_allocated() / (1024**3)  # Convert to GB
        reserved = torch.cuda.memory_reserved() / (1024**3)  # Convert to GB
        print(
            f""GPU Memory after optimizer offload: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved""
        )",Offload the optimizer and buffers to the CPU.,???Optimize GPU memory by offloading optimizer state to CPU if offload is enabled.???
2150,get_referenced_tweet_id,"def get_referenced_tweet_id(tweet_id: str, ref_type: str = ""replied_to"") -> Optional[str]:
    
    try:
        tweet = client.get_tweet(tweet_id, expansions=[""referenced_tweets""])

        if not tweet.data or not tweet.data.referenced_tweets:
            return None

        for ref in tweet.data.referenced_tweets:
            if (ref_type == ""replied_to"" and ref.type == ""replied_to"") or (
                ref_type == ""quoted"" and ref.type == ""quoted""
            ):
                return ref.id

        return None

    except Exception as e:
        print(f""Error getting referenced tweet for {tweet_id}: {str(e)}"")
        return None","Get ID of referenced tweet (reply to, quote, etc) ref_type can be 'replied_to' or 'quoted'",???Retrieve specific referenced tweet ID based on type from a given tweet.???
2151,_save_html,"def _save_html(self, html_data: str) -> str:
        
        # Randomly generate a filename.
        filename = f""{uuid.uuid4().hex}.html""
        path = os.path.join(self._output_dir, filename)
        with open(path, ""w"") as f:
            f.write(html_data)
        return os.path.abspath(path)",Save html data to a file.,??? Save HTML content to a uniquely named file and return its absolute path. ???
2152,mock_batch_embedding_response,"def mock_batch_embedding_response(self) -> dict[str, Any]:
        
        return {
            ""object"": ""list"",
            ""data"": [
                {
                    ""object"": ""embedding"",
                    ""index"": 0,
                    ""embedding"": [0.1] * 1024
                },
                {
                    ""object"": ""embedding"", 
                    ""index"": 1,
                    ""embedding"": [0.2] * 1024
                },
                {
                    ""object"": ""embedding"",
                    ""index"": 2,
                    ""embedding"": [0.3] * 1024
                }
            ],
            ""model"": ""jina-embeddings-v3"",
            ""usage"": {
                ""total_tokens"": 30,
                ""prompt_tokens"": 30
            }
        }",Mock response for batch embedding.,???Generate a mock response simulating batch embeddings with metadata and usage details.???
2153,mark_tracks_as_not_present_in_spotify,"def mark_tracks_as_not_present_in_spotify(
    playlist_spotify_id: str, track_ids_to_mark: list
):
    
    table_name = f""playlist_{playlist_spotify_id.replace('-', '_')}""
    if not track_ids_to_mark:
        return
    try:
        with _get_playlists_db_connection() as conn:  # Use playlists connection
            cursor = conn.cursor()
            placeholders = "","".join(""?"" for _ in track_ids_to_mark)
            sql = f""UPDATE {table_name} SET is_present_in_spotify = 0 WHERE spotify_track_id IN ({placeholders})""
            cursor.execute(sql, track_ids_to_mark)
            conn.commit()
            logger.info(
                f""Marked {cursor.rowcount} tracks as not present in Spotify for playlist {playlist_spotify_id} in {PLAYLISTS_DB_PATH}.""
            )
    except sqlite3.Error as e:
        logger.error(
            f""Error marking tracks as not present for playlist {playlist_spotify_id} in {PLAYLISTS_DB_PATH}: {e}"",
            exc_info=True,
        )",Marks specified tracks as not present in the Spotify playlist anymore in playlists.db.,???Update database to mark specified tracks as absent in Spotify playlist.???
2154,save_masks_to_dir,"def save_masks_to_dir(
    output_mask_dir,
    video_name,
    frame_name,
    per_obj_output_mask,
    height,
    width,
    per_obj_png_file,
    output_palette,
):
    
    os.makedirs(os.path.join(output_mask_dir, video_name), exist_ok=True)
    if not per_obj_png_file:
        output_mask = put_per_obj_mask(per_obj_output_mask, height, width)
        output_mask_path = os.path.join(
            output_mask_dir, video_name, f""{frame_name}.png""
        )
        save_ann_png(output_mask_path, output_mask, output_palette)
    else:
        for object_id, object_mask in per_obj_output_mask.items():
            object_name = f""{object_id:03d}""
            os.makedirs(
                os.path.join(output_mask_dir, video_name, object_name),
                exist_ok=True,
            )
            output_mask = object_mask.reshape(height, width).astype(np.uint8)
            output_mask_path = os.path.join(
                output_mask_dir, video_name, object_name, f""{frame_name}.png""
            )
            save_ann_png(output_mask_path, output_mask, output_palette)",Save masks to a directory as PNG files.,???Save object masks as PNG files in a structured directory hierarchy.???
2155,_generate_field_map,"def _generate_field_map(self, force_regenerate: bool = False) -> dict[str, str]:
        
        if self._field_name_to_id_map is not None and not force_regenerate:
            return self._field_name_to_id_map

        # Ensure fields are loaded into cache first
        fields = (
            self.get_fields()
        )  # Uses cache if available unless force_regenerate was True
        if not fields:
            self._field_name_to_id_map = {}
            return {}

        name_map: dict[str, str] = {}
        id_map: dict[str, str] = {}  # Also map ID to ID for consistency
        for field in fields:
            field_id = field.get(""id"")
            field_name = field.get(""name"")
            if field_id:
                id_map[field_id] = field_id  # Map ID to itself
                if field_name:
                    # Store lowercase name -> ID. Handle potential name collisions if necessary.
                    name_map.setdefault(field_name.lower(), field_id)

        # Combine maps, ensuring IDs can also be looked up directly
        self._field_name_to_id_map = name_map | id_map
        logger.debug(
            f""Generated/Updated field name map: {len(self._field_name_to_id_map)} entries""
        )
        return self._field_name_to_id_map",Generates and caches a map of lowercase field names to field IDs.,"???Generate or update a mapping of field names to IDs, optionally forcing regeneration.???"
2156,chat_and_close,"def chat_and_close(self, text):
        
        try:
            # Use the existing chat method
            self.chat(text)

            # After chat is complete, close the connection
            self.close_after_chat = True
        except Exception as e:
            self.logger.bind(tag=TAG).error(f""Chat and close error: {str(e)}"")",Chat with the user and then close the connection,"???Invoke chat method and ensure connection closure, handling errors.???"
2157,parse_from_scheme,"def parse_from_scheme(self, scheme: str, repr: str) -> SequentialWorkFlowGraph:
        
        if scheme not in VALID_SCHEMES:
            raise ValueError(f""Invalid scheme: {scheme}. The scheme should be one of {VALID_SCHEMES}."")
        if scheme == ""python"":
            graph = self.parse_workflow_python_repr(repr)
        elif scheme == ""yaml"":
            graph = self.parse_workflow_yaml_repr(repr)
        elif scheme == ""code"":
            graph = self.parse_workflow_code_repr(repr)
        elif scheme == ""core"":
            graph = self.parse_workflow_core_repr(repr)
        elif scheme == ""bpmn"":
            graph = self.parse_workflow_bpmn_repr(repr)
        return graph",Parse the SequentialWorkFlowGraph from the given scheme and representation.,???Parse workflow representation into a graph based on the specified scheme.???
2158,get_videos,"def get_videos(self):
        
        videos = self.collection.get_videos()
        return [
            {
                ""id"": video.id,
                ""name"": video.name,
                ""description"": video.description,
                ""collection_id"": video.collection_id,
                ""stream_url"": video.stream_url,
                ""length"": video.length,
                ""thumbnail_url"": video.thumbnail_url,
                ""type"": ""video"",
            }
            for video in videos
        ]",Get all videos in a collection.,???Retrieve and format video details from a collection for output.???
2159,_create_usage_cost_table,"def _create_usage_cost_table(services_info: Dict[str, ServiceInfo]) -> str:
    
    if not services_info:
        return 'Cost scaling information not available. See Custom Analysis Data section for detailed cost information.'

    USAGE_TIERS = {
        'Low': 0.5,  # 50% of estimated
        'Medium': 1.0,  # 100% of estimated
        'High': 2.0,  # 200% of estimated
    }

    table = [
        '| Service | Low Usage | Medium Usage | High Usage |',
        '|---------|-----------|--------------|------------|',
    ]

    for service in services_info.values():
        min_cost, max_cost = _parse_cost_value(service.estimated_cost)

        if min_cost == 0 and max_cost == 0:
            table.append(f'| {service.name} | Varies | Varies | Varies |')
            continue

        # Use average if range provided
        base_cost = max_cost if min_cost == max_cost else (min_cost + max_cost) / 2

        costs = {
            tier: f'${int(base_cost * multiplier)}/month'
            for tier, multiplier in USAGE_TIERS.items()
        }

        table.append(f'| {service.name} | {costs[""Low""]} | {costs[""Medium""]} | {costs[""High""]} |')

    return '\n'.join(table)",Create the usage cost table with different usage tiers.,???Generate a usage cost table for services based on estimated cost tiers.???
2160,evaluator,"def evaluator(
        self, task: AllTaskTypes, candidate: AllCandidateTypes
    ) -> AllEvalResultTypes:
        
        # cast to WebVoyagerTask and WebVoyagerCandidate if dicts
        if isinstance(task, dict):
            task = WebVoyagerTask(**task)  # type: ignore
        if isinstance(candidate, dict):
            candidate = WebVoyagerCandidate(**candidate)  # type: ignore
        if self.eval_method == ""exact_match"":
            score = gaia_evaluator(task.ground_truth, candidate.answer)
            return WebVoyagerEvalResult(score=score, reasoning="""")
        elif self.eval_method == ""gpt_eval"":
            score, gpt_response_text = asyncio.run(
                self.gpt_evaluator_async(task, candidate)
            )
            return WebVoyagerEvalResult(score=score, reasoning=gpt_response_text)
        raise ValueError(f""Unknown eval_method: {self.eval_method}"")",Evaluate how 'correct' the candidate answer is relative to the gold_answer.,"???Evaluate task and candidate using specified method, returning score and reasoning.???"
2161,delete_all_llm_thread_id,"def delete_all_llm_thread_id(self):
        
        session = self.DBSession()
        try:
            session.query(User).update({User.llm_thread_id: {}})
            session.query(Chatroom).update({Chatroom.llm_thread_id: {}})
            session.commit()
            return True
        except Exception as e:
            session.rollback()
            logger.error(f""数据库: 清除所有用户llm thread id失败, 错误: {e}"")
            return False
        finally:
            session.close()",Clear llm thread id for everyone,???Clear all LLM thread IDs from User and Chatroom database tables.???
2162,_get_engine_instance,"def _get_engine_instance(self, engine_name: str) -> Optional[BaseSearchEngine]:
        
        # Return cached instance if available
        if engine_name in self.engine_cache:
            return self.engine_cache[engine_name]

        # Create a new instance
        engine = None
        try:
            # Only pass parameters that all engines accept
            common_params = {""llm"": self.llm, ""max_results"": self.max_results}

            # Add max_filtered_results if specified
            if self.max_filtered_results is not None:
                common_params[""max_filtered_results""] = self.max_filtered_results

            engine = create_search_engine(engine_name, **common_params)
        except Exception:
            logger.exception(f""Error creating engine instance for {engine_name}"")
            return None

        if engine:
            # Cache the instance
            self.engine_cache[engine_name] = engine

        return engine",Get or create an instance of the specified search engine,???Retrieve or create and cache a search engine instance by name.???
2163,set_active_adapter,"def set_active_adapter(self, model: PeftModel, adapter_id: str = None) -> bool:
        
        if not isinstance(model, PeftModel):
            logger.warning(""Model is not a PeftModel, cannot set active adapter"")
            return False
            
        available_adapters = self.loaded_adapters.get(model, [])
        
        if not available_adapters:
            logger.warning(""No adapters loaded in model"")
            return False
            
        if adapter_id is None:
            adapter_id = available_adapters[-1]
            
        if adapter_id in available_adapters:
            try:
                model.set_adapter(self._get_adapter_name(adapter_id))
                logger.info(f""Successfully set active adapter to: {adapter_id}"")
                return True
            except Exception as e:
                logger.error(f""Error setting adapter {adapter_id}: {str(e)}"")
                return False
        else:
            logger.warning(f""Requested adapter {adapter_id} not loaded. Available adapters: {available_adapters}"")
            return False",Set a specific adapter as active with error handling,"???Set active adapter for a model, handling errors and logging outcomes.???"
2164,ensure_container,"def ensure_container(self) -> None:
        
        try:
            from docker.errors import NotFound
        except ImportError as e:
            raise ImportError(
                ""Install 'docker' package with 'pip install docker'.""
            ) from e

        try:
            self.execution_container = self.docker_client.containers.get(
                self.container_name
            )
            logger.info(f""Reusing existing container: {self.container_name}"")
        except NotFound:
            logger.info(f""Creating a new container: {self.container_name}"")
            self.create_container()
            self.execution_container.start()
            logger.info(f""Started container: {self.container_name}"")",Ensures that the execution container exists.,"???Ensure Docker container existence, create and start if not found???"
2165,build_tree,"def build_tree(self, model_name: str = None) -> None:
        
        if model_name:
            self.model_name = model_name

        print(f""Building the topic tree with model: {self.model_name}"")

        try:
            self.tree_paths = self.build_subtree(
                [self.args.root_prompt],
                self.system_prompt,
                self.args.tree_degree,
                self.args.tree_depth,
                model_name=self.model_name,
            )

            print(f""Tree building complete. Generated {len(self.tree_paths)} paths."")
            if self.failed_generations:
                print(
                    f""Warning: {len(self.failed_generations)} subtopic generations failed.""
                )

        except Exception as e:
            print(f""Error building tree: {str(e)}"")
            if self.tree_paths:
                print(""Saving partial tree..."")
                self.save(""partial_tree.jsonl"")
            raise",Build the complete topic tree.,???Constructs a topic tree using specified model parameters and handles potential errors.???
2166,do_Ts,"def do_Ts(self, rise: PDFStackT) -> None:
        
        rise_f = safe_float(rise)

        if rise_f is None:
            log.warning(
                f""Could not set text rise because {rise!r} is an invalid float value""
            )
        else:
            self.textstate.rise = rise_f",Set the text rise :param rise: a number expressed in unscaled text space units,"???Set text rise if valid float, log warning otherwise.???"
2167,_prepare_batch,"def _prepare_batch(self, si, batch):
        
        idx = batch[""batch_idx""] == si
        cls = batch[""cls""][idx].squeeze(-1)
        bbox = batch[""bboxes""][idx]
        ori_shape = batch[""ori_shape""][si]
        imgsz = batch[""img""].shape[2:]
        ratio_pad = batch[""ratio_pad""][si]
        if len(cls):
            bbox = ops.xywh2xyxy(bbox)  # target boxes
            bbox[..., [0, 2]] *= ori_shape[1]  # native-space pred
            bbox[..., [1, 3]] *= ori_shape[0]  # native-space pred
        return {""cls"": cls, ""bbox"": bbox, ""ori_shape"": ori_shape, ""imgsz"": imgsz, ""ratio_pad"": ratio_pad}",Prepares a batch for training or inference by applying transformations.,???Prepares and transforms batch data for object detection processing.???
2168,handle_file_upload,"def handle_file_upload(files: list[bytes | io.BytesIO | FileData]) -> list[FileData]:
    
    files_data = []
    for file in files:
        if isinstance(file, FileData):
            files_data.append(file)
        elif isinstance(file, bytes | io.BytesIO):
            file_name = getattr(file, ""name"", generate_fallback_filename(file))
            description = getattr(file, ""description"", generate_file_description(file))
            files_data.append(
                FileData(
                    data=file.getvalue() if isinstance(file, io.BytesIO) else file,
                    name=file_name,
                    description=description,
                )
            )
        else:
            raise ValueError(f""Error: Invalid file data type: {type(file)}. Expected bytes or BytesIO or FileData."")

    return files_data",Handles file uploading with additional metadata.,???Process and convert various file inputs into standardized file data objects.???
2169,get_endpoint_limiter,"def get_endpoint_limiter(self, api_name: str) -> RateLimiter:
        
        rate_limit = self.config.get_api_endpoint_rate_limit(api_name)
        return self.get_or_create_limiter(f""{api_name}_endpoint"", rate_limit)",Get or create a rate limiter for the API endpoint.,???Retrieve or initialize a rate limiter for a specified API endpoint.???
2170,_dereference_sd_image_t_p,"def _dereference_sd_image_t_p(self, c_image: sd_cpp.sd_image_t) -> Dict:
        

        # Calculate the size of the data buffer
        buffer_size = c_image.channel * c_image.width * c_image.height

        image = {
            ""width"": c_image.width,
            ""height"": c_image.height,
            ""channel"": c_image.channel,
            ""data"": self._c_array_to_bytes(c_image.data, buffer_size),
        }
        return image","Dereference a C sd_image_t pointer to a Python dictionary with height, width, channel and data (bytes).",???Convert structured image data into a dictionary format.???
2171,list_keyspaces,"def list_keyspaces(self) -> List[KeyspaceInfo]:
        
        keyspaces = []

        try:
            query = 'SELECT keyspace_name, replication FROM system_schema.keyspaces'
            rows = self.session.execute(query)

            for row in rows:
                name = row.keyspace_name
                replication = row.replication

                keyspace_info = KeyspaceInfo(name=name)
                keyspace_info.replication_strategy = replication.get('class', '')

                rf_string = replication.get('replication_factor', '0')
                try:
                    keyspace_info.replication_factor = int(rf_string)
                except (ValueError, TypeError):
                    keyspace_info.replication_factor = 0

                keyspaces.append(keyspace_info)

            return keyspaces
        except Exception as e:
            logger.error(f'Error listing keyspaces: {str(e)}')
            raise RuntimeError(f'Failed to list keyspaces: {str(e)}')",List all keyspaces in the database.,???Retrieve and process keyspace metadata from a database schema.???
2172,_load_images,"def _load_images(self):
        
        if self.images_loaded: # Don't reload if already loaded
             return True
        try:
            self.img_originals['wall'] = pygame.image.load(self.img_paths['wall']).convert_alpha()
            self.img_originals['floor'] = pygame.image.load(self.img_paths['floor']).convert_alpha()
            self.img_originals['box'] = pygame.image.load(self.img_paths['box']).convert_alpha()
            self.img_originals['box_docked'] = pygame.image.load(self.img_paths['box_docked']).convert_alpha()
            self.img_originals['worker'] = pygame.image.load(self.img_paths['worker']).convert_alpha()
            self.img_originals['worker_docked'] = pygame.image.load(self.img_paths['worker_docked']).convert_alpha()
            self.img_originals['docker'] = pygame.image.load(self.img_paths['docker']).convert_alpha()
            self.images_loaded = True
            # print(""DEBUG: Custom images loaded successfully."")
            return True
        except pygame.error as e:
            # Fallback to default gym-sokoban colors if images fail
            print(f""Error loading images from {self.image_dir}: {e}"", file=sys.stderr)
            print(""Rendering will fall back to default gym-sokoban colors."", file=sys.stderr)
            self.images_loaded = False
            return False
        except FileNotFoundError as e:
             # Fallback to default gym-sokoban colors if images fail
             print(f""Error finding image file in {self.image_dir}: {e}"", file=sys.stderr)
             print(""Rendering will fall back to default gym-sokoban colors."", file=sys.stderr)
             self.images_loaded = False
             return False",Loads the original images from paths.,"???Load and convert game images, with error handling for missing files.???"
2173,gen_version,"def gen_version() -> Version:
    
    now = datetime.datetime.now()
    return Version(f""{now.year}.{now.month}.{now.day}"")",Generate version based on current date,???Generate a version string based on the current date.???
2174,query,"def query(sql: str, source_name: str) -> pd.DataFrame:
    
    try:
        service = PreswaldService.get_instance()
        df_result = service.data_manager.query(sql, source_name)
        logger.info(f""Successfully queried data source: {source_name}"")
        return df_result
    except Exception as e:
        logger.error(f""Error querying data source: {e}"")",Query a data source using sql from preswald.toml by name,???Execute SQL query on data source and return results as DataFrame???
2175,convert_weights_to_lp,"def convert_weights_to_lp(model: nn.Module, dtype=torch.float16):
    

    def _convert_weights(l):

        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            l.weight.data = l.weight.data.to(dtype)
            if l.bias is not None:
                l.bias.data = l.bias.data.to(dtype)

        if isinstance(l, (nn.MultiheadAttention, Attention)):
            for attr in [*[f""{s}_proj_weight"" for s in [""in"", ""q"", ""k"", ""v""]], ""in_proj_bias"", ""bias_k"", ""bias_v""]:
                tensor = getattr(l, attr, None)
                if tensor is not None:
                    tensor.data = tensor.data.to(dtype)

        if isinstance(l, nn.Parameter):
            l.data = l.data.to(dtype)

        for name in [""text_projection"", ""proj""]:
            if hasattr(l, name) and isinstance(l, nn.Parameter):
                attr = getattr(l, name, None)
                if attr is not None:
                    attr.data = attr.data.to(dtype)

    model.apply(_convert_weights)",Convert applicable model parameters to low-precision (bf16 or fp16),???Convert neural network model weights to specified data type for optimization.???
2176,_update_state_and_attributes,"def _update_state_and_attributes(self) -> None:
        
        data = self.coordinator.data or {}
        if not self.coordinator.last_update_success:
            self._attr_native_value = PROVIDER_STATUS_ERROR
        elif not data:
            self._attr_native_value = PROVIDER_STATUS_INITIALIZING
        elif data.get(""last_error""):
            self._attr_native_value = PROVIDER_STATUS_ERROR
        elif ""suggestions"" in data:
             self._attr_native_value = PROVIDER_STATUS_CONNECTED
        else:
            self._attr_native_value = PROVIDER_STATUS_DISCONNECTED

        self._attr_extra_state_attributes = {
            ""last_error_message"": data.get(""last_error"", None),
            ""last_attempted_update"": data.get(""last_update""),
        }",Update sensor state and attributes.,???Update status and attributes based on coordinator data and error conditions.???
2177,get_track_info,"def get_track_info():
    
    spotify_id = request.args.get(""id"")

    if not spotify_id:
        return Response(
            json.dumps({""error"": ""Missing parameter: id""}),
            status=400,
            mimetype=""application/json"",
        )

    try:
        # Import and use the get_spotify_info function from the utility module.
        from routes.utils.get_info import get_spotify_info

        track_info = get_spotify_info(spotify_id, ""track"")
        return Response(json.dumps(track_info), status=200, mimetype=""application/json"")
    except Exception as e:
        error_data = {""error"": str(e), ""traceback"": traceback.format_exc()}
        return Response(json.dumps(error_data), status=500, mimetype=""application/json"")",Retrieve Spotify track metadata given a Spotify track ID.,"???Fetch and return Spotify track details using a provided ID, handling errors.???"
2178,get_local_version,"def get_local_version():
    
    if os.path.exists(VERSION_FILE):
        try:
            with open(VERSION_FILE, ""r"") as f:
                data_content = json.load(f)
                return data_content.get(""version"", ""0.0.0""), data_content.get(""changelog"", [])
        except json.JSONDecodeError:
            print(f""Error: Invalid JSON in local file: {VERSION_FILE}"")
            return ""0.0.0"", []
        except Exception as e:
            print(f""Error reading local version file {VERSION_FILE}: {e}"")
            return ""0.0.0"", []
    return ""0.0.0"", []","Reads the local version file and returns (version, changelog).","???Retrieve software version and changelog from a local JSON file, handling errors gracefully.???"
2179,google_search,"def google_search(query: str) -> str:
    
    global BROWSER_TYPE
    stop_spinner()
    print(f""{MAGENTA}Google search is: {query}{RESET}"")
    encoded_query = quote_plus(query)
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=[""--disable-blink-features=AutomationControlled""])
        if BROWSER_TYPE == 'chrome':
            context = browser.new_context(
                user_agent=""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ...""
            )
        if BROWSER_TYPE == 'chromium':
            context = browser.new_context(
                user_agent=""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ...""
            )
        page = context.new_page()
        page.goto(url)
        page.wait_for_load_state(""networkidle"")
        html = page.content()
        browser.close()
    soup = BeautifulSoup(html, 'html.parser')
    text = soup.get_text()
    cleaned_text = ' '.join(text.split())[0:5000]
    print(cleaned_text)
    return cleaned_text",Performs a Google search using Playwright and scrapes text from the first page.,"???Perform a Google search, retrieve, and clean webpage content.???"
2180,_get_deterministic_port,"def _get_deterministic_port(test_name: str, framework_name: str) -> int:
    
    # Create a unique string by combining test name and framework
    unique_string = f""{test_name}_{framework_name}""

    # Generate a hash and convert to a port number in the range 6000-9999
    hash_value = int(hashlib.md5(unique_string.encode()).hexdigest()[:4], 16)  # noqa: S324
    return 6000 + (hash_value % 4000)",Generate a deterministic port number based on test name and framework.,???Generate a consistent port number from test and framework names.???
2181,_build_orthocenter_task,"def _build_orthocenter_task(self, rng: random.Random, A: Point, B: Point, C: Point):
        
        # Convert segments to lines
        BC_line = sympy.Line(B, C)
        CA_line = sympy.Line(C, A)

        # Calculate altitudes by creating lines perpendicular from each vertex
        alt_A = BC_line.perpendicular_line(A)
        alt_B = CA_line.perpendicular_line(B)

        # Find orthocenter (intersection of any two altitudes, e.g. alt_A and alt_B)
        ortho = alt_A.intersection(alt_B)[0]

        x_ortho_approx = float(ortho.x.evalf())
        y_ortho_approx = float(ortho.y.evalf())

        question_template = rng.choice(self._prompt_templates[""orthocenter""])
        question = question_template.format(A=(A.x, A.y), B=(B.x, B.y), C=(C.x, C.y), a=""a"", b=""b"")
        answer_str = f""({x_ortho_approx:.3f}, {y_ortho_approx:.3f})""
        metadata = {
            ""A"": (str(A.x), str(A.y)),
            ""B"": (str(B.x), str(B.y)),
            ""C"": (str(C.x), str(C.y)),
            ""ortho"": (str(ortho.x), str(ortho.y)),
            ""orthocenter_approx"": (x_ortho_approx, y_ortho_approx),
        }
        return question, answer_str, metadata",Build a question about finding the orthocenter of triangle ABC.,???Calculate triangle orthocenter and generate formatted question-answer pair.???
2182,clear_device_orientation_override,"def clear_device_orientation_override() -> typing.Generator[
    T_JSON_DICT, T_JSON_DICT, None
]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DeviceOrientation.clearDeviceOrientationOverride"",
    }
    json = yield cmd_dict",Clears the overridden Device Orientation.,???Clear the device's orientation override setting using a command dictionary.???
2183,build_description,"def build_description(self):
        
        # A sequence of string(s) representing the options of the expected response.
        self._constrained_responses = _CONSTRAINED_RESPONSE_OPTIONS
        self._description_pattern = (
            ""Answer with one of the following options: {response_options}""
        )
        return self._description_pattern.format(
            response_options=self._constrained_responses
        )",Build the instruction description.,???Generate a formatted response description using predefined options.???
2184,prompt,"def prompt(self, problem):
        
        test_case = json.loads(problem[""input_output""])
        starter_code = problem[""starter_code""]
        prompt_text = generate_prompt(test_case, problem[""question""], starter_code)
        return [{""role"": ""system"", ""content"": SKY_T1_SYSTEM_PROMPT}, {""role"": ""user"", ""content"": prompt_text}]",Parse test cases and starter code from problem to create a prompt for the LLM.,???Generate a structured prompt for a coding problem using test cases and starter code.???
2185,setup_agent,"def setup_agent():
    
    llm = setup_llm()

    try:
        aws_connection = AWS()

        dynamo_db = DynamoDB(
            connection=aws_connection,
            table_name=""default"",
            create_if_not_exist=True,
        )
        print(""DynamoDB backend initialized. Connection test performed during init."")

    except Exception as e:
        print(f""FATAL: Failed to initialize DynamoDB backend: {e}"", file=sys.stderr)
        print(""Please ensure DynamoDB is running and accessible with correct credentials."", file=sys.stderr)
        raise

    memory = Memory(backend=dynamo_db, message_limit=50)

    AGENT_ROLE = ""Helpful assistant focusing on the current conversation.""
    agent = SimpleAgent(
        name=""ChatAgent"",
        llm=llm,
        role=AGENT_ROLE,
        id=""agent"",
        memory=memory,
    )
    return agent",Sets up the SimpleAgent with DynamoDB memory.,???Initialize a conversational agent with memory using AWS DynamoDB as storage.???
2186,_prepend_after_frontmatter,"def _prepend_after_frontmatter(self, current_content: str, content: str) -> str:
        

        # Check if file has frontmatter
        if has_frontmatter(current_content):
            try:
                # Parse and separate frontmatter from body
                frontmatter_data = parse_frontmatter(current_content)
                body_content = remove_frontmatter(current_content)

                # Prepend content to the body
                if content and not content.endswith(""\n""):
                    new_body = content + ""\n"" + body_content
                else:
                    new_body = content + body_content

                # Reconstruct file with frontmatter + prepended body
                yaml_fm = yaml.dump(frontmatter_data, sort_keys=False, allow_unicode=True)
                return f""---\n{yaml_fm}---\n\n{new_body.strip()}""

            except Exception as e:  # pragma: no cover
                logger.warning(
                    f""Failed to parse frontmatter during prepend: {e}""
                )  # pragma: no cover
                # Fall back to simple prepend if frontmatter parsing fails  # pragma: no cover

        # No frontmatter or parsing failed - do simple prepend  # pragma: no cover
        if content and not content.endswith(""\n""):  # pragma: no cover
            return content + ""\n"" + current_content  # pragma: no cover
        return content + current_content","Prepend content after frontmatter, preserving frontmatter structure.","???Prepend content to a document, handling frontmatter if present.???"
2187,_validate_update_ids,"def _validate_update_ids(self, collection_name: str, ids: list[str]) -> bool:
        
        retrieved_ids = [
            point.id for point in self.client.retrieve(collection_name, ids=ids, with_payload=False, with_vectors=False)
        ]

        if missing_ids := set(ids) - set(retrieved_ids):
            logger.log(f""Missing IDs: {missing_ids}. Skipping update"", level=logging.WARN)
            return False

        return True",Validates all the IDs exist in the collection,???Check if all specified IDs exist in the collection before updating.???
2188,merge_args,"def merge_args(args1, args2):
    
    if args2 is None:
        return args1

    for k in args2._content.keys():
        if k in args1.__dict__:
            v = getattr(args2, k)
            if isinstance(v, ListConfig) or isinstance(v, DictConfig):
                v = OmegaConf.to_object(v)
            setattr(args1, k, v)
        else:
            raise RuntimeError(f""Unknown argument {k}"")

    return args1",Merge two argparse Namespace objects.,???Merge and validate configuration arguments with error handling.???
2189,remove_observer,"def remove_observer(self, observer: WorkflowObserver) -> None:
        
        if observer in self.observers:
            self.observers.remove(observer)
            logger.debug(f""Removed observer: {observer}"")",Remove an event observer callback.,???Remove a specified observer from the list and log the action.???
2190,check_hourly_reset,"def check_hourly_reset():
    
    global last_hour_checked, next_reset_check
    
    current_time = datetime.datetime.now()
    current_hour = current_time.hour
    
    # Skip if we've already checked this hour
    if last_hour_checked == current_hour:
        return
    
    # Only reset at the top of the hour (00 minute mark)
    if current_time.minute == 0:
        logger.info(f""Hour changed to {current_hour}:00, resetting hourly API caps"")
        reset_hourly_caps()
        last_hour_checked = current_hour",Check if we need to reset hourly caps based on the current hour,???Check and reset API limits at the start of each new hour.???
2191,multiple_messages_log,"def multiple_messages_log() -> LLMMessageLogType:
    
    return [
        {
            ""input_ids"": torch.tensor([1, 2]),
            ""attention_mask"": torch.tensor([1, 1]),
            ""text"": ""first"",
        },
        {
            ""input_ids"": torch.tensor([3, 4]),
            ""attention_mask"": torch.tensor([1, 1]),
            ""text"": ""second"",
        },
    ]",Fixture for multiple messages with tensor and text data.,"???Function returns a list of message logs with input IDs, attention masks, and text.???"
2192,is_configured,"def is_configured(self, verbose = False) -> bool:
        
        try:
            load_dotenv()
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                return False

            client = Anthropic(api_key=api_key)
            client.models.list()
            return True
            
        except Exception as e:
            if verbose:
                logger.debug(f""Configuration check failed: {e}"")
            return False",Check if Anthropic API key is configured and valid,???Check if API configuration is valid and log errors if verbose.???
2193,from_json,"def from_json(cls, json_input: Union[str, Path]) -> ""SpecSchema"":
        
        data: Dict[str, Any]
        try:
            file_path: Path | None = None
            if isinstance(json_input, Path):
                file_path = json_input
            elif isinstance(json_input, str):
                try:
                    possible_path = Path(json_input)
                    if possible_path.is_file() or possible_path.exists():
                        file_path = possible_path
                except OSError:
                    file_path = None

            if file_path is not None:
                if not file_path.exists():
                    raise FileNotFoundError(f""JSON file not found: {file_path}"")
                with file_path.open(""r"", encoding=""utf-8"") as f:
                    data = json.load(f)
            else:
                # Assume it's a JSON string
                data = json.loads(str(json_input))
            return cls.from_dict(data)
        except json.JSONDecodeError as e:
            print(f""Error decoding JSON: {e}"")
            raise
        except (FileNotFoundError, ValidationError) as e:
            print(f""Error processing JSON input: {e}"")
            raise",Creates a SpecSchema instance from a JSON string or file path.,"???  
Convert JSON input (file or string) into a SpecSchema object.  
???"
2194,validate_optim_states_are_reset,"def validate_optim_states_are_reset(self):
    

    param_groups_cnt = len(self.vl_optim.param_groups)
    param_group_states = list(self.vl_optim.state.values())[:param_groups_cnt]
    for i, state in enumerate(param_group_states):
        if state[""step""] != 1:
            raise ValueError(f""optimizer reset didn't seem to work: state={i} step={state['step']}"")
        if not all(state[""exp_avg""] == 0):
            raise ValueError(f""optimizer reset didn't seem to work: state={i} step={state['exp_avg']}"")
        if not all(state[""exp_avg_sq""] == 0):
            raise ValueError(f""optimizer reset didn't seem to work: state={i} step={state['exp_avg_sq']}"")",for a new or fully reset optimizer we expect all zeros `exp_avg` and `exp_avg_sq` state tensors and step=1,???Ensure optimizer states are reset by checking step and averages???
2195,save_levels_dimensions,"def save_levels_dimensions(levels_filename, max_level=52):
    
    dims = {}
    os.makedirs(CACHE_DIR, exist_ok=True)
    outpath = os.path.join(CACHE_DIR, ""levels_dim.json"")

    for lvl in range(1, max_level + 1):
        g = game(levels_filename, lvl)
        # load_size() returns pixel size (width, height) = (cols*32, rows*32)
        pixel_width, pixel_height = g.load_size()

        # Convert pixel dimensions back to tile counts

        # Store data in a dict keyed by level number
        dims[f""level_{lvl}""] = {""cols"": cols, ""rows"": rows}

    with open(outpath, ""w"") as f:
        json.dump(dims, f, indent=2)

    print(f""Level dimensions saved to: {outpath}"")","Reads each level from 1..max_level, retrieves its matrix dimension, and saves all dimensions in JSON form to ""cache/sokoban/levels_dim.json"".",???Store game level dimensions in JSON for caching purposes.???
2196,file_upload_limit_check,"def file_upload_limit_check(self, base64_image: str) -> None:
        
        provider = self.config.model.split(""/"")[0]
        if provider in _FILE_UPLOAD_LIMIT_PROVIDERS:
            mb = get_base64_size(base64_image)
            limit = _FILE_UPLOAD_LIMIT_PROVIDERS[provider]
            if mb > limit:
                raise ValueError(f""Uploaded object size is {mb} MB,"", f""which is greater than the allowed size of {limit} MB."")",Check if the image size is within the allowed limit.,???Check if uploaded image size exceeds provider's limit and raise error if so.???
2197,run_command,"def run_command(command: list[str], return_stdout=False, env=None):
    
    for i, c in enumerate(command):
        if isinstance(c, Path):
            command[i] = str(c)

    if env is None:
        env = os.environ.copy()

    try:
        output = subprocess.check_output(command, stderr=subprocess.STDOUT, env=env)
        if return_stdout:
            if hasattr(output, ""decode""):
                output = output.decode(""utf-8"")
            return output
    except subprocess.CalledProcessError as e:
        raise SubprocessCallException(
            f""Command `{' '.join(command)}` failed with the following error:\n\n{e.output.decode()}""
        ) from e",Runs command with subprocess.check_output and returns stdout if requested.,???Execute shell command with optional output capture and error handling.???
2198,process_next_frame,"def process_next_frame(self):
        

        if self.current_frame_number > self.max_frame_number:
            # print(""Stopping frame_read_timer as all frames have been read!"")
            self.frame_read_timer.stop()
            return

        if self.frame_queue.qsize() >= self.num_threads:
            # print(f""Queue is full ({self.frame_queue.qsize()} frames). Throttling frame reading."")
            return

        if self.file_type == 'video' and self.media_capture:
            ret, frame = misc_helpers.read_frame(self.media_capture, preview_mode = not self.recording)
            if ret:
                frame = frame[..., ::-1]  # Convert BGR to RGB
                # print(f""Enqueuing frame {self.current_frame_number}"")
                self.frame_queue.put(self.current_frame_number)
                self.start_frame_worker(self.current_frame_number, frame)
                self.current_frame_number += 1
            else:
                print(""Cannot read frame!"", self.current_frame_number)
                self.stop_processing()
                self.main_window.display_messagebox_signal.emit('Error Reading Frame', f'Error Reading Frame {self.current_frame_number}.\n Stopped Processing...!', self.main_window)",Read the next frame and add it to the queue for processing.,"???Process video frames, enqueue them, and handle errors if reading fails.???"
2199,get_edit_form_get,"def get_edit_form_get(self, field_name: str, shared_pdf: SharedPdf):
        

        form_dict = self.get_edit_form_dict()

        initial_dict = {
            'name': {'name': shared_pdf.name},
            'description': {'description': shared_pdf.description},
            'max_views': {'max_views': shared_pdf.max_views},
            'password': {'password': ''},
            'expiration_date': {'expiration_date': ''},
            'deletion_date': {'deletion_date': ''},
        }

        form = form_dict[field_name](initial=initial_dict[field_name])

        return form",Get the form belonging to the specified field.,???Generate an edit form for a PDF field with initial values.???
2200,construct,"def construct(
        self,
        trajectory: Trajectory,
        intent: str,
        meta_data: dict[str, Any] = {},
    ) -> APIInput:
        
        intro = self.instruction[""intro""]
        examples = self.instruction[""examples""]
        template = self.instruction[""template""]
        keywords = self.instruction[""meta_data""][""keywords""]
        state_info: StateInfo = trajectory[-1]  # type: ignore[assignment]

        obs = state_info[""observation""][self.obs_modality]
        max_obs_length = self.lm_config.gen_config[""max_obs_length""]
        if max_obs_length:
            obs = self.tokenizer.decode(self.tokenizer.encode(obs)[:max_obs_length])  # type: ignore[arg-type]

        page = state_info[""info""][""page""]
        url = page.url
        previous_action_str = meta_data[""action_history""][-1]

        # input x
        current = template.format(
            objective=intent,
            url=self.map_url_to_real(url),
            observation=obs,
            previous_action=previous_action_str,
        )

        # make sure all keywords are replaced
        assert all([f""{{k}}"" not in current for k in keywords])
        prompt = self.get_lm_api_input(intro, examples, current)
        return prompt",Construct prompt given the trajectory,???Generate API input by formatting trajectory data and intent into a template.???
2201,format_tree,"def format_tree(tree: Dict, start: int, end: int) -> str:
    
    if not tree:  # Handle empty or None tree
        return ""==== No files to display ====""
    
    lines = []
    _format_tree_recursive(tree, lines, 0)
    total_lines = len(lines)
    
    if total_lines == 0:  # Handle case with no valid lines
        return ""==== No files to display ====""
    
    is_last_block = end >= total_lines
    output = ""\n"".join(lines[start - 1 : end])

    header = f""==== Lines: {start}-{end} of {total_lines} ====""
    if is_last_block:
        header = f""==== Lines: {start}-{total_lines} of {total_lines} ====""
        header += f"" [LAST BLOCK] (total_lines: {total_lines})""
    return f""{header}\n{output}\n==== End of Block ====""",Format tree structure into string output with line information.,???Format and display a tree structure with pagination and header information.???
2202,_build_hybrid_index,"def _build_hybrid_index(self, document_paths: List[str]):
        
        # Load documents if needed
        if not self.document_store:
            documents = self._load_documents(document_paths)
            
            # Store documents and their text for BM25
            tokenized_corpus = []
            
            for doc in documents:
                # Process text for BM25
                text = doc.text.lower()
                tokens = text.split()
                
                # Store document info
                self.document_store.append({
                    'text': doc.text,
                    'metadata': doc.metadata,
                    'tokens': tokens
                })
                
                tokenized_corpus.append(tokens)
            
            # Create BM25 index
            self.bm25_index = BM25Okapi(tokenized_corpus)
        
        # Rebuild embedding index if needed
        if self.force_reindex or not self.index:
            self._create_index(document_paths)",Build BM25 index and optionally rebuild embedding index.,???Builds a hybrid search index using BM25 and embeddings for documents.???
2203,_print_response,"def _print_response(self, ai_name: str, response: str):
        
        self.console.print(f""\n[bold blue]{ai_name}[/bold blue]:"")
        self.console.print(Markdown(response))
        # Store in history
        self.conversation_history.append((ai_name, response))",Pretty prints an AI response using Rich.,???Display AI response and log it in conversation history.???
2204,format_state_machine_response,"def format_state_machine_response(state_machine_name: str, payload: bytes) -> str:
    
    try:
        # Try to parse the payload as JSON
        payload_json = json.loads(payload)
        return f'State machine {state_machine_name} returned: {json.dumps(payload_json, indent=2)}'
    except (json.JSONDecodeError, UnicodeDecodeError):
        # Return raw payload if not JSON
        return f'State machine {state_machine_name} returned payload: {payload}'",Format the Step Functions state machine response payload.,???Format state machine response by attempting JSON parsing or returning raw payload.???
2205,create_conversation,"def create_conversation(self, **kwargs) -> Conversation:
        
        merged_kwargs = {**self.default_kwargs, **kwargs}
        return create_conversation(
            llm_provider=self.llm_provider, llm_model=self.llm_model, **merged_kwargs
        )",Create a conversation using the session's default provider and model.,"???  
Initialize a conversation using default and provided parameters.  
???"
2206,format_value,"def format_value(self, value):
        
        if value:
            self.attrs[""data-value""] = (
                value  # We use this to dynamically select the initial date on AirDatePicker
            )

        if value is None:
            return """"
        if isinstance(value, str):
            try:
                value = datetime.datetime.strptime(value, ""%Y-%m-%d"").date()
            except ValueError:
                return value
        if isinstance(value, (datetime.datetime, datetime.date)):
            # Use Django's date translation
            month_name = dates.MONTHS[value.month]
            return f""{month_name} {value.year}""
        return value",Format the value for display in the widget.,???Format and validate date input for dynamic date selection and display???
2207,deliver_push_message,"def deliver_push_message(
    origin: str, registration_id: RegistrationID, data: str
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""origin""] = origin
    params[""registrationId""] = registration_id.to_json()
    params[""data""] = data
    cmd_dict: T_JSON_DICT = {
        ""method"": ""ServiceWorker.deliverPushMessage"",
        ""params"": params,
    }
    json = yield cmd_dict",:param origin: :param registration_id: :param data:,???Send push notification using service worker with specified parameters???
2208,_initialize_aave_contracts,"def _initialize_aave_contracts(self, web3: Web3):
        
        try:
            return get_helper_contracts(web3)
        except AaveContractsNotConfigured as e:
            raise RuntimeError(f""Aave v3 not supported on chain ID {web3.eth.chain_id}"") from e",Initialize Aave contracts for a given Web3 instance.,"???Initialize Aave contracts using Web3, handling unsupported chain exceptions.???"
2209,check_outputs_equal,"def check_outputs_equal(
    *,
    outputs_0_lst: Sequence[TokensText],
    outputs_1_lst: Sequence[TokensText],
    name_0: str,
    name_1: str,
):
    
    assert len(outputs_0_lst) == len(outputs_1_lst)

    for prompt_idx, (outputs_0,
                     outputs_1) in enumerate(zip(outputs_0_lst,
                                                 outputs_1_lst)):
        output_ids_0, output_str_0 = outputs_0
        output_ids_1, output_str_1 = outputs_1

        # The text and token outputs should exactly match
        fail_msg = (f""Test{prompt_idx}:""
                    f""\n{name_0}:\t{output_str_0!r}""
                    f""\n{name_1}:\t{output_str_1!r}"")

        assert output_str_0 == output_str_1, fail_msg
        assert output_ids_0 == output_ids_1, fail_msg","Compare the two sequences generated by different models, which should be equal.",???Verify equality of paired text and token outputs across two sequences???
2210,_initialize_web3,"def _initialize_web3(self) -> None:
        
        if not self._web3:
            for attempt in range(3):
                try:
                    self._web3 = Web3(Web3.HTTPProvider(self.rpc_url))
                    self._web3.middleware_onion.inject(geth_poa_middleware, layer=0)
                    
                    if not self._web3.is_connected():
                        raise MonadConnectionError(""Failed to connect to Monad network"")
                    
                    chain_id = self._web3.eth.chain_id
                    if chain_id != self.chain_id:
                        raise MonadConnectionError(f""Connected to wrong chain. Expected {self.chain_id}, got {chain_id}"")
                        
                    logger.info(f""Connected to Monad network with chain ID: {chain_id}"")
                    break
                    
                except Exception as e:
                    if attempt == 2:
                        raise MonadConnectionError(f""Failed to initialize Web3 after 3 attempts: {str(e)}"")
                    logger.warning(f""Web3 initialization attempt {attempt + 1} failed: {str(e)}"")
                    time.sleep(1)",Initialize Web3 connection with retry logic,???Initialize and verify a Web3 connection to a specific blockchain network with retry logic.???
2211,_cdp_url_logic,"def _cdp_url_logic(self) -> str:
        
        cdp_url = self.cdp_url
        if self.nstbrowser_mode:
            if self.nstbrowser_config and isinstance(self.nstbrowser_config, dict):
                config = self.nstbrowser_config
            else:
                query = NSTBROWSER_DEFAULT_QUERY.copy()
                if self.stealth:
                    flags = self.__set_flags()
                    query.update({
                        ""args"": dict(zip(flags, [''] * len(flags))),  # browser args should be a dictionary
                    })

                config = {
                    'config': json.dumps(query),
                    # 'token': ''
                }
            cdp_url = construct_cdp_url(cdp_url, config)
        else:
            # To validate it
            cdp_url = construct_cdp_url(cdp_url)

        return cdp_url",Constructs new CDP URL if NSTBrowser is enabled otherwise return CDP URL as it is :return: CDP URL,???Constructs a browser URL based on configuration and mode settings.???
2212,profile_command,"def profile_command(session: CodegenSession):
    
    repo_config = session.config.repository
    rich.print(
        Panel(
            f""[cyan]Name:[/cyan]  {repo_config.user_name}\n[cyan]Email:[/cyan] {repo_config.user_email}\n[cyan]Repo:[/cyan]  {repo_config.repo_name}"",
            title=""🔑 [bold]Current Profile[/bold]"",
            border_style=""cyan"",
            box=box.ROUNDED,
            padding=(1, 2),
        )
    )",Display information about the currently authenticated user.,???Display user profile details from session configuration in a styled panel.???
2213,check_model,"def check_model(self, model: str, **kwargs) -> bool:
        
        try:
            client = self._get_client()
            try:
                client.models.retrieve(model_id=model)
                return True
            except NotFoundError:
                logging.error(""Model not found."")
                return False
            except Exception as e:
                raise AnthropicAPIError(f""Model check failed: {e}"")
                
        except Exception as e:
            raise AnthropicAPIError(f""Model check failed: {e}"")",Check if a specific model is available,???Check if a specified model exists using a client retrieval method.???
2214,rewrite_query,"    
    messages = format_prompt(query)
    
    payload = {
        ""model"": ""DeepRetrieval/DeepRetrieval-TriviaQA-BM25-3B"",
        ""messages"": messages,
        ""temperature"": 0.7,
        ""max_tokens"": 512
    }
    
    headers = {""Content-Type"": ""application/json""}
    
    try:
        response = requests.post(api_url, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        
        # Extract the generated text from the response
        generated_text = result['choices'][0]['message']['content']
        
        # Extract the rewritten query
        rewritten_query = extract_query(generated_text)
        return rewritten_query
        
    except requests.exceptions.RequestException as e:
        raise Exception(f""API request failed: {e}"")
    except Exception as e:
        raise Exception(f""Failed to process response: {e}"")",Send the query to the vLLM API and get the rewritten version.,"???Send query to AI model, extract and return rewritten query text.???"
2215,is_configured,"def is_configured(self, verbose = False) -> bool:
        
        logger.debug(""Checking Twitter configuration status"")
        try:
            # check if credentials exist
            self._get_credentials()

            # Test the configuration by making a simple API call
            self._get_authenticated_user_info()
            logger.debug(""Twitter configuration is valid"")
            return True

        except Exception as e:
            if verbose:
                error_msg = str(e)
                if isinstance(e, TwitterConfigurationError):
                    error_msg = f""Configuration error: {error_msg}""
                elif isinstance(e, TwitterAPIError):
                    error_msg = f""API validation error: {error_msg}""
                logger.error(f""Configuration validation failed: {error_msg}"")
            return False",Check if Twitter credentials are configured and valid,???Check Twitter API configuration validity and log errors if verbose???
2216,ask_camera,"def ask_camera() -> Dict[str, Any]:
    
    body = app.current_event.json_body or {}
    camera_location = body.get(""camera"")
    question = body.get(""question"")

    # Validate input
    is_valid, error_message = validate_camera_request(
        camera_location, question, CAMERA_STREAMS
    )
    if not is_valid:
        return {""error"": error_message}, 400

    try:
        logger.info(f""Asking {camera_location} camera question: {question}"")
        response = get_latest_frame_from_camera(camera_location)

        if ""Images"" not in response or not response[""Images""]:
            return {
                ""error"": f""No video data currently available in stream for {camera_location} camera""
            }, 200

        base64_encoded_image = response[""Images""][0][""ImageContent""]

        response = analyze_camera_image(base64_encoded_image, camera_location, question)

        return {""message"": response}, 200

    except Exception as e:
        logger.error(f""Error processing camera request for {camera_location}: {str(e)}"")
        return {""error"": f""Internal server error: {str(e)}""}, 500",Process questions about camera images.,"???Process and analyze camera data based on user queries, returning results or errors.???"
2217,create_output_dirs,"def create_output_dirs() -> Tuple[Path, Path]:
    
    date_str = datetime.now().strftime(""%Y%m%d"")
    base_dir = Path(""evaluation"") / ""default"" / f""{date_str}_raaid""
    log_dir = base_dir / ""logs""
    base_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    return base_dir, log_dir",Create base/log directory structure.,???Create timestamped directories for evaluation and logging???
2218,safely_close_audio_stream,"def safely_close_audio_stream(stream):
        
        if stream:
            try:
                if stream.is_active():
                    stream.stop_stream()
                stream.close()
            except Exception as e:
                print(f""Error closing audio stream: {e}"")",Safely close an audio stream with error handling,"???Safely terminate and close an active audio stream, handling exceptions.???"
2219,add_communication_response_data,"def add_communication_response_data() -> Dict[str, Any]:
    
    return {
        ""result"": True,
        ""status"": ""success"",
        ""message"": ""Communication added successfully to case: case-12345678910-2013-c4c1d2bf33c5cf47"",
    }",Return a dictionary with sample add communication response data.,???Function returns a success message for adding communication to a case???
2220,get_device_uuid,"def get_device_uuid(device_idx: int) -> str:
    
    # Convert logical device index to physical device index
    global_device_idx = device_id_to_physical_device_id(device_idx)

    # Get the device handle and UUID
    with nvml_context():
        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(global_device_idx)
            uuid = pynvml.nvmlDeviceGetUUID(handle)
            # Ensure the UUID is returned as a string, not bytes
            if isinstance(uuid, bytes):
                return uuid.decode(""utf-8"")
            elif isinstance(uuid, str):
                return uuid
            else:
                raise RuntimeError(
                    f""Unexpected UUID type: {type(uuid)} for device {device_idx} (global index: {global_device_idx})""
                )
        except pynvml.NVMLError as e:
            raise RuntimeError(
                f""Failed to get device UUID for device {device_idx} (global index: {global_device_idx}): {e}""
            )",Get the UUID of a CUDA device using NVML.,"???Retrieve the UUID of a device using its logical index, handling errors and type conversions.???"
2221,model_name_to_classes,"def model_name_to_classes(model_name_or_path):
    

    model_name_lowcase = model_name_or_path.lower()
    for rx, classes in model_name2classes.items():
        if re.search(rx, model_name_lowcase):
            return classes
    else:
        raise ValueError(
            f""Unknown type of backbone LM. Got {model_name_or_path}, supported regexes:""
            f"" {list(model_name2classes.keys())}.""
        )","returns config_class, model_class for a given model name or path",???Map model name to corresponding class list using regex patterns.???
2222,check_model,"def check_model(self, model: str, **kwargs) -> bool:
        
        try:
            client = self._get_client()
            try:
                models = client.models.list()
                for groq_model in models.data:
                    if groq_model.id == model:
                        return True
                return False
            except Exception as e:
                raise GroqAPIError(f""Model check failed: {e}"")
                
        except Exception as e:
            raise GroqAPIError(f""Model check failed: {e}"")",Check if a specific model is available,???Check if a specified model exists in the client's model list.???
2223,load_strike_data,"def load_strike_data(app_name):
    
    app_state_dir = ensure_state_directory(app_name)
    strike_file = os.path.join(app_state_dir, ""strikes.json"")
    
    if not os.path.exists(strike_file):
        return {}
    
    try:
        with open(strike_file, 'r') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        swaparr_logger.error(f""Error loading strike data for {app_name}: {str(e)}"")
        SWAPARR_STATS['errors_encountered'] += 1
        return {}",Load strike data for a specific app,"???Load and parse strike data from a JSON file, handling errors gracefully.???"
2224,file_upload_limit_check,"def file_upload_limit_check(self, base64_image: str) -> None:
        
        mb = get_base64_size(base64_image)
        if mb > _OPENAI_ALLOWED_IMAGE_SIZE_MB:
            raise ValueError(f""Image size is {mb} MB, which is greater than the allowed size of {_OPENAI_ALLOWED_IMAGE_SIZE_MB} MB."")",Check if the image size is within the allowed limit.,???Check if the image size exceeds the allowed upload limit and raise an error if it does.???
2225,get_file_add_date,"def get_file_add_date(file_path):
        
        date_str = _run_cmd(f""git log --follow --format=%as -- {file_path} | tail -n 1"")
        return datetime.datetime.strptime(date_str, ""%Y-%m-%d"")",Return the date a given file was added to git,???Retrieve the initial commit date of a file using Git commands.???
2226,structured_response,"def structured_response(self, prompt: str, response_model: Type[T], **kwargs) -> T:
        
        # Only try to pop if the key exists
        kwargs.pop(""llm_model"", None)  # Add default value of None

        try:
            response = self.structured_client.chat.completions.create(
                messages=[{""role"": ""user"", ""content"": prompt}],
                response_model=response_model,
                **kwargs,
            )
        except Exception as e:
            # Handle the exception appropriately, e.g., log the error or raise a custom exception
            raise RuntimeError(
                f""Failed to send structured response to Gemini API: {e}""
            ) from e
        return response_model.model_validate(response)",Send a structured response to the Gemini API.,???Generate structured API response using a specified model and handle exceptions.???
2227,load_tuning_state,"def load_tuning_state(self, path: str):
        
        if path.startswith(""http""):
            state = torch.hub.load_state_dict_from_url(path, map_location=""cpu"")
        else:
            state = torch.load(path, map_location=""cpu"")

        module = dist_utils.de_parallel(self.model)

        # Load the appropriate state dict
        if ""ema"" in state:
            pretrain_state_dict = state[""ema""][""module""]
        else:
            pretrain_state_dict = state[""model""]

        # Adjust head parameters between datasets
        try:
            adjusted_state_dict = self._adjust_head_parameters(
                module.state_dict(), pretrain_state_dict
            )
            stat, infos = self._matched_state(module.state_dict(), adjusted_state_dict)
        except Exception:
            stat, infos = self._matched_state(module.state_dict(), pretrain_state_dict)

        module.load_state_dict(stat, strict=False)
        print(f""Load model.state_dict, {infos}"")",Load model for tuning and adjust mismatched head parameters,???Load and adjust model state from a given path or URL for tuning.???
2228,reset_language_server,"def reset_language_server(self) -> None:
        
        # stop the language server if it is running
        if self.is_language_server_running():
            assert self.language_server is not None
            log.info(f""Stopping the current language server at {self.language_server.repository_root_path} ..."")
            self.language_server.stop()
            self.language_server = None

        # instantiate and start the language server
        assert self._active_project is not None
        multilspy_config = MultilspyConfig(
            code_language=self._active_project.project_config.language,
            ignored_paths=self._active_project.project_config.ignored_paths,
            trace_lsp_communication=self.serena_config.trace_lsp_communication,
        )
        ls_logger = MultilspyLogger(log_level=self.serena_config.log_level)
        log.info(f""Starting language server for {self._active_project.project_root}."")
        self.language_server = SyncLanguageServer.create(
            multilspy_config,
            ls_logger,
            self._active_project.project_root,
            add_gitignore_content_to_config=self._active_project.project_config.ignore_all_files_in_gitignore,
        )
        self.language_server.start()
        if not self.language_server.is_running():
            raise RuntimeError(
                f""Failed to start the language server for {self._active_project.project_name} at {self._active_project.project_root}""
            )",Starts/resets the language server for the current project,???Restart the language server by stopping the current instance and initializing a new one.???
2229,save_model_weights,"def save_model_weights(gpt_weights: Dict[str, torch.Tensor],
                      xtts_weights: Dict[str, torch.Tensor],
                      output_dir: str) -> Tuple[str, str]:
    
    gpt_dir = os.path.join(output_dir, ""gpt"")
    xtts_dir = os.path.join(output_dir, ""core_xttsv2"")
    os.makedirs(gpt_dir, exist_ok=True)
    os.makedirs(xtts_dir, exist_ok=True)

    gpt_path = os.path.join(gpt_dir, 'gpt2_model.safetensors')
    save_file(gpt_weights, gpt_path)
    print(f""GPT weights saved to: {gpt_path}"")
    print(f""GPT weight keys: {list(gpt_weights.keys())}"")

    xtts_path = os.path.join(xtts_dir, 'xtts-v2.safetensors')
    save_file(xtts_weights, xtts_path)
    print(f""XTTS weights saved to: {xtts_path}"")
    print(f""XTTS weight keys: {list(xtts_weights.keys())}"")

    return gpt_path, xtts_path",Saves model weights in SafeTensors format.,???Save GPT and XTTS model weights to specified directories.???
2230,create_embedded_resource,"def create_embedded_resource(
    resource_path: str, content: str, mime_type: str, is_binary: bool = False
) -> EmbeddedResource:
    
    # Format a valid resource URI string
    resource_uri_str = create_resource_uri(resource_path)

    # Create common resource args dict to reduce duplication
    resource_args = {
        ""uri"": resource_uri_str,  # type: ignore
        ""mimeType"": mime_type,
    }

    if is_binary:
        return EmbeddedResource(
            type=""resource"",
            resource=BlobResourceContents(
                **resource_args,
                blob=content,
            ),
        )
    else:
        return EmbeddedResource(
            type=""resource"",
            resource=TextResourceContents(
                **resource_args,
                text=content,
            ),
        )",Create an embedded resource content object,"???Create an embedded resource object with URI, content, and MIME type.???"
2231,get_conditioning_sample,"def get_conditioning_sample(self, original_sample_path: str) -> str:
        
        # strip leading /
        original_sample_path = original_sample_path.lstrip(""/"")
        full_path = os.path.join(
            self.metadata_backend.instance_data_dir, original_sample_path
        )
        try:
            conditioning_sample_data = self.data_backend.read_image(full_path)
        except Exception as e:
            self.logger.error(f""Could not fetch conditioning sample: {e}"")

            return None
        if not conditioning_sample_data:
            self.debug_log(f""Could not fetch conditioning sample from {full_path}."")
            return None

        conditioning_sample = TrainingSample(
            image=conditioning_sample_data,
            data_backend_id=self.id,
            image_metadata=self.metadata_backend.get_metadata_by_filepath(full_path),
            image_path=full_path,
            conditioning_type=self.conditioning_type,
        )
        return conditioning_sample","Given an original dataset sample path, return a TrainingSample",???Fetch and return a conditioning sample image with metadata from a specified path.???
2232,get_tree_string,"def get_tree_string(self) -> str:
        
        output = [""Directory Structure:"", ""="" * 50]
        output.extend(self.tree)
        output.extend([""="" * 50, f""Total files found: {self.files_found}"", """"])
        return ""\n"".join(output)",Get the tree structure as a string,???Generate a formatted string representing a directory's structure and file count.???
2233,_generate_basic_docs,"def _generate_basic_docs(self, spec: ToolSpecification) -> str:
        
        lines: List[str] = [
            f""# {spec.name}"",
            """",
            spec.purpose,
            """",
            ""## Inputs"",
        ]
        for p in spec.input_parameters:
            req = ""(Required)"" if p.required else ""(Optional)""
            lines.append(f""- {p.name}: {p.description or 'No description'} {req}"")
        lines.append("""")
        lines.append(""## Output"")
        lines.append(str(spec.output_format))
        return ""\n"".join(lines)",Return minimal documentation for a generated tool.,???Generate documentation string from tool specification details.???
2234,cancel_prompt,"def cancel_prompt(id_: RequestId) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""id""] = id_.to_json()
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DeviceAccess.cancelPrompt"",
        ""params"": params,
    }
    json = yield cmd_dict",Cancel a prompt in response to a DeviceAccess.deviceRequestPrompted event.,???Generate a command to cancel a device access prompt using a request ID.???
2235,_chat,"def _chat(
            config_file: str = typer.Option(""server_config.json"", help=""Configuration file path""),
            server: Optional[str] = typer.Option(None, help=""Server to connect to""),
            provider: Optional[str] = typer.Option(None, help=""LLM provider name""),
            model: Optional[str] = typer.Option(None, help=""Model name""),
            api_base: Optional[str] = typer.Option(None, ""--api-base"", help=""API base URL""),
            api_key: Optional[str] = typer.Option(None, ""--api-key"", help=""API key""),
            disable_filesystem: bool = typer.Option(False, help=""Disable filesystem access""),
            logging_level: str = typer.Option(""WARNING"", help=""Set logging level""),
        ) -> None:
            
            _set_logging(logging_level)

            # Use ModelManager to determine provider/model if not specified
            model_manager = ModelManager()
            effective_provider = provider or model_manager.get_active_provider()
            effective_model = model or model_manager.get_active_model()

            servers, _, server_names = process_options(
                server, disable_filesystem, effective_provider, effective_model, config_file
            )

            extra = {
                ""provider"": provider,  # Pass None if not specified - let ModelManager decide
                ""model"": model,        # Pass None if not specified - let ModelManager decide
                ""api_base"": api_base,
                ""api_key"": api_key,
                ""server_names"": server_names,
            }
            
            run_command_func(self.wrapped_execute, config_file, servers, extra_params=extra)",Start interactive chat mode.,???Configure and execute a chat server connection using specified or default settings.???
2236,check_environment_variables,"def check_environment_variables():
    
    print(""\n5. Environment Variables"")
    print(""-"" * 25)
    
    env_vars = [
        'MCP_TOOL_TIMEOUT',
        'CHUK_TOOL_TIMEOUT',
        'CHUK_LOG_LEVEL',
        'PERPLEXITY_API_KEY',
        'OPENAI_API_KEY'
    ]
    
    for var in env_vars:
        value = os.getenv(var)
        if value:
            # Hide sensitive values
            if 'API_KEY' in var:
                display_value = f""{value[:8]}..."" if len(value) > 8 else ""***""
            else:
                display_value = value
            print(f""   ✅ {var}: {display_value}"")
        else:
            print(f""   ⚠️ {var}: Not set"")",Check relevant environment variables.,"???Check and display status of specific environment variables, masking sensitive data.???"
2237,is_empty,"def is_empty(self) -> bool:
        
        try:
            query = self.CHECK_IF_EMPTY_QUERY.format(index_name=self.index_name)
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(query)
                count = cursor.fetchone()[0]
            return count == 0

        except sqlite3.Error as e:
            raise SQLiteError(f""Error checking if database is empty: {e}"") from e",Checks if the SQLite database is empty.,???Check if a database table is empty and handle errors.???
2238,add_result,"def add_result(
        cls,
        test_name: str,
        input_description: str,
        resource_type: str,
        expected_yaml: Dict[str, Any],
        actual_yaml: Dict[str, Any],
        similarity_score: float,
        diff_details: str,
    ):
        
        cls.results.append(
            {
                ""test_name"": test_name,
                ""input_description"": input_description,
                ""resource_type"": resource_type,
                ""expected_yaml"": expected_yaml,
                ""actual_yaml"": actual_yaml,
                ""similarity_score"": similarity_score,
                ""diff_details"": diff_details,
                ""timestamp"": datetime.now().isoformat(),
            }
        )",Add a result to the results list.,???Log test results with metadata and timestamp in a class list.???
2239,perform_action,"def perform_action(element: MacElementNode, action: str) -> bool:
	
	try:
		if not element._element:
			logger.error(f'❌ Cannot perform action: Element reference is missing for {element}')
			return False

		# Check if the element supports this action
		available_actions = element.actions
		if action not in available_actions:
			logger.error(f'❌ Action {action} not supported by element {element}. Available actions: {available_actions}')
			return False

		result = AXUIElementPerformAction(element._element, action)
		if result == 0:
			logger.debug(f'✅ Successfully performed {action} on element: {element}')
			return True
		else:
			logger.error(f'❌ Failed to perform {action} on element: {element}, error code: {result}')
			return False
	except Exception as e:
		logger.error(f'❌ Error performing {action} on element: {element}, {e}')
		return False",Performs a specified accessibility action on an element.,"???Attempt to execute a specified action on a UI element, logging success or failure.???"
2240,_gen_name,"def _gen_name(self, name: str | None, prefix: str | None) -> str:
        
        if name:
            return name

        if not prefix:
            prefix = self.__class__.__name__

        identifier: str | None = None
        if not self.context or not self.context.executor:
            import uuid

            identifier = str(uuid.uuid4())
        else:
            identifier = str(self.context.executor.uuid())

        return f""{prefix}-{identifier}""",Generate a name for the LLM based on the provided name or the default prefix.,"???Generate a unique identifier using a name, prefix, or UUID.???"
2241,check_thresholds,"def check_thresholds(self, metrics: Dict[str, Any]) -> Optional[Alert]:
        
        alerts = []
        
        # Check CPU usage
        if metrics.get('performance', {}).get('cpu_percent', 0) > self.thresholds['cpu_percent']:
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                message=f""High CPU usage: {metrics['performance']['cpu_percent']}%"",
                timestamp=datetime.now(),
                metrics=metrics
            ))
        
        # Check memory usage
        if metrics.get('performance', {}).get('memory_percent', 0) > self.thresholds['memory_percent']:
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                message=f""High memory usage: {metrics['performance']['memory_percent']}%"",
                timestamp=datetime.now(),
                metrics=metrics
            ))
        
        # Return most severe alert if any
        return max(alerts, key=lambda x: x.level.value) if alerts else None",Check if metrics exceed defined thresholds,???Evaluate system metrics against thresholds to generate performance alerts.???
2242,_get_month_names,"def _get_month_names():
        
        return {dates.MONTHS[i]: i for i in range(1, 13)}",Get month names using Django's date translation,???Creates a dictionary mapping month names to their corresponding numbers.???
2243,_populate_min_entrypoint,"def _populate_min_entrypoint(self):
        
        entrypoint_path = frameworks.get_entrypoint_path(self.framework)
        shutil.copy(BASE_PATH / f""fixtures/frameworks/{self.framework}/entrypoint_min.py"", entrypoint_path)",This entrypoint does not have any tools or agents.,???Copy minimal entrypoint script for a specified framework to its designated path.???
2244,untrack_cache_storage_for_origin,"def untrack_cache_storage_for_origin(
    origin: str,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""origin""] = origin
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Storage.untrackCacheStorageForOrigin"",
        ""params"": params,
    }
    json = yield cmd_dict",Unregisters origin from receiving notifications for cache storage.,???Generate command to stop tracking cache storage for a given origin.???
2245,get_config,"def get_config(self):
        
        local_cfg_path = self._closest_config()
        try:
            with open(local_cfg_path) as f:
                local_cfg = yaml.load(f)
        except Exception as e:
            raise Exception(f""failed to load config from {local_cfg_path}, error: {e}"")
        projdir = """"
        if local_cfg_path:
            projdir = str(Path(local_cfg_path).parent)
            if projdir not in sys.path:
                sys.path.append(projdir)

        return local_cfg",Get knext config file as a ConfigParser.,"???Load and return YAML configuration, updating system path if necessary.???"
2246,find_all_collaborations,"def find_all_collaborations(self, orchestrator_id):
        
        collaborations = []
        
        # Check all versions including DRAFT
        versions = self.find_all_versions(orchestrator_id)
        
        for version in versions:
            try:
                response = self.bedrock_agent.list_agent_collaborators(
                    agentId=orchestrator_id,
                    agentVersion=version
                )
                
                for collab in response.get('collaboratorSummaries', []):
                    collaborations.append({
                        'id': collab['collaboratorId'],
                        'name': collab['collaboratorName'],
                        'version': version,
                        'arn': collab.get('agentDescriptor', {}).get('aliasArn', None)
                    })
                    print(f""Found collaborator: {collab['collaboratorName']} (ID: {collab['collaboratorId']}) for version {version}"")
            except Exception as e:
                print(f""Error finding collaborators for agent {orchestrator_id} version {version}: {str(e)}"")
        
        return collaborations",Attempt to find all collaborations for all versions of an orchestrator,???Retrieve all collaborator details for each version of a specified orchestrator.???
2247,convert_weights,"def convert_weights(model: nn.Module):
    

    def _convert_weights_to_fp16(l):
        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            l.weight.data = l.weight.data.half()
            if l.bias is not None:
                l.bias.data = l.bias.data.half()

        if isinstance(l, nn.MultiheadAttention):
            for attr in [*[f""{s}_proj_weight"" for s in [""in"", ""q"", ""k"", ""v""]], ""in_proj_bias"", ""bias_k"", ""bias_v""]:
                tensor = getattr(l, attr)
                if tensor is not None:
                    tensor.data = tensor.data.half()

        for name in [""text_projection"", ""proj""]:
            if hasattr(l, name):
                attr = getattr(l, name)
                if attr is not None:
                    attr.data = attr.data.half()

    model.apply(_convert_weights_to_fp16)",Convert applicable model parameters to fp16.,???Convert neural network model weights to half-precision for efficiency.???
2248,_get_or_create_episode,"def _get_or_create_episode(
        self, episode_id: int, topic: Optional[str] = None
    ) -> EpisodeState:
        
        if episode_id not in self.episodes:
            if topic is None:
                topic = random.choice(self.topics)

            ep = EpisodeState(episode_id, topic)

            # Initialize with system prompt
            ep.message_history = [{""role"": ""system"", ""content"": SYSTEM_PROMPT}]

            # Add initial user prompt with the topic
            ep.message_history.append(
                {
                    ""role"": ""user"",
                    ""content"": f'Research and write a comprehensive Wikipedia-style article about: ""{topic}""',
                }
            )

            self.episodes[episode_id] = ep

        return self.episodes[episode_id]",Get an existing episode or create a new one,???Create or retrieve an episode with a topic and initialize its message history.???
2249,dbg_run_to,"def dbg_run_to(
    address: Annotated[str, ""Run the debugger to the specified address""],
) -> str:
    
    ea = parse_address(address)
    if idaapi.run_to(ea):
        return f""Debugger run to {hex(ea)}""
    return f""Failed to run to address {hex(ea)}""",Run the debugger to the specified address,???Run debugger to specified memory address and return status???
2250,run_simpleqa_benchmark,"def run_simpleqa_benchmark(
    num_examples, output_dir, model=None, provider=None, endpoint_url=None, api_key=None
):
    
    from local_deep_research.benchmarks.benchmark_functions import evaluate_simpleqa

    logger.info(f""Starting SimpleQA benchmark with {num_examples} examples"")
    start_time = time.time()

    # Run the benchmark
    results = evaluate_simpleqa(
        num_examples=num_examples,
        search_iterations=2,
        questions_per_iteration=3,
        search_strategy=""source_based"",
        search_tool=""searxng"",
        search_model=model,
        search_provider=provider,
        endpoint_url=endpoint_url,
        output_dir=os.path.join(output_dir, ""simpleqa""),
        evaluation_provider=""ANTHROPIC"",
        evaluation_model=""claude-3-7-sonnet-20250219"",
    )

    duration = time.time() - start_time
    logger.info(f""SimpleQA benchmark completed in {duration:.1f} seconds"")

    if results and isinstance(results, dict):
        logger.info(f""SimpleQA accuracy: {results.get('accuracy', 'N/A')}"")

    return results",Run SimpleQA benchmark with specified number of examples.,???Execute a SimpleQA benchmark test with specified parameters and log results.???
2251,handle_help_command,"def handle_help_command(
    lines: List[str], args: List[str], console: Console, session: PromptSession, history_manager: InputHistoryManager
) -> None:
    
    help_content = ""\n"".join([f""  {name}: {cmd['help']}"" for name, cmd in registry.commands.items()])
    console.print(Panel(f""Available commands:\n{help_content}"", title=""Help Menu"", border_style=""green""))",Display auto-generated help from registered commands.,???Display a help menu with available commands and descriptions.???
2252,get_response,"def get_response(
        self,
        user_message=None,
        image=None,
        messages=None,
        temperature=0.0,
        max_new_tokens=None,
        **kwargs,
    ):
        
        if messages is None:
            messages = self.messages
        if user_message:
            messages.append(
                {""role"": ""user"", ""content"": [{""type"": ""text"", ""text"": user_message}]}
            )

        return self.engine.generate(
            messages,
            temperature=temperature,
            max_new_tokens=max_new_tokens,
            **kwargs,
        )",Generate the next response based on previous messages,???Generate AI response based on user input and conversation context.???
2253,embed,"def embed(self, text: str) -> ""np.ndarray"":
        
        token_count = self.count_tokens(text)
        if (
            token_count > 512 and self._show_warnings
        ):  # Cohere models max_context_length
            warnings.warn(
                f""Text has {token_count} tokens which exceeds the model's context length of 512.""
                ""Generation may not be optimal""
            )

        for _ in range(self._max_retries):
            try:
                response = self.client.embed(
                    model=self.model,
                    input_type=""search_document"",
                    embedding_types=[""float""],
                    texts=[text],
                )

                return np.array(response.embeddings.float_[0], dtype=np.float32)  # type: ignore[index]
            except Exception as e:
                if self._show_warnings:
                    warnings.warn(
                        f""There was an exception while generating embeddings. Exception: {str(e)}. Retrying...""
                    )

        raise RuntimeError(""Unable to generate embeddings through Cohere."")",Generate embeddings for a single text.,???Generate text embeddings with retry and warning mechanisms for token limits.???
2254,hsl_to_rgb_hex,"def hsl_to_rgb_hex(self, h: float, s: float = 1.0, l: float = 0.5) -> str:
        
        # colorsys uses HLS, not HSL, and expects values between 0.0 and 1.0
        hue = h / 360.0
        r, g, b = colorsys.hls_to_rgb(hue, l, s) # Note the order: H, L, S
        r_int, g_int, b_int = int(r * 255), int(g * 255), int(b * 255)
        return f""#{r_int:02X}{g_int:02X}{b_int:02X}""",Converts HSL color value to RGB HEX string.,???Convert HSL color values to RGB hexadecimal format.???
2255,_check_dependencies,"def _check_dependencies(self) -> Optional[bool]:
        
        dependencies = [""huggingface_hub"", ""jsonschema""]
        for dependency in dependencies:
            if importutil.find_spec(dependency) is None:
                raise ImportError(f""Tried importing {dependency} but it is not installed. Please install it via `pip install chonkie[hub]`"")
        return True",Check if the required dependencies are available.,"???  
Verify required libraries are installed, raising error if missing.  
???"
2256,get_snooze_file_path,"def get_snooze_file_path():
    
    cache_dir_base = data.CACHE_DIR or os.path.expanduser(f""~/.cache/{data.APP_NAME}"")
    try:
        os.makedirs(cache_dir_base, exist_ok=True)
    except Exception as e:
        print(f""Error creating cache directory {cache_dir_base}: {e}"")
    return os.path.join(cache_dir_base, SNOOZE_FILE_NAME)",Returns the path to the 'snooze' file inside ~/.cache/APP_NAME.,???Determine the file path for storing snooze data within a cache directory.???
2257,_convert_image_content,"def _convert_image_content(content: ImageContent) -> ContentBlock:
        
        # Get image data using helper
        image_data = get_image_data(content)

        # OpenAI requires image URLs or data URIs for images
        image_url = {""url"": f""data:{content.mimeType};base64,{image_data}""}

        # Check if the image has annotations for detail level
        if hasattr(content, ""annotations"") and content.annotations:
            if hasattr(content.annotations, ""detail""):
                detail = content.annotations.detail
                if detail in (""auto"", ""low"", ""high""):
                    image_url[""detail""] = detail

        return {""type"": ""image_url"", ""image_url"": image_url}",Convert ImageContent to OpenAI image_url content block.,???Convert image content to a structured block with URL and detail metadata.???
2258,to_markdown,"def to_markdown(self):
        
        logger.debug(""Creating Markdown representation of tool dictionary"")
        markdown = """"
        index: int = 1
        for tool_name, tool in self.tools.items():
            # use the tool's to_markdown method
            markdown += f""### {index}. {tool_name}\n""
            markdown += tool.to_markdown()
            markdown += ""\n""
            index += 1
        return markdown",Create a comprehensive Markdown representation of the tool dictionary.,???Convert tool dictionary to Markdown format using tool-specific methods.???
2259,request_api_key,"def request_api_key(self, max_attempts=3):
        
        import getpass

        for attempts in range(max_attempts):
            LOGGER.info(f""{PREFIX}Login. Attempt {attempts + 1} of {max_attempts}"")
            input_key = getpass.getpass(f""Enter API key from {API_KEY_URL} "")
            self.api_key = input_key.split(""_"")[0]  # remove model id if present
            if self.authenticate():
                return True
        raise ConnectionError(emojis(f""{PREFIX}Failed to authenticate ❌""))",Prompt the user to input their API key.,???Prompt user for API key and authenticate within limited attempts???
2260,mixed_quant_predicate,"def mixed_quant_predicate(
        path: str,
        module: nn.Module,
        config: dict,
    ) -> Union[bool, dict]:
        

        if not hasattr(module, ""to_quantized""):
            return False

        index = (
            int(path.split(""."")[layer_location])
            if len(path.split(""."")) > layer_location
            else 0
        )
        use_more_bits = (
        )
        if ""v_proj"" in path and use_more_bits:
            return {""group_size"": group_size, ""bits"": high_bits}
        if ""down_proj"" in path and use_more_bits:
            return {""group_size"": group_size, ""bits"": high_bits}
        if ""lm_head"" in path:
            return {""group_size"": group_size, ""bits"": high_bits}

        return {""group_size"": group_size, ""bits"": low_bits}","Implements mixed quantization predicates with similar choices to, for example, llama.cpp's Q4_K_M.",???Determine quantization settings for neural network layers based on path and configuration.???
2261,request,"def request(self):
        
        if self.request_id is not None:
            return
        response = requests.post(
            f""{API_ENDPOINT}/v1/image"",
            headers={
                ""accept"": ""application/json"",
                ""x-key"": self.api_key,
                ""Content-Type"": ""application/json"",
            },
            json=self.request_json,
        )
        result = response.json()
        if response.status_code != 200:
            raise ApiException(
                status_code=response.status_code, detail=result.get(""detail"")
            )
        self.request_id = response.json()[""id""]",Request to generate the image.,???Send a POST request to an API to retrieve and store an image request ID.???
2262,load_file_from_path,"def load_file_from_path(file_path: Path) -> UploadFile:
    
    try:
        file_path_md = file_path.split(""."")[0] + "".md""
        if not os.path.exists(file_path_md):
            raise HTTPException(status_code=404, detail=""File not found"")

        with open(file_path_md, ""rb"") as file:
            content = file.read()

        return UploadFile(filename=file_path_md, file=io.BytesIO(content))

    except Exception as e:
        logger.error(f""Error loading file from path: {e}"")
        raise e",this functions loads the markdown file from the file_path and returns an UploadFile object,"???Load and return a markdown file as an uploadable object, handling errors.???"
2263,validate_manifest,"def validate_manifest(manifest_path: Path, schema: Dict) -> Tuple[bool, str]:
    
    try:
        with open(manifest_path, ""r"") as f:
            manifest = json.load(f)
    except json.JSONDecodeError as e:
        return False, f""Invalid JSON: {e}""
    except Exception as e:
        return False, f""Error reading file: {e}""

    try:
        jsonschema.validate(manifest, schema)
        return True, """"
    except jsonschema.exceptions.ValidationError as e:
        return False, f""{e.json_path}: {e.message}""
    except jsonschema.exceptions.SchemaError as e:
        return False, f""Schema error: {e}""",Validate a single manifest file against the schema,"???Validate JSON manifest against schema, returning success status and error message.???"
2264,run_multiple_images_workflow,"def run_multiple_images_workflow():
    
    llm = setup_llm(model_provider=""gpt"", model_name=""gpt-4o"", temperature=1)
    agent = ReActAgent(
        name=""MultiImageAgent"",
        id=""multi_image_agent"",
        llm=llm,
        inference_mode=InferenceMode.XML,
    )

    tracing = TracingCallbackHandler()
    wf = Workflow(flow=Flow(nodes=[agent]))

    with open(IMAGE_FILE, ""rb"") as f:
        image1_data = f.read()

    result = wf.run(
        input_data={""input"": ""Compare these two images."", ""images"": [image1_data, IMAGE_URL]},
        config=RunnableConfig(callbacks=[tracing]),
    )

    agent_output = result.output[agent.id][""output""][""content""]
    print(""Multiple images workflow response:"", agent_output)

    return agent_output, tracing.runs",Example workflow with multiple images of different types,???Execute a workflow comparing two images using a language model agent and return the analysis.???
2265,format_observations,"def format_observations(self, observations: list[Observation]) -> str:
        
        lines = [f""{obs}"" for obs in observations]
        return ""\n"".join(lines) + ""\n""",Format observations section in standard way.,???Converts a list of observations into a newline-separated string.???
2266,perform_action,"def perform_action(self, action_name: str, kwargs) -> Any:
        
        if action_name not in self.actions:
            raise KeyError(f""Unknown action: {action_name}"")

        # Explicitly reload environment variables
        load_dotenv()
        
        if not self.is_configured(verbose=True):
            raise HyperbolicConfigurationError(""Hyperbolic is not properly configured"")

        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f""Invalid parameters: {', '.join(errors)}"")

        # Call the appropriate method based on action name
        method_name = action_name.replace('-', '_')
        method = getattr(self, method_name)
        return method(**kwargs)",Execute a Hyperbolic action with validation,???Execute validated action by dynamically invoking corresponding method with parameters.???
2267,_from_yaml,"def _from_yaml(self, cfg, ch, nc, verbose):
        
        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # cfg dict

        # Define model
        ch = self.yaml[""ch""] = self.yaml.get(""ch"", ch)  # input channels
        if nc and nc != self.yaml[""nc""]:
            LOGGER.info(f""Overriding model.yaml nc={self.yaml['nc']} with nc={nc}"")
            self.yaml[""nc""] = nc  # override YAML value
        elif not nc and not self.yaml.get(""nc"", None):
            raise ValueError(""nc not specified. Must specify nc in model.yaml or function arguments."")
        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist
        self.stride = torch.Tensor([1])  # no stride constraints
        self.names = {i: f""{i}"" for i in range(self.yaml[""nc""])}  # default names dict
        self.info()",Set YOLOv8 model configurations and define the model architecture.,"???Load and configure a model from YAML, adjusting channels and class count as needed.???"
2268,setup_resources,"def setup_resources(self):
        

        self._resource = Resource.create(
            attributes={
                SERVICE_NAME: str(self.service_name),
            }
        )",Set up the resource for OpenTelemetry.,???Initialize a resource with service name attributes for configuration.???
2269,get_document,"def get_document(self, document_id: str | List[str], user_id: str = None) -> Optional[SimbaDoc] | List[Optional[SimbaDoc]]:
        
        try:
            session = self._Session()
            if isinstance(document_id, list):
                query = session.query(SQLDocument).filter(SQLDocument.id.in_(document_id))
                if user_id:
                    query = query.filter(SQLDocument.user_id == user_id)
                docs = query.all()
                # Map id to doc for fast lookup
                doc_map = {doc.id: doc for doc in docs}
                # Return in the same order as input list, None if not found
                return [doc_map.get(doc_id).to_simbadoc() if doc_map.get(doc_id) else None for doc_id in document_id]
            else:
                query = session.query(SQLDocument).filter(SQLDocument.id == document_id)
                if user_id:
                    query = query.filter(SQLDocument.user_id == user_id)
                doc = query.first()
                return doc.to_simbadoc() if doc else None
        except Exception as e:
            logger.error(f""Failed to get document(s) {document_id}: {e}"")
            if isinstance(document_id, list):
                return [None for _ in document_id]
            return None
        finally:
            session.close()",Retrieve a document by ID or a list of documents by IDs using SQLAlchemy ORM.,"???Retrieve documents by ID, optionally filtering by user, and handle exceptions.???"
2270,parse,"def parse(self, input: dict, response: Poems) -> dict:
        
        return [{""topic"": input[""topic""], ""poem"": p} for p in response.poems_list]",Parse the model response along with the input to the model into the desired output format..,???Transforms input data into a list of topic-poem dictionaries.???
2271,rebuild,"def rebuild(self) -> None:
        
        self._index = []
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                version = version_info[""version""]
                path = self.registry.templates_dir / version_info[""path""]
                metadata_path = path.parent / METADATA_FILE_NAME
                try:
                    content = path.read_text(encoding=""utf-8"")
                except OSError:  # pragma: no cover - file missing
                    continue
                checksum = sha256(content.encode(""utf-8"")).hexdigest()
                try:
                    with open(metadata_path, ""r"", encoding=""utf-8"") as f:
                        metadata = json.load(f)
                except (OSError, json.JSONDecodeError):
                    metadata = {}
                self._index.append(
                    {
                        ""slug"": slug,
                        ""version"": version,
                        ""path"": str(path.relative_to(self.registry.templates_dir)),
                        ""checksum"": checksum,
                        ""metadata"": metadata,
                        ""content"": content,
                    }
                )
        self.save()",Rebuild the index from all registered templates.,"???Rebuilds index by processing template entries, calculating checksums, and loading metadata.???"
2272,_serialize_data,"def _serialize_data(obj: Any) -> Any:
    
    if isinstance(obj, BaseModel):
        # Convert Pydantic model to dict
        return obj.model_dump()

    if isinstance(obj, dict):
        return {str(k): _serialize_data(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple, set)):
        return [_serialize_data(item) for item in obj]

    if isinstance(obj, (str, int, float, bool, type(None))):
        return obj

    # Fallback
    return str(obj)",Convert `obj` into a structure that can be passed to `json.dumps(...)`.,"???  
Recursively serialize complex data structures into JSON-compatible formats.  
???"
2273,send_message,"def send_message(self, content: str) -> Dict[str, Any]:
        
        try:
            url = f""{self.api_url}/api/rooms/{self.room}/message""
            data = {
                ""content"": content,
                ""sender"": {
                    ""username"": self.sender_username,
                    ""model"": self.sender_model
                }
            }
            response = self._make_request(""POST"", url, json=data)
            self.metrics['messages_sent'] += 1
            
            # Add to sent messages history
            self.sent_messages.append({
                ""content"": content,
                ""timestamp"": time.time()
            })
            
            return response
        except Exception as e:
            self.metrics['messages_failed'] += 1
            self._handle_error(""Failed to send message"", e)
            raise",Send a message to the room,???Send a message to a chat room and update metrics and history.???
2274,get_template_path,"def get_template_path(agent_name: str, debug: bool = False) -> pathlib.Path:
    
    current_dir = pathlib.Path(__file__).parent.parent.parent.parent
    template_path = current_dir / ""agents"" / agent_name / ""template""
    if debug:
        logging.debug(f""Looking for template in: {template_path}"")
        logging.debug(f""Template exists: {template_path.exists()}"")
        if template_path.exists():
            logging.debug(f""Template contents: {list(template_path.iterdir())}"")

    if not template_path.exists():
        raise ValueError(f""Template directory not found at {template_path}"")

    return template_path",Get the absolute path to the agent template directory.,"???Determine the file path for an agent's template directory, with optional debug logging.???"
2275,verify_chunk_indices,"def verify_chunk_indices(chunks: List[Chunk], original_text: str):
    
    for i, chunk in enumerate(chunks):
        # Extract text using the indices
        extracted_text = original_text[chunk.start_index : chunk.end_index]
        # Remove any leading/trailing whitespace from both texts for comparison
        chunk_text = chunk.text.strip()
        extracted_text = extracted_text.strip()

        assert chunk_text == extracted_text, (
            f""Chunk {i} text mismatch:\n""
            f""Chunk text: '{chunk_text}'\n""
            f""Extracted text: '{extracted_text}'\n""
            f""Indices: [{chunk.start_index}:{chunk.end_index}]""
        )",Verify that chunk indices correctly map to the original text.,???Verify extracted text matches chunk text using specified indices.???
2276,screenshot,"def screenshot(self, filename:str = 'updated_screen.png') -> bool:
        
        self.logger.info(""Taking full page screenshot..."")
        time.sleep(0.1)
        try:
            original_zoom = self.driver.execute_script(""return document.body.style.zoom || 1;"")
            self.driver.execute_script(""document.body.style.zoom='75%'"")
            time.sleep(0.1)
            path = os.path.join(self.screenshot_folder, filename)
            if not os.path.exists(self.screenshot_folder):
                os.makedirs(self.screenshot_folder)
            self.driver.save_screenshot(path)
            self.logger.info(f""Full page screenshot saved as {filename}"")
        except Exception as e:
            self.logger.error(f""Error taking full page screenshot: {str(e)}"")
            return False
        finally:
            self.driver.execute_script(f""document.body.style.zoom='1'"")
        return True","Take a screenshot of the current page, attempt to capture the full page by zooming out.",???Capture and save a full-page screenshot with zoom adjustment and error handling.???
2277,emit_status,"def emit_status(self):
        
        if self.stt_enabled:
            pretty_print(f""Text-to-speech trigger is {self.ai_name}"", color=""status"")
        if self.tts_enabled:
            self.speech.speak(""Hello, we are online and ready. What can I do for you ?"")
        pretty_print(""AgenticSeek is ready."", color=""status"")",Print the current status of agenticSeek.,??? Emit status messages based on text-to-speech and speech settings ???
2278,get_all,"def get_all(self, limit: int | None = None) -> list[Message]:
        
        try:
            documents = self.vector_store.list_documents(include_embeddings=False)
            messages = [self._document_to_message(doc) for doc in documents]
            return sorted(messages, key=lambda msg: msg.metadata.get(""timestamp"", 0))
        except Exception as e:
            raise QdrantError(f""Failed to retrieve messages from Qdrant: {e}"") from e",Retrieves all messages from Qdrant.,???Retrieve and sort messages from a document store by timestamp.???
2279,_load_data_into_duckdb,"def _load_data_into_duckdb(self):
        
        try:
            # API request
            response = self._make_api_request()
            data = response.json()

            # Convert JSON to DF
            df = pd.json_normalize(data)  # noqa: F841

            # Create a table in DB
            self._duckdb.execute(f)
        except Exception as e:
            logger.error(f""Error loading API data into DuckDB: {e}"")
            raise",Fetch data from the API and load it into DuckDB,"???Load API data into DuckDB, handling errors gracefully.???"
2280,mock_config_repository,"def mock_config_repository():
    
    with patch('ra_aid.database.repositories.config_repository.config_repo_var') as mock_repo_var:
        # Setup a mock repository
        mock_repo = MagicMock()
        
        # Create a dictionary to simulate config
        config = {
            ""cowboy_mode"": False
        }
        
        # Setup get method to return config values (already set up in this file)
        
        # Note: get_all is deprecated, but kept for backward compatibility
        # Setup get_all method to return a reference to the config dict
        mock_repo.get_all.return_value = config
        
        # Setup get method to return config values
        def get_config(key, default=None):
            return config.get(key, default)
        mock_repo.get.side_effect = get_config
        
        # Setup set method to update config values
        def set_config(key, value):
            config[key] = value
        mock_repo.set.side_effect = set_config
        
        # Make the mock context var return our mock repo
        mock_repo_var.get.return_value = mock_repo
        
        yield mock_repo",Mock the ConfigRepository to avoid database operations during tests,???Simulates a configuration repository using mock objects for testing purposes.???
2281,get_attachment,"def get_attachment(attachment_id):
    
    try:
        command_history = CommandHistory(db_path)
        data, name, type = command_history.get_attachment_data(attachment_id)

        if data:
            # Convert binary data to base64 for sending
            base64_data = base64.b64encode(data).decode(""utf-8"")
            return jsonify(
                {""data"": base64_data, ""name"": name, ""type"": type, ""error"": None}
            )
        return jsonify({""error"": ""Attachment not found""}), 404
    except Exception as e:
        return jsonify({""error"": str(e)}), 500",Get specific attachment data,???Retrieve and encode attachment data as base64 for JSON response.???
2282,get_blender_connection,"def get_blender_connection():
    
    global _blender_connection, _polyhaven_enabled  # Add _polyhaven_enabled to globals
    
    # If we have an existing connection, check if it's still valid
    if _blender_connection is not None:
        try:
            # First check if PolyHaven is enabled by sending a ping command
            result = _blender_connection.send_command(""get_polyhaven_status"")
            # Store the PolyHaven status globally
            _polyhaven_enabled = result.get(""enabled"", False)
            return _blender_connection
        except Exception as e:
            # Connection is dead, close it and create a new one
            logger.warning(f""Existing connection is no longer valid: {str(e)}"")
            try:
                _blender_connection.disconnect()
            except:
                pass
            _blender_connection = None
    
    # Create a new connection if needed
    if _blender_connection is None:
        _blender_connection = BlenderConnection(host=""localhost"", port=9876)
        if not _blender_connection.connect():
            logger.error(""Failed to connect to Blender"")
            _blender_connection = None
            raise Exception(""Could not connect to Blender. Make sure the Blender addon is running."")
        logger.info(""Created new persistent connection to Blender"")
    
    return _blender_connection",Get or create a persistent Blender connection,"???Establish or validate a persistent connection to Blender, checking PolyHaven status.???"
2283,_parse_response,"def _parse_response(self, response, query, topk=-1):
        
        # achieve maximum retrieval number
        if topk != -1 and topk <= 0:
            return []
        
        results = json.loads(response)
        total_count = results.get(""totalCount"", 0)
        study_results = results.get(""studies"", {})
        nextPageToken = results.get(""nextPageToken"", """")

        studies = []
        for res in study_results:
            res = self._parse_json_response(res)
            studies.append(res)

        if studies != []:
            studies = pd.concat(studies, axis=0).reset_index(drop=True)
        else:
            return pd.DataFrame()

        # recursion to get results from all pages
        if nextPageToken != """":
            query_next_page = query + f""&pageToken={nextPageToken}""
            response_next_page = requests.get(query_next_page)
            if topk == -1:
                studies_next_page = self._parse_response(response_next_page.text, query, topk=-1)
            else:
                studies_next_page = self._parse_response(response_next_page.text, query, topk=topk-len(studies))

            if isinstance(studies_next_page, pd.DataFrame):
                studies = pd.concat([studies, studies_next_page], ignore_index=True)

        return studies",Parse the response to pd dataframe from the API call.,"???Parse and aggregate paginated API response data into a DataFrame, handling recursive retrieval.???"
2284,remove_task_event_queue,"def remove_task_event_queue(self, task_id: str):
        
        with self.queue_lock:
            if task_id in self.task_queues:
                del self.task_queues[task_id]
                logger.debug(f""Removed event queue for task_id: {task_id}"")",Remove a task-specific event queue.,"???  
Safely deletes a task's event queue from a synchronized collection.  
???"
2285,_handle_unknown_command,"def _handle_unknown_command(self, command: str) -> None:
        
        logger.warning(f""Unknown command: '{command}'"") 

        # Suggest similar commands using basic string similarity
        suggestions = self._get_command_suggestions(command)
        if suggestions:
            logger.info(""Did you mean one of these?"")
            for suggestion in suggestions:
                logger.info(f""  - {suggestion}"")
        logger.info(""Use 'help' to see all available commands."")",Handle unknown command with suggestions,???Log unknown command and suggest similar alternatives for user guidance.???
2286,set_breakpoints_active,"def set_breakpoints_active(
    active: bool,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""active""] = active
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Debugger.setBreakpointsActive"",
        ""params"": params,
    }
    json = yield cmd_dict",Activates / deactivates all breakpoints on the page.,???Enable or disable debugger breakpoints via a command dictionary generator.???
2287,_create_client,"def _create_client(self) -> httpx.AsyncClient:
        
        proxy_timeout = self.config.get_default_timeout()
        timeout = self._get_timeout()

        client_kwargs = {
            ""follow_redirects"": True,
            ""timeout"": timeout,
            ""limits"": httpx.Limits(
                max_connections=2000,
                max_keepalive_connections=500,
                keepalive_expiry=min(120.0, proxy_timeout),
            ),
        }

        # Add proxy support if configured
        if self.config.get_proxy_enabled():
            proxy_address = self.config.get_proxy_address()
            if proxy_address:
                else:
                    client_kwargs[""proxies""] = {
                    }

        return httpx.AsyncClient(**client_kwargs)",Create HTTP client with optimized settings.,"???Initialize an asynchronous HTTP client with configurable timeout, connection limits, and optional proxy support.???"
2288,print_status,"def print_status(self) -> None:
        
        current = self.get_current_revision()
        head = self.get_head_revision()
        differences = self.get_schema_differences()
        pending = self.get_pending_migrations()

        logger.info(""=== Database Schema Status ==="")
        logger.info(f""Current revision: {current}"")
        logger.info(f""Head revision: {head}"")
        logger.info(f""Pending migrations: {len(pending)}"")
        for rev in pending:
            logger.info(f""  - {rev}"")
        logger.info(f""Unmigrated changes: {len(differences)}"")
        for diff in differences:
            logger.info(f""  - {diff}"")",Prints current migration status information to logger.,"???Log database schema status, including revisions, pending migrations, and differences.???"
2289,to_html,"def to_html(self, report: SummaryReport) -> str:
        
        return (
            ""<html><body>""
            ""<h2>Evaluation Report</h2>""
            f""<p>Status: {'PASSED' if report.passed else 'FAILED'}</p>""
            f""<p>Exit Code: {report.exit_code}</p>""
            f""<p>Duration: {report.duration:.2f}s</p>""
            f""<h3>stdout</h3><pre>{html.escape(report.stdout)}</pre>""
            f""<h3>stderr</h3><pre>{html.escape(report.stderr)}</pre>""
            ""</body></html>""
        )",Return a simple HTML representation of the report.,??? Convert evaluation report data into an HTML formatted string. ???
2290,write_state,"def write_state(key: str, value: Any) -> None:
    
    try:
        with open(config_file(), ""r"") as f:
            config = yaml.safe_load(f) or {}
    except FileNotFoundError:
        config = {}

    keys = key.split(""."")
    d = config.setdefault(""state"", {})
    for k in keys[:-1]:
        d = d.setdefault(k, {})
    d[keys[-1]] = value

    with open(config_file(), ""w"") as f:
        yaml.dump(config, f, default_flow_style=False)

    # update the global config object
    global CONFIG
    d = CONFIG.state
    for k in keys[:-1]:
        d = getattr(d, k)
    setattr(d, keys[-1], value)","Write a state value to the config.yaml file, supporting nested keys with dot operator.",???Update nested configuration state in YAML file and global object.???
2291,get_tensor_signature,"def get_tensor_signature(a: torch.Tensor | torch.nn.Parameter) -> str:
    
    while isinstance(a, torch.nn.Parameter):
        a = a.data

    if isinstance(a, DTensor):
        a = a.full_tensor()

    if a.numel() < TENSOR_SIG_SAMPLE_SIZE:
        b = a.as_strided(size=(a.numel(),), stride=(1,))
    else:
        b = a.as_strided(size=(TENSOR_SIG_SAMPLE_SIZE,), stride=(step_size,))
    element_str = """".join([f""{x:.3e}"" for x in b])
    element_hash = hashlib.md5(element_str.encode(""utf-8"")).hexdigest()
    return f""{a.dtype}{a.shape}{a.stride()}<{element_hash}>""",Get the tensor signature,???Generate a unique hash-based signature for a given tensor's structure and content.???
2292,validate_predictions,"def validate_predictions(self, predictions, expected_columns):
        
        missing_cols = [col for col in expected_columns if col not in predictions.columns]
        if missing_cols:
            raise ValueError(f""Predictions missing required columns: {missing_cols}"")
        return True",Validate prediction data has required columns and format,"???Ensure predictions contain all required columns, raising error if any are missing.???"
2293,structure,"def structure(self) -> DatasetStructure:
        
        return DatasetStructure(
            modality=""table"",
            features=list(self._data.columns),
            details={
                ""num_rows"": len(self._data),
                ""num_columns"": self._data.shape[1],
                ""column_names"": list(self._data.columns),
                ""column_types"": self._data.dtypes.astype(str).to_dict(),
            },
        )",Return structural metadata for the dataset.,"???  
Generate a dataset structure summary with modality, features, and details.  
???"
2294,_generate_metadata,"def _generate_metadata(self, chunk: Chunk) -> Dict[str, Any]:
        
        metadata = {
            ""text"": chunk.text,
            ""start_index"": chunk.start_index,
            ""end_index"": chunk.end_index,
            ""token_count"": chunk.token_count,
            ""chunk_type"": type(chunk).__name__,
        }
        
        # Add chunk-specific metadata
        if hasattr(chunk, ""sentences"") and chunk.sentences:
            metadata[""sentence_count""] = len(chunk.sentences)
        
        if hasattr(chunk, ""words"") and chunk.words:
            metadata[""word_count""] = len(chunk.words)
            
        if hasattr(chunk, ""language"") and chunk.language:
            metadata[""language""] = chunk.language
            
        return metadata",Generate metadata for the chunk.,???Generate metadata dictionary from text chunk attributes and properties.???
2295,parse_json_from_llm_output,"def parse_json_from_llm_output(text: str) -> dict:
    
    json_list = parse_json_from_text(text=text)
    if json_list:
        json_text = json_list[0]
        try:
            data = yaml.safe_load(json_text)
        except Exception:
            raise ValueError(f""The following generated text is not a valid JSON string!\n{json_text}"")
    else:
        raise ValueError(f""The follwoing generated text does not contain JSON string!\n{text}"")
    return data",Extract JSON str from LLM outputs and convert it to dict.,"???  
Extracts and validates JSON data from a text string using YAML parsing.  
???"
2296,apply_context_parallel,"def apply_context_parallel(
    model: torch.nn.Module,
    mesh: torch.distributed.device_mesh.DeviceMesh,
    plan: Optional[Dict[str, ContextParallelModelPlan]] = None,
) -> None:
    
    logger.debug(f""Applying context parallel with CP mesh: {mesh}"")
    model_cls = unwrap_module(model).__class__

    if plan is None:
        plan = TransformerRegistry.get(model_cls).cp_plan

    for module_id, cp_model_plan in plan.items():
        module = get_submodule_by_name(model, module_id)
        if not isinstance(module, list):
            module = [module]
        logger.debug(f""Applying ContextParallelHook to {module_id=} identifying a total of {len(module)} modules"")
        for m in module:
            registry = HookRegistry.check_if_exists_or_initialize(m)
            if isinstance(cp_model_plan, list):
                # Metadata can only be a list when it is a list of CPOutput
                assert all(isinstance(x, CPOutput) for x in cp_model_plan)
                hook = ContextParallelGatherHook(cp_model_plan, mesh)
                hook_name = f""cp_output---{module_id}""
            else:
                hook = ContextParallelSplitHook(cp_model_plan, mesh)
                hook_name = f""cp_input---{module_id}""
            registry.register_hook(hook, hook_name)",Apply context parallel on a model.,???Integrate context parallelism into a model using a device mesh and optional execution plan.???
2297,get_contents,"def get_contents(self, file_path: str, ref: str | None = None) -> str | None:
        
        if not ref:
            ref = self.default_branch
        try:
            file = self.repo.get_contents(file_path, ref=ref)
            file_contents = file.decoded_content.decode(""utf-8"")  # type: ignore[union-attr]
            return file_contents
        except UnknownObjectException:
            logger.info(f""File: {file_path} not found in ref: {ref}"")
            return None
        except GithubException as e:
            if e.status == 404:
                logger.info(f""File: {file_path} not found in ref: {ref}"")
                return None
            raise",Returns string file content on a given ref,"???Retrieve file contents from a repository, handling exceptions for missing files.???"
2298,print_overview,"def print_overview(self) -> None:
        
        print(f""{self.name}:\n {self.description}"")
        if self.excluded_tools:
            print("" excluded tools:\n  "" + "", "".join(sorted(self.excluded_tools)))",Print an overview of the mode.,"???Outputs an overview of an object's name, description, and excluded tools.???"
2299,process_foreign_key,"def process_foreign_key(
    table: str, fk: dict, inspector: Inspector, graph: nx.DiGraph, fk_relationships: List[dict]
) -> None:
    
    src_col = fk[""constrained_columns""][0]
    tgt_table = fk[""referred_table""]
    tgt_col = fk[""referred_columns""][0]

    # Check uniqueness and nullability in source column
    src_columns = inspector.get_columns(table)
    src_col_meta = next(c for c in src_columns if c[""name""] == src_col)
    is_unique = src_col_meta.get(""unique"", False) or src_col in inspector.get_pk_constraint(table).get(
        ""constrained_columns"", []
    )
    is_nullable = src_col_meta[""nullable""]

    fk_relationships.append(
        {
            ""source_table"": table,
            ""source_column"": src_col,
            ""target_table"": tgt_table,
            ""target_column"": tgt_col,
            ""constraint_name"": fk[""name""],
            ""is_unique"": is_unique,
            ""is_nullable"": is_nullable,
        }
    )
    graph.add_edge(table, tgt_table)",Process and record foreign key relationships with cardinality information.,???Analyze foreign key constraints and update relationships in a graph structure.???
2300,create_model,"def create_model(model_type: str) -> ForecastingBaseModel:
    
    if model_type == ""linear"":
        return LinearModel()
    if model_type == ""randomforest"":
        return RandomForestModel()

    raise ValueError(f""Unsupported model_type: {model_type}"")",A simple factory method that returns a model instance based on the input string.,"???  
Selects and returns a forecasting model based on specified type.  
???"
2301,convert_safetensors_to_hf_name,"def convert_safetensors_to_hf_name(self, sft_name):
        
        name_mapping = {
            ""model."": """",
            ""layers."": ""decoder_layers."",
            ""embed_tokens"": ""embedding"",
            ""self_attn."": ""attention."",
            ""o_proj"": ""out_proj"",
            ""lm_head"": ""final_proj"",
            ""input_layernorm"": ""input_layernorm"",
            ""post_attention_layernorm"": ""post_attention_layernorm"",
            r'^norm': 'final_norm'
        }
        
        result = sft_name
        for pattern, replacement in name_mapping.items():
            result = re.sub(pattern, replacement, result)
        return result",Convert safetensors naming convention to HuggingFace naming convention.,???Map tensor names to Hugging Face format using regex substitutions.???
2302,select_account,"def select_account(
    dialog_id: str, account_index: int
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""dialogId""] = dialog_id
    params[""accountIndex""] = account_index
    cmd_dict: T_JSON_DICT = {
        ""method"": ""FedCm.selectAccount"",
        ""params"": params,
    }
    json = yield cmd_dict",:param dialog_id: :param account_index:,???Generate a command to select a user account in a dialog interface.???
2303,_apply_header_to_chunks,"def _apply_header_to_chunks(self, chunks: List[Dict[str, Any]], header_text: str) -> List[Dict[str, Any]]:
    
    if not self.add_split_context or not header_text:
      return chunks
    
    result_chunks = []
    for i, chunk_data in enumerate(chunks):
      if i == 0:
        # First chunk: header + content (no breadcrumb)
        combined_text = header_text + ""\n\n"" + chunk_data['text'].lstrip()
      else:
        # Subsequent chunks: header + breadcrumb + content  
        combined_text = header_text + ""\n\n\t...\n\n"" + chunk_data['text'].lstrip()
      
      result_chunks.append({
        **chunk_data,
        'text': combined_text,
      })
    
    return result_chunks",Apply header context to chunks when add_split_context is enabled.,???Function prepends a header and optional breadcrumb to text chunks based on context settings.???
2304,add_guardrail_event,"def add_guardrail_event(
        self, base_trace_id: str, trace_data: Dict, content: Optional[str] = None
    ) -> None:
        
        if base_trace_id not in self.guardrail_buffer:
            self.guardrail_buffer[base_trace_id] = []

        # Store event with timestamp and content
        event_data = {
            ""trace_data"": trace_data,
            ""timestamp"": datetime.now().isoformat(),
            ""content"": content,
        }
        self.guardrail_buffer[base_trace_id].append(event_data)",Add event to guardrail buffer with associated content chunk,???Store trace events with timestamps in a guardrail buffer for tracking.???
2305,add_dataframe,"def add_dataframe(self, df):
    
    html_table = df.to_html(classes='dataframe', index=False, escape=False)
    self.log_messages.append(f""<div style='color:lightgreen;'>{html_table}</div>"")",Add a DataFrame to the log display as a table.,???Convert a DataFrame to HTML and log it with styling.???
2306,label_loss_items,"def label_loss_items(self, loss_items=None, prefix=""train""):
        
        keys = [f""{prefix}/{x}"" for x in self.loss_names]
        if loss_items is None:
            return keys
        loss_items = [round(float(loss_items), 5)]
        return dict(zip(keys, loss_items))",Returns a loss dict with labelled training loss items tensor.,??? Generate labeled dictionary of loss metrics with optional prefix. ???
2307,filter_by_prefixes,"def filter_by_prefixes(strings: Set[str], prefixes: Set[str]) -> Set[str]:
    
    return {s for s in strings if any(s.startswith(p) for p in prefixes)}",Return strings filtered down to only those that start with any of the prefixes.,???Filter strings by checking if they start with any given prefixes.???
2308,_load_cycle_data,"def _load_cycle_data() -> Dict[str, Any]:
    
    if os.path.exists(_CYCLE_DATA_PATH):
        try:
            with open(_CYCLE_DATA_PATH, 'r') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f""Error loading cycle data: {e}"")
    return {}",Load cycle data from disk,"???Load cycle data from file, handling errors and returning a dictionary.???"
2309,action_goToTab,"def action_goToTab(self, tab_number: int) -> None:
        
        tabs = self.query_one(Tabs)
        tabs.active = f""t{tab_number}""",Go to the specified tab.,???Switches the active tab to the specified tab number.???
2310,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Network.disable"",
    }
    json = yield cmd_dict","Disables network tracking, prevents network events from being sent to the client.","??? 
Generates a command to disable network monitoring in a JSON format. 
???"
2311,parse,"def parse(self, input, response):
        
        return [
            {
                ""problem"": input[""problem""],
                ""deepseek_reasoning"": response[""choices""][0][""message""][""reasoning""],
                ""deepseek_solution"": response[""choices""][0][""message""][""content""],
            }
        ]",Parse the LLM response to extract reasoning and solution.,???Extracts problem details and reasoning from input and response data???
2312,get_confirmation_message,"def get_confirmation_message(self) -> str:
        
        if hasattr(self, 'confirmation_message_callable') and self.confirmation_message_callable:
            return self.confirmation_message_callable()
        return self.confirmation_message or f""Tool '{self.name}' requires confirmation: Proceed? (yes/no)""","Return the confirmation message, invoking callable if present.","???  
Generate a confirmation prompt using a callable or default message.  
???"
2313,_check_model_available,"def _check_model_available(self) -> bool:
        
        try:
            response = requests.get(f""{self.base_url}/engines/llama.cpp/v1/models"", timeout=10)
            if response.status_code == 200:
                models_data = response.json()
                available_models = [model[""id""] for model in models_data.get(""data"", [])]
                return self.model_name in available_models
            else:
                print(f""Failed to check available models: HTTP {response.status_code}"")
                return False
        except requests.exceptions.ConnectionError:
            print(f""Cannot connect to Docker Model Runner at {self.base_url}. Is Docker Desktop running with Model Runner enabled?"")
            return False
        except requests.exceptions.RequestException as e:
            print(f""Error checking available models: {e}"")
            return False",Check if the model is available in Docker Model Runner.,???Check if a specified model is available on a remote server via HTTP request.???
2314,format_example,"def format_example(example: SerializedExample, language: str) -> str:
    
    name = example.name if example.name else ""Untitled""

    sections = [f""{name}-({example.language})"", format_section(""Description"", example.description), format_section(""Docstring"", example.docstring)]

    return '""' * 3 + ""\n"".join(filter(None, sections)) + '""' * 3 + ""\n\n"" + convert_to_cli(example.source, language, ""demo-function"")",Format a single example.,???Format serialized example with sections and convert source to CLI.???
2315,get_opportunities,"def get_opportunities(
        self,
        name: Optional[str] = None,
        chainId: Optional[str] = None,
        action: Optional[str] = None,
        tags: Optional[List[str]] = None,
        test: Optional[bool] = None,
        minimumTvl: Optional[float] = None,
        status: Optional[str] = None,
        tokens: Optional[List[str]] = None,
        sort: Optional[str] = None,
        order: Optional[str] = None,
        mainProtocolId: Optional[str] = None,
        page: Optional[int] = None,
        items: Optional[int] = None,
    ) -> Dict:
        
        params = {k: v for k, v in locals().items() if v is not None and k != ""self""}

        return self._sync_request(""get"", ""/opportunities/"", params=params)",Get list of DeFi opportunities with optional filters,???Fetch investment opportunities based on various optional filters and sorting criteria.???
2316,calculate_answer_score,"def calculate_answer_score(pred_sql, gold_sql, db_path, do_print=False):
    
    try:
        pred_results = execute_sql(pred_sql, db_path)
        gold_results = execute_sql(gold_sql, db_path)
        
        # bird
        answer_score = 1 if set(pred_results) == set(gold_results) else 0.3
        # answer_score = 2 if set(pred_results) == set(gold_results) else 0.5

        # spider
        # answer_score = 1 if set(pred_results) == set(gold_results) else 0
        
    except Exception as e:
        if do_print:
            print(f""[Error] Error in executing SQL: {e}"")
        pred_results = []
        gold_results = []

        answer_score = 0
        # if 'syntax' in str(e):
        #     answer_score = 0
        # else:
        #     answer_score = 0.1

    if do_print:
        # print(f""Retrieved results: {pred_results}"")
        # print(f""Target: {gold_results} "")
        print(f""Answer score: {answer_score}"")

    
    return answer_score",Calculate answer score based on final_prediction idx.,???Evaluate SQL query accuracy by comparing predicted and actual results.???
2317,load_task_from_yaml,"def load_task_from_yaml(yaml_path: str) -> tuple[list, str, str, str, list]:
    
    try:
        with open(yaml_path, 'r') as f:
            data = yaml.safe_load(f)
            # Get task configuration
            task_id = data.get('task_id')
            task_description = data.get('task_description')
            function_name = data.get('function_name')
            allowed_imports = data.get('allowed_imports', [])
            
            # Convert test cases from YAML format to input_output_examples format
            input_output_examples = []
            for test_group in data.get('tests', []):
                for test_case in test_group.get('test_cases', []):
                    if 'output' in test_case:
                        input_output_examples.append({
                            'input': test_case['input'],
                            'output': test_case['output']
                        })
                    elif 'validation_func' in test_case:
                        input_output_examples.append({
                            'input': test_case['input'],
                            'validation_func': test_case['validation_func']
                        })
            
            return input_output_examples, task_id, task_description, function_name, allowed_imports
    except Exception as e:
        logger.error(f""Error loading task from YAML: {e}"")
        return [], """", """", """", []",Load task configuration and test cases from a YAML file.,???Parse YAML to extract task details and test cases for execution???
2318,get_observation,"def get_observation(env_id):
    
    print_step(f""Getting observation for environment {env_id}..."")
    
    observation = make_request(""GET"", ""observation"", params={""env_idx"": env_id})
    
    if observation:
        print_success(""Observation received"")
        print(f""\n{Colors.CYAN}{'='*40} OBSERVATION {'='*40}{Colors.ENDC}"")
        print(f""{Colors.CYAN}{observation}{Colors.ENDC}"")
        print(f""{Colors.CYAN}{'='*90}{Colors.ENDC}\n"")
        return observation
    else:
        print_error(""Failed to get observation"")
        return None",Get observation for environment,"???Retrieve and display environment observation using API request, with success/error feedback.???"
2319,light_text,"def light_text(placeholder):
    
    if sys.platform == ""win32"":
        return f""\033[90m{placeholder} (type \""/exit\"" to quit)\033[0m""
    else:
        return HTML(f'<style color=""#777777"">{placeholder} (type ""/exit"" to quit)</style>')",Apply light text style to the placeholder.,"??? 
Format placeholder text with platform-specific styling and exit instructions. 
???"
2320,_add_prefix,"def _add_prefix(file: TextIO, worker_name: str, pid: int) -> None:
    

    prefix = f""{CYAN}({worker_name} pid={pid}){RESET} ""
    file_write = file.write

    def write_with_prefix(s: str):
        if not s:
            return
        if file.start_new_line:  # type: ignore[attr-defined]
            file_write(prefix)
        idx = 0
        while (next_idx := s.find(""\n"", idx)) != -1:
            next_idx += 1
            file_write(s[idx:next_idx])
            if next_idx == len(s):
                file.start_new_line = True  # type: ignore[attr-defined]
                return
            file_write(prefix)
            idx = next_idx
        file_write(s[idx:])
        file.start_new_line = False  # type: ignore[attr-defined]

    file.start_new_line = True  # type: ignore[attr-defined]
    file.write = write_with_prefix",Prepend each output line with process-specific prefix,???Add worker prefix to each new line in file output???
2321,clean_nan_values,"def clean_nan_values(obj):
    
    import numpy as np

    if isinstance(obj, (float, np.floating)):
        return None if np.isnan(obj) else float(obj)
    elif isinstance(obj, (list, tuple)):
        return [clean_nan_values(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: clean_nan_values(v) for k, v in obj.items()}
    elif isinstance(obj, np.ndarray):
        if obj.dtype.kind in [""f"", ""c""]:  # Float or complex
            obj = np.where(np.isnan(obj), None, obj)
        return obj.tolist()
    return obj",Clean NaN values from an object recursively.,???Recursively replace NaN values with None in various data structures.???
2322,to_openai_request,"def to_openai_request(self) -> Dict[str, Any]:
        
        oai_dict = {
            k: v for k, v in self.model_dump().items()
            if k not in [""speaker_files"", ""openai_api_url"", ""vocalize_at_every_n_words"", 'modalities'] and
               not k in tts_defaults.keys()
        }
        oai_dict.update({""stream"": True})
        return oai_dict",Convert to OpenAI API compatible request format,"???  
Filter and prepare model data for OpenAI API request.  
???"
2323,batch_evaluate_html,"def batch_evaluate_html(self, pred_htmls, true_htmls):
        
        if self.n_jobs == 1:
            scores = [
                self.evaluate(pred_html, true_html)
                for (pred_html, true_html) in zip(pred_htmls, true_htmls)
            ]
        else:
            inputs = [
                {""pred"": pred_html, ""true"": true_html}
                for (pred_html, true_html) in zip(pred_htmls, true_htmls)
            ]

            scores = parallel_process(
                inputs, self.evaluate, use_kwargs=True, n_jobs=self.n_jobs, front_num=1
            )
        return scores",Computes TEDS score between the prediction and the ground truth of a batch of samples,??? Evaluate HTML predictions against true values using parallel processing if needed. ???
2324,_create_client_params,"def _create_client_params(self):
        
        params = {}
        logger.debug(f""Creating client parameters for endpoint: {self.endpoint_name}"")

        # Check for URL-based connection
        url = self.api_endpoint
        api_key = self.api_key
        path = self.database_path

        # Decide whether to use URL or path-based connection
            logger.debug(f""Using Qdrant server URL: {url}"")
            params[""url""] = url
            if api_key:
                params[""api_key""] = api_key
        elif path:
            # Resolve relative paths for local file-based storage
            resolved_path = self._resolve_path(path)
            logger.debug(f""Using local Qdrant database path: {resolved_path}"")
            params[""path""] = resolved_path
        else:
            # Default to a local path if neither URL nor path is specified
            default_path = self._resolve_path(""../data/db"")
            logger.debug(f""Using default local Qdrant database path: {default_path}"")
            params[""path""] = default_path
        
        logger.debug(f""Final client parameters: {params}"")
        return params",Extract client parameters from endpoint config.,???Determine client connection parameters based on URL or local path configuration.???
2325,analyze_sentiment,"def analyze_sentiment(news_items: list) -> dict:
    
    if not news_items:
        return {""score"": 5, ""details"": ""No news data; defaulting to neutral sentiment""}

    negative_keywords = [""lawsuit"", ""fraud"", ""negative"", ""downturn"", ""decline"", ""investigation"", ""recall""]
    negative_count = 0
    for news in news_items:
        title_lower = (news.title or """").lower()
        if any(word in title_lower for word in negative_keywords):
            negative_count += 1

    details = []
    if negative_count > len(news_items) * 0.3:
        score = 3
        details.append(f""High proportion of negative headlines: {negative_count}/{len(news_items)}"")
    elif negative_count > 0:
        score = 6
        details.append(f""Some negative headlines: {negative_count}/{len(news_items)}"")
    else:
        score = 8
        details.append(""Mostly positive/neutral headlines"")

    return {""score"": score, ""details"": ""; "".join(details)}",Basic news sentiment: negative keyword check vs.,???Evaluate news sentiment by counting negative keywords in headlines to assign a sentiment score.???
2326,create_tool_use_message,"def create_tool_use_message(call_count, usage):
        
        return Message(
            role=""assistant"",
            content=[
                ToolUseBlock(
                    type=""tool_use"",
                    name=""search_tool"",
                    input={""query"": ""test query""},
                    id=f""tool_{call_count}"",
                )
            ],
            model=""claude-3-7-sonnet-latest"",
            stop_reason=""tool_use"",
            id=f""resp_{call_count}"",
            type=""message"",
            usage=usage,
        )",Creates a tool use message for testing.,???Generate a message detailing tool usage with specific identifiers and parameters.???
2327,execute_agent,"def execute_agent(system_prompt: str, user_prompt: str, to_json: bool = False) -> dict:
    

    llm = OpenAI(
        model=""gpt-4o-mini"",
        connection=OpenAIConnection(),
        max_tokens=3000,
        prompt=Prompt(
            messages=[
                Message(role=""system"", content=system_prompt),
                Message(role=""user"", content=user_prompt),
            ]
        ),
    )

    response = llm.run(input_data={}).output.get(""content"", """").strip()

    if to_json:
        try:
            response = _extract_code_block(response)
            return json.loads(response.lstrip(""'`json"").rstrip(""'`""))
        except json.JSONDecodeError as e:
            print(f""JSON decoding failed: {e}. Response: {response}"")
            return {}

    return response",Executes an LLM request,"???  
Execute AI model with prompts, optionally parse response as JSON.  
???"
2328,click_element,"def click_element(self, query, click_command, action_name=""click""):
        
        self.screenshot()
        position = grounding_model.call(query, self.latest_screenshot)
        dot_image = draw_big_dot(Image.open(self.latest_screenshot), position)
        filepath = self.save_image(dot_image, ""location"")
        logger.log(f""{action_name} {filepath})"", ""gray"")

        x, y = position
        self.sandbox.move_mouse(x, y)
        click_command()
        return f""The mouse has {action_name}ed.""",Base method for all click operations,???Automates mouse click on a UI element based on a visual query.???
2329,handle_models_command,"def handle_models_command(
    lines: List[str], args: List[str], console: Console, session: PromptSession, history_manager: InputHistoryManager
) -> None:
    
    from quantalogic.utils.get_all_models import get_all_models

    try:
        models = get_all_models()
        if models:
            # Group models by provider
            provider_groups = {}
            for model in models:
                provider = model.split(""/"")[0] if ""/"" in model else ""default""
                if provider not in provider_groups:
                    provider_groups[provider] = []
                provider_groups[provider].append(model)

            # Create formatted output
            output = ""[bold #00cc66]Available AI Models:[/bold #00cc66]\n""
            for provider, model_list in provider_groups.items():
                output += f""\n[bold #ffaa00]{provider.upper()}[/bold #ffaa00]\n""
                for model in sorted(model_list):
                    output += f""  • {model}\n""

            console.print(Panel(output, border_style=""green""))
        else:
            console.print(""[yellow]No models available.[/yellow]"")
    except Exception as e:
        console.print(f""[red]Error retrieving models: {str(e)}[/red]"")",Display all available AI models supported by the system.,???Display available AI models grouped by provider using console output.???
2330,_process_guardrail_span,"def _process_guardrail_span(self, span: Dict[str, Any]) -> None:
        
        guardrail_name = span.get(""data"", {}).get(""name"", ""unknown_guardrail"")
        guardrail_type = span.get(""data"", {}).get(""type"", ""unknown_type"")
        triggered = span.get(""data"", {}).get(""tripwire_triggered"", False)
        
        logger.debug(f""Guardrail: {guardrail_name} ({guardrail_type}), Triggered: {triggered}"")",Process a guardrail span,"???Log guardrail details including name, type, and trigger status from span data.???"
2331,reset_demo_data,"def reset_demo_data(timestamp=None):
    
    if not settings.DEMO:
        return  # Exit if not in demo mode

    logger.info(""Demo mode active. Starting daily data reset..."")

    try:
        # 1. Flush the database (wipe all data)
        logger.info(""Flushing the database..."")

        management.call_command(
            ""flush"", ""--noinput"", database=DEFAULT_DB_ALIAS, verbosity=1
        )
        logger.info(""Database flushed successfully."")

        # 2. Load data from the fixture
        # TO-DO: Roll dates over based on today's date
        fixture_name = ""fixtures/demo_data.json""
        logger.info(f""Loading data from fixture: {fixture_name}..."")
        management.call_command(
            ""loaddata"", fixture_name, database=DEFAULT_DB_ALIAS, verbosity=1
        )
        logger.info(f""Data loaded successfully from {fixture_name}."")

        logger.info(""Daily demo data reset completed."")

    except Exception as e:
        logger.exception(f""Error during daily demo data reset: {e}"")
        raise",Wipes the database and loads fresh demo data if DEMO mode is active.,???Reset demo data daily by flushing the database and reloading from a fixture.???
2332,category,"def category(self) -> str:
        
        module_name = self.__class__.__module__
        parts = module_name.split(""."")
        if len(parts) >= 3:
            return parts[1]  # reasoning_gym.{category}.dataset_name
        return ""other""",Extract category from the module name.,???Determine category from module name structure???
2333,attempt_install,"def attempt_install(packages, commands):
        
        return subprocess.check_output(f""pip install --no-cache-dir {packages} {commands}"", shell=True).decode()",Attempt pip install command with retries on failure.,???Executes a pip installation command for specified packages and options.???
2334,ensure_all_methods_implemented,"def ensure_all_methods_implemented(
    source_cls: Type[object],
) -> Callable[[Type[R]], Type[R]]:
    

    def check_all_methods_implemented(target_cls: R) -> R:
        for name, _ in inspect.getmembers(source_cls, inspect.isfunction):
            if name.startswith(""_""):
                continue
            if name not in target_cls.__dict__ or not callable(target_cls.__dict__[name]):
                raise NotImplementedError(f""{name} is not implemented in {target_cls}"")

        return target_cls

    return check_all_methods_implemented",A decorator to ensure that all methods of source_cls class are implemented in the decorated class.,???Decorator ensures target class implements all public methods of source class???
2335,undo_edit,"def undo_edit(self, path: Path):
        
        if not self._file_history[path]:
            raise ToolError(f""No edit history found for {path}."")

        old_text = self._file_history[path].pop()
        self.write_file(path, old_text)

        return CLIResult(
            output=f""Last edit to {path} undone successfully. {self._make_output(old_text, str(path))}""
        )",Implement the undo_edit command.,???Revert file to previous state using edit history.???
2336,_prepare_update_document_with_text_request,"def _prepare_update_document_with_text_request(
        self,
        document_id: str,
        content: str,
        filename: Optional[str],
        metadata: Optional[Dict[str, Any]],
        rules: Optional[List],
        update_strategy: str,
        use_colpali: Optional[bool],
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        
        request = IngestTextRequest(
            content=content,
            filename=filename,
            metadata=metadata or {},
            rules=[self._convert_rule(r) for r in (rules or [])],
            use_colpali=use_colpali if use_colpali is not None else True,
        )

        params = {}
        if update_strategy != ""add"":
            params[""update_strategy""] = update_strategy

        return params, request.model_dump()",Prepare request for update_document_with_text endpoint,"???Prepare a document update request with content, metadata, and strategy options.???"
2337,run_file_mode,"def run_file_mode(agent, console: Console, file: str, config: QLConfig) -> None:
    
    task_content = get_task_from_file(file)
    
    # Clear any existing handlers to prevent duplicates
    agent.event_emitter.clear(""stream_chunk"")
    
    # Register stream handler
    agent.event_emitter.on(""stream_chunk"", create_stream_handler(console))
    
    # Execute task from file
    logger.debug(f""Solving task with agent: {task_content}"")
    if config.max_iterations < 1:
        raise ValueError(""max_iterations must be greater than 0"")
        
    result = agent.solve_task(
        task=task_content,
        max_iterations=config.max_iterations,
        streaming=not config.no_stream
    )
    
    logger.debug(f""Task solved with result: {result} using {config.max_iterations} iterations"")
    console.print(
        Panel.fit(
            f""[bold]Task Result:[/bold]\n{result}"",
            title=""[bold]Execution Output[/bold]"",
            border_style=""green""
        )
    )",Run a task from a file.,???Execute a task from a file using an agent with streaming and iteration control.???
2338,register_executor,"def register_executor(self, executor: BaseExecutor, name: str) -> None:
        
        try:
            self.plugin_manager.executors[name] = executor.__class__
        except Exception as e:
            logger.error(f""Failed to register executor {name}: {e}"")
            raise",Register a new executor dynamically at runtime.,???Register an executor in the plugin manager with error handling.???
2339,_initialize_alembic,"def _initialize_alembic(self) -> bool:
        
        try:
            # Ensure parent directory exists
            self.alembic_dir.parent.mkdir(exist_ok=True)

            # Run alembic init to create fresh directory structure
            # logger.info(""Initializing alembic directory structure..."")

            # Create initial config file for alembic init
            config_content = self._generate_alembic_ini_content()
            with open(self.alembic_ini_path, ""w"") as f:
                f.write(config_content)

            # Use the config we just created
            config = Config(str(self.alembic_ini_path))

            with redirect_stdout(io.StringIO()):
                command.init(config, str(self.alembic_dir))

            # Update script template after initialization
            self.update_script_template()

            # Update env.py with our customizations
            self._update_env_py(self.alembic_dir / ""env.py"")

            logger.info(""Alembic initialization complete"")
            return True

        except Exception as e:
            # Explicitly convert error to string
            logger.error(f""Failed to initialize alembic: {str(e)}"")
            return False",Initialize alembic structure and configuration,"???Initialize Alembic directory and configuration, handling errors gracefully.???"
2340,escape_code_brackets,"def escape_code_brackets(text: str) -> str:
    

    def replace_bracketed_content(match):
        content = match.group(1)
        cleaned = re.sub(
            r""bold|red|green|blue|yellow|magenta|cyan|white|black|italic|dim|\s|#[0-9a-fA-F]{6}"", """", content
        )
        return f""\\[{content}\\]"" if cleaned.strip() else f""[{content}]""

    return re.sub(r""\[([^\]]*)\]"", replace_bracketed_content, text)",Escapes square brackets in code segments while preserving Rich styling tags.,"???  
Sanitize and escape bracketed text by removing specific keywords and colors.  
???"
2341,get_action,"def get_action(self, obs):
        
        if len(self.action_cache) > 0:
            curr_action = self.action_cache.pop(0)
        else:
            processed_obs = self.process_obs(obs)
            action_chunk = self.predict_action(processed_obs)  # shape: (action_chunk_steps, num_envs, action_dim)
            if self.policy_cfg.action_config.temporal_agg:
                curr_action = self.get_temporal_agg_action(action_chunk)
                curr_action = self.process_action([curr_action], obs)[0]
            else:
                qpos_action = self.process_action(action_chunk, obs)
                assert len(qpos_action) == self.policy_cfg.action_config.action_chunk_steps, (
                    f""Expected {self.policy_cfg.action_config.action_chunk_steps} actions, got {len(qpos_action)}""
                )
                self.action_cache = qpos_action
                curr_action = self.action_cache.pop(0)

        self.step += 1
        assert curr_action.shape == (self.num_envs, len(self.scenario.robots[0].joint_limits.keys())), (
            f""Expected num_envs X n_dof : {self.num_envs} X {len(self.scenario.robots[0].joint_limits.keys())}, got {curr_action.shape} instead""
        )

        actions = self.action_to_dict(curr_action)
        return actions",Returns a single action to be directly executed.,???Determine and execute the next action based on observation and cached actions.???
2342,mock_weaviate_client,"def mock_weaviate_client():
    
    mock_client = MagicMock()

    # Mock collections API
    mock_client.collections.exists.return_value = False

    # Mock collection objects
    mock_collection = MagicMock()
    mock_client.collections.__getitem__.return_value = mock_collection
    mock_client.collections.get.return_value = mock_collection

    # Mock multi-tenancy config in collection
    mock_config = {""multi_tenancy_config"": {""enabled"": True}}
    mock_collection.config.return_value = mock_config

    # Set up tenants
    mock_tenant1 = MagicMock()
    mock_tenant1.name = ""tenant1""
    mock_tenant2 = MagicMock()
    mock_tenant2.name = ""tenant2""
    mock_tenants = {""tenant1"": mock_tenant1, ""tenant2"": mock_tenant2}
    mock_collection.tenants.get.return_value = mock_tenants

    return mock_client",Create a mock Weaviate client with necessary structures for tenant testing.,???Create a mock Weaviate client with predefined collection and tenant configurations.???
2343,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DOMStorage.enable"",
    }
    json = yield cmd_dict","Enables storage tracking, storage events will now be delivered to the client.",???Enable DOM storage by sending a command and yielding a JSON response.???
2344,analyze_cash_flow,"def analyze_cash_flow(financial_line_items: list) -> dict[str, any]:
    
    if not financial_line_items:
        return {""score"": 0, ""details"": ""No cash flow data""}

    latest = financial_line_items[0]
    score = 0
    reasoning = []

    # Free cash flow analysis
    if getattr(latest, ""free_cash_flow"", None) and latest.free_cash_flow:
        if latest.free_cash_flow > 0:
            score += 2
            reasoning.append(f""Positive free cash flow: {latest.free_cash_flow}"")
        else:
            reasoning.append(f""Negative free cash flow: {latest.free_cash_flow}"")
    else:
        reasoning.append(""Free cash flow data not available"")

    # Dividend analysis
    if getattr(latest, ""dividends_and_other_cash_distributions"", None) and latest.dividends_and_other_cash_distributions:
        if latest.dividends_and_other_cash_distributions < 0:  # Negative indicates cash outflow for dividends
            score += 1
            reasoning.append(""Company pays dividends to shareholders"")
        else:
            reasoning.append(""No significant dividend payments"")
    else:
        reasoning.append(""No dividend payment data available"")

    return {""score"": score, ""details"": ""; "".join(reasoning)}",Evaluate free cash flow and dividend behavior.,???Evaluate financial health by scoring cash flow and dividend data.???
2345,set_modes,"def set_modes(self, modes: list[SerenaAgentMode]) -> None:
        
        self._modes = modes
        self._update_active_tools()

        log.info(f""Set modes to {[mode.name for mode in modes]}"")",Set the current mode configurations.,"???Sets operational modes and updates active tools, logging changes.???"
2346,_initialize,"def _initialize(self):
        
        if self._pc is None:
            try:
                self._pc = Pinecone(api_key=self._config.api_key.get_secret_value(), host=self._config.index_host)
                self._index = self._pc.Index(host=self._config.index_host)
            except Exception as e:
                logger.error(f""Failed to initialize Pinecone: {e}"")
                raise Exception(f""Failed to initialize Pinecone: {e}"") from e",Initialize Pinecone if not already done.,???Initialize Pinecone connection using API key and handle exceptions.???
2347,initialize_models,"def initialize_models(model_name: str = ""gpt-4o-mini"", provider: str = ""openai"", temperature: float = 0.5, max_results: int = 2):
    
    if provider == ""openai"":
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(model=model_name, temperature=temperature)

    elif provider == ""google_genai"":
        from langchain_google_genai import ChatGoogleGenerativeAI
        llm = ChatGoogleGenerativeAI(model=model_name, temperature=temperature)
    
    # elif provider == ""google_vertexai"":
    #     from langchain_google_vertexai import ChatVertexAI
    #     llm = ChatVertexAI(model=model_name, google_api_key=os.getenv(""GOOGLE_API_KEY""))

    # elif provider == ""azure"":
    #     # Example for Azure OpenAI (adjust as needed)
    #     llm = ChatOpenAI(
    #         model=model_name,
    #         temperature=temperature,
    #         openai_api_base=os.getenv(""AZURE_OPENAI_ENDPOINT""),
    #         openai_api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
    #         openai_api_version=""2024-08-01-preview""
    #     )
    # elif provider == ""anthropic"":
    #     from langchain_anthropic import ChatAnthropic
    #     llm = ChatAnthropic(model=model_name, temperature=temperature)
    else:
        raise ValueError(f""Unsupported provider: {provider}"")
    
    tavily_tool = TavilySearchResults(max_results=max_results)
    return llm, tavily_tool",Initialize the language model and search tool based on the provider.,???Initialize language models and search tools based on specified provider and parameters.???
2348,handle_decompress,"def handle_decompress(args: argparse.Namespace) -> int:
    
    try: result = decompress(args.string); print(f""Decompressed string: {result}""); return 0
    except Exception as e: logger.error(f""Error decompressing: {e}""); print(f""Error: {e}""); return 1",Handle the decompress command.,"???Decompress input string, log errors, and return status code.???"
2349,ensure_config_dir,"def ensure_config_dir():
    
    if not os.path.exists(CONFIG_DIR):
        os.makedirs(CONFIG_DIR, exist_ok=True)
        scheduler_logger.info(f""Created config directory: {CONFIG_DIR}"")",Ensure the config directory exists,"???Ensure configuration directory exists, creating it if necessary and logging the action.???"
2350,create_knowledge_base,"def create_knowledge_base(kb_name, kb_description, suffix):
    
    bucket_name = f'{kb_name}-{suffix}'
    knowledge_base_metadata = BedrockKnowledgeBase(
        kb_name=f'{kb_name}-{suffix}',
        kb_description=kb_description,
        data_bucket_name=bucket_name,
        chunking_strategy=""FIXED_SIZE"",
        suffix=suffix
    )
    return knowledge_base_metadata, bucket_name",Create a knowledge base,???Create a structured knowledge base with metadata and storage configuration.???
2351,convert_multiline_to_single_line,"def convert_multiline_to_single_line(file_path, output_path=None):
    
    if output_path is None:
        output_path = file_path

    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        
        # Preserve the original file permissions
        original_mode = os.stat(file_path).st_mode
            
        # Use regular expression to find lines ending with \ and remove newlines and \
        pattern = r'\\\s*\n\s*'
        converted_content = re.sub(pattern, ' ', content)
            
        # Write the converted content to the output file
        with open(output_path, 'w', encoding='utf-8') as file:
            file.write(converted_content)
            
        # Restore the original file permissions
        os.chmod(output_path, original_mode)
        
        print(f""Successfully converted {file_path} to single-line command format, saved to {output_path}"")
        return True
    except Exception as e:
        print(f""Error: {str(e)}"")
        return False",Convert multi-line commands (ending with backslash) to single-line commands,"???Convert multiline commands in a file to single-line format, preserving permissions.???"
2352,init_executors,"def init_executors(self) -> dict[AgentTools]:
        
        rows_data = self.env.data_examples
        table_insertion_tools = {}
        agent_executors = {}
        for table_name, example in rows_data.items():
            cur_tool, tool_schema = self.get_insertion_function(table_name)
            table_insertion_tools[table_name] = StructuredTool.from_function(
                cur_tool,
                None,
                name='add_row_to_table',
                description='Add a row to the table in json format. The row should be a **json string** with the same schema as in the provided example.',
                infer_schema=True,
            )

            system_messages = hub.pull(self.config['event_graph']['prompt_executors']['prompt_hub_name'])
            system_messages = system_messages.partial(schema=self.env.data_schema[table_name],
                                                      example=json.dumps(rows_data[table_name]))
            agent_executor = AgentTools(llm=self.llm, tools=[think, table_insertion_tools[table_name]],
                                        system_prompt=system_messages)
            agent_executors[table_name] = agent_executor
        return agent_executors",Initialize the database plane executors.,???Initialize agent executors for table data insertion using structured tools and system prompts.???
2353,_get_system_architecture,"def _get_system_architecture() -> str:
    
    system = platform.system().lower()
    machine = platform.machine().lower()

    # Map machine architecture to supported plugin architecture
    arch_map = {
        ""x86_64"": ""amd64"",
        ""aarch64"": ""arm64"",
        ""armv7l"": ""arm"",
    }

    # Determine the system architecture
    arch = arch_map.get(machine, machine)

    # Handle different operating systems
    if system == ""windows"":
        return f""windows-{arch}.exe""
    elif system == ""darwin"":
        return f""darwin-{arch}""
    elif system == ""linux"":
        return f""linux-{arch}""
    else:
        raise ValueError(f""Unsupported system: {system}"")",Determine the system architecture for plugin download.,???Determine executable format based on system and architecture mapping???
2354,normalize_text,"def normalize_text(text: str) -> str:
    
    text = re.sub(r""\s+"", "" "", text)
    replacements = {""'"": ""'"", ""‚"": ""'"", '""': '""', ""„"": '""', ""＿"": ""_"", ""–"": ""-"", ""—"": ""-"", ""‑"": ""-"", ""‒"": ""-""}
    for fancy_char, ascii_char in replacements.items():
        text = text.replace(fancy_char, ascii_char)
    return text",Normalize text for better matching.,???Standardize text by replacing fancy characters with ASCII equivalents.???
2355,op_description_dict,"def op_description_dict(self) -> Dict[str, str]:
        
        return {k: v.description for k, v in self._operation_dict.items()}",Property to retrieve a dictionary mapping operation keys to their descriptions.,???Creates a dictionary mapping operation names to their descriptions.???
2356,run_evals,"def run_evals(predictions_jsonl, logs_dir: Path, dataset: SWEBenchDataset, run_id: str):
    
    run_evals_cmd = f
    run_evals_cmd = "" "".join([line.strip() for line in run_evals_cmd.split() if line.strip()])
    print(""Running evaluation command:"", run_evals_cmd)

    subprocess.run(run_evals_cmd.split(), check=True)",Run the evaluations on the predictions on modal.,???Execute evaluation command using predictions and dataset parameters???
2357,ensure_history_dir,"def ensure_history_dir():
    
    try:
        # Create base directory
        HISTORY_BASE_PATH.mkdir(exist_ok=True, parents=True)
        
        # Create app-specific directories
        for app in history_locks.keys():
            app_dir = HISTORY_BASE_PATH / app
            app_dir.mkdir(exist_ok=True, parents=True)
                    
        return True
    except Exception as e:
        logger.error(f""Failed to create history directory: {str(e)}"")
        return False",Ensure the history directory exists with app-specific subdirectories,"???Ensure application history directories exist, logging errors if creation fails.???"
2358,_create_domain_ssl_context,"def _create_domain_ssl_context(
        self, cert_path: str, key_path: str, domain: str
    ) -> ssl.SSLContext:
        

        logger.debug(f""Loading cert chain from {cert_path} for domain {domain}"")
        context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
        try:
            context.load_cert_chain(cert_path, key_path)
        except ssl.SSLError as e:
            logger.error(f""Failed to load cert chain for {domain}: {e}"")
            raise

        context.minimum_version = ssl.TLSVersion.TLSv1_2
        context.set_ciphers(""ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20"")
        context.options |= (
            ssl.OP_NO_SSLv2
            | ssl.OP_NO_SSLv3
            | ssl.OP_NO_COMPRESSION
            | ssl.OP_CIPHER_SERVER_PREFERENCE
        )
        return context",Domain SNI Context Setting,???Create a secure SSL context for a domain using specified certificate paths.???
2359,generate_data,"def generate_data(
        self, prompt: str, response_model: Type[BaseModel], **kwargs
    ) -> BaseModel:
        
        merged_kwargs = {**self.default_kwargs, **kwargs}
        return generate_data(
            prompt=prompt,
            response_model=response_model,
            llm_provider=self.llm_provider,
            llm_model=self.llm_model,
            **merged_kwargs,
        )",Generate structured data using the session's default provider and model.,???Generate data using a prompt and response model with merged parameters.???
2360,_init_image_cond_model,"def _init_image_cond_model(self, name: str):
        
        if not hasattr(self, 'model_dir'):
            raise AttributeError(""Pipeline model_dir not set. Please ensure from_pretrained() is called first."")
            
        # Create model manager instance with proper config
        config = getattr(self, 'config', {})
        model_manager = TrellisModelManager(self.model_dir, config=config)
        
        try:
            # This will handle downloading if needed
            dinov2_model = model_manager.load_dinov2(name)
            
            # Ensure model is in consistent dtype based on config
            if getattr(config, 'use_fp16', True):
                dinov2_model = dinov2_model.half()
            else:
                dinov2_model = dinov2_model.float()
                
            self.models['image_cond_model'] = dinov2_model
            
            transform = transforms.Compose([
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ])
            self.image_cond_model_transform = transform
            
        except Exception as e:
            logger.error(f""Error loading DINOv2 model: {str(e)}"")
            raise",Initialize the image conditioning model.,???Initialize and configure an image conditioning model with error handling and transformation setup.???
2361,_prepare_for_ast,"def _prepare_for_ast(self, expr: str) -> str:
        
        # Replace ${var_name} with var_name for AST parsing
        processed_expr = expr
        for var_name in self._variable_names:
            processed_expr = processed_expr.replace(f""${{{var_name}}}"", var_name)

        return processed_expr",Convert the expression to valid Python for AST parsing by replacing variables with placeholders.,"???  
Transform template variables in expression for AST compatibility.  
???"
2362,_get_output_path,"def _get_output_path(self, uid: str) -> str:
        
        return str(self.output_dir / f""{uid}.png"")",Generate unique output path for image,"???  
Generates a file path for a PNG image using a unique identifier.  
???"
2363,check_request_exists,"def check_request_exists(self, media_type, media_id):
        
        self.logger.debug(f""Checking if request exists: {media_type} {media_id}"")
        
        query = 
        params = (str(media_id), media_type)
        result = self.execute_query(query, params)
        return result is not None and len(result) > 0",Check if a media request already exists in the database.,???Determine if a media request exists by querying with media type and ID.???
2364,introductions_msg,"def introductions_msg(self, agents: Optional[list[Agent]] = None) -> str:
        
        if agents is None:
            agents = self.agents

        # Use the class attribute instead of a hardcoded string
        intro_msg = self.DEFAULT_INTRO_MSG
        participant_roles = self._participant_roles(agents)

        return f""{intro_msg}\n\n{participant_roles}""",Return the system message for selecting the next speaker.,???Generate an introduction message incorporating agent roles and a default greeting.???
2365,get_browser_use_version,"def get_browser_use_version() -> str:
	
	try:
		package_root = Path(__file__).parent.parent
		pyproject_path = package_root / 'pyproject.toml'

		# Try to read version from pyproject.toml
		if pyproject_path.exists():
			import re

			with open(pyproject_path, encoding='utf-8') as f:
				content = f.read()
				match = re.search(r'version\s*=\s*[""\']([^""\']+)[""\']', content)
				if match:
					return f'{match.group(1)}'

		# If pyproject.toml doesn't exist, try getting version from pip
		from importlib.metadata import version as get_version

		return str(get_version('browser-use'))

	except Exception as e:
		logger.debug(f'Error detecting browser-use version: {type(e).__name__}: {e}')
		return 'unknown'",Get the browser-use package version using the same logic as Agent._set_browser_use_version_and_source,???Determine browser-use version from pyproject.toml or package metadata???
2366,is_configured,"def is_configured(self, verbose = False) -> bool:
        
        try:
            load_dotenv()
            api_key = os.getenv('PERPLEXITY_API_KEY')
            if not api_key:
                return False

            client = self._get_client()
            self.search(""test"")  # Quick test query
            return True
            
        except Exception as e:
            if verbose:
                logger.debug(f""Configuration check failed: {e}"")
            return False",Check if Perplexity API key is configured and valid,???Check if API configuration is valid and operational with a test query.???
2367,reply_to_cast,"def reply_to_cast(self, parent_fid: int, parent_hash: str, text: str, embeds: Optional[List[str]] = None, channel_key: Optional[str] = None) -> CastContent:
        
        logger.debug(f""Replying to cast: {parent_hash}, text: {text}"")
        parent = Parent(fid=parent_fid, hash=parent_hash)
        return self._client.post_cast(text, embeds, parent, channel_key)",Reply to an existing cast,???Function to post a reply to a specific cast with optional media and channel.???
2368,remove,"def remove(self, tool_name: str) -> bool:
        
        logger.debug(f""Removing tool: {tool_name} from tool dictionary"")
        del self.tools[tool_name]
        return True",Remove a tool from the tool dictionary.,???Remove a specified tool from the tool dictionary and log the action.???
2369,parse_provider_model,"def parse_provider_model(model_id: str) -> tuple[str, str]:
    
    # most providers are in the format ""<provider>/<model-name>""
    # openrouter models are in the format ""openrouter/<provider>/<model-name>""
    parts = tuple(model_id.split('/'))
    if len(parts) == 2:
        return parts
    elif len(parts) == 3:
        return '/'.join(parts[:2]), parts[2]
    else:
        raise ValidationError(f""Model id '{model_id}' does not match expected format."")",Parse the provider and model name from the model ID,"???Parse model identifier into provider and model name components, handling different formats.???"
2370,_too_many_failures,"def _too_many_failures(self) -> bool:
		
		if self.consecutive_failures >= self.max_failures:
			logger.error(f'❌ Stopping due to {self.max_failures} consecutive failures')
			return True
		return False",Check if we should stop due to too many failures,???Check if consecutive failures exceed the maximum allowed threshold and log an error.???
2371,generate_cases,"def generate_cases(extra_repos: bool = False):
    
    repos = find_repos(extra_repos=extra_repos)
    for codemod in find_codemods():
        for repo_name, repo in repos.items():
            (codemod.test_dir / f""test_{repo_name}"").mkdir(parents=True, exist_ok=True)
    _generate_diffs(extra_repos=extra_repos)
    _clean_diffs(aggressive=True)",Generate cases for codemod tests.,???Create test directories for repositories and manage code modifications.???
2372,_generate_api_key,"def _generate_api_key(self) -> GeneratedAPIKey:
        
        key = f""wai-{secrets.token_urlsafe(32)}""
        hashed = self._get_hashed_key(key)
        partial = f""{key[:9]}****""
        return GeneratedAPIKey(key=key, hashed=hashed, partial=partial)","Generate a new API key, its hash, and partial display version.",???Generate a secure API key with hashed and partial representations.???
2373,register_actions,"def register_actions(self) -> None:
        
        self.actions = {
            ""generate-text"": Action(
                name=""generate-text"",
                parameters=[
                    ActionParameter(""prompt"", True, str, ""The input prompt for text generation""),
                    ActionParameter(""system_prompt"", True, str, ""System prompt to guide the model""),
                    ActionParameter(""model"", False, str, ""Model to use for generation"")
                ],
                description=""Generate text using Together AI models""
            ),
            ""check-model"": Action(
                name=""check-model"",
                parameters=[
                    ActionParameter(""model"", True, str, ""Model name to check availability"")
                ],
                description=""Check if a specific model is available""
            ),
            ""list-models"": Action(
                name=""list-models"",
                parameters=[],
                description=""List all available Together AI models""
            )
        }",Register available Together AI actions,???Define and register actions for text generation and model management.???
2374,trace_mcp_cli_imports,"def trace_mcp_cli_imports():
    
    print(""🔍 Tracing MCP CLI imports and timeout sources..."")
    
    try:
        # Try to import the actual MCP CLI components
        components_to_check = [
            'mcp_cli',
            'mcp_cli.tools.manager',
            'mcp_cli.run_command', 
            'chuk_tool_processor',
            'chuk_tool_processor.execution.strategies.inprocess_strategy'
        ]
        
        for component in components_to_check:
            try:
                module = importlib.import_module(component)
                file_path = getattr(module, '__file__', 'Unknown')
                print(f""✅ {component}: {file_path}"")
                
                # Check for timeout-related attributes
                for attr in dir(module):
                    if 'timeout' in attr.lower():
                        value = getattr(module, attr, None)
                        print(f""   - {attr}: {value}"")
                        
            except ImportError as e:
                print(f""❌ {component}: Not found ({e})"")
    
    except Exception as e:
        print(f""Error during import tracing: {e}"")",Trace where MCP CLI components are loaded from.,???Trace and report import status and timeout attributes of specified MCP CLI components.???
2375,get_azure_endpoint,"def get_azure_endpoint(cls) -> str:
        
        logger.debug(""Retrieving Llama Azure endpoint from config"")
        provider_config = CONFIG.llm_endpoints.get(""llama_azure"")
        if provider_config and provider_config.endpoint:
            endpoint = provider_config.endpoint
            if endpoint:
                endpoint = endpoint.strip('""')
                logger.debug(f""Llama Azure endpoint found: {endpoint[:20]}..."")
                return endpoint
        logger.warning(""Llama Azure endpoint not found in config"")
        return None",Get Llama Azure endpoint from config,???Retrieve and return the Llama Azure endpoint from configuration settings.???
2376,track_network_usage,"def track_network_usage(self) -> Dict[str, Optional[float]]:
        
        default_response = {'uploads': None, 'downloads': None}
        try:
            net_io = psutil.net_io_counters()
            return {
                'uploads': net_io.bytes_sent / (1024 * 1024),  # Convert to MB
                'downloads': net_io.bytes_recv / (1024 * 1024)  # Convert to MB
            }
        except Exception as e:
            logger.warning(f""Failed to track network usage: {str(e)}"")
            return default_response",Track network I/O in MB,"???Monitor network data transfer, returning upload/download metrics in megabytes.???"
2377,get_internal_messages,"def get_internal_messages(self, markdown: bool = False):
        
        pretty_print = ""\n\n"".join(
            [
                f""### {msg.type.upper()}\n\nID: {msg.id}\n\nContent:\n\n{msg.content}""
                for msg in self.response[""internal_messages""]
            ]
        )
        if markdown:
            return Markdown(pretty_print)
        else:
            return self.response[""internal_messages""]",Returns internal messages from the agent response.,???Retrieve and format internal messages with optional Markdown conversion.???
2378,load_index,"def load_index(index_path):
    
    print(f""Loading index from {index_path}..."")
    index = faiss.read_index(index_path)
    
    # Load metadata if available
    meta_path = f""{index_path}.meta""
    if os.path.exists(meta_path):
        with open(meta_path, ""r"") as f:
            meta = json.load(f)
            if ""nprobe"" in meta and hasattr(index, ""nprobe""):
                index.nprobe = meta[""nprobe""]
                print(f""Setting nprobe to {meta['nprobe']}"")
    
    return index",Load a FAISS index from disk,???Load and configure a search index from a specified file path.???
2379,_get_strategy_prompt,"def _get_strategy_prompt(self, strategy: str, df: pd.DataFrame, timeframe: str) -> str:
        
        base_prompt = (
            f""Analyze the following market data using the {strategy} strategy on {timeframe} timeframe. ""
            ""Provide analysis in JSON format with the following structure:\n""
            ""{\n""
            '  ""signals"": {""primary"": ""buy/sell/hold"", ""secondary"": ""string""},\n'
            '  ""confidence"": float between 0 and 1,\n'
            '  ""reasoning"": ""detailed explanation"",\n'
            '  ""metrics"": {""risk_reward"": float, ""probability"": float}\n'
            ""}\n\n""
            ""Market Data Summary:\n""
        )
        
        # Add strategy-specific data points
        if strategy == ""trend_following"":
            base_prompt += (
                f""Current Price: {df['close'].iloc[-1]}\n""
                f""SMA20: {df['sma_20'].iloc[-1]}\n""
                f""SMA50: {df['sma_50'].iloc[-1]}\n""
                f""SMA200: {df['sma_200'].iloc[-1]}\n""
            )
        elif strategy == ""momentum"":
            base_prompt += (
                f""RSI: {df['rsi'].iloc[-1]}\n""
                f""MACD: {df['macd'].iloc[-1]}\n""
                f""Recent Price Change: {(df['close'].iloc[-1] / df['close'].iloc[-5] - 1) * 100}%\n""
            )
        
        return base_prompt",Generate strategy-specific analysis prompt.,"???Generate a market analysis prompt based on strategy, timeframe, and data.???"
2380,display_conversation,"def display_conversation(chat_history):
    
    if not chat_history:
        st.info(""No conversation available"")
        return
    
    st.markdown(""<div class='conversation-container'>"", unsafe_allow_html=True)
    
    for idx, message in enumerate(chat_history):
        round_num = idx + 1
        st.markdown(f""<div class='round-header'>Round {round_num}</div>"", unsafe_allow_html=True)
        
        # Display user message
        if ""user"" in message and message[""user""]:
            st.markdown(f""<div class='user-message'><b>🧑‍💼 Job Seeker:</b><br>{message['user']}</div>"", unsafe_allow_html=True)
        
        # Display assistant message
        if ""assistant"" in message and message[""assistant""]:
            assistant_content = message[""assistant""]
            # Remove any note about truncation for cleaner display
            if ""[Note: This conversation was limited"" in assistant_content:
                assistant_content = assistant_content.replace(""[Note: This conversation was limited to maintain response quality. The complete thought process is available in the logs.]"", """")
            
            st.markdown(f""<div class='assistant-message'><b>🦉 Interview Coach:</b><br>{assistant_content}</div>"", unsafe_allow_html=True)
        
        # Display tool calls if any
        if ""tool_calls"" in message and message[""tool_calls""]:
            for tool in message[""tool_calls""]:
                tool_name = tool.get('name', 'Unknown Tool')
                st.markdown(f""<div class='tool-call'><b>🔧 Tool Used: {tool_name}</b></div>"", unsafe_allow_html=True)
    
    st.markdown(""</div>"", unsafe_allow_html=True)",Display the conversation history in a structured format,"???Render chat history with user, assistant, and tool interactions in a structured format.???"
2381,raise_for_response,"def raise_for_response(
      cls, response: Union['ReplayResponse', httpx.Response]
  ) -> None:
    
    if response.status_code == 200:
      return

    if isinstance(response, httpx.Response):
      try:
        response.read()
        response_json = response.json()
      except json.decoder.JSONDecodeError:
        message = response.text
        response_json = {
            'message': message,
            'status': response.reason_phrase,
        }
    else:
      response_json = response.body_segments[0].get('error', {})

    status_code = response.status_code
    if 400 <= status_code < 500:
      raise ClientError(status_code, response_json, response)
    elif 500 <= status_code < 600:
      raise ServerError(status_code, response_json, response)
    else:
      raise cls(status_code, response_json, response)",Raises an error with detailed error message if the response has an error status.,???Handle HTTP response errors by raising appropriate exceptions based on status codes.???
2382,_round,"def _round(labels):
            
            if self.task == ""detect"":
                coordinates = labels[""bboxes""]
            elif self.task in {""segment"", ""obb""}:  # Segment and OBB use segments. OBB segments are normalized xyxyxyxy
                coordinates = [x.flatten() for x in labels[""segments""]]
            elif self.task == ""pose"":
                n, nk, nd = labels[""keypoints""].shape
                coordinates = np.concatenate((labels[""bboxes""], labels[""keypoints""].reshape(n, nk * nd)), 1)
            else:
                raise ValueError(f""Undefined dataset task={self.task}."")
            zipped = zip(labels[""cls""], coordinates)
            return [[int(c[0]), *(round(float(x), 4) for x in points)] for c, points in zipped]",Update labels to integer class and 4 decimal place floats.,???Process and round coordinates based on task type for various label formats.???
2383,summarize_failures,"def summarize_failures(self) -> dict:
        
        summary = {
            ""total_failures"": len(self.failed_samples),
            ""failure_types"": {k: len(v) for k, v in self.failure_analysis.items()},
            ""failure_examples"": {},
        }

        # Add example failures for each category
        for category, failures in self.failure_analysis.items():
            if failures:
                # Get up to 3 examples for each category
                examples = failures[:3]
                summary[""failure_examples""][category] = [
                    (
                        str(ex)[:200] + ""...""
                        if len(str(ex)) > 200  # noqa: PLR2004
                        else str(ex)  # noqa: PLR2004
                    )  # noqa: PLR2004
                    for ex in examples
                ]
        return summary",Generate a summary of all failures.,???Summarize and categorize failure data with examples in a dictionary.???
2384,temp_template_file,"def temp_template_file(self):
        
        with tempfile.NamedTemporaryFile(mode=""w+"", suffix="".txt"", delete=False) as tf:
            tf.write()
            tf_path = Path(tf.name)

        yield tf_path

        # Cleanup
        os.unlink(tf_path)",Create a temporary template file for testing,???Create and manage a temporary text file for use and cleanup.???
2385,fgpartition,"def fgpartition(grid: Grid) -> Objects:
    
    return frozenset(
        frozenset((v, (i, j)) for i, r in enumerate(grid) for j, v in enumerate(r) if v == value)
        for value in palette(grid) - {mostcolor(grid)}
    )",each cell with the same value part of the same object without background,"???Partition grid into sets of coordinates grouped by unique values, excluding the most common.???"
2386,parse_resolution,"def parse_resolution(resolution_str):
    
    try:
        # Extract the resolution part before the parenthesis
        res_part = resolution_str.split("" ("")[0].strip()
        # Replace 'x' with '×' for consistency if needed
        parts = res_part.replace('x', '×').split(""×"")
        
        if len(parts) != 2:
            raise ValueError(f""Expected format 'width × height', got '{res_part}'"")
            
        width_str = parts[0].strip()
        height_str = parts[1].strip()
        
        width = int(width_str)
        height = int(height_str)
        print(f""Successfully parsed resolution: {width}x{height}"")
        return height, width
    except Exception as e:
        print(f""Error parsing resolution '{resolution_str}': {e}. Falling back to 1024x1024."")
        return 1024, 1024",Parse resolution string into height and width dimensions.,"???Parse and validate resolution strings, defaulting to 1024x1024 on error.???"
2387,find_chrome,"def find_chrome():
    
    possiblePaths = [
        r""C:\Program Files\Google\Chrome\Application\chrome.exe"",
        r""C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"",
        r""C:\ProgramData\chocolatey\bin\chrome.exe"",
        r""C:\Users\%USERNAME%\AppData\Local\Google\Chrome\Application\chrome.exe"",
        ""/usr/bin/google-chrome"",
        ""/usr/local/bin/google-chrome"",
        ""/opt/google/chrome/chrome"",
        ""/snap/bin/chromium"",
        ""/Applications/Google Chrome.app/Contents/MacOS/Google Chrome""
    ]

    # Check predefined paths
    for path in possiblePaths:
        if os.path.exists(path):
            return path

    # Use system command to find Chrome
    try:
        if platform.system() == ""Windows"":
            chrome_path = shutil.which(""chrome"")
        else:
            chrome_path = shutil.which(""google-chrome"") or shutil.which(""chromium"")
        if chrome_path:
            return chrome_path
    except Exception as e:
        print(f""[ChromeDriver] Error while searching system paths: {e}"")

    return None",Find Chrome executable using known paths and system commands.,???Locate the Chrome browser executable across various system paths and platforms.???
2388,_get_reliable_tool_context,"def _get_reliable_tool_context(context_variables: ContextVariables, context_key: str) -> ReliableToolContext:
    
    context_data = context_variables.get(context_key)
    if context_data is None:
        raise KeyError(f""ReliableToolContext key '{context_key}' not found in ContextVariables."")
    try:
        if isinstance(context_data, str):
            return ReliableToolContext.model_validate_json(context_data)
        raise TypeError(
            f""Unexpected type {type(context_data)} for context key '{context_key}'. Expected ReliableToolContext, str, or dict.""
        )
    except (ValidationError, json.JSONDecodeError, TypeError) as e:
        preview = f"" Preview: '{str(context_data)[:100]}...'"" if isinstance(context_data, (str, dict)) else """"
        # Logged error level changed to warning as this function re-raises.
        logger.warning(
            ""Failed loading ReliableToolContext '%s'. Error: %s. Type: %s.%s"",
            context_key,
            e,
            type(context_data).__name__,
            preview,
        )
        raise ValueError(f""Failed loading ReliableToolContext key '{context_key}': {e}"") from e",Retrieve and validate the ReliableToolContext from ContextVariables.,"???Retrieve and validate context data, raising errors for invalid or missing entries.???"
2389,research_sub_questions,"def research_sub_questions(state: ResearchState) -> ResearchState:
    
    answers = []
    for question in state[""sub_questions""]:
        search_results = tavily_tool.invoke(question)
        prompt = PromptTemplate(
            input_variables=[""question"", ""search_results""],
            template=""Answer '{question}' concisely based on: {search_results}""
        )
        answer = llm.invoke(prompt.format(
            question=question,
            search_results=[r[""content""] for r in search_results]
        ))
        answers.append({
            ""question"": question,
            ""answer"": answer.content,
            ""sources"": [r[""url""] for r in search_results]
        })
    return {""answers"": answers, ""status"": ""researched""}",Research each sub-question using Tavily.,???Generate concise answers to research questions using search results and a language model.???
2390,dont_throw,"def dont_throw(func):
    

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.warning(f""Exception caught in {func.__name__}: {e}"")
            logger.debug(f""Exception details:"", exc_info=True)

    return wrapper",Decorator to catch exceptions and log them without raising.,???Decorator to handle exceptions and log warnings for a function.???
2391,get_task_description,"def get_task_description(self):
        
        if self.task_description is None:
            llm = get_llm(self.config['task_description']['llm'])
            task_extractor = set_llm_chain(llm, **self.config['task_description']['extraction_prompt'])
            self.task_description = task_extractor.invoke({'prompt': self.prompt}).content
        return self.task_description",Generate task description if not exists,??? Retrieve or generate task description using language model chain ???
2392,_append_library_version_headers,"def _append_library_version_headers(headers: dict[str, str]) -> None:
  
  library_label = f'google-genai-sdk/{version.__version__}'
  language_label = 'gl-python/' + sys.version.split()[0]
  version_header_value = f'{library_label} {language_label}'
  if (
      'user-agent' in headers
      and version_header_value not in headers['user-agent']
  ):
    headers['user-agent'] = f'{version_header_value} ' + headers['user-agent']
  elif 'user-agent' not in headers:
    headers['user-agent'] = version_header_value
  if (
      'x-goog-api-client' in headers
      and version_header_value not in headers['x-goog-api-client']
  ):
    headers['x-goog-api-client'] = f'{version_header_value} ' + headers['x-goog-api-client']
  elif 'x-goog-api-client' not in headers:
    headers['x-goog-api-client'] = version_header_value",Appends the telemetry header to the headers dict.,???Add SDK and Python version info to HTTP headers if missing???
2393,handle_ordered_list,"def handle_ordered_list(self, element: lxml_html.HtmlElement) -> str:
        
        start = element.get(""start"", ""1"")
        try:
            start = int(start)
        except ValueError:
            start = 1

        self.list_state.append({""type"": ""ol"", ""index"": start - 1})
        content = self.process_children(element, True)
        self.list_state.pop()
        return f""\n{content}\n""",Handle ordered list elements,"???Processes an HTML ordered list element, managing list state and content.???"
2394,compose,"def compose(self) -> ComposeResult:
        
        yield Header(show_clock=True)
        with TabbedContent():
            if self.title_slide:
                with TabPane(""Title Slide"", id=""title-slide-tab""):
                    yield self.title_slide
            for i, chapter in enumerate(self.chapters):
                with TabPane(chapter.title, id=f""chapter_{i}""):
                    yield chapter
        yield Footer()",Create child widgets for the app.,"???Generate a UI layout with a header, tabbed content, and footer.???"
2395,route_map,"def route_map(parsed_routes):
    
    return {r.operation_id: r for r in parsed_routes if r.operation_id is not None}",Return a dictionary of routes by operation ID.,???Create a dictionary mapping operation IDs to routes from a list of parsed routes.???
2396,_load_from_url,"def _load_from_url(self, file_url: str, expected_hash: str) -> bytes:
        
        resp = requests.get(file_url)
        resp.raise_for_status()
        content = resp.content

        if not self._is_valid_model(model_data=content, expected_hash=expected_hash):
            actual_hash = hashlib.sha256(content).hexdigest()
            raise ValueError(
                f""Downloaded model file is corrupted.""
                f"" Expected hash {expected_hash}. Got file hash {actual_hash}.""
            )
        return content",Loads model bytes from the given file url.,???Download and validate file from URL using hash verification???
2397,flush_telemetry,"def flush_telemetry():
    
    try:
        # Get the tracer provider
        trace_provider = trace.get_tracer_provider()

        # Check if we have processors with the exporter
        if hasattr(trace_provider, ""_active_span_processor""):
            processor = trace_provider._active_span_processor

        # Set a longer timeout (30 seconds) to ensure all data is flushed
        success = trace_provider.force_flush(timeout_millis=30000)

        if success:
            logger.info(""🟢 Telemetry data flushed successfully to Langfuse"")
        else:
            logger.warning(
                ""🔶 Telemetry flush timed out or failed - data may not have been sent completely""
            )

        # Also try to force the process to be more aggressive
        from opentelemetry.sdk.trace.export import (
            SimpleSpanProcessor,
            BatchSpanProcessor,
        )

        if hasattr(trace_provider, ""_active_span_processor""):
            processor = trace_provider._active_span_processor
            if hasattr(processor, ""force_flush""):
                try:
                    processor.force_flush(300)
                except Exception as e:
                    logger.warning(f""Error during explicit processor flush: {e}"")
    except Exception as e:
        logger.error(f""🔴 Error flushing telemetry: {str(e)}"", exc_info=True)",Force flush all pending telemetry data to Langfuse,???Ensure telemetry data is flushed to Langfuse with extended timeout and error handling.???
2398,change_extension,"def change_extension(file_path, new_extension, version=None):
    
    # Remove leading '.' from the new extension if present
    new_extension = new_extension.lstrip('.')

    # Create the new file path with the version before the extension, if provided
    if version:
        new_file_path = Path(file_path).with_suffix(f'.{version}.{new_extension}')
    else:
        new_file_path = Path(file_path).with_suffix(f'.{new_extension}')

    return str(new_file_path)",Change the extension of the file path and optionally prepend a version.,???Modify file path to update its extension and optionally add a version.???
2399,_setup_data_channel,"def _setup_data_channel(self, remote_peer_id: str, channel: RTCDataChannel):
        

        @channel.on(""open"")
        def on_open():
            self.live_connections += 1

        # manually call on_open if the channel is already open by the time event is attached
        if channel.readyState == ""open"":
            on_open()

        @channel.on(""message"")
        def on_message(message):
            self._receive(message)

        @channel.on(""close"")
        def on_close():
            self.live_connections -= 1

        @channel.on(""error"")
        def on_error(e):
            logging.error(f""[data] Data channel error with {remote_peer_id}: {e}"")","Set up handlers for an RTCDataChannel (open, message, close, error).",???Initialize and manage data channel events for peer communication and error handling.???
2400,add_comfyui_directory_to_sys_path,"def add_comfyui_directory_to_sys_path() -> None:
    
    comfyui_path = find_path(""ComfyUI"")
    if comfyui_path is not None and os.path.isdir(comfyui_path):
        sys.path.append(comfyui_path)
        print(f""'{comfyui_path}' added to sys.path"")",Add 'ComfyUI' to the sys.path,"???  
Add ""ComfyUI"" directory to system path if it exists.  
???"
2401,remove_agent_from_automation,"def remove_agent_from_automation(self, automation_name: str, agent_index: int) -> list:
        
        if automation_name not in self.automations:
            raise ValueError(f""Automation '{automation_name}' does not exist"")
        
        if not isinstance(agent_index, int):
            raise ValueError(""Agent index must be an integer"")
            
        if agent_index < 0 or agent_index >= len(self.automations[automation_name][""agents""]):
            raise ValueError(f""Invalid agent index {agent_index}"")
        
        self.automations[automation_name][""agents""].pop(agent_index)
        return self.automations[automation_name][""agents""]",Remove an agent from an automation flow,"???Remove specified agent from automation by index, ensuring valid inputs.???"
2402,transformers_tokenizer,"def transformers_tokenizer() -> PreTrainedTokenizerFast:
    
    try:
        return cast(PreTrainedTokenizerFast, AutoTokenizer.from_pretrained(""gpt2""))
    except (OSError, ValueError) as e:
        pytest.skip(f""Could not load HuggingFace tokenizer: {e}"")",Fixture that returns a GPT-2 tokenizer from the transformers library.,"???Load GPT-2 tokenizer, skip test if loading fails.???"
2403,validate_indexing_args,"def validate_indexing_args(args):
    
    if args.include and args.exclude:
        raise ValueError(""At most one of indexing.include and indexing.exclude can be specified."")
    if not args.include and not args.exclude:
        args.exclude = str(resources.files(""sage"").joinpath(""sample-exclude.txt""))
    if args.include and not os.path.exists(args.include):
        raise ValueError(f""Path --include={args.include} does not exist."")
    if args.exclude and not os.path.exists(args.exclude):
        raise ValueError(f""Path --exclude={args.exclude} does not exist."")
    if not args.index_repo and not args.index_issues:
        raise ValueError(""Either --index_repo or --index_issues must be set to true."")
    if args.index_issues and not os.getenv(""GITHUB_TOKEN""):
        raise ValueError(""Please set the GITHUB_TOKEN environment variable."")",Validates the indexing configuration and sets defaults.,"???Ensure valid indexing arguments, checking paths and environment variables.???"
2404,upload_to_imgbb,"def upload_to_imgbb(image_url: str) -> Optional[str]:
    
    try:
        api_key = os.getenv(""IMGBB_API_KEY"")
        if not api_key:
            raise ValueError(""IMGBB_API_KEY not found"")

        image_data = urlopen(image_url).read()
        response = requests.post(
            data={""key"": api_key, ""image"": base64.b64encode(image_data).decode(""utf-8"")},
        )

        return response.json()[""data""][""url""] if response.status_code == 200 else None

    except Exception as e:
        logger.error(f""IMGBB upload error: {str(e)}"")
        return None",Upload an image to IMGBB with caching for repeated uploads,???Uploads image to IMGBB and returns the hosted URL or logs an error.???
2405,set_interrupt_trigger,"def set_interrupt_trigger(self, message_index: int, during_execution: bool = False):
        
        self.interrupt_triggers[message_index] = {
            'during_execution': during_execution
        }",Set when to trigger a KeyboardInterrupt.,???Configure interrupt triggers based on message index and execution state???
2406,_prepare_filters,"def _prepare_filters(self, filters: dict | None = None) -> dict | None:
        
        if not filters:
            return None

        if all(isinstance(v, (str, int, float, bool)) for v in filters.values()):
            conditions = []
            for key, value in filters.items():
                conditions.append({""field"": key, ""operator"": ""=="", ""value"": value})
            return {""operator"": ""AND"", ""conditions"": conditions}
        return filters",Convert simple filters to Pinecone filter format.,"???Transform input filters into query conditions if valid, else return unchanged.???"
2407,update_env_file,"def update_env_file(agent_engine_id, env_file_path):
    
    try:
        set_key(env_file_path, ""AGENT_ENGINE_ID"", agent_engine_id)
        print(f""Updated AGENT_ENGINE_ID in {env_file_path} to {agent_engine_id}"")
    except Exception as e:
        print(f""Error updating .env file: {e}"")",Updates the .env file with the agent engine ID.,"???Update environment file with new agent engine ID, handling errors???"
2408,_format_list_section,"def _format_list_section(self, title: str, items: List[str]) -> str:
        
        if not items:
            return """"

        formatted_items = ""\n"".join([f""- `{item}`"" for item in items])
        return f""\n## {title}\n\n{formatted_items}\n""",Format a list section with title.,???Formats a titled markdown list section from given items.???
2409,load_puzzle_matrix,"def load_puzzle_matrix(self, matrix):
        
        try:
            # Convert numpy arrays to list of lists
            if isinstance(matrix, np.ndarray):
                data = matrix.tolist()
            else:
                data = matrix

            # Validate and process
            self._process_puzzle_data(data)
        except ValueError as e:
            print(f""{e}"")
            return",New method: Load puzzle directly from a matrix (list/numpy array),"???Load and validate puzzle matrix data, handling errors gracefully.???"
2410,batch_mode,"def batch_mode(minion, context_chunks):
    
    test_queries = [
        ""What is the main topic of this document?"",
        ""Can you provide more details about it?"",
        ""What were the key points mentioned earlier?"",
        ""Based on what you told me before, what should I focus on?""
    ]
    
    for i, query in enumerate(test_queries):
        print(f""\n=== Query {i+1}: {query} ==="")
        result = minion(
            task=query,
            context=context_chunks,
            max_rounds=2
        )
        
        print(f""\n[Answer]:\n{result['final_answer']}"")
        print(f""\nTokens: {result['remote_usage'].total_tokens} remote, {result['local_usage'].total_tokens} local"")
        
        show_history(minion)",Run in batch mode with predefined queries.,???Iteratively query a document analysis tool for insights using predefined questions.???
2411,_format_observation,"def _format_observation(self, cube_state: CubeState) -> str:
        
        cube_visualization = cube_state.get_cube_state_visualization()

        moves_made = "", "".join(cube_state.actions) if cube_state.actions else ""None""
        steps_remaining = cube_state.max_steps - cube_state.num_steps

        # Add scramble info for debugging and learning
        scramble_length = len(cube_state.scramble_sequence)

        message = (
            f""Current state of the Rubik's cube:\n\n""
            f""```\n{cube_visualization}\n```\n\n""
            f""Previous moves: {moves_made}\n""
            f""Steps remaining: {steps_remaining}\n""
        )

        # Add curriculum level info if using curriculum
        if self.use_curriculum and cube_state.curriculum_level > 0:
            current_level = self.curriculum.levels[cube_state.curriculum_level]
            message += f""\nDifficulty level: {current_level.description}\n""
            message += f""Scramble depth: {scramble_length} moves\n""

        # Show the current solved percentage to help the LLM
        solved_percentage = cube_state.cube.count_solved_cubies() * 100
        message += (
            f""\nCurrent progress: {solved_percentage:.1f}% of cubies correctly placed\n""
        )

        if cube_state.is_solved():
            message += ""\nCongratulations! The cube is now solved.""

        return message",Format the cube state as a string observation for the LLM,???Generate a detailed status report of a Rubik's cube solving process.???
2412,recv_obj,"def recv_obj(self, src: int) -> Any:
        
        obj = pickle.loads(
            self.store.get(f""send_to/{self.rank}/{self.recv_src_counter[src]}""))
        self.recv_src_counter[src] += 1
        return obj",Receive an object from a source rank.,???Deserialize and retrieve an object from a storage system using a source identifier.???
2413,_get_user_timezone,"def _get_user_timezone():
    
    try:
        from src.primary.utils.timezone_utils import get_user_timezone
        return get_user_timezone()
    except Exception as e:
        logger.warning(f""Could not get user timezone, defaulting to UTC: {e}"")
        import pytz
        return pytz.UTC",Get the user's selected timezone from general settings,"???  
Retrieve user timezone, default to UTC on failure.  
???"
2414,collect_megatron_compute_data_proto,"def collect_megatron_compute_data_proto(worker_group, output):
    
    from verl.protocol import DataProto
    import ray

    output = collect_megatron_compute(worker_group, output)
    for o in output:
        assert isinstance(o, (DataProto, ray.ObjectRef)), f""expecting {o} to be DataProto, but got {type(o)}""

    return _concat_data_proto_or_future(output)",Each output must be a DataProto.,"???Collect and validate compute data, returning concatenated results.???"
2415,inspect_inprocess_strategy,"def inspect_inprocess_strategy():
    
    print(""\n🔧 Inspecting InProcessStrategy..."")
    
    try:
        from chuk_tool_processor.execution.strategies.inprocess_strategy import InProcessStrategy
        
        # Get the signature of __init__
        sig = inspect.signature(InProcessStrategy.__init__)
        print(f""   InProcessStrategy.__init__ signature: {sig}"")
        
        # Check default values
        for param_name, param in sig.parameters.items():
            if param.default != inspect.Parameter.empty:
                print(f""   - {param_name} default: {param.default}"")
                
        # Create an instance to see actual values
        try:
            # We need a mock registry, so let's just examine the class
            print(f""   InProcessStrategy source: {inspect.getfile(InProcessStrategy)}"")
            
        except Exception as e:
            print(f""   Could not create instance: {e}"")
            
    except ImportError as e:
        print(f""   Could not import InProcessStrategy: {e}"")",Inspect the InProcessStrategy class for timeout configuration.,???Inspect and report on the initialization and source of the InProcessStrategy class.???
2416,write_deps,"def write_deps(deps: Iterable[str], added_deps: set[str], label: str = """", indent: int = 2) -> str:
    
    deps_str = """"
    space = "" "" * indent
    if label:
        deps_str += f""  # {label}\n""
    for dep in deps:
        if dep in added_deps:
            continue
        deps_str += f""{space}- {dep}\n""
        added_deps.add(dep)
    return deps_str",Write dependencies with optional label.,???Formats and appends unique dependencies to a string with optional labeling and indentation.???
2417,_center_window,"def _center_window(self):
        
        self.update_idletasks()  # Ensure accurate dimensions
        window_width = self.winfo_width()
        window_height = self.winfo_height()
        screen_width = self.winfo_screenwidth()
        screen_height = self.winfo_screenheight()

        # Calculate position

        # Enforce maximum height if needed
        if window_height > self.max_window_height:
            window_height = self.max_window_height

        # Always set geometry to the current (or adjusted) size and center it
        self.geometry(f""{window_width}x{window_height}+{x}+{y}"")",Centers the window on the screen based on its current size.,"???Center a window on the screen, adjusting dimensions if necessary.???"
2418,_populate_max_entrypoint,"def _populate_max_entrypoint(self):
        
        entrypoint_path = frameworks.get_entrypoint_path(self.framework)
        shutil.copy(BASE_PATH / f""fixtures/frameworks/{self.framework}/entrypoint_max.py"", entrypoint_path)
        shutil.copy(BASE_PATH / 'fixtures/agents_max.yaml', self.project_dir / AGENTS_FILENAME)
        shutil.copy(BASE_PATH / 'fixtures/tasks_max.yaml', self.project_dir / TASKS_FILENAME)",This entrypoint has tools and agents.,???Copy framework-specific configuration files to project directory.???
2419,mock_role_members,"def mock_role_members():
    
    return {
        ""actors"": [
            {""id"": ""user1"", ""name"": ""user1"", ""displayName"": ""User One""},
            {""id"": ""user2"", ""name"": ""user2"", ""displayName"": ""User Two""},
        ]
    }",Fixture to return mock project role members.,"??? 
Generate a mock list of actor roles with user details. 
???"
2420,process_mesh,"def process_mesh(vertices, faces, quantization_bits=8, augment=True, augment_dict=None):
    

    # Transpose so that z-axis is vertical.
    vertices = vertices[:, [2, 0, 1]]

    # Translate the vertices so that bounding box is centered at zero.
    vertices = center_vertices(vertices)

    if augment:
        vertices = augment_mesh(vertices, **augment_dict)

    # Scale the vertices so that the long diagonal of the bounding box is equal
    # to one.
    vertices = normalize_vertices_scale(vertices)

    # Quantize and sort vertices, remove resulting duplicates, sort and reindex
    # faces.
    vertices, faces = quantize_process_mesh(
        vertices, faces, quantization_bits=quantization_bits
    )
    vertices = undiscretize(vertices, num_discrete=2**quantization_bits)


    # Discard degenerate meshes without faces.
    return {
        ""vertices"": vertices,
        ""faces"": faces,
    }",Process mesh vertices and faces.,"???Normalize, augment, and quantize 3D mesh vertices and faces for processing.???"
2421,create_or_replace_table,"def create_or_replace_table(db_path, table_name, data):
    
    conn = sqlite3.connect(os.path.expanduser(db_path))
    try:
        data.to_sql(table_name, conn, if_exists=""replace"", index=False)
        print(f""Table '{table_name}' created/replaced successfully."")
        return True
    except Exception as e:
        print(f""Error creating/replacing table '{table_name}': {e}"")
        return False
    finally:
        conn.close()",Creates or replaces a table in the SQLite database,??? Replace or create a database table with new data using SQLite. ???
2422,get_module_config,"def get_module_config(self, module_name: str) -> Dict[str, Any]:
        
        modules = self.config[""logging""].get(""modules"", {})
        return modules.get(module_name, {})",Get configuration for a specific module,???Retrieve configuration settings for a specified logging module from a configuration dictionary.???
2423,get_framework_module,"def get_framework_module(framework: str) -> FrameworkModule:
    
    try:
        return import_module(f"".{framework}"", package=__package__)
    except ImportError:
        raise Exception(f""Framework {framework} could not be imported."")",Get the module for a framework.,"???  
Imports a specified framework module, raising an exception if import fails.  
???"
2424,mock_firecrawl_response,"def mock_firecrawl_response(mocker):
    
    return {
        ""success"": True,
        ""data"": {
            ""markdown"": ""# Test Page\n\nThis is a test page with some content."",
            ""html"": ""<html><body><h1>Test Page</h1><p>This is a test page with some content.</p></body></html>"",
            ""links"": [
            ],
            ""metadata"": {""title"": ""Test Page"", ""description"": ""Test page description""},
        },
    }",Mock response from Firecrawl API.,???Simulate a successful web crawl response with test content and metadata.???
2425,extract_native_id,"def extract_native_id(url, platform):
    
    if platform == ""google_meet"":
        try:
            # Handle potential trailing slash and query parameters
            path_part = url.split('?')[0]
            if path_part.endswith('/'):
                path_part = path_part[:-1]
            return path_part.split('/')[-1]
        except Exception:
            return None
    # Add logic for other platforms (Zoom, Teams) if necessary
    print(f""Warning: Native ID extraction not implemented for platform '{platform}'. Returning None."", file=sys.stderr)
    return None",Basic extraction of native ID from URL (adjust if needed).,"???Extracts a unique identifier from a Google Meet URL, with error handling.???"
2426,create_content_metadata,"def create_content_metadata(document_type: str) -> dict:
    
    # Use the mapping; if document_type is not found, fallback to ""unknown"".
    content_type = DOCUMENT_TO_CONTENT_MAPPING.get(document_type, ContentTypeEnum.UNKNOWN)
    return {
        ""type"": content_type,
        ""description"": """",
        ""page_number"": -1,
        ""hierarchy"": {
            ""page_count"": -1,
            ""page"": -1,
            ""block"": -1,
            ""line"": -1,
            ""span"": -1,
            ""nearby_objects"": {
                ""text"": {""content"": [], ""bbox"": [], ""type"": []},
                ""images"": {""content"": [], ""bbox"": [], ""type"": []},
                ""structured"": {""content"": [], ""bbox"": [], ""type"": []},
            },
        },
        ""subtype"": """",
    }",Creates a content metadata dictionary for a file based on its document type.,???Generate metadata dictionary for a given document type with default values.???
2427,get_status_summary,"def get_status_summary(self) -> Dict[str, Any]:
        
        return {
            ""provider"": self.provider,
            ""model"": self.model,
            ""tool_count"": len(self.tools),
            ""server_count"": len(self.server_info),
            ""conversation_length"": self.get_conversation_length(),
            ""tools_adapted"": bool(self.openai_tools),
            ""exit_requested"": self.exit_requested,
        }",Get status summary for debugging.,"???Generate a summary of system status including provider, model, and tool metrics.???"
2428,convert_jsonl_for_pyserini,"def convert_jsonl_for_pyserini(input_file, output_file):
    
    docs = []

    with open(input_file, ""r"", encoding=""utf-8"") as f:
        for line in f:
            data = json.loads(line.strip())
            
            # Create JSON document with a clear structure
            doc = {
                ""id"": data[""_id""],  # Unique identifier for search results
                ""contents"": data['text'],  # Required field for Pyserini
            }
            
            docs.append(json.dumps(doc))

    with open(output_file, ""w"", encoding=""utf-8"") as f:
        for doc in docs:
            f.write(doc + ""\n"")

    print(f""✅ Converted JSONL saved to {output_file}"")",Convert JSONL data to Pyserini-compatible format with a structured 'contents' field,???Transform JSONL data into Pyserini-compatible JSON format for search indexing.???
2429,add_list,"def add_list(self, tools: list[Tool]):
        
        logger.debug(f""Adding {len(tools)} tools to tool dictionary"")
        for tool in tools:
            self.add(tool)",Add a list of tools to the tool dictionary.,???Add multiple tools to the tool dictionary with logging for debugging???
2430,build_list_keyspaces_context,"def build_list_keyspaces_context(keyspaces: List[KeyspaceInfo]) -> str:
    
    context = {
        'cassandra_knowledge': build_cassandra_knowledge(),
        'amazon_keyspaces_knowledge': build_amazon_keyspaces_knowledge(),
    }

    # Add keyspace-specific guidance
    list_keyspaces_guidance = {
        'compatibility': 'Amazon Keyspaces is compatible with Apache Cassandra 3.11. This means that it supports most '
        'of the same CQL language features and is driver-protocol compatible with Cassandra 3.11.',
        'limitations': ""Amazon Keyspaces doesn't support all Apache Cassandra 3.11 features. Unsupported features ""
        'include logged batches, materialized views, indexes, aggregate functions like COUNT and SUM, prepared '
        'statements for DDL operations, DROP COLUMN, TRUNCATE TABLE, user-defined functions, the inequality operator '
        'for user-defined types, or the IN keyword in INSERT and UPDATE statements. Keyspaces uses AWS IAM for '
        ""authentication and authorization, and not Cassandra's security configuration and commands. Additionally, ""
        'some operations that are synchronous in Cassandra are asynchronous in Keyspaces, such as DDL operations '
        'and range delete operations.',
        'replication_strategy': 'In Cassandra, common replication strategies include SimpleStrategy and NetworkTopologyStrategy. '
        'Amazon Keyspaces uses a single-region replication strategy with 3x replication for durability.',
        'naming_conventions': 'Keyspace names typically use snake_case and represent logical data domains.',
    }
    context['list_keyspaces_guidance'] = list_keyspaces_guidance

    return dict_to_markdown(context)",Provide LLM context for Amazon Keyspaces and Apache Cassandra.,???Generate a markdown summary of keyspace compatibility and limitations.???
2431,_create_streaming_response,"def _create_streaming_response(
        self,
        stream: AsyncIterator[Any],
        client_type: ClientType = ClientType.GENERIC,
        stream_generator: Callable | None = None,
    ) -> StreamingResponse:
        
        return StreamingResponse(
            stream_generator(stream) if stream_generator else openai_stream_generator(stream),
            headers={
                ""Cache-Control"": ""no-cache"",
                ""Connection"": ""keep-alive"",
                ""Transfer-Encoding"": ""chunked"",
            },
            status_code=200,
        )",Create a streaming response from a stream generator.,???Create a streaming HTTP response with optional custom generator and headers.???
2432,log_generations_to_swanlab,"def log_generations_to_swanlab(self, samples, step):
        
        import swanlab

        swanlab_text_list = []
        for i, sample in enumerate(samples):
            row_text = f
            swanlab_text_list.append(swanlab.Text(row_text, caption=f""sample {i + 1}""))

        # Log to swanlab
        swanlab.log({""val/generations"": swanlab_text_list}, step=step)",Log samples to swanlab as text,???Log sample generations to Swanlab with captions and step tracking.???
2433,_create_collection,"def _create_collection(self):
        
        # Create schema that matches our data structure
        schema = self.client.create_schema(
            auto_id=True,  # Auto-generate IDs
            enable_dynamic_fields=True,  # Allow dynamic fields
        )

        # Add fields that match our data structure
        schema.add_field(field_name=""id"", datatype=DataType.INT64, is_primary=True, auto_id=True)
        schema.add_field(field_name=""vector"", datatype=DataType.FLOAT_VECTOR, dim=settings.VECTOR_DIMENSIONS)
        schema.add_field(field_name=""document_id"", datatype=DataType.VARCHAR, max_length=255)
        schema.add_field(field_name=""chunk_number"", datatype=DataType.INT32)
        schema.add_field(field_name=""content"", datatype=DataType.VARCHAR, max_length=65535)
        schema.add_field(field_name=""metadata"", datatype=DataType.VARCHAR, max_length=65535)

        # Create the collection
        self.client.create_collection(collection_name=""vector_store"", schema=schema)

        # Create index for vector field
        index_params = self.client.prepare_index_params()
        index_params.add_index(field_name=""vector"", index_type=""IVF_FLAT"", metric_type=""COSINE"", params={""nlist"": 1024})

        self.client.create_index(collection_name=""vector_store"", index_params=index_params)

        logger.info(""Created vector_store collection with proper schema"")",Create the vector_store collection with the proper schema.,???Define and initialize a vector-based data collection with schema and indexing.???
2434,validate_port,"def validate_port(ctx: click.Context, param: click.Parameter, value: int) -> int:
    
    cli_logger = structlog.get_logger(""codegate"").bind(origin=""cli"")
    cli_logger.debug(f""Validating port number: {value}"")
    if value is not None and not (1 <= value <= 65535):
        raise click.BadParameter(""Port must be between 1 and 65535"")
    return value",Validate the port number is in valid range.,"???  
Validates a port number within a specified range using logging and error handling.  
???"
2435,optimizer_parameters,"def optimizer_parameters(optimizer, args):
    
    if optimizer in optimizer_choices:
        optimizer_details = optimizer_choices.get(optimizer)
        optimizer_class = optimizer_choices.get(optimizer).get(""class"")
        optimizer_params = optimizer_choices.get(optimizer).get(""default_settings"")
        optimizer_params.update(convert_arg_to_parameters(args))
        if args.optimizer_release_gradients and ""optimi-"" in optimizer:
            optimizer_params[""gradient_release""] = True
        optimizer_details[""default_settings""] = optimizer_params
        return optimizer_class, optimizer_details
    else:
        raise ValueError(f""Optimizer {optimizer} not found."")",Return the parameters for the optimizer,??? Configure optimizer settings based on user arguments and predefined choices ???
2436,get_token_by_ticker,"def get_token_by_ticker(agent, **kwargs):
    
    try:
        ticker = kwargs.get(""ticker"")
        if not ticker:
            logger.error(""No ticker provided"")
            return None
            
        # Direct passthrough to connection method - add your logic before/after this call!
        agent.connection_manager.connections[""sonic""].get_token_by_ticker(ticker)

        return

    except Exception as e:
        logger.error(f""Failed to get token by ticker: {str(e)}"")
        return None",Get token address by ticker symbol,???Retrieve a token using a ticker symbol via an agent's connection manager???
2437,discover_unprocessed_files,"def discover_unprocessed_files(self, directory: str = None):
        
        all_image_files = set(StateTracker.get_image_files(data_backend_id=self.id))
        existing_cache_files = set(
            StateTracker.get_vae_cache_files(data_backend_id=self.id)
        )
        # Convert cache filenames to their corresponding image filenames
        already_cached_images = []
        for cache_file in existing_cache_files:
            try:
                n = self._image_filename_from_vaecache_filename(cache_file)
                if n is None:
                    continue
                already_cached_images.append(n)
            except Exception as e:
                logger.error(
                    f""Could not find image path for cache file {cache_file}: {e}""
                )
                continue

        # Identify unprocessed files
        self.local_unprocessed_files = list(
            set(all_image_files) - set(already_cached_images)
        )

        return self.local_unprocessed_files",Identify files that haven't been processed yet.,???Identify unprocessed image files by comparing with cached entries.???
2438,get_next_strategy_id,"def get_next_strategy_id(self) -> str:
        
        self.metrics[""last_strategy_id""] += 1
        new_id = f""strategy_{self.metrics['last_strategy_id']}""
        logger.info(f""Generated new strategy ID: {new_id}"")
        return new_id",Generate a unique ID for a new strategy.,"???  
Generate and log a new unique strategy identifier.  
???"
2439,generate_decl,"def generate_decl(self) -> str:
        
        code = inline_doc(self.description)
        if code:
            code += ""\n""
        code += f""{self.py_name}: {self.py_annotation}""
        if self.optional:
            code += "" = None""
        return code",Generate the code that declares this property.,"???  
Create a Python variable declaration string with optional documentation.  
???"
2440,load_image,"def load_image(relative_path, size=None):
    
    full_path = get_resource_path(relative_path)
    if os.path.exists(full_path):
        pixmap = QPixmap(full_path)
        if size:
            pixmap = pixmap.scaled(size[0], size[1], Qt.AspectRatioMode.KeepAspectRatio)
        return pixmap
    else:
        print(f""Image not found at: {full_path}"")
        return QPixmap()",Load an image from a relative path and optionally resize it,???Load and optionally resize an image from a given path.???
2441,handle_reset_config,"def handle_reset_config(args: argparse.Namespace) -> int:
    
    config_manager = ConfigManager()
    try:
        success = config_manager.reset_to_defaults()
        if success: print(""Config reset to defaults.""); return 0
        else: print(""Error: Failed reset config.""); return 1
    except Exception as e: logger.exception(f""Error reset_config: {e}""); print(f""Error: {e}""); return 1",Handle the reset-config command.,"???Reset configuration to defaults, handle success or log errors.???"
2442,_revise_draft,"def _revise_draft(context: dict, **kwargs) -> dict:
    
    review = context.get(""review"")
    draft_report = context.get(""draft"")

    user_prompt = f
    system_prompt = ""You are an expert writer. Your goal is to revise drafts based on reviewer notes.""

    response = execute_agent(system_prompt, user_prompt, to_json=True)

    return {""draft"": response.get(""draft"", """"), ""revision_notes"": response.get(""revision_notes"", """"), ""result"": """"}",Revises the draft based on reviewer feedback.,???Refine draft reports using expert feedback and reviewer notes.???
2443,ask,"def ask(self, prompt: str) -> str:
        
        try:
            return input(f""{prompt.strip()} "")
        except (EOFError, KeyboardInterrupt) as e:  # pragma: no cover - user interrupt
            raise InteractiveError(""input interrupted"") from e",Return a response to the given prompt.,"???  
Handles user input with error management for interruptions.  
???"
2444,get_token_from_config,"def get_token_from_config():
    
    try:
        from config import get_config
        config = get_config()
        if not config:
            return None
            
        system = platform.system()
        if system == ""Windows"" and config.has_section('WindowsPaths'):
            return {
                'storage_path': config.get('WindowsPaths', 'storage_path'),
                'sqlite_path': config.get('WindowsPaths', 'sqlite_path'),
                'session_path': os.path.join(os.getenv(""APPDATA""), ""Cursor"", ""Session Storage"")
            }
        elif system == ""Darwin"" and config.has_section('MacPaths'):  # macOS
            return {
                'storage_path': config.get('MacPaths', 'storage_path'),
                'sqlite_path': config.get('MacPaths', 'sqlite_path'),
                'session_path': os.path.expanduser(""~/Library/Application Support/Cursor/Session Storage"")
            }
        elif system == ""Linux"" and config.has_section('LinuxPaths'):
            return {
                'storage_path': config.get('LinuxPaths', 'storage_path'),
                'sqlite_path': config.get('LinuxPaths', 'sqlite_path'),
                'session_path': os.path.expanduser(""~/.config/Cursor/Session Storage"")
            }
    except Exception as e:
        logger.error(f""Get config path failed: {str(e)}"")
    
    return None",get path info from config,???Retrieve platform-specific storage paths from configuration settings.???
2445,subsentence_rules,"def subsentence_rules() -> RecursiveRules:
    
    subsentence_level = RecursiveLevel(
        delimiters=[
            "","",
            "";"",
            "":"",
            ""-"",
            ""/"",
            ""|"",
            ""("",
            "")"",
            ""["",
            ""]"",
            ""{"",
            ""}"",
            ""<"",
            "">"",
        ],
        whitespace=False,
    )
    return RecursiveRules(levels=[subsentence_level])",Return a subsentence set of rules.,??? Define recursive parsing rules for subsentence delimiters. ???
2446,remove_null_values,"def remove_null_values(d: Dict):
    
    return {k: v for k, v in d.items() if v}",Return a new dictionary with the key-value pair of any null value removed.,"???  
Filter out dictionary entries with null values.  
???"
2447,synchronize_projects,"def synchronize_projects() -> None:
    
    # Call the API to synchronize projects

    project_url = config.project_url

    try:
        response = asyncio.run(call_post(client, f""{project_url}/project/sync""))
        result = ProjectStatusResponse.model_validate(response.json())

        console.print(f""[green]{result.message}[/green]"")
    except Exception as e:  # pragma: no cover
        console.print(f""[red]Error synchronizing projects: {str(e)}[/red]"")
        console.print(""[yellow]Note: Make sure the Basic Memory server is running.[/yellow]"")
        raise typer.Exit(1)",Synchronize projects between configuration file and database.,???Synchronize projects via API call and handle potential errors.???
2448,validate_raw_math,"def validate_raw_math(item: Dict) -> Tuple[bool, List[str]]:
    
    errors = []
    required = {""query_id"": str, ""prompt"": str, ""solutions"": list}

    for field, typ in required.items():
        if field not in item:
            errors.append(f""Missing required field: {field}"")
        elif not isinstance(item[field], typ):
            type_names = (
                [t.__name__ for t in typ] if isinstance(typ, tuple) else typ.__name__
            )
            errors.append(f""Invalid type for {field}: expected {type_names}"")

    return (not errors, errors)",Validate raw math item structure,"???Validate dictionary fields and types, returning success status and error messages.???"
2449,analyze_sentiment,"def analyze_sentiment(news_items: list) -> dict:
    
    if not news_items:
        return {""score"": 5, ""details"": ""No news data; defaulting to neutral sentiment""}

    negative_keywords = [""lawsuit"", ""fraud"", ""negative"", ""downturn"", ""decline"", ""investigation"", ""recall""]
    negative_count = 0
    for news in news_items:
        title_lower = (news.title or """").lower()
        if any(word in title_lower for word in negative_keywords):
            negative_count += 1

    details = []
    if negative_count > len(news_items) * 0.3:
        # More than 30% negative => somewhat bearish => 3/10
        score = 3
        details.append(f""High proportion of negative headlines: {negative_count}/{len(news_items)}"")
    elif negative_count > 0:
        # Some negativity => 6/10
        score = 6
        details.append(f""Some negative headlines: {negative_count}/{len(news_items)}"")
    else:
        # Mostly positive => 8/10
        score = 8
        details.append(""Mostly positive/neutral headlines"")

    return {""score"": score, ""details"": ""; "".join(details)}",Basic news sentiment: negative keyword check vs.,???Evaluate news sentiment by counting negative keywords to assign a sentiment score.???
2450,unregister_agent,"def unregister_agent(session_id: int) -> None:
    
    logger.info(f""Unregistering agent for session_id {session_id}"")
    with _registry_lock:
        agent_thread_registry.pop(session_id, None)",Remove an agent entry from the registry once it has finished or is no longer needed.,???Safely remove an agent from the registry using a session identifier.???
2451,execute,"def execute(self,
                input_values, 
                jinxs_dict, 
                jinja_env = None,
                npc = None,
                messages=None):
        
        if jinja_env is None:
            jinja_env = Environment(
                loader=FileSystemLoader([npc.npc_directory, npc.jinxs_directory]),
                undefined=SilentUndefined,
            )
        # Create context with input values and jinxs
        context = (npc.shared_context.copy() if npc else {})
        context.update(input_values)
        context.update({
            ""jinxs"": jinxs_dict,
            ""llm_response"": None,
            ""output"": None
        })
        
        # Process each step in sequence
        for i, step in enumerate(self.steps):
            context = self._execute_step(
                step, 
                context,
                jinja_env, 
                npc=npc, 
                messages=messages, 

            )            

        return context",Execute the jinx with given inputs,???Execute a sequence of steps using a Jinja environment and input context???
2452,openrouter_fn,"def openrouter_fn(self, history, verbose=False):
        
        if self.is_local:
            # This case should ideally not be reached if unsafe_providers is set correctly
            # and is_local is False in config for openrouter
            raise Exception(""OpenRouter is not available for local use. Change config.ini"")
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=history,
            )
            if response is None:
                raise Exception(""OpenRouter response is empty."")
            thought = response.choices[0].message.content
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""OpenRouter API error: {str(e)}"") from e",Use OpenRouter API to generate text.,???Handle remote chat completions via OpenRouter API with error management.???
2453,execute_query,"def execute_query(cls, query, params=None):
        
        with cls.get_connection() as conn:
            try:
                with conn.cursor() as cursor:
                    logger.info(f""Executing query: {query}"")
                    logger.info(f""Parameters: {params}"")
                    cursor.execute(query, params or ())
                    rowcount = cursor.rowcount
                    logger.info(f""Query executed successfully. Affected rows: {rowcount}"")
                conn.commit()
                logger.info(""Transaction committed"")
                return rowcount
            except Exception as e:
                conn.rollback()
                logger.error(f""Query execution error: {str(e)}"")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f""Database query failed: {str(e)}""
                )","Run an INSERT, UPDATE, or DELETE query.","???Execute a database query with logging, error handling, and transaction management.???"
2454,register_tooltips,"def register_tooltips():
    

    # PoseInterpolator tooltips (inherits from: RyanOnTheInside)
    TooltipManager.register_tooltips(""PoseInterpolator"", {
        ""pose_1"": ""First pose keypoint sequence to interpolate between"",
        ""pose_2"": ""Second pose keypoint sequence to interpolate between"",
        ""feature"": ""Feature that controls the interpolation amount between poses"",
        ""strength"": ""Overall strength of the interpolation effect (0.0 to 1.0)"",
        ""interpolation_mode"": ""Method for interpolating between poses: Linear (straight interpolation) or Spherical (curved path interpolation)"",
        ""omit_missing_points"": ""When enabled, missing keypoints in either pose will be set to zero in the output. When disabled, uses the available keypoint from either pose."",
    }, inherits_from='RyanOnTheInside')",Register tooltips for preprocessors nodes,???Register tooltips for pose interpolation settings in a user interface.???
2455,did_close,"def did_close(server: CodegenLanguageServer, params: types.DidCloseTextDocumentParams) -> None:
    
    logger.info(f""Document closed: {params.text_document.uri}"")
    # The document is automatically removed from the workspace by pygls
    # We can perform any additional cleanup here if needed
    path = get_path(params.text_document.uri)
    server.io.close_file(path)",Handle document close notification.,???Handle document closure by logging and closing file resources.???
2456,get_element_bounds,"def get_element_bounds(element):
    
    bounds = {
        ""x"": 0,
        ""y"": 0,
        ""width"": 0,
        ""height"": 0
    }
    
    # Get position
    position_value = element_attribute(element, kAXPositionAttribute)
    if position_value:
        position_value = element_value(position_value, kAXValueCGPointType)
        if position_value:
            bounds[""x""] = position_value.x
            bounds[""y""] = position_value.y
    
    # Get size
    size_value = element_attribute(element, kAXSizeAttribute)
    if size_value:
        size_value = element_value(size_value, kAXValueCGSizeType)
        if size_value:
            bounds[""width""] = size_value.width
            bounds[""height""] = size_value.height
            
    return bounds",Get the bounds of an accessibility element,???Extracts and returns position and size attributes of a UI element.???
2457,_get_first_value,"def _get_first_value(self, series_or_scalar):
        
        if isinstance(series_or_scalar, pd.Series):
            return int(series_or_scalar.iloc[0])
        elif isinstance(series_or_scalar, str):
            # Convert to int if the input is a string representing a number
            return int(series_or_scalar)
        elif isinstance(series_or_scalar, (int, float)):
            return series_or_scalar
        elif isinstance(series_or_scalar, numpy.int64):
            new_type = int(series_or_scalar)
            if type(new_type) != int:
                raise ValueError(f""Unsupported data type: {type(series_or_scalar)}."")
            return new_type
        else:
            raise ValueError(f""Unsupported data type: {type(series_or_scalar)}."")","Extract the first value if the input is a Series, else return the value itself.","???  
Extracts the first numeric value from various input types, ensuring integer conversion.  
???"
2458,get_default_setting,"def get_default_setting(self, setting_path: str, default_value: Any = None) -> Any:
        
        return self.config.get(f""default_settings.{setting_path}"", default_value)",Get a default setting value.,???Retrieve configuration setting with fallback to default value???
2459,extract_numbers,"def extract_numbers(string):
    
    # Pattern for numbers with commas
    pattern_commas = r""-?\b\d{1,3}(?:,\d{3})+\b""
    # Pattern for scientific notation
    pattern_scientific = r""-?\d+(?:\.\d+)?[eE][+-]?\d+""
    # Pattern for simple numbers without commas
    pattern_simple = r""-?(?:\d+\.\d+|\.\d+|\d+\b)(?![eE][+-]?\d+)(?![,\d])""

    # Extract numbers with commas
    numbers_with_commas = re.findall(pattern_commas, string)
    # Extract numbers in scientific notation
    numbers_scientific = re.findall(pattern_scientific, string)
    # Extract simple numbers without commas
    numbers_simple = re.findall(pattern_simple, string)

    # Combine all extracted numbers
    all_numbers = numbers_with_commas + numbers_scientific + numbers_simple
    return all_numbers",Exact all forms of numbers from a string with regex.,???Extracts various formatted numbers from a string using regex patterns.???
2460,states_descriptions,"def states_descriptions(self, states: list[str]) -> str:
        
        return ""\n"".join(
            [f""'{self._state_by_id[state].name}': {self._state_by_id[state].description}"" for state in states]
        )",Get a formatted string of states descriptions.,???Generate a formatted string of state names and descriptions from a list of state identifiers.???
2461,perform_action,"def perform_action(self, action_name: str, kwargs) -> Any:
        
        if action_name not in self.actions:
            raise KeyError(f""Unknown action: {action_name}"")

        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f""Invalid parameters: {', '.join(errors)}"")

        method_name = action_name.replace(""-"", ""_"")
        method = getattr(self, method_name)
        return method(**kwargs)",Execute a Solana action with validation,???Execute validated action by invoking corresponding method with parameters.???
2462,generate_image_with_retry,"def generate_image_with_retry(prompt: str, max_retries: int = 3, delay: int = 2) -> dict:
    
    for attempt in range(max_retries):
        try:
            result = generate_image(prompt=prompt)
            if result:
                return result
        except Exception as e:
            logger.warning(f""Image generation attempt {attempt + 1} failed: {str(e)}"")

        if attempt < max_retries - 1:
            time.sleep(delay)

    logger.error(f""Image generation failed after {max_retries} attempts"")
    return None",Generate an image with retry mechanism,???Attempt image generation with retries and delay on failure???
2463,_handle_union_type,"def _handle_union_type(self, union_node: ast.BinOp) -> dict[str, Any]:
        
        # For now, just extract the first non-None type
        left_type = self._extract_type_from_node(union_node.left)
        right_type = self._extract_type_from_node(union_node.right)

        # If one side is None, return the other type
        if isinstance(right_type, str) and right_type == ""null"":
            return left_type if isinstance(left_type, dict) else {""type"": left_type}
        elif isinstance(left_type, str) and left_type == ""null"":
            return right_type if isinstance(right_type, dict) else {""type"": right_type}

        # Otherwise, return the first type
        return left_type if isinstance(left_type, dict) else {""type"": left_type}",Handle union types like str | None.,???Extracts and returns the first non-null type from a union node.???
2464,_convert_to_flat_dict,"def _convert_to_flat_dict(yaml_dict: Dict[str, Any], parent_key: str = """") -> Dict[str, Any]:
        
        items = []
        for key, value in yaml_dict.items():
            new_key = f""{parent_key}.{key}"" if parent_key else key

            if isinstance(value, dict):
                items.extend(YAMLComparer._convert_to_flat_dict(value, new_key).items())
            elif isinstance(value, list):
                if all(isinstance(item, dict) for item in value):
                    for i, item in enumerate(value):
                        list_key = f""{new_key}[{i}]""
                        items.extend(YAMLComparer._convert_to_flat_dict(item, list_key).items())
                else:
                    items.append((new_key, value))
            else:
                items.append((new_key, value))

        return dict(items)",Convert a nested YAML dict to a flat dictionary with dot-notation keys.,???Flatten nested YAML structure into a single-level dictionary with composite keys.???
2465,_get_similarity_cache_key,"def _get_similarity_cache_key(key1_str: str, key2_str: str, embeddings_dir: str,
                              path_to_key_info: Dict[str, KeyInfo], project_root: str,
                              code_roots: List[str], doc_roots: List[str], **kwargs) -> str:
    
    norm_embeddings_dir = normalize_path(embeddings_dir)
    norm_project_root = normalize_path(project_root)

    def get_npy_mtime(key_str: str) -> float:
        
        key_info = next((info for info in path_to_key_info.values() if info.key_string == key_str), None)
        if not key_info or not key_info.norm_path.startswith(norm_project_root):
            return 0.0
        try:
            relative_file_path = os.path.relpath(key_info.norm_path, norm_project_root)
            npy_path = normalize_path(os.path.join(norm_embeddings_dir, relative_file_path) + "".npy"")
            if os.path.exists(npy_path):
                return os.path.getmtime(npy_path)
            else:
                return 0.0
        except (ValueError, OSError):
            return 0.0 # Error calculating path or getting mtime

    # Sort keys to ensure consistent key order
    # Use hierarchical sorting for key strings
    sorted_keys = sort_key_strings_hierarchically([key1_str, key2_str])
    mtime1 = get_npy_mtime(sorted_keys[0])
    mtime2 = get_npy_mtime(sorted_keys[1])

    return f""similarity:{sorted_keys[0]}:{sorted_keys[1]}:{norm_embeddings_dir}:{mtime1}:{mtime2}""","Generates a cache key for calculate_similarity, including .npy mtimes.",???Generate a unique cache key for similarity comparison using normalized paths and file modification times.???
2466,get_cache,"def get_cache(self, cache_name: str, ttl: int = DEFAULT_TTL) -> Cache:
        
        if cache_name not in self.caches or self.caches[cache_name].is_expired():
            self.caches[cache_name] = Cache(cache_name, ttl)
            logger.debug(f""Spun up new cache: {cache_name} with TTL {ttl}s"")
        return self.caches[cache_name]",Retrieve or create a cache by name.,"???Initialize or retrieve a cache by name, renewing if expired.???"
2467,get_manager_status,"def get_manager_status() -> dict:
    
    return {
        ""database_manager"": _db_manager is not None,
        ""websocket_manager"": _websocket_manager is not None,
        ""team_manager"": _team_manager is not None,
        ""auth_manager"": _auth_manager is not None,
    }",Get the initialization status of all managers,???Check if various system managers are initialized and return their statuses.???
2468,agents_descriptions,"def agents_descriptions(self) -> str:
        
        return ""\n"".join([f""{i}. {agent.name}"" for i, agent in enumerate(self.agents)]) if self.agents else """"",Get a formatted string of agent descriptions.,???Generate a numbered list of agent names from a collection.???
2469,_create_model_from_path,"def _create_model_from_path(self, model_path: str, args: SFTConfig) -> PreTrainedModel:
        
        model_init_kwargs = args.model_init_kwargs or {}
        # Handle torch dtype
        torch_dtype = model_init_kwargs.get(""torch_dtype"")
        if isinstance(torch_dtype, torch.dtype) or torch_dtype == ""auto"" or torch_dtype is None:
            pass  # torch_dtype is already a torch.dtype or ""auto"" or None
        elif isinstance(torch_dtype, str):  # it's a str, but not ""auto""
            torch_dtype = getattr(torch, torch_dtype)
            model_init_kwargs[""torch_dtype""] = torch_dtype
        else:
            raise ValueError(
                ""Invalid `torch_dtype` passed to `SFTConfig`. Expected either 'auto' or a string representing ""
                f""a `torch.dtype` (e.g., 'float32'), but got {torch_dtype}.""
            )
        # Disable caching if gradient checkpointing is enabled (not supported)
        if args.gradient_checkpointing:
            model_init_kwargs[""use_cache""] = False

        # Create model
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_init_kwargs)
        return model",Creates a model from a path or model identifier.,???Initialize a pre-trained language model with configuration handling for data type and caching.???
2470,invoke_bedrock_agent,"def invoke_bedrock_agent(
    inputText: str, agentId: str, agentAliasId: str, sessionId: str, **kwargs
):
    
    # Create Bedrock client
    bedrock_rt_client = boto3.client(""bedrock-agent-runtime"")
    use_streaming = kwargs.get(""streaming"", False)
    invoke_params = {
        ""inputText"": inputText,
        ""agentId"": agentId,
        ""agentAliasId"": agentAliasId,
        ""sessionId"": sessionId,
        ""enableTrace"": True,  # Required for instrumentation
    }

    # Add streaming configurations if needed
    if use_streaming:
        invoke_params[""streamingConfigurations""] = {
            ""applyGuardrailInterval"": 10,
            ""streamFinalResponse"": use_streaming,
        }
    response = bedrock_rt_client.invoke_agent(**invoke_params)
    return response",Invoke a Bedrock Agent with instrumentation for Langfuse.,???Invoke a Bedrock agent with optional streaming configurations using AWS client.???
2471,calculate_answer_score,"def calculate_answer_score(pred_sql, gold_sql, db_path, do_print=False):
    
    try:
        pred_results = execute_sql(pred_sql, db_path)
        gold_results = execute_sql(gold_sql, db_path)
        
        # answer_score = 2 if set(pred_results) == set(gold_results) else 0.5
        answer_score = 1 if set(pred_results) == set(gold_results) else 0
        
    except Exception as e:
        if do_print:
            print(f""[Error] Error in executing SQL: {e}"")
        pred_results = []
        gold_results = []

        answer_score = 0
        # if 'syntax' in str(e):
        #     answer_score = 0
        # else:
        #     answer_score = 0.1

    if do_print:
        # print(f""Retrieved results: {pred_results}"")
        # print(f""Target: {gold_results} "")
        print(f""Answer score: {answer_score}"")

    
    return answer_score",Calculate answer score based on final_prediction idx.,???Evaluate SQL query accuracy by comparing predicted and actual results.???
2472,load_yaml_file,"def load_yaml_file(file_path):
    
    try:
        with open(os.path.expanduser(file_path), 'r') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f""Error loading YAML file {file_path}: {e}"")
        return None",Load a YAML file with error handling,"???Load and parse a YAML file, handling errors gracefully.???"
2473,_get_ref_audio,"def _get_ref_audio(task_df, min_duration=8, max_duration=14.5) -> str:
    
    rprint(f""[blue]🎯 Starting reference audio selection process..."")
    
    duration = 0
    selected = []
    
    for _, row in task_df.iterrows():
        current_duration = row['duration']
        
        # Skip if adding this segment would exceed max duration
        if current_duration + duration > max_duration:
            continue
            
        # Add segments until we exceed min duration
        selected.append(row)
        duration += current_duration
        
        # Once we exceed min duration and are under max, we're done
        if duration > min_duration and duration < max_duration:
            break
    
    if not selected:
        rprint(f""[red]❌ No valid segments found (could not reach minimum {min_duration}s duration)"")
        return None
        
    rprint(f""[blue]📊 Selected {len(selected)} segments, total duration: {duration:.2f}s"")
    
    audio_files = [f""{_AUDIO_REFERS_DIR}/{row['number']}.wav"" for row in selected]
    rprint(f""[yellow]🎵 Audio files to merge: {audio_files}"")
    
    combined_audio = f""{_AUDIO_REFERS_DIR}/refer.wav""
    success = _merge_audio(audio_files, combined_audio)
    
    if not success:
        rprint(f""[red]❌ Error: Failed to merge audio files"")
        return False
    
    rprint(f""[green]✅ Successfully created combined audio: {combined_audio}"")
    
    return combined_audio","Get reference audio, ensuring the combined audio duration is > min_duration and < max_duration",???Select and merge audio segments to create a reference file within duration limits.???
2474,web_ui,"def web_ui(self):
        
        menu_style_cfg =   # Hide main menu style

        # Main title of streamlit application
        main_title_cfg = 

        # Subtitle of streamlit application
        sub_title_cfg = 

        # Set html page configuration and append custom HTML
        self.st.set_page_config(page_title=""Ultralytics Streamlit App"", layout=""wide"")
        self.st.markdown(menu_style_cfg, unsafe_allow_html=True)
        self.st.markdown(main_title_cfg, unsafe_allow_html=True)
        self.st.markdown(sub_title_cfg, unsafe_allow_html=True)",Sets up the Streamlit web interface with custom HTML elements.,???Configure and display a Streamlit web UI with custom titles and styles.???
2475,setup_experiment_dirs,"def setup_experiment_dirs(self) -> None:
        
        if not self.base_dir:
            return

        # Create base experiments directory if it doesn't exist
        os.makedirs(self.base_dir, exist_ok=True)

        # Create timestamped run directory
        timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
        self.run_dir = os.path.join(self.base_dir, timestamp)
        os.makedirs(self.run_dir, exist_ok=True)
        logger.info(f""Created run directory: {self.run_dir}"")

        # Create first turn directory
        self.create_turn_dir()",Setup the experiment directory structure.,???Initialize experiment directories with timestamped and turn-specific subdirectories.???
2476,predict,"def predict(self, img_path):
        
        try:
            image = Image.open(img_path).convert(""RGB"")
            image_tensor = self.transform(image).unsqueeze(0)
            input_tensor = Variable(image_tensor).to(self.device)
            
            with torch.no_grad():
                out = self.model(input_tensor)
                _, preds = torch.max(out, 1)
                idx = preds.cpu().numpy()[0]
                pred_class = self.class_names[idx]
                
            # # Display Image
            # plt.imshow(np.array(image))
            # plt.title(f""Predicted: {pred_class}"")
            # plt.show()

            self.logger.info(f""Predicted Class: {pred_class}"")
            
            return pred_class
        except Exception as e:
            self.logger.error(f""Error during prediction Covid Chest X-ray: {str(e)}"")
            return None",Predict the class of a given image.,???Predicts image class using a pre-trained model and logs the result.???
2477,reset,"def reset(self, seed: Optional[int] = None):
        
        def _expand_seed(seed: int):
            seeds = [[seed + i] * self.group_size for i in range(self.env_groups)] # [[seed, ..., seed], [seed+1, ..., seed+1], ...]
            return sum(seeds, [])

        envs = self.envs
        rollout_cache = [{""env_id"": entry['env_id'], ""history"": [], ""group_id"": entry['group_id'], ""tag"": entry['tag'], ""penalty"": 0} for entry in envs]

        # reset all environments
        if self.mode == ""train"":
            seed = random.randint(0, 1000000) if seed is None else seed # get a random seed
        else:
            seed = 123
        seeds = _expand_seed(seed)
        for seed, entry in zip(seeds, envs):
            entry['env'].reset(seed=seed, mode=self.mode)
            entry['status'] = EnvStatus(seed=seed)

        # update rollout cache
        for cache, env in zip(rollout_cache, envs):
            next_state = self._handle_mm_state(env['env'].render())
            cache['history'] = self._update_cache_history(cache['history'], next_state=next_state, actions_left=env['max_actions_per_traj'], num_actions_info=None)
            
        self.rollout_cache = rollout_cache
        return rollout_cache","Reset the environments and get initial observation build up rollout cache like [{""env_id"": int, ""history"": List[Dict], ""group_id"": int}, ...]",???Reset and initialize environments with expanded seeds and update rollout history.???
2478,show_status,"def show_status():
    
    running_servers = check_flask_servers()

    if running_servers:
        print(f""Found {len(running_servers)} running Flask server(s):"")
        for pid in running_servers:
            try:
                proc = psutil.Process(pid)
                cmdline = proc.cmdline()
                port = None

                # Try to extract the port number
                for i, arg in enumerate(cmdline):
                    if arg == ""--port"" and i + 1 < len(cmdline):
                        port = cmdline[i + 1]

                if port:
                    print(f""  - PID {pid}: Running on port {port}"")
                else:
                    print(f""  - PID {pid}: Running (port unknown)"")

            except (psutil.NoSuchProcess, psutil.AccessDenied):
                print(f""  - PID {pid}: [Process information unavailable]"")
    else:
        print(""No Flask servers currently running."")

    return len(running_servers) > 0",Show status of running Flask servers.,???Check and display status of running Flask servers with port details.???
2479,load_state_dict,"def load_state_dict(self, state_dict):
        
        super().load_state_dict(state_dict)  # Call the parent class method

        for state in self.state.values():
            for key in [
                ""exp_avg"",
                ""exp_avg_sq"",
                ""exp_avg_sq_row"",
                ""exp_avg_sq_col"",
                ""exp_avg_res_row"",
                ""exp_avg_res_col"",
            ]:
                if key in state:
                    if isinstance(state[key], list):
                        state[key] = [
                            {
                                ""data"": exp[""data""].byte(),  # Directly convert data to 8-bit
                                ""scale"": exp[""scale""],  # Keep scale unchanged
                                ""min"": exp[""min""],  # Keep min unchanged
                            }
                            for exp in state[key]
                        ]
                    elif isinstance(state[key], torch.Tensor):
                        # If it's a tensor, keep it as 32-bit
                        state[key] = state[key].float()  # Ensure it's 32-bit

        del state_dict
        torch.cuda.empty_cache()",Loads the state dictionary and converts the corresponding states to 8-bit,???Optimize state dictionary by converting data types and clearing cache.???
2480,_get_info,"def _get_info(self):
        
        print(f""[DEBUG] CustomSokobanEnv._get_info(): self.boxes_on_target = {getattr(self, 'boxes_on_target', 'N/A')}, self.num_boxes = {getattr(self, 'num_boxes', 'N/A')}"")
        return {
            ""action.name"": ACTION_LOOKUP.get(0, ""NoOp""), # Default NoOp for reset state
            ""action.moved_player"": False,
            ""action.moved_box"": False,
            ""steps"": self.num_env_steps,
            ""boxes_on_target"": self.boxes_on_target,
        }",Returns the info dictionary (currently basic).,??? Retrieve and log Sokoban environment state details for debugging purposes. ???
2481,input_mapper_for_xtts,"def input_mapper_for_xtts(ctx: InputContext, data: Union[Dict, List[Tensor]]) -> MultiModalKwargs:
    

    if not isinstance(data, list):
        data = [data]

    if len(data) == 0:
        return MultiModalKwargs()

    assert is_list_of(data, dict, check=""all""), (f""Expected a list of dictionaries, ""
                                                 f""but got a list of {[type(dat) for dat in data if type(dat) != dict][0]}"")

    embeds = [dat[""embeds""] for dat in data]
    is_logits_only_mode = [dat.get(""is_logits_only_mode"", False) for dat in data]
    sequence_length = [dat.get(""sequence_length"", -1) for dat in data]
    return MultiModalKwargs(
        {
            ""cond_latents"": embeds,
            ""is_logits_only_mode"": is_logits_only_mode,
            ""sequence_length"": sequence_length
        }
    )",Map input data to XTTS format.,???Transforms input data into a structured format for multi-modal processing.???
2482,load_checkpoint,"def load_checkpoint(self):
        
        # Call the base class method first to load the data
        super().load_checkpoint()

        # The base class loads data into attributes, so we can access them directly
        # if they were saved in save_checkpoint
        if hasattr(self, ""iter""):
            # Restore complexity system state if available
            if hasattr(self, ""task_complexity_levels"") and self.task_complexity_levels:
                self.logger.info(
                    f""Restored complexity levels for {len(self.task_complexity_levels)} tasks""
                )
            if (
                hasattr(self, ""task_performance_history"")
                and self.task_performance_history
            ):
                total_scores = sum(
                    len(scores) for scores in self.task_performance_history.values()
                )
                self.logger.info(
                    f""Restored performance history with {total_scores} total scores""
                )
            if hasattr(self, ""task_group_counts"") and self.task_group_counts:
                total_groups = sum(self.task_group_counts.values())
                self.logger.info(
                    f""Restored group counts with {total_groups} total groups processed""
                )
            # Data was loaded successfully
            pass","Load checkpoint including iteration number, completion lengths, data dumping state, and complexity system.",???Restore system state by loading checkpoint data and logging task metrics.???
2483,create_indexes_and_triggers,"def create_indexes_and_triggers(db_path: str):
    
    logging.info(f""Creating indexes and triggers for database: {db_path}..."")
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.executescript(SQL_CREATE_INDEXES_TRIGGERS)
    conn.commit()
    conn.close()
    logging.info(""Indexes and triggers created successfully."")",Creates indexes and triggers on the populated database.,???Establishes database connection to create indexes and triggers.???
2484,run_browsecomp_benchmark_wrapper,"def run_browsecomp_benchmark_wrapper(args: Tuple) -> Dict[str, Any]:
    
    num_examples, output_dir, resume_from, search_config, evaluation_config = args

    logger.info(f""Starting BrowseComp benchmark with {num_examples} examples"")
    start_time = time.time()

    # BrowseComp needs more iterations
    browsecomp_config = {**search_config, ""iterations"": 3}

    results = run_resumable_benchmark(
        dataset_type=""browsecomp"",
        num_examples=num_examples,
        output_dir=os.path.join(output_dir, ""browsecomp""),
        search_config=browsecomp_config,
        evaluation_config=evaluation_config,
        resume_from=resume_from,
    )

    duration = time.time() - start_time
    logger.info(f""BrowseComp benchmark completed in {duration:.1f} seconds"")

    return results",Wrapper for running BrowseComp benchmark in parallel.,???Execute and log a resumable BrowseComp benchmark with custom configurations.???
2485,find_model,"def find_model(model_name):
    
    if model_name in pretrained_models:  # Find/download our pre-trained G.pt checkpoints
        return download_model(model_name)
    else:  # Load a custom Sana checkpoint:
        assert os.path.isfile(model_name), f""Could not find Sana checkpoint at {model_name}""
        return torch.load(model_name, map_location=lambda storage, loc: storage)","Finds a pre-trained G.pt model, downloading it if necessary.","???Determine and load a specified model, either pre-trained or custom.???"
2486,main_shutdown_handler,"def main_shutdown_handler(signum, frame):
    
    huntarr_logger.info(f""Received signal {signum}. Initiating graceful shutdown..."")
    
    # Set both shutdown events
    if not stop_event.is_set():
        stop_event.set()
    if not shutdown_requested.is_set():
        shutdown_requested.set()
    
    # Also shutdown the Waitress server directly if it exists
    global waitress_server
    if waitress_server:
        try:
            huntarr_logger.info(""Signaling Waitress server to shutdown..."")
            waitress_server.close()
        except Exception as e:
            huntarr_logger.warning(f""Error closing Waitress server: {e}"")",Gracefully shut down the application.,"???Handle graceful shutdown on receiving termination signal, including server closure.???"
2487,parse_qrel,"def parse_qrel(qrel):
    
    query_dict = {}

    # Skip the header
    for line in qrel:
        query_id, corpus_id, score = line

        if query_id not in query_dict:
            query_dict[query_id] = {""targets"": [], ""scores"": []}

        query_dict[query_id][""targets""].append(corpus_id)
        query_dict[query_id][""scores""].append(int(score))

    return query_dict",Parse a TSV file and return a dictionary where: - Keys are query IDs.,???Convert relevance data into a structured dictionary by query ID.???
2488,query_gpt4o,"def query_gpt4o(self, prompt: str) -> str:
        

        try:
            response = self.gru_client.chat.completions.create(
                model=self.gru_model,  # Use the specified teacher model
                messages=[{""role"": ""user"", ""content"": prompt}],
                temperature=0.2,
                max_tokens=2000,
                response_format={""type"": ""json_object""},
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f""Error querying {self.gru_model}: {e}"")
            return """"",Query the teacher model (GPT-4o).,"???Invoke AI model to generate text response from user prompt, handling errors.???"
2489,parse_sheet,"def parse_sheet(api_sheet: dict) -> dict:
    
    props = api_sheet.get(""properties"", {})
    grid_props = props.get(""gridProperties"", {})
    cell_data = convert_api_grid_data_to_dict(api_sheet.get(""data"", []))

    return {
        ""sheetId"": props.get(""sheetId""),
        ""title"": props.get(""title"", """"),
        ""rowCount"": grid_props.get(""rowCount"", 0),
        ""columnCount"": grid_props.get(""columnCount"", 0),
        ""data"": cell_data,
    }",Parse an individual sheet's data from the Google Sheets 'get spreadsheet',???Transforms API sheet data into a structured dictionary format.???
2490,_log_exclusion_reason,"def _log_exclusion_reason(self, item, reason, content_type):
        
        title = item.get('title' if content_type == 'movie' else 'name')
        self.logger.info(f""Excluding {title} due to {reason}."")",Logs the reason for excluding a content item.,???Log exclusion reasons for items based on content type and title.???
2491,renew_vae_resnet_paths,"def renew_vae_resnet_paths(old_list, n_shave_prefix_segments=0):
    
    mapping = []
    for old_item in old_list:
        new_item = old_item

        new_item = new_item.replace(""nin_shortcut"", ""conv_shortcut"")
        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)

        mapping.append({""old"": old_item, ""new"": new_item})

    return mapping",Updates paths inside resnets to the new naming scheme (local renaming),??? Update and map old paths to new VAE ResNet paths ???
2492,new_prediction,"def new_prediction(self, cache_device='cpu'):
        
        self.cache_device = cache_device
        pred_id = self._next_pred_id
        self._next_pred_id += 1
        self.states[pred_id] = {
            'previous_residual': None,
            'accumulated_rel_l1_distance': 0,
            'previous_modulated_input': None,
            'skipped_steps': [],
        }
        return pred_id",Create new prediction state and return its ID,???Initialize prediction state with unique ID and default values.???
2493,delete_audio,"def delete_audio(collection_id, audio_id):
    
    try:
        if not audio_id:
            return {""message"": ""Video ID is required""}, 400
        videodb = VideoDBHandler(collection_id)
        result = videodb.delete_audio(audio_id)
        return result, 200
    except Exception as e:
        return {""message"": str(e)}, 500",Delete a audio by ID from a specific collection.,"???  
Function to remove audio by ID from a specified collection.  
???"
2494,check_argument_types,"def check_argument_types(parser: argparse.ArgumentParser):
    
    for action in parser._actions:
        if action.dest != ""help"" and not action.const:
            if action.type is None:
                raise ValueError(f""Argument '{action.dest}' doesn't have a type specified."")
            else:
                continue","Check to make sure all CLI args are typed, raises error if not","???Validate argument types in command-line parser, raising error if unspecified???"
2495,_validate_value,"def _validate_value(self, raw_value: Any, field_type: Type, module) -> Any:
        
        # Handle basic types
        if field_type in (str, int, float, bool):
            return field_type(raw_value)

        # Handle Lists
        if hasattr(field_type, ""__origin__"") and field_type.__origin__ is list:
            if not isinstance(raw_value, list):
                raise ValueError(f""Expected list, got {type(raw_value).__name__}"")

            element_type = field_type.__args__[0]
            return [
                self._validate_value(item, element_type, module) for item in raw_value
            ]

        # Handle dynamic types (classes/types that need to be imported)
        if isinstance(raw_value, str):
            return self._resolve_type(raw_value, module)

        raise ValueError(f""Unsupported type: {field_type}"")",Validate and convert a value to its expected type,"???Validate and convert raw input to specified data type, handling lists and dynamic types.???"
2496,search_with_parameter_override,"def search_with_parameter_override():
    
    tavily_connection = Tavily()

    tavily_tool = TavilyTool(connection=tavily_connection, search_depth=""basic"", max_results=5, include_answer=False)

    result = tavily_tool.run(
        input_data={
            ""query"": ""Latest developments in quantum computing"",
            ""search_depth"": ""advanced"",
            ""max_results"": 3,
            ""include_answer"": True,
            ""exclude_domains"": [""wikipedia.org""],
        }
    )

    print(""Search Results with Parameter Override:"")
    print(result.output.get(""content""))",Example demonstrating parameter override during execution.,???Override default search parameters for tailored quantum computing results.???
2497,set_document_cookie_disabled,"def set_document_cookie_disabled(
    disabled: bool,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""disabled""] = disabled
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Emulation.setDocumentCookieDisabled"",
        ""params"": params,
    }
    json = yield cmd_dict",EXPERIMENTAL** :param disabled: Whether document.coookie API should be disabled.,???Disable or enable document cookies via emulation command generator.???
2498,create_requirements_file,"def create_requirements_file(example_dir: Path, use_local: bool, version: str | None) -> Path:
    
    temp_req = tempfile.NamedTemporaryFile(mode=""w"", delete=False, suffix="".txt"")

    with open(file=example_dir / ""requirements.txt"", mode=""r"", encoding=""utf-8"") as f:
        requirements = f.readlines()

    with open(file=temp_req.name, mode=""w"", encoding=""utf-8"") as f:
        # TODO: saqadri - consider just copying the original requirements file
        # f.writelines(requirements)
        for req in requirements:
            if not (req.strip().startswith(""-e"") or req.strip().startswith(""mcp-agent"")):
                f.write(req)

        f.write(""\n"")

        if use_local:
            # Add the local source
            f.write(""-e ../../\n"")
        else:
            # Add the PyPI version
            version_str = f""=={version}"" if version else """"
            f.write(f""mcp-agent{version_str}\n"")

    return Path(temp_req.name)",Create a temporary requirements file with the correct mcp-agent source.,"???Generate a temporary requirements file, optionally including local or specific version dependencies.???"
2499,process_node,"def process_node(
        self, node, current_class, definitions, process_method, process_function, process_class, process_class_variable
    ):
        
        if node.type == ""function_definition"":
            if current_class:
                process_method(node, definitions[""classes""][current_class][""methods""])
            else:
                process_function(node, definitions[""functions""])
            return ""function""
        elif node.type == ""struct_specifier"":
            class_name = process_class(node)
            definitions[""classes""][class_name] = {
                ""line"": (node.start_point[0] + 1, node.end_point[0] + 1),
                ""methods"": [],
                ""variables"": [],
            }
            return ""class""",Processes a node in a C syntax tree.,???Process nodes to categorize and store functions or classes in definitions.???
2500,filter_dict,"def filter_dict(map: dict[str, T | V], value_to_exclude: V = None) -> dict[str, V]:
    
    return {filter: value for filter, value in map.items() if value is not value_to_exclude}",Remove entries with unwanted values (None by default) from dictionary.,"???Filters a dictionary to exclude specified values, returning the modified dictionary.???"
2501,task_gravity_antigravity,"def task_gravity_antigravity(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    density = 0.5
    question = [rng.randint(1, 2) if rng.random() < density else 0 for _ in range(size)]

    color1 = [x for x in question if x == 1]
    color2 = [x for x in question if x == 2]
    answer = [2] * len(color2) + [0] * (size - len(color1) - len(color2)) + [1] * len(color1)

    return {""input"": question, ""output"": answer}",Generate a task where color 1 moves right and color 2 moves left.,??? Generate a randomized sequence and transform it based on predefined rules. ???
2502,get_import_module_name_for_file,"def get_import_module_name_for_file(self, filepath: str, ctx: CodebaseContext) -> str:
        
        # TODO: support relative and absolute module path
        import_path = filepath

        # Apply path import aliases to import_path
        if self.ts_config:
            import_path = self.ts_config.translate_absolute_path(import_path)

        # Remove file extension
        import_path = os.path.splitext(import_path)[0]
        return f""'{import_path}'""",Returns the module name that this file gets imported as,???Determine module import path from file path using configuration context.???
2503,_update_cache_entry,"def _update_cache_entry(self, hash_key: str, context: PipelineContext):
        
        existing_entry = self.cache.get(hash_key)
        if existing_entry is not None:
            # Update critical alerts while retaining the original timestamp.
            critical_alerts = [
                alert
                for alert in context.alerts_raised
                if alert.trigger_category == AlertSeverity.CRITICAL.value
            ]
            # Update the entry in the cache with new critical alerts but keep the old timestamp.
            updated_cache = CachedFim(
                timestamp=existing_entry.timestamp,
                critical_alerts=critical_alerts,
                initial_id=existing_entry.initial_id,
            )
            self.cache[hash_key] = updated_cache
            logger.info(f""Updated cache entry for hash key: {hash_key}"")
        else:
            # Log a warning if trying to update a non-existent entry - ideally should not happen.
            logger.warning(f""Attempted to update non-existent cache entry for hash key: {hash_key}"")",Update an existing cache entry without changing the timestamp.,???Update cache with new critical alerts while preserving original timestamp???
2504,copy_defaults,"def copy_defaults(self, from_spec: ""TaskDataSpec"") -> None:
        
        default_attrs = {
            ""system_prompt"": from_spec.system_prompt,
            ""prompt"": from_spec.prompt,
        }

        for attr_name, default_value in default_attrs.items():
            if getattr(self, attr_name) is None:
                setattr(self, attr_name, default_value)",Apply default values from another Task instance for any None attributes.,???Copy default attributes from a source object if they are unset.???
2505,_parse_value,"def _parse_value(self, value: str) -> Dict[str, Any]:
        
        if not value:
            return {""value"": """"}
            
        try:
            # Try parsing as JSON first
            return json.loads(value)
        except json.JSONDecodeError:
            # If not valid JSON, treat as template or simple value
            return {""value"": value, ""is_template"": bool(re.search(r'\$\w+\$', value))}",Parse the column_value string into a dictionary.,???Parse input string as JSON or template with placeholders???
2506,classify_text,"def classify_text(self, text: str, classification_prompt: str) -> str:
        
        try:
            text_response, _, _ = self.call(
                system_prompt=classification_prompt, user_prompt=text, temperature=0.3, model_id=self.small_model_id
            )
            return text_response.strip().upper()

        except Exception as e:
            logger.error(f""Text classification failed: {str(e)}"")
            return ""UNKNOWN""",Classify text using the small model,???Classify input text using a model and handle errors.???
2507,_verify_gateway_plugin,"def _verify_gateway_plugin(
    version: Annotated[Optional[str], ""Version of the Gateway API plugin to verify. If None, checks latest""] = None,
    namespace: Annotated[str, ""Namespace where Argo Rollouts is installed""] = ""argo-rollouts"",
    should_install: Annotated[bool, ""Flag to determine if the plugin should be installed if not present""] = True,
) -> GatewayPluginStatus:
    
    # First check if the ConfigMap exists and is properly configured
    cmd = [""get"", ""configmap"", ""argo-rollouts-config"", ""-n"", namespace, ""-o"", ""yaml""]
    try:
        config_map = run_command(command=""kubectl"", args=cmd)
        if ""argoproj-labs/gatewayAPI"" not in config_map:
            if should_install:
                return _configure_gateway_plugin(version, namespace)
            else:
                return GatewayPluginStatus(
                    installed=False, error_message=""Gateway API plugin is not configured and installation is disabled""
                )
        return GatewayPluginStatus(installed=True, error_message=""Gateway API plugin is already configured"")
    except Exception as e:
        if should_install:
            return _configure_gateway_plugin(version, namespace)
        else:
            return GatewayPluginStatus(installed=False, error_message=f""Error verifying plugin: {str(e)}"")",Verify Gateway API plugin installation for Argo Rollouts.,???Verify and optionally install the Gateway API plugin for Argo Rollouts in a specified namespace.???
2508,render_mdx_inheritence_section,"def render_mdx_inheritence_section(cls_doc: ClassDoc) -> str:
    
    # Filter on parents who we have docs for
    parents = cls_doc.inherits_from
    if not parents:
        return """"
    parents_string = "", "".join([parse_link(parent) for parent in parents])
    return f",Renders the MDX for the inheritence section,??? Generate a formatted string of documented parent classes for a given class. ???
2509,_download_googlesource_file,"def _download_googlesource_file(download_session, repo_url, version, relative_path):
    
    if 'googlesource.com' not in repo_url:
        raise ValueError('Repository URL is not a googlesource.com URL: {}'.format(repo_url))
    full_url = repo_url + '/+/{}/{}?format=TEXT'.format(version, str(relative_path))
    get_logger().debug('Downloading: %s', full_url)
    response = download_session.get(full_url)
    if response.status_code == 404:
        raise _NotInRepoError()
    response.raise_for_status()
    # Assume all files that need patching are compatible with UTF-8
    return base64.b64decode(response.text, validate=True).decode('UTF-8')",Returns the contents of the text file with path within the given  repo as a string.,???Download and decode a file from a Google Source repository using a session object.???
2510,_process_handoff_span,"def _process_handoff_span(self, span: Dict[str, Any]) -> None:
        
        from_agent = span.get(""data"", {}).get(""from_agent"", ""unknown"")
        to_agent = span.get(""data"", {}).get(""to_agent"", ""unknown"")
        
        logger.debug(f""Handoff: {from_agent} → {to_agent}"")",Process a handoff span,???Log agent handoff transition from source to destination???
2511,get_im_features,"def get_im_features(self, im):
        
        assert isinstance(self.imgsz, (tuple, list)) and self.imgsz[0] == self.imgsz[1], (
            f""SAM models only support square image size, but got {self.imgsz}.""
        )
        self.model.set_imgsz(self.imgsz)
        return self.model.image_encoder(im)",Extracts image features using the SAM model's image encoder for subsequent mask prediction.,??? Validate square image size and encode image features using a model. ???
2512,read_prompt_file,"def read_prompt_file(prompt_path):
    
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except Exception as e:
        print(f""Error reading prompt file {prompt_path}: {e}"")
        return None",Read and return the content of a prompt file,"???Read and return text from a file, handling errors gracefully.???"
2513,reset_cooldown,"def reset_cooldown() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""FedCm.resetCooldown"",
    }
    json = yield cmd_dict","Resets the cooldown time, if any, to allow the next FedCM call to show a dialog even if one was recently dismissed by the user.",???Resets a cooldown by sending a command and yielding a JSON response???
2514,inject_misconfig_k8s,"def inject_misconfig_k8s(self, microservices: list[str]):
        
        for service in microservices:
            service_config = self._modify_target_port_config(
                from_port=9090,
                to_port=9999,
                configs=self.kubectl.get_service_json(service, self.testbed),
            )

            print(f""Misconfig fault for service: {service} | namespace: {self.testbed}"")
            self.kubectl.patch_service(service, self.testbed, service_config)",Inject a fault to misconfigure service's target port in Kubernetes.,???Injects port misconfiguration into Kubernetes microservices for testing purposes.???
2515,get_available_sparse_backends,"def get_available_sparse_backends() -> Dict[str, bool]:
    
    return {
        'spconv': _try_import_spconv(),
        'torchsparse': _try_import_torchsparse()
    }",Return dict of available sparse backends and their status,???Determine availability of sparse computation libraries in a dictionary.???
2516,git_repo_with_aider_files,"def git_repo_with_aider_files(sample_git_repo):
    
    # Create .aider files
    aider_files = [
        "".aider.chat.history.md"",
        "".aider.input.history"",
        "".aider.tags.cache.v3/some_file"",
        ""src/.aider.local.settings"",
    ]

    # Create regular files
    regular_files = [""main.cpp"", ""src/helper.cpp""]

    # Create all files
    for file_path in aider_files + regular_files:
        full_path = sample_git_repo / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text(f""Content of {file_path}"")

    # Add all files (both .aider and regular) to git
    subprocess.run([""git"", ""add"", "".""], cwd=sample_git_repo)
    subprocess.run(
        [""git"", ""commit"", ""-m"", ""Add files including .aider""],
        cwd=sample_git_repo,
        env={
            ""GIT_AUTHOR_NAME"": ""Test"",
            ""GIT_AUTHOR_EMAIL"": ""test@example.com"",
            ""GIT_COMMITTER_NAME"": ""Test"",
            ""GIT_COMMITTER_EMAIL"": ""test@example.com"",
        },
    )

    return sample_git_repo",Create a git repository with .aider files that should be ignored.,???Initialize a Git repository with specific aider and regular files committed.???
2517,_save_conversation,"def _save_conversation(self, input_messages: list[BaseMessage], response: Any) -> None:
		
		if not self.save_conversation_path:
			return

		# create folders if not exists
		os.makedirs(os.path.dirname(self.save_conversation_path), exist_ok=True)

		with open(
			self.save_conversation_path + f'_{self.n_steps}.txt',
			'w',
			encoding=self.save_conversation_path_encoding,
		) as f:
			self._write_messages_to_file(f, input_messages)
			self._write_response_to_file(f, response)",Save conversation history to file if path is specified,???Save conversation data to a file if a path is specified.???
2518,get_user_meetings,"def get_user_meetings(user_id):
        
        session = get_session()
        try:
            meetings = session.query(Meeting).filter_by(user_id=user_id).all()
            
            result = []
            for m in meetings:
                result.append({
                    ""id"": m.id,
                    ""title"": m.title,
                    ""start_time"": m.start_time.isoformat() if m.start_time else None,
                    ""end_time"": m.end_time.isoformat() if m.end_time else None
                })
            
            return result
        except SQLAlchemyError as e:
            logger.error(f""Error retrieving user meetings: {e}"")
            raise
        finally:
            session.close()",Get all meetings for a user,???Retrieve and format a user's meeting details from the database.???
2519,get_env_var,"def get_env_var(
    name: str, expert: bool = False, default: Optional[str] = None
) -> Optional[str]:
    
    prefix = ""EXPERT_"" if expert else """"
    value = os.getenv(f""{prefix}{name}"")

    # If expert mode and no expert value, fall back to base value
    if expert and not value:
        value = os.getenv(name)

    return value if value is not None else default",Get environment variable with optional expert prefix and fallback.,???Retrieve environment variable with optional expert mode and default value.???
2520,sodac_llm_score,"def sodac_llm_score(iou_matrix, score_matrix, predicted_captions, gt_captions, iou_thresholds=(0.0,)):
    

    if not predicted_captions:
        return 0

    res = {str(index): [p] for index, p in enumerate(predicted_captions)}
    fs = [0] * len(iou_thresholds)
    gts = [{index: [x] for index in res} for x in gt_captions]
    for i, threshold in enumerate(iou_thresholds):
        iou_cur = np.copy(iou_matrix)
        iou_cur[iou_cur < threshold] = 0.0
        max_score, _ = chased_dp_assignment(iou_cur * score_matrix)
        (n_g, n_p) = iou_cur.shape
        p = max_score / n_p
        r = max_score / n_g
        fs[i] = 2 * p * r / (p + r) if p + r > 0 else 0

    mean_fs = np.mean(fs)

    return mean_fs",SODA_c with score matrix computed from LLM.,??? Calculate mean F-score for predicted captions using IoU and score matrices. ???
2521,get_polyhaven_status,"def get_polyhaven_status(ctx: Context) -> str:
    
    try:
        blender = get_blender_connection()
        result = blender.send_command(""get_polyhaven_status"")
        enabled = result.get(""enabled"", False)
        message = result.get(""message"", """")
        
        return message
    except Exception as e:
        logger.error(f""Error checking PolyHaven status: {str(e)}"")
        return f""Error checking PolyHaven status: {str(e)}""",Check if PolyHaven integration is enabled in Blender.,"???Check PolyHaven status via Blender connection, returning status message.???"
2522,run_tests,"def run_tests():
    
    start_time = time.time()
    
    # Print test header
    print_section(""GAIA Environment Trajectory Tests"")
    print(f""{get_timestamp()}Starting tests at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"")
    print(f""{get_timestamp()}Server URL: {BASE_URL}"")
    
    # Test server connection
    if not test_server_connection():
        print_error(""Aborting tests due to connection failure"")
        return False
    
    # Run trajectory tests
    tester = TrajectoryTester()
    success = tester.test_all_trajectories()
    
    # Calculate test duration
    end_time = time.time()
    test_duration = end_time - start_time
    
    # Print test summary
    print(f""\n{get_timestamp()}Tests completed at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"")
    print(f""{get_timestamp()}Total test duration: {test_duration:.2f} seconds"")
    
    return success",Run all trajectory tests,"???Execute and log results of GAIA environment trajectory tests, including server connection validation.???"
2523,get_server_status,"def get_server_status(self) -> ServerStatus:
        
        try:
            base_dir = os.getcwd()
            server_path = os.path.join(base_dir, ""llama.cpp"", ""build"", ""bin"", ""llama-server"")
            server_exec_name = os.path.basename(server_path)
            
            for proc in psutil.process_iter([""pid"", ""name"", ""cmdline""]):
                try:
                    cmdline = proc.cmdline()
                    # Check both for the executable name and the full path
                    if any(server_exec_name in cmd for cmd in cmdline) or any(""llama-server"" in cmd for cmd in cmdline):
                        with proc.oneshot():
                            process_info = ProcessInfo(
                                pid=proc.pid,
                                cpu_percent=proc.cpu_percent(),
                                memory_percent=proc.memory_percent(),
                                create_time=proc.create_time(),
                                cmdline=cmdline,
                            )
                            return ServerStatus.running(process_info)
                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                    continue
                    
            return ServerStatus.not_running()
            
        except Exception as e:
            logger.error(f""Error checking llama-server status: {str(e)}"")
            return ServerStatus.not_running()",Get the current status of llama-server,???Check if the llama-server process is running and return its status.???
2524,log_tool_usage,"def log_tool_usage(func):
    
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        tool_name = func.__name__
        logging.info(f""🔧 TOOL TRIGGERED: {tool_name}"")
        try:
            # Sanitize arguments to avoid logging sensitive info
            safe_args = sanitize_args(args)
            safe_kwargs = {k: sanitize_value(v) for k, v in kwargs.items()}
            logging.info(f""🔍 TOOL ARGS: {tool_name} called with {len(safe_kwargs)} parameters"")
            
            result = await func(*args, **kwargs)
            
            # Log completion but not the actual result content (might be large or sensitive)
            logging.info(f""✅ TOOL COMPLETED: {tool_name}"")
            return result
        except Exception as e:
            logging.error(f""❌ TOOL ERROR: {tool_name} - {str(e)}"")
            raise
    return wrapper",Decorator to log when a tool is being used.,"???Asynchronously logs tool usage, sanitizes inputs, and handles exceptions.???"
2525,_get_table_parts,"def _get_table_parts(cls, table_name: str) -> tuple[str | None, str | None, str]:
        
        table_parts = table_name.split(""."")
        if len(table_parts) == 3:
            return table_parts
        elif len(table_parts) == 2:
            return None, *table_parts
        elif len(table_parts) == 1:
            return None, None, *table_parts
        else:
            raise ValueError(f""Invalid table name: {table_name}"")",Returns the table parts from the table name.,"???Parse table name into schema, database, and table components.???"
2526,extract_prefixes,"def extract_prefixes(cls, values):
        
        query_string = values['queryString']
        if query_string:
            # Match the SOURCE ... pattern and extract the content inside parentheses
            source_match = re.search(r'SOURCE\s+logGroups\((.*?)\)', query_string)
            if source_match:
                content = source_match.group(1)
                # Extract namePrefix and its values
                prefix_match = re.search(r'namePrefix:\s*\[(.*?)\]', content)
                if prefix_match:
                    # Split the prefixes and strip whitespace and quotes
                    values['logGroupPrefixes'] = {
                        p.strip().strip('\'""') for p in prefix_match.group(1).split(',')
                    }
        return values","Extract log group prefixes by parsing the SOURCE command of the query string, if present.",???Extract log group prefixes from query string pattern in values dictionary???
2527,process_mmlu,"def process_mmlu(item: Dict[str, Any]) -> Tuple[str, str]:
    
    question = item['question']
    choices = [item['choices'][i] for i in range(len(item['choices']))]
    formatted_question = question + ""\n"" + ""\n"".join([f""{chr(65+i)}. {choice}"" for i, choice in enumerate(choices)])
    answer = chr(65 + item['answer'])  # Convert to A, B, C, D format
    return formatted_question, answer",Process MMLU dataset with multiple choice format.,"???  
Format multiple-choice questions and determine the correct answer.  
???"
2528,get_branding_config,"def get_branding_config(self, script_path: str | None = None) -> dict[str, Any]:
        
        branding = {
            ""name"": ""Preswald"",
            ""logo"": ""/images/logo.png"",
            ""favicon"": f""/images/favicon.ico?timestamp={time.time()}"",
            ""primaryColor"": ""#000000"",
        }

        if script_path:
            try:
                script_dir = os.path.dirname(script_path)
                config_path = os.path.join(script_dir, ""preswald.toml"")
                if os.path.exists(config_path):
                    config = toml.load(config_path)
                    logger.info(f""Loading config from {config_path}"")

                    if ""branding"" in config:
                        branding_config = config[""branding""]
                        branding[""name""] = branding_config.get(""name"", branding[""name""])
                        self._handle_logo(branding_config, script_dir, branding)
                        self._handle_favicon(branding_config, script_dir, branding)
                        branding[""primaryColor""] = branding_config.get(
                            ""primaryColor"", branding[""primaryColor""]
                        )
            except Exception as e:
                logger.error(f""Error loading branding config: {e}"")
                self._ensure_default_assets()

        logger.info(f""Final branding configuration: {branding}"")
        return branding",Get branding configuration from config file or defaults,???Load and merge branding settings from a configuration file if available???
2529,custom_json_serializer,"def custom_json_serializer(obj):
    
    if isinstance(obj, datetime):
        return obj.isoformat()
    if isinstance(obj, stripe.StripeObject):
        return json.loads(str(obj))
    raise TypeError(f""Object of type {type(obj)} is not JSON serializable"")",Custom JSON serializer for objects not serializable by default json code,"???  
Defines a function to serialize datetime and Stripe objects into JSON-compatible formats.  
???"
2530,loss_cardinality,"def loss_cardinality(self, outputs, targets, indices, num_boxes):
        
        pred_logits = outputs['pred_logits']
        device = pred_logits.device
        tgt_lengths = torch.as_tensor([len(v[""labels""]) for v in targets], device=device)
        # Count the number of predictions that are NOT ""no-object"" (which is the last class)
        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)
        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())
        losses = {'cardinality_error': card_err}
        return losses","Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes",???Calculate cardinality error between predicted and target object counts.???
2531,_load_local_spec,"def _load_local_spec(self) -> dict[str, Any]:
        
        try:
            with open(LOCAL_SPEC_PATH) as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f""Local spec not found at {LOCAL_SPEC_PATH}"")
            raise
        except json.JSONDecodeError as e:
            logger.error(f""Invalid JSON in local spec: {e}"")
            raise",Load OpenAPI spec from local file.,"??? Load and parse local JSON configuration, handling errors. ???"
2532,get_available_actions,"def get_available_actions(self):
        
        html_obj = self._parse_html()

        # Collect search bar, buttons, links, and options as clickables
        search_bar = html_obj.find(id=""search_input"")
        has_search_bar = True if search_bar is not None else False
        buttons = html_obj.find_all(class_=""btn"")
        product_links = html_obj.find_all(class_=""product-link"")
        buying_options = html_obj.select('input[type=""radio""]')

        self.text_to_clickable = {
            f""{b.get_text()}"".lower(): b for b in buttons + product_links
        }
        for opt in buying_options:
            opt_value = opt.get(""value"")
            self.text_to_clickable[f""{opt_value}""] = opt
        return dict(
            has_search_bar=has_search_bar,
            clickables=list(self.text_to_clickable.keys()),
        )",Returns list of available actions at the current step,???Extracts interactive elements from HTML for user actions mapping.???
2533,exclude_keys,"def exclude_keys(d: dict[str, Any], keys: set[str]) -> dict[str, Any]:
    
    return {k: v for k, v in d.items() if k not in keys}",Returns a copy of the dictionary without the keys in the set.,???Filter dictionary by removing specified keys???
2534,resolve_case_response_data,"def resolve_case_response_data() -> Dict[str, Any]:
    
    return {
        ""initial_case_status"": ""opened"",
        ""final_case_status"": ""resolved"",
        ""status"": ""success"",
        ""message"": ""Support case resolved successfully: case-12345678910-2013-c4c1d2bf33c5cf47"",
    }",Return a dictionary with sample resolve case response data.,???Generate a response dictionary indicating successful case resolution.???
2535,get_pr_details,"def get_pr_details(self, owner: str, repo: str, pr_number: int) -> Dict[str, Any]:
        
        url = f""{self.config.github.base_url}/repos/{owner}/{repo}/pulls/{pr_number}""
        response = self.github_session.get(url)
        response.raise_for_status()
        return response.json()",Get PR details from GitHub API.,???Fetches and returns details of a specific GitHub pull request by its number.???
2536,_load_plugin_commands,"def _load_plugin_commands(self) -> None:
        
        try:
            eps = entry_points(group=""quantalogic.shell.commands"")
            for ep in eps:
                try:
                    cmd_func = ep.load()
                    self.command_registry.register(ep.name, cmd_func, f""Plugin command: {ep.name}"", args=None)
                except Exception as e:
                    console.print(Panel(f""Failed to load plugin command {ep.name}: {e}"", title=""Error"", border_style=""red""))
        except Exception as e:
            console.print(Panel(f""Error retrieving command entry points: {e}"", title=""Error"", border_style=""red""))",Load plugin commands from entry points.,"???Load and register plugin commands, handling errors gracefully.???"
2537,print_trainable_parameters,"def print_trainable_parameters(self) -> None:
    
    # Check embed_tokens
    is_embed_trainable = any(
        param.requires_grad for param in self.embed_tokens.parameters()
    )
    print(f""LLM Module - Embed Tokens Trainable: {is_embed_trainable}"")

    # Check each decoder layer
    trainable_layers = []
    non_trainable_layers = []

    for layer_idx, layer in enumerate(self.layers):
        is_trainable = any(param.requires_grad for param in layer.parameters())
        if is_trainable:
            trainable_layers.append(layer_idx)
        else:
            non_trainable_layers.append(layer_idx)

    # Print layer status
    print(
        f""LLM Module - Trainable Layer Indices: {trainable_layers if trainable_layers else 'None'}""
    )
    print(
        f""LLM Module - Non-Trainable Layer Indices: {non_trainable_layers if non_trainable_layers else 'None'}""
    )","Prints the trainable status of all LLM components including embeddings, layers, and normalization.",???Determine and display trainability status of embedding tokens and decoder layers in a model.???
2538,_construct_request,"def _construct_request(self, reqid) -> dict:
        
        return {
            ""app"": {
                ""appid"": f""{self.appid}"",
                ""cluster"": self.cluster,
                ""token"": self.access_token,
            },
            ""user"": {
                ""uid"": str(uuid.uuid4()),
            },
            ""request"": {
                ""reqid"": reqid,
                ""show_utterances"": False,
                ""sequence"": 1,
                ""boosting_table_name"": self.boosting_table_name,
                ""correct_table_name"": self.correct_table_name,
            },
            ""audio"": {
                ""format"": ""raw"",
                ""rate"": 16000,
                ""language"": ""zh-CN"",
                ""bits"": 16,
                ""channel"": 1,
                ""codec"": ""raw"",
            },
        }",Construct the request payload.,??? Construct a request dictionary for an application with user and audio details. ???
2539,send_message,"def send_message(session_service: VertexAiSessionService, resource_id: str, message: str) -> None:
    

    session = asyncio.run(session_service.create_session(
            app_name=resource_id,
            user_id=""traveler0115""
        )
    )

    remote_agent = agent_engines.get(resource_id)

    print(f""Trying remote agent: {resource_id}"")
    for event in remote_agent.stream_query(
        user_id=""traveler0115"",
        session_id=session.id,
        message=message,
    ):
        print(event)
    print(""Done."")",Send a message to the deployed agent.,???The function initiates a session and streams messages to a remote agent using a session service.???
2540,get_latest_tweets,"def get_latest_tweets(self,
                          username: str,
                          count: int = 10,
                          **kwargs) -> list:
        
        logger.debug(f""Getting latest tweets for {username}, count: {count}"")

        credentials = self._get_credentials()
        params = {
            ""tweet.fields"": ""created_at,text"",
            ""max_results"": min(count, 100),
            ""query"": f""from:{username} -is:retweet -is:reply""
        }

        response = self._make_request('get',
                                      f""tweets/search/recent"",
                                      params=params)

        tweets = response.get(""data"", [])
        logger.debug(f""Retrieved {len(tweets)} tweets"")
        return tweets",Get latest tweets for a user,???Fetch recent tweets for a user using API with specified count.???
2541,is_valid_email,"def is_valid_email(email: str) -> bool:
    
    pattern = r""^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$""
    return re.match(pattern, email) is not None",Simple regex-based email validation.,???Determine if a string is a valid email format using regex.???
2542,get_per_obj_mask,"def get_per_obj_mask(mask):
    
    object_ids = np.unique(mask)
    object_ids = object_ids[object_ids > 0].tolist()
    per_obj_mask = {object_id: (mask == object_id) for object_id in object_ids}
    return per_obj_mask",Split a mask into per-object masks.,???Generate binary masks for each unique object in the input mask.???
2543,generate_policies_graph,"def generate_policies_graph(self, override=False):
        
        logger = get_logger()
        if override or not hasattr(self, 'flows'):
            logger.info(f""{ConsoleColor.WHITE}Step 1: Breaking prompt to flows{ConsoleColor.RESET}"")
            self.flows = self.extract_flows()
            logger.info(f""{ConsoleColor.WHITE}Finish step 1{ConsoleColor.RESET}"")
        if override or not hasattr(self, 'policies'):
            logger.info(f""{ConsoleColor.WHITE}Step 2: Breaking each flow to policies{ConsoleColor.RESET}"")
            self.policies = self.extract_policies()
            logger.info(f""{ConsoleColor.WHITE}Finish step 2{ConsoleColor.RESET}"")
        if override or not hasattr(self, 'relations'):
            logger.info(f""{ConsoleColor.WHITE}Step 3: Building the relations graph{ConsoleColor.RESET}"")
            self.extract_graph()
            logger.info(f""{ConsoleColor.WHITE}Finish step 3{ConsoleColor.RESET}"")",Generate the policies graph,"???Generate a policies graph by extracting flows, policies, and building relations???"
2544,check_metadata_serializable_all_types,"def check_metadata_serializable_all_types(metadata: dict):
    
    def convert_to_serializable(obj):
        if isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_to_serializable(v) for v in obj]
        elif isinstance(obj, (str, int, float, bool, type(None))):
            return obj
        else:
            return str(obj)

    try:
        json.dumps(metadata)
        return metadata
    except (TypeError, OverflowError) as e:
        print(f""[WARNING] Metadata is not JSON serializable, error: {str(e)}"")
        # Convert non-serializable values to strings recursively
        converted_metadata = convert_to_serializable(metadata)
        print(
            f""[WARNING] Metadata now converted to be JSON serializable: {converted_metadata}""
        )
        return converted_metadata","Ensure metadata is JSON serializable, if not, convert non-serializable values to strings recursively","???Ensure metadata is JSON serializable, converting non-serializable types to strings.???"
2545,list_litellm_models,"def list_litellm_models() -> List[str]:
    
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        # Handle JSON structures: top-level dict or list of entries
        if isinstance(data, dict):
            return [mid for mid in data.keys() if mid != 'sample_spec']
        models = []
        for entry in data:
            provider = entry.get(""litellm_provider"")
            model_name = entry.get(""model"")
            if provider and model_name and model_name != 'sample_spec':
                models.append(f""{provider}/{model_name}"")
        return models
    except requests.RequestException as e:
        print(f""Error fetching the model list: {e}"")
        return []",Fetches and returns a list of all models supported by litellm.,"???Fetch and parse model names from a remote JSON endpoint, handling errors gracefully.???"
2546,set_command,"def set_command(key: str, value: str):
    
    config = _get_user_config()
    if not config.has_key(key):
        rich.print(f""[red]Error: Configuration key '{key}' not found[/red]"")
        return

    cur_value = config.get(key)
    if cur_value is None or str(cur_value).lower() != value.lower():
        try:
            config.set(key, value)
        except Exception as e:
            logging.exception(e)
            rich.print(f""[red]{e}[/red]"")
            return

    rich.print(f""[green]Successfully set {key}=[magenta]{value}[/magenta] and saved to {ENV_FILENAME}[/green]"")",Set a configuration value and write to .env,???Update configuration key-value pair with error handling and feedback display.???
2547,serialize,"def serialize(self) -> dict[str, Any]:
        
        return {
            ""bash_command_mode"": self._bash_command_mode.serialize(),
            ""file_edit_mode"": self._file_edit_mode.serialize(),
            ""write_if_empty_mode"": self._write_if_empty_mode.serialize(),
            ""whitelist_for_overwrite"": {
                k: v.serialize() for k, v in self._whitelist_for_overwrite.items()
            },
            ""mode"": self._mode,
            ""workspace_root"": self._workspace_root,
            ""chat_id"": self._current_thread_id,
        }",Serialize BashState to a dictionary for saving,???Serialize object state into a dictionary with configuration details.???
2548,get_stats_api,"def get_stats_api():
    
    try:
        # Import here to avoid circular imports
        from ..stats_manager import get_stats
        
        # Get stats from stats_manager
        stats = get_stats()
        logger.debug(f""Retrieved stats for API response: {stats}"")
        
        # Return success response with stats
        return jsonify({""success"": True, ""stats"": stats})
    except Exception as e:
        logger.error(f""Error retrieving stats: {e}"", exc_info=True)
        return jsonify({""success"": False, ""error"": str(e)}), 500",API endpoint to get media statistics,"???Retrieve and return statistical data via an API, handling errors gracefully.???"
2549,replay,"def replay():
    
    try:
        WebresearcherCrew().crew().replay(task_id=sys.argv[1])

    except Exception as e:
        raise Exception(f""An error occurred while replaying the crew: {e}"")",Replay the crew execution from a specific task.,"???  
Executes a task replay using a crew system, handling exceptions.  
???"
2550,remove_project,"def remove_project(self, name: str) -> None:
        
        if name not in self.config.projects:  # pragma: no cover
            raise ValueError(f""Project '{name}' not found"")

        if name == self.config.default_project:  # pragma: no cover
            raise ValueError(f""Cannot remove the default project '{name}'"")

        del self.config.projects[name]
        self.save_config(self.config)",Remove a project from the configuration.,"???Remove a project from configuration, ensuring it's not the default.???"
2551,extract_numbers,"def extract_numbers(string):
    
    # Pattern for numbers with commas
    pattern_commas = r""-?\b\d{1,3}(?:,\d{3})+\b""
    # Pattern for scientific notation
    pattern_scientific = r""-?\d+(?:\.\d+)?[eE][+-]?\d+""
    # Pattern for simple numbers without commas
    pattern_simple = r""-?(?:\d+\.\d+|\.\d+|\d+\b)(?![eE][+-]?\d+)(?![,\d])""

    # Extract numbers with commas
    numbers_with_commas = re.findall(pattern_commas, string)
    # Extract numbers in scientific notation
    numbers_scientific = re.findall(pattern_scientific, string)
    # Extract simple numbers without commas
    numbers_simple = re.findall(pattern_simple, string)

    # Combine all extracted numbersz
    all_numbers = numbers_with_commas + numbers_scientific + numbers_simple
    return all_numbers",Exact all forms of numbers from a string with regex.,???Extracts various formatted numbers from a string using regex patterns.???
2552,api_get_app_cycle_status,"def api_get_app_cycle_status(app_name):
    
    try:
        from src.primary.cycle_tracker import get_cycle_status
        status = get_cycle_status(app_name)
        return jsonify(status), 200
    except Exception as e:
        web_logger = get_logger(""web_server"")
        web_logger.error(f""Error getting cycle status for {app_name}: {e}"")
        return jsonify({""error"": f""Failed to retrieve cycle status for {app_name}.""}), 500",API endpoint to get cycle status for a specific app.,"???Retrieve and return application cycle status, handling errors with logging.???"
2553,set_start_node,"def set_start_node(self, name: str) -> None:
        
        if name not in self.workflow.nodes:
            raise ValueError(f""Node '{name}' does not exist"")
        self.workflow.workflow.start = name",Set the start node of the workflow.,???Set the starting node of a workflow if it exists???
2554,get_agent_tools,"def get_agent_tools(self, agent_name: str) -> ast.List:
        
        method = asttools.find_method(self.get_agent_methods(), agent_name)
        if method is None:
            raise ValidationError(f""Agent method `{agent_name}` does not exist in {ENTRYPOINT}"")

        # find the `functions` keyword argument to `Agent` inside method
        try:
            agent_init = asttools.find_method_calls(method, 'Agent')[0]
        except IndexError:
            raise ValidationError(f""Agent method `{agent_name}` does not instantiate `Agent` in {ENTRYPOINT}"")

        tools_kwarg = asttools.find_kwarg_in_method_call(agent_init, 'functions')

        if not tools_kwarg:
            raise ValidationError(f""`Agent` does not have a keyword argument `functions` in {ENTRYPOINT}"")

        if not isinstance(tools_kwarg.value, ast.List):
            raise ValidationError(f""`Agent` must define a list for kwarg `tools` in {ENTRYPOINT}"")

        return tools_kwarg.value",Get the list of tools used by an agent as an AST List node.,???Extracts and validates a list of tools from an agent's method definition.???
2555,download_cif_file,"def download_cif_file(pdb_id: str, directory: Path) -> Path:
    
    outfile = directory / f""{pdb_id}.cif.gz""
    download_if_not_exists(source_url, outfile)
    assert outfile.exists() and outfile.stat().st_size > 0
    return outfile",Download the cif file for the given PDB ID from RCSB into the directory.,"???  
Download and verify a CIF file from a source URL using a PDB identifier.  
???"
2556,pretty_name,"def pretty_name(
    name: str, pretty_names: dict | str = {}  # type: ignore pylint: disable=dangerous-default-value
) -> str:
    

    # reading from yaml and caching the dictionary
    label_file: Path = evaluation_path() / ""labels.yaml""
    if isinstance(pretty_names, str):
        label_file = Path(pretty_names)

    if pretty_names == {} or isinstance(pretty_names, str):
        yaml_parser = YAMLParser()
        yaml_content = yaml_parser.parse_yaml(label_file)
        pretty_names: dict[str, str] = yaml_content[""names""]

    # applying pretty names
    name_without_yaml_prefix = name.split(""."")[-1]
    if name in pretty_names.keys():
        name = pretty_names[name]
    elif name_without_yaml_prefix in pretty_names.keys():
        name = pretty_names[name_without_yaml_prefix]
    else:
        name = name.replace(""_"", "" "").title()
    return name","Tries to use a mapping for the name, else will do some general replacement.",???Transform input name using a YAML-based pretty name mapping or default formatting.???
2557,setup_urls,"def setup_urls(self, httpbin):
        
        self.status_200 = f'{httpbin.url}/status/200'
        self.status_404 = f'{httpbin.url}/status/404'
        self.status_501 = f'{httpbin.url}/status/501'
        self.basic_url = f'{httpbin.url}/get'
        self.html_url = f'{httpbin.url}/html'
        self.delayed_url = f'{httpbin.url}/delay/10'  # 10 Seconds delay response
        self.cookies_url = f""{httpbin.url}/cookies/set/test/value""",Fixture to set up URLs for testing,"???  
Configure endpoint URLs for various HTTP status and response scenarios.  
???"
2558,connect_db,"def connect_db(self) -> None:
        
        if self.name in self.falkordb.list_graphs():
            try:
                self.ontology = self._load_ontology_from_db()
            except Exception:
                warnings.warn(""Graph Ontology is not loaded."")

            if self.ontology is None:
                raise ValueError(f""Ontology of the knowledge graph '{self.name}' can't be None."")

            self.knowledge_graph = KnowledgeGraph(
                name=self.name,
                host=self.host,
                port=self.port,
                username=self.username,
                password=self.password,
                model_config=self.model_config,
                ontology=self.ontology,
            )

            # Establishing a chat session will maintain the history
            self._chat_session = self.knowledge_graph.chat_session()
        else:
            raise ValueError(f""Knowledge graph '{self.name}' does not exist"")",Connect to an existing knowledge graph.,"???Establishes a connection to a knowledge graph database, loading ontology and initiating a chat session.???"
2559,get_playlist_track_ids_from_db,"def get_playlist_track_ids_from_db(playlist_spotify_id: str):
    
    table_name = f""playlist_{playlist_spotify_id.replace('-', '_')}""
    track_ids: set[str] = set()
    try:
        with _get_playlists_db_connection() as conn:  # Use playlists connection
            cursor = conn.cursor()
            cursor.execute(
                f""SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}';""
            )
            if cursor.fetchone() is None:
                logger.warning(
                    f""Track table {table_name} does not exist in {PLAYLISTS_DB_PATH}. Cannot fetch track IDs.""
                )
                return track_ids
            cursor.execute(
                f""SELECT spotify_track_id FROM {table_name} WHERE is_present_in_spotify = 1""
            )
            rows = cursor.fetchall()
            for row in rows:
                track_ids.add(row[""spotify_track_id""])
        return track_ids
    except sqlite3.Error as e:
        logger.error(
            f""Error retrieving track IDs for playlist {playlist_spotify_id} from table {table_name} in {PLAYLISTS_DB_PATH}: {e}"",
            exc_info=True,
        )
        return track_ids",Retrieves all track Spotify IDs from a specific playlist's tracks table in playlists.db.,???Retrieve Spotify track IDs from a database table for a given playlist ID.???
2560,get_model_loader,"def get_model_loader(load_config: LoadConfig) -> BaseModelLoader:
    

    if isinstance(load_config.load_format, type):
        return load_config.load_format(load_config)

    if load_config.load_format == LoadFormat.AUTO:
        update_megatron_weight_loader()
        return MegatronLoader(load_config)

    # NOTE(sgm): change the weight_loader function in runtime
    if load_config.load_format == LoadFormat.MEGATRON:
        update_megatron_weight_loader()
        return MegatronLoader(load_config)

    if load_config.load_format == LoadFormat.HF:
        update_hf_weight_loader()
        return HFLoader(load_config)

    if load_config.load_format == LoadFormat.DTENSOR:
        update_dtensor_weight_loader()
        return DTensorLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_HF:
        update_hf_weight_loader()
        return DummyModelLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_MEGATRON:
        update_megatron_weight_loader()
        return DummyModelLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_DTENSOR:
        update_dtensor_weight_loader()
        return DummyModelLoader(load_config)

    raise ValueError(""load format not supported in verl: {}, only support {} and {}"".format(
        load_config.load_format, LoadFormat.MEGATRON, LoadFormat.HF))",Get a model loader based on the load format.,???Determine and return the appropriate model loader based on the specified configuration.???
2561,create_logger,"def create_logger(logging_dir):
    
    if dist.get_rank() == 0:  # real logger
        logging.basicConfig(
            level=logging.INFO,
            format=""[\033[34m%(asctime)s\033[0m] %(message)s"",
            datefmt=""%Y-%m-%d %H:%M:%S"",
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(f""{logging_dir}/log.txt""),
            ],
        )
        logger = logging.getLogger(__name__)
    else:  # dummy logger (does nothing)
        logger = logging.getLogger(__name__)
        logger.addHandler(logging.NullHandler())
    return logger",Create a logger that writes to a log file and stdout.,???Initialize a logger for distributed systems with file and stream handlers.???
2562,add_flow,"def add_flow(
        from_node_id: str, to_node_id: str, amount: Decimal, currency, is_income: bool
    ) -> None:
        
        total_volume = total_volume_by_currency.get(currency, Decimal(""0""))
        percentage = (amount / total_volume) * 100 if total_volume else 0
        scaled_flow = percentage / 100

        flows.append(
            {
                ""from_node"": from_node_id,
                ""to_node"": to_node_id,
                ""flow"": float(scaled_flow),
                ""currency"": {
                    ""code"": currency.code,
                    ""name"": currency.name,
                    ""prefix"": currency.prefix,
                    ""suffix"": currency.suffix,
                    ""decimal_places"": currency.decimal_places,
                },
                ""original_amount"": float(amount),
                ""percentage"": float(percentage),
            }
        )",Add flow with percentage based on total transaction volume for the specific currency.,???Add a financial flow between nodes with currency details and percentage calculation.???
2563,_generate_sign_parameters,"def _generate_sign_parameters(self, url: str, pf: str = '4', appvr: str = '6.6.0', tdid='') -> \
            Tuple[str, str]:
        
        current_time = str(int(time.time()))
        data = {
            'url': url,
            'current_time': current_time,
            'pf': pf,
            'appvr': appvr,
            'tdid': self.tdid
        }
        headers = {
            'User-Agent': f""VideoCaptioner/{VERSION}"",
            'tdid': self.tdid,
            't': current_time
        }
        # Replace with your actual endpoint URL
        try:
            response = requests.post(get_sign_url, json=data, headers=headers)
            response.raise_for_status()
            response_data = response.json()
            sign = response_data.get('sign')
            if not sign:
                raise ValueError(""No 'sign' in response"")
        except requests.exceptions.RequestException as e:
            raise SystemExit(f""HTTP Request failed: {e}"")
        except ValueError as ve:
            raise SystemExit(f""Invalid response: {ve}"")
        return sign.lower(), current_time",Generate signature and timestamp via an HTTP request,???Generate authentication parameters for a URL request with error handling.???
2564,calculate_stats,"def calculate_stats() -> dict:
    
    global PDF_TESTS

    total_tests = 0
    null_status = 0
    verified_status = 0
    rejected_status = 0

    for pdf_tests in PDF_TESTS.values():
        total_tests += len(pdf_tests)

        for test in pdf_tests:
            status = test.get(""checked"")
            if status is None:
                null_status += 1
            elif status == ""verified"":
                verified_status += 1
            elif status == ""rejected"":
                rejected_status += 1

    completion = 0
    if total_tests > 0:
        completion = (verified_status + rejected_status) / total_tests * 100

    return {""total"": total_tests, ""null"": null_status, ""verified"": verified_status, ""rejected"": rejected_status, ""completion"": completion}",Calculate statistics for all tests in the dataset.,???Calculate and return PDF test status statistics and completion percentage.???
2565,check_cache_disk,"def check_cache_disk(self, safety_margin=0.5):
        
        import shutil

        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        n = min(self.ni, 30)  # extrapolate from 30 random images
        for _ in range(n):
            im_file = random.choice(self.im_files)
            im = cv2.imread(im_file)
            if im is None:
                continue
            b += im.nbytes
            if not os.access(Path(im_file).parent, os.W_OK):
                self.cache = None
                LOGGER.info(f""{self.prefix}Skipping caching images to disk, directory not writeable ⚠️"")
                return False
        disk_required = b * self.ni / n * (1 + safety_margin)  # bytes required to cache dataset to disk
        total, used, free = shutil.disk_usage(Path(self.im_files[0]).parent)
        if disk_required > free:
            self.cache = None
            LOGGER.info(
                f""{self.prefix}{disk_required / gb:.1f}GB disk space required, ""
                f""with {int(safety_margin * 100)}% safety margin but only ""
                f""{free / gb:.1f}/{total / gb:.1f}GB free, not caching images to disk ⚠️""
            )
            return False
        return True",Check image caching requirements vs available disk space.,???Check if sufficient disk space is available to cache images with a safety margin.???
2566,load_events,"def load_events(path: Path) -> List[Event]:
    
    events = []
    print(f""Loading events from {path}"")  # Debug
    try:
        with open(path) as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        raw_event = json.loads(line)
                        # Convert from log format to event format
                        event = Event(
                            type=raw_event.get(""level"", ""info"").lower(),
                            namespace=raw_event.get(""namespace"", """"),
                            message=raw_event.get(""message"", """"),
                            timestamp=datetime.fromisoformat(raw_event[""timestamp""]),
                            data=raw_event.get(""data"", {}),
                        )
                        events.append(event)
                    except Exception as e:
                        print(f""Error on line {line_num}: {e}"")
                        print(f""Line content: {line.strip()}"")
                        raise
    except Exception as e:
        print(f""Error loading file: {e}"")
        raise

    print(f""Loaded {len(events)} events"")  # Debug
    return events",Load events from JSONL file.,???Parse and convert log file entries into structured event objects.???
2567,_find_conversion_path,"def _find_conversion_path(
        graph, from_id, to_id
    ) -> Tuple[Optional[list], Optional[Decimal]]:
        
        if from_id not in graph or to_id not in graph:
            return None, None

        queue = [(from_id, [from_id], Decimal(""1""))]
        visited = {from_id}

        while queue:
            current, path, current_rate = queue.pop(0)

            if current == to_id:
                return path, current_rate

            for neighbor, rate in graph.get(current, {}).items():
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, path + [neighbor], current_rate * rate))

        return None, None",Find the shortest path between currencies using breadth-first search,???Finds conversion path and rate in a weighted graph using BFS.???
2568,get_balance,"def get_balance(self, token_address: Optional[str] = None) -> float:
        
        try:
            account = self._get_current_account()
            
            if token_address is None or token_address.lower() == self.NATIVE_TOKEN.lower():
                raw_balance = self._web3.eth.get_balance(account.address)
                return self._web3.from_wei(raw_balance, 'ether')
            
            contract = self._web3.eth.contract(
                address=Web3.to_checksum_address(token_address), 
                abi=ERC20_ABI 
            )
            decimals = contract.functions.decimals().call()
            raw_balance = contract.functions.balanceOf(account.address).call()
            return raw_balance / (10 ** decimals)
            
        except Exception as e:
            logger.error(f""Failed to get balance: {str(e)}"")
            return 0",Get native or token balance for the configured wallet,???Retrieve cryptocurrency balance for a given account and token address???
2569,select_cuda_devices,"def select_cuda_devices(device_key):
    
    if torch.cuda.is_available():
        cuda_devices = [f""cuda:{i}"" for i in range(torch.cuda.device_count())]
    else:
        cuda_devices = [t(""No CUDA devices available"")]
    selected_devices = st.selectbox(
        t('Select CUDA Devices for') + f"" {device_key}"",
        cuda_devices
    )
    return selected_devices",Create a selectbox for CUDA device selection.,???Selects available CUDA devices for a given key using a dropdown interface.???
2570,_board_to_string,"def _board_to_string(self, board: list[list[str]]) -> str:
        
        size = len(board)
        # Column labels
        cols = ""   "" + "" "".join(chr(ord(""A"") + i) for i in range(size)) + ""\n""
        # Board with row numbers
        rows = [f""{size-i:2d} {' '.join(row)}"" for i, row in enumerate(board)]
        return cols + ""\n"".join(rows)",Convert board to string representation,"???  
Convert a 2D board array into a formatted string with labels.  
???"
2571,mock_related_files_repository,"def mock_related_files_repository():
    
    with patch('ra_aid.database.repositories.related_files_repository.related_files_repo_var') as mock_repo_var:
        # Setup a mock repository
        mock_repo = MagicMock()
        
        # Create a dictionary to simulate stored files
        related_files = {}
        
        # Setup get_all method to return the files dict
        mock_repo.get_all.return_value = related_files
        
        # Setup format_related_files method
        mock_repo.format_related_files.return_value = [f""ID#{file_id} {filepath}"" for file_id, filepath in sorted(related_files.items())]
        
        # Make the mock context var return our mock repo
        mock_repo_var.get.return_value = mock_repo
        
        yield mock_repo",Mock the RelatedFilesRepository to avoid database operations during tests,???Mock a file repository to simulate and format stored file data for testing purposes.???
2572,is_configured,"def is_configured(self, verbose = False) -> bool:
        
        try:
            load_dotenv()
            api_key = os.getenv('GROQ_API_KEY')
            if not api_key:
                return False

            client = OpenAI(
                api_key=api_key,
            )
            client.models.list()
            return True
            
        except Exception as e:
            if verbose:
                logger.debug(f""Configuration check failed: {e}"")
            return False",Check if Groq API key is configured and valid,"???Check if API configuration is valid and accessible, logging errors if verbose.???"
2573,handle_error,"def handle_error(self, message: str, error=None) -> ImportResult:
        
        import logging

        logger = logging.getLogger(__name__)

        error_message = f""{message}""
        if error:
            error_message += f"": {str(error)}""

        logger.error(error_message)
        return ImportResult(
            import_count={},
            success=False,
            error_message=error_message,
        )",Implement the abstract handle_error method.,???Log error details and return a failed import result.???
2574,step_over,"def step_over(
    skip_list: typing.Optional[typing.List[LocationRange]] = None,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    if skip_list is not None:
        params[""skipList""] = [i.to_json() for i in skip_list]
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Debugger.stepOver"",
        ""params"": params,
    }
    json = yield cmd_dict",Steps over the statement.,"???Generate a debugger command to step over code, optionally skipping specified ranges.???"
2575,transfer,"def transfer(
        self,
        to_address: str,
        amount: float,
        token_address: Optional[str] = None
    ) -> str:
        
        try:
            account = self._get_current_account()

            # Check balance including gas cost since Monad charges on gas limit
            gas_cost = Web3.to_wei(MONAD_BASE_GAS_PRICE * 21000, 'gwei')
            gas_cost_eth = float(self._web3.from_wei(gas_cost, 'ether'))
            total_required = float(amount) + gas_cost_eth
            
            current_balance = float(self.get_balance(token_address=token_address))
            if current_balance < total_required:
                raise ValueError(
                    f""Insufficient balance. Required: {total_required}, Available: {current_balance}""
                )

            # Prepare and send transaction
            tx = self._prepare_transfer_tx(to_address, amount, token_address)
            signed = account.sign_transaction(tx)
            tx_hash = self._web3.eth.send_raw_transaction(signed.rawTransaction)
            
            tx_url = self._get_explorer_link(tx_hash.hex())
            return f""Transaction sent: {tx_url}""

        except Exception as e:
            logger.error(f""Transfer failed: {str(e)}"")
            raise",Transfer tokens with Monad-specific balance validation,"???Facilitates cryptocurrency transfer by verifying balance, preparing transaction, and returning transaction link.???"
2576,_load_spec,"def _load_spec(
      self, spec_str: str, spec_type: Literal[""json"", ""yaml""]
  ) -> Dict[str, Any]:
    
    if spec_type == ""json"":
      return json.loads(spec_str)
    elif spec_type == ""yaml"":
      return yaml.safe_load(spec_str)
    else:
      raise ValueError(f""Unsupported spec type: {spec_type}"")",Loads the OpenAPI spec string into a dictionary.,"???Function parses a string into a dictionary based on specified format, JSON or YAML.???"
2577,copy_texture_file,"def copy_texture_file(xml_path: str, texture_path: str, dst_mesh_dir: str) -> str:
    
    # Get absolute paths
    xml_dir = os.path.dirname(os.path.abspath(xml_path))
    abs_texture_path = os.path.abspath(os.path.join(xml_dir, texture_path))

    # Create textures subdirectory in the destination
    dst_texture_dir = os.path.join(dst_mesh_dir, ""textures"")
    os.makedirs(dst_texture_dir, exist_ok=True)

    # Get destination path
    texture_filename = os.path.basename(texture_path)
    dst_texture_path = os.path.abspath(os.path.join(dst_texture_dir, texture_filename))

    if os.path.exists(abs_texture_path):
        # Only copy if source and destination are different
        if abs_texture_path != dst_texture_path:
            try:
                shutil.copy2(abs_texture_path, dst_texture_path)
            except shutil.SameFileError:
                # File already exists at destination, that's okay
                pass
            except Exception as e:
                print(f""Warning: Failed to copy texture file: {e}"")

        # Return path relative to package
        return f""textures/{texture_filename}""
    else:
        print(f""Warning: Texture file not found: {abs_texture_path}"")
        return None",Copy texture file to destination directory and return the relative path.,"???Copy texture file to destination directory, ensuring unique paths and handling errors.???"
2578,setup_posthog,"def setup_posthog() -> Any | None:
    
    if not TELEMETRY_ENABLED:
        return None

    try:
        client: Any = posthog.Posthog(
            api_key=POSTHOG_API_KEY,
            host=POSTHOG_HOST,
            disable_geoip=False,
        )

        if not DEBUG_LOGGING:
            posthog_logger = logging.getLogger(""posthog"")
            posthog_logger.disabled = True

        return client
    except Exception as e:
        logger.debug(f""Failed to initialize PostHog: {e}"")
        return None",Set up the PostHog client if enabled.,"???Initialize PostHog client if telemetry is enabled, handling exceptions gracefully.???"
2579,translate,"def translate(self, text, ignore_cache=False, rate_limit_params: dict = None):
        
        self.translate_call_count += 1
        if not (self.ignore_cache or ignore_cache):
            try:
                cache = self.cache.get(text)
                if cache is not None:
                    self.translate_cache_call_count += 1
                    return cache
            except Exception as e:
                logger.debug(f""try get cache failed, ignore it: {e}"")
        _translate_rate_limiter.wait()
        translation = self.do_translate(text, rate_limit_params)
        if not (self.ignore_cache or ignore_cache):
            self.cache.set(text, translation)
        return translation","Translate the text, and the other part should call this method.",???Translate text with optional caching and rate limiting???
2580,renew_attention_paths,"def renew_attention_paths(old_list, n_shave_prefix_segments=0):
    
    mapping = []
    for old_item in old_list:
        new_item = old_item

        #         new_item = new_item.replace('norm.weight', 'group_norm.weight')
        #         new_item = new_item.replace('norm.bias', 'group_norm.bias')

        #         new_item = new_item.replace('proj_out.weight', 'proj_attn.weight')
        #         new_item = new_item.replace('proj_out.bias', 'proj_attn.bias')

        #         new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)

        mapping.append({""old"": old_item, ""new"": new_item})

    return mapping",Updates paths inside attentions to the new naming scheme (local renaming),"???  
Generate a mapping of old to new attention paths without modification.  
???"
2581,_process_tool_call_chunk,"def _process_tool_call_chunk(self, tool_call_data: Dict[str, Any], tool_calls: List[Dict[str, Any]]):
        
        try:
            if isinstance(tool_call_data, list):
                # Array of tool calls
                for tc_item in tool_call_data:
                    self._accumulate_tool_call(tc_item, tool_calls)
            elif isinstance(tool_call_data, dict):
                # Single tool call or function call
                if ""function_call"" in tool_call_data:
                    # Legacy function_call format - convert to tool_calls format
                    fc = tool_call_data[""function_call""]
                    converted = {
                        ""id"": f""call_{len(self._accumulated_tool_calls)}"",
                        ""type"": ""function"",
                        ""function"": fc
                    }
                    self._accumulate_tool_call(converted, tool_calls)
                else:
                    # Direct tool call
                    self._accumulate_tool_call(tool_call_data, tool_calls)
                    
        except Exception as e:
            logger.warning(f""Error processing tool call chunk: {e}"")",Process tool call chunk data and accumulate complete tool calls.,"???Process and accumulate tool call data into a structured list, handling exceptions.???"
2582,_extract_user_id,"def _extract_user_id(self, api_key: str) -> Optional[str]:
        
        if not api_key:
            return None
        try:
            user_id = api_key.split(""-"", 1)[0]
            return user_id
        except Exception:
            logger.warning(f""Invalid API key format: {api_key}"")
            return None",Extract user_id from API key,"???Extract user ID from API key, handling errors and logging warnings.???"
2583,_get_ligand_features,"def _get_ligand_features(
    struc: structure.Structure,
) -> Mapping[str, Mapping[str, np.ndarray | bytes]]:
  
  ligand_struc = struc.filter_to_entity_type(ligand=True)
  assert ligand_struc.coords is not None
  assert ligand_struc.atom_name is not None
  assert ligand_struc.atom_occupancy is not None

  ligand_features = {}
  for ligand_chain_id in ligand_struc.chains:
    idxs = np.where(ligand_struc.chain_id == ligand_chain_id)[0]
    if idxs.shape[0]:
      ligand_features[ligand_chain_id] = {
          'ligand_atom_positions': (
              ligand_struc.coords[idxs, :].astype(np.float32)
          ),
          'ligand_atom_names': ligand_struc.atom_name[idxs].astype(object),
          'ligand_atom_occupancies': (
              ligand_struc.atom_occupancy[idxs].astype(np.float32)
          ),
          'ccd_id': ligand_struc.res_name[idxs][0].encode(),
      }
  return ligand_features",Returns features for the ligands in this structure.,???Extracts and organizes ligand structural features into a nested dictionary format.???
2584,clear,"def clear(self) -> None:
        
        try:
            self._store = []
            logger.debug(""Cleared task history"")
        except Exception as e:
            logger.error(f""Error clearing task history: {e}"")",Clear the task history for a new task.,???This function resets a task history list and logs the action or any errors.???
2585,stop_loading,"def stop_loading() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Page.stopLoading"",
    }
    json = yield cmd_dict",Force the page stop all navigations and pending resource fetches.,???Generate a command to halt webpage loading and yield JSON response???
2586,_initialize_workflow_state,"def _initialize_workflow_state(
        self, item_id: str, target_sequence: str, ground_truth_binder: Optional[str]
    ) -> Dict:
        
        return {
            ""item_id"": item_id,
            ""current_internal_step"": 0,
            ""target_sequence"": target_sequence,
            ""ground_truth_binder_sequence"": ground_truth_binder,
            ""target_pdb_content"": None,
            ""target_chain_details"": None,
            ""binder_backbone_pdb_content"": None,
            ""designed_binder_sequence"": None,
            ""complex_pdb_content_path"": None,
            ""af2_multimer_plddt"": 0.0,
            ""target_structure_predicted"": False,
            ""binder_backbone_designed"": False,
            ""binder_sequence_designed"": False,
            ""complex_evaluated"": False,
            ""workflow_complete_flag"": False,
            ""last_tool_success"": True,
            ""cumulative_reward"": 0.0,
            ""turn_messages_history"": [],
            ""retry_count_this_internal_step"": 0,
            ""previous_tool_error_message"": None,
        }",Initializes or resets the state for a new workflow.,???Initialize and return a dictionary to track workflow state and progress.???
2587,inject_pod_failure,"def inject_pod_failure(self, microservices: List[str], duration: str = ""200s""):
        
        chaos_experiment = {
            ""apiVersion"": ""chaos-mesh.org/v1alpha1"",
            ""kind"": ""PodChaos"",
            ""metadata"": {""name"": ""pod-failure-experiment"", ""namespace"": self.namespace},
            ""spec"": {
                ""action"": ""pod-failure"",
                ""mode"": ""one"",
                ""duration"": duration,
                ""selector"": {
                    ""labelSelectors"": {""io.kompose.service"": "", "".join(microservices)}
                },
            },
        }

        self.create_chaos_experiment(chaos_experiment, ""pod-failure"")",Inject a pod failure fault.,???Simulate pod failure in specified microservices for a set duration using chaos engineering.???
2588,fetch_kernel_from_disk,"def fetch_kernel_from_disk(run_dir: str, level: int, problem_id: int, sample_id: int) -> str | None:
    
    kernel_path = os.path.join(run_dir, f""level_{level}_problem_{problem_id}_sample_{sample_id}_kernel.py"")
    
    if os.path.exists(kernel_path):
        return read_file(kernel_path)
    else:
        return None",Fetch kernel file from disk (stored in runs/{run_name}),??? Retrieve kernel file path based on directory and identifiers ???
2589,set_skip_all_pauses,"def set_skip_all_pauses(skip: bool) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""skip""] = skip
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Debugger.setSkipAllPauses"",
        ""params"": params,
    }
    json = yield cmd_dict","Makes page not interrupt on any pauses (breakpoint, exception, dom exception etc).",???Configure debugger to skip or not skip all pauses dynamically???
2590,get_available_models,"def get_available_models() -> Dict[str, dict]:
    
    # check whether the model list file exists:
    if not NEXA_MODEL_LIST_PATH.exists():
        st.error(""Model list file not found"")
        return {}  # empty dict

    try:
        # read model list from the JSON file:
        with open(NEXA_MODEL_LIST_PATH, ""r"") as f:
            available_models = json.load(f)
            return available_models

    except json.JSONDecodeError as e:
        logging.error(f""Invalid JSON in model list file: {e}"")
        return {}
    except Exception as e:
        logging.error(f""Error loading available models: {e}"")
        return {}",Get list of available computer vision (cv) models from the model list JSON file.,"???Retrieve and return available models from a JSON file, handling errors.???"
2591,get_default_config,"def get_default_config() -> dict[str, Any]:
	
	return {
		'model': {
			'name': None,
			'temperature': 0.0,
			'api_keys': {
				'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY', ''),
				'ANTHROPIC_API_KEY': os.getenv('ANTHROPIC_API_KEY', ''),
				'GOOGLE_API_KEY': os.getenv('GOOGLE_API_KEY', ''),
				'DEEPSEEK_API_KEY': os.getenv('DEEPSEEK_API_KEY', ''),
				'GROK_API_KEY': os.getenv('GROK_API_KEY', ''),
			},
		},
		'agent': {},  # AgentSettings will use defaults
		'browser': {
			'headless': True,
			'keep_alive': True,
			'ignore_https_errors': False,
		},
		'command_history': [],
	}",Return default configuration dictionary.,???Initialize default configuration with model settings and API keys.???
2592,list_strings_filter,"def list_strings_filter(
    offset: Annotated[int, ""Offset to start listing from (start at 0)""],
    count: Annotated[int, ""Number of strings to list (100 is a good default, 0 means remainder)""],
    filter: Annotated[str, ""Filter to apply to the list (required parameter, empty string for no filter). Case-insensitive contains or /regex/ syntax""],
) -> Page[String]:
    
    strings = []
    for item in idautils.Strings():
        try:
            string = str(item)
            if string:
                strings.append({
                    ""address"": hex(item.ea),
                    ""length"": item.length,
                    ""string"": string,
                })
        except:
            continue
    strings = pattern_filter(strings, filter, ""string"")
    return paginate(strings, offset, count)","List matching strings in the database (paginated, filtered)",???Filter and paginate a list of strings based on user-defined criteria.???
2593,get_upload_paths,"def get_upload_paths(input_dir, categories, skip_patterns, verbose=False):
    
    upload_paths = []
    
    for category in categories:
        category_path = os.path.join(input_dir, category)
        if not os.path.exists(category_path):
            logger.warning(f""Category directory '{category_path}' doesn't exist. Skipping."")
            continue
            
        # Add the category path itself
        upload_paths.append({
            'source_path': category_path,
            'target_path': category
        })
        
        if verbose:
            logger.info(f""Added category '{category}' for upload."")
    
    # Filter out paths that should be skipped
    filtered_paths = []
    for path_info in upload_paths:
        if not should_skip_path(path_info['source_path'], skip_patterns):
            filtered_paths.append(path_info)
        elif verbose:
            logger.info(f""Skipping path '{path_info['source_path']}'"")
    
    return filtered_paths",Get paths of all categories to upload.,"???Generate filtered upload paths from directories, excluding specified patterns.???"
2594,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Database.disable"",
    }
    json = yield cmd_dict","Disables database tracking, prevents database events from being sent to the client.",???Disables a database feature by sending a command dictionary.???
2595,instantiate_cond_stage_2,"def instantiate_cond_stage_2(self, config: Dict[str, Any]):
        
        self.cond_stage_2_model = None
        if config is not None:
            logger.info(""creating cond stage 2"")
            model = instantiate_from_config(config)
            self.cond_stage_2_model = model.eval()
            for param in self.cond_stage_2_model.parameters():
                param.requires_grad = False
            self.components.append(Component.COND_STAGE_2_MODEL.value)
            self.cond_stage_2_model_path = config.get(""ckpt_path"", f""{Component.COND_STAGE_2_MODEL.value}.ckpt"")
            logger.info(f""self.cond_stage_2_model_path: {self.cond_stage_2_model_path}"")",Instantiates the conditional stage model of the generative process.,???Initialize and configure a conditional stage 2 model with evaluation mode and frozen parameters.???
2596,_process_forbidden_keys,"def _process_forbidden_keys(schema: dict[str, Any]) -> None:
    
    agg_description: list[str] = []
    if description := schema.get(""description""):
        agg_description.append(description)

    for key in _INLINE_IN_DESCRIPTION_KEYS:
        value = schema.pop(key, None)
        if value is None:
            continue
        if isinstance(value, list):
            value = ""\n"" + ""\n"".join(str(item) for item in value)  # pyright: ignore[reportUnknownArgumentType, reportUnknownVariableType]
        agg_description.append(f""{key}: {value}"")

    if agg_description:
        schema[""description""] = ""\n"".join(agg_description)

    for key in _UNSUPPORTED_KEYS:
        schema.pop(key, None)",Inject unsupported keys details into the description and remove them from the schema.,???Refine schema by aggregating descriptions and removing unsupported keys.???
2597,get_app,"def get_app(
    app_name: str,
) -> None:
    
    with utils.create_db_session(config.DB_FULL_URL) as db_session:
        app = crud.apps.get_app(
            db_session,
            app_name,
            public_only=False,
            active_only=False,
        )

        if app is None:
            console.rule(f""[bold red]App '{app_name}' not found[/bold red]"")
            return

        console.rule(f""[bold green]App: {app.name}[/bold green]"")

        # print without excluded fields
        excluded_fields = [""functions"", ""_sa_instance_state""]
        app_dict = {}
        for key, value in vars(app).items():
            if key not in excluded_fields:
                app_dict[key] = value

        # Add function count
        app_dict[""function_count""] = len(app.functions) if hasattr(app, ""functions"") else 0

        # Convert to JSON string with nice formatting
        json_str = json.dumps(app_dict, indent=2, default=str)

        # Print with syntax highlighting
        console.print(Syntax(json_str, ""json"", theme=""monokai""))",Get an app by name from the database.,"???Retrieve and display app details from the database, excluding specific fields, with JSON formatting.???"
2598,remove_tool,"def remove_tool(tool_name: str):
    
    conf.assert_project()

    repo.commit_user_changes()
    with repo.Transaction() as commit:
        commit.add_message(f""Removed tool {tool_name}"")
        generation.remove_tool(tool_name)",Remove a tool from the user's project.,???Remove a specified tool from the project and commit the changes.???
2599,start_listener,"def start_listener(self):
        
        self.running = True
        try:
            self.socket.bind((self.host, self.port))
            self.socket.listen(1)
            while self.running:
                client_socket, _ = self.socket.accept()
                client_handler = threading.Thread(
                    target=self.handle_client,
                    args=(client_socket,)
                )
                client_handler.daemon = True
                client_handler.start()
        except OSError as e:
            print(f""Error in listener: {str(e)}"")
        finally:
            if not self.running:
                self.socket.close()",Start listener thread in background,???Initialize and manage a server socket to handle incoming client connections.???
2600,set_addresses,"def set_addresses(
    addresses: typing.List[Address],
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""addresses""] = [i.to_json() for i in addresses]
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Autofill.setAddresses"",
        ""params"": params,
    }
    json = yield cmd_dict",Set addresses so that developers can verify their forms implementation.,???Generate a command to update autofill settings with a list of addresses.???
2601,get_pending_tweet_ids,"def get_pending_tweet_ids(self) -> set:
        
        data = self.read_data()
        pending_ids = {reply[""tweet_id""] for reply in data[""pending_replies""]}
        return pending_ids",Get set of all tweet IDs from pending replies,???Extracts unique tweet IDs from pending replies in data storage.???
2602,severity_level_data,"def severity_level_data() -> Dict[str, Any]:
    
    return {""code"": ""urgent"", ""name"": ""Production system down""}",Return a dictionary with sample severity level data.,???Returns a dictionary with severity level details for system issues.???
2603,_map_python_type_to_schema_type,"def _map_python_type_to_schema_type(python_type: str) -> str:
    
    type_mapping = {
        'str': 'string',
        'int': 'integer',
        'float': 'number',
        'bool': 'boolean',
        'list': 'array',
        'dict': 'object',  # though not in allowed list, included for completeness
    }
    return type_mapping.get(python_type, 'string')",Map Python type names to JSON schema type names.,???Map Python data types to corresponding schema types.???
2604,load_existing_results,"def load_existing_results():
    
    summary_file = ""results/overall_summary_evi_no_reasoning.json""
    if os.path.exists(summary_file):
        try:
            with open(summary_file, 'r') as f:
                return json.load(f)
        except:
            return {}
    return {}",Load existing overall results if they exist.,"???Load and return JSON data from a summary file if it exists, else return an empty dictionary.???"
2605,add_to_history,"def add_to_history(action_entry, status, message):
    
    # Use user's selected timezone for display
    user_tz = _get_user_timezone()
    now = datetime.datetime.now(user_tz)
    time_str = now.strftime(""%Y-%m-%d %H:%M:%S"")
    
    # Add timezone information to the timestamp for clarity
    timezone_name = str(user_tz)
    time_str_with_tz = f""{time_str} {timezone_name}""
    
    history_entry = {
        ""timestamp"": time_str,
        ""timestamp_tz"": time_str_with_tz,  # Include timezone-aware timestamp
        ""id"": action_entry.get(""id"", ""unknown""),
        ""action"": action_entry.get(""action"", ""unknown""),
        ""app"": action_entry.get(""app"", ""unknown""),
        ""status"": status,
        ""message"": message
    }
    
    execution_history.appendleft(history_entry)
    scheduler_logger.debug(f""Scheduler history: {time_str_with_tz} - {action_entry.get('action')} for {action_entry.get('app')} - {status} - {message}"")",Add an action execution to the history log,???Log user actions with timestamps and timezone details into execution history.???
2606,list_reasoners,"def list_reasoners() -> None:
    
    console.print(""[bold cyan]Available Reasoners:[/bold cyan]"")
    for name in plugin_manager.reasoners.keys():
        console.print(f""- {name}"")",List all available reasoners.,???Display available reasoning plugins using a console output.???
2607,generate_json,"def generate_json(self, prompt: str, schema: ""BaseModel"") -> Dict[str, Any]:
        
        response = self.client.beta.chat.completions.parse(
            model=self.model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            response_format=schema  # type: ignore[arg-type]
        )
        content = response.choices[0].message.parsed
        if content is None:
            raise ValueError(""OpenAI response content is None"")
        return content.model_dump()",Generate a JSON response based on the given prompt and schema.,???Generate JSON output from AI model response based on user prompt and schema.???
2608,sample_git_repo,"def sample_git_repo(empty_git_repo):
    
    # Create some files
    files = [
        ""README.md"",
        ""src/main.py"",
        ""src/utils.py"",
        ""tests/test_main.py"",
        ""docs/index.html"",
    ]

    for file_path in files:
        full_path = empty_git_repo / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text(f""Content of {file_path}"")

    # Add and commit files
    subprocess.run([""git"", ""add"", "".""], cwd=empty_git_repo)
    subprocess.run(
        [""git"", ""commit"", ""-m"", ""Initial commit""],
        cwd=empty_git_repo,
        env={
            ""GIT_AUTHOR_NAME"": ""Test"",
            ""GIT_AUTHOR_EMAIL"": ""test@example.com"",
            ""GIT_COMMITTER_NAME"": ""Test"",
            ""GIT_COMMITTER_EMAIL"": ""test@example.com"",
        },
    )

    return empty_git_repo",Create a git repository with sample files.,???Initialize and commit a sample project structure in a Git repository.???
2609,log_new_wrapper,"def log_new_wrapper(self, wrapper: OpenAIWrapper, init_args: dict[str, LLMConfig | list[LLMConfig]] = {}) -> None:
        
        thread_id = threading.get_ident()

        try:
            log_data = json.dumps({
                ""wrapper_id"": id(wrapper),
                ""session_id"": self.session_id,
                ""json_state"": json.dumps(init_args),
                ""timestamp"": get_current_ts(),
                ""thread_id"": thread_id,
            })
            self.logger.info(log_data)
        except Exception as e:
            self.logger.error(f""[file_logger] Failed to log event {e}"")",Log a new wrapper instance.,???Log OpenAIWrapper initialization details with session and thread identifiers???
2610,agent_eval_artifacts_in_fixture,"def agent_eval_artifacts_in_fixture():
  
  agent_eval_artifacts = []
  fixture_dir = os.path.join(os.path.dirname(__file__), 'fixture')
  for agent_name in os.listdir(fixture_dir):
    agent_dir = os.path.join(fixture_dir, agent_name)
    if not os.path.isdir(agent_dir):
      continue
    for filename in os.listdir(agent_dir):
      # Evaluation test files end with test.json
      if not filename.endswith('test.json'):
        continue
      agent_eval_artifacts.append((
          f'tests.integration.fixture.{agent_name}',
          f'tests/integration/fixture/{agent_name}/{filename}',
      ))

  # This method gets invoked twice, sorting helps ensure that both the
  # invocations have the same view.
  agent_eval_artifacts = sorted(
      agent_eval_artifacts, key=lambda item: f'{item[0]}|{item[1]}'
  )
  return agent_eval_artifacts",Get all agents from fixture folder.,???Collects and sorts evaluation test artifacts from fixture directories for integration testing.???
2611,parse,"def parse(self, input: dict, response: Recipe) -> dict:
        
        return {
            ""title"": response.title,
            ""ingredients"": response.ingredients,
            ""instructions"": response.instructions,
            ""prep_time"": response.prep_time,
            ""cook_time"": response.cook_time,
            ""servings"": response.servings,
        }",Parse the model response along with the input to the model into the desired output format..,"???  
Extracts recipe details from response into a structured dictionary.  
???"
2612,validate_file,"def validate_file(file_path):
    
    # Check if the file exists
    if not os.path.isfile(file_path):
        raise argparse.ArgumentTypeError(f""The file {file_path} does not exist."")

    # Try to open and parse the file content as JSON
    try:
        with open(file_path, ""r"") as f:
            scheme = json.load(f)

            # Ensure the scheme is a dictionary
            if not isinstance(scheme, dict):
                raise argparse.ArgumentTypeError(
                    ""The scheme in the file must be a valid JSON object (dict).""
                )

            return scheme
    except json.JSONDecodeError:
        raise argparse.ArgumentTypeError(f""Invalid JSON format in {file_path}."")",Validate if the file exists and contains a valid JSON scheme.,"???Validate and parse a file as a JSON object, ensuring it exists and is correctly formatted.???"
2613,get_im_features,"def get_im_features(self, im):
        
        assert (
            isinstance(self.imgsz, (tuple, list)) and self.imgsz[0] == self.imgsz[1]
        ), f""SAM 2 models only support square image size, but got {self.imgsz}.""
        self.model.set_imgsz(self.imgsz)

        backbone_out = self.model.forward_image(im)
        _, vision_feats, _, _ = self.model._prepare_backbone_features(backbone_out)
        if self.model.directly_add_no_mem_embed:
            vision_feats[-1] = vision_feats[-1] + self.model.no_mem_embed
        feats = [
            feat.permute(1, 2, 0).view(1, -1, *feat_size)
            for feat, feat_size in zip(vision_feats[::-1], self._bb_feat_sizes[::-1])
        ][::-1]
        return {""image_embed"": feats[-1], ""high_res_feats"": feats[:-1]}",Extracts image features from the SAM image encoder for subsequent processing.,??? Extracts and processes image features for a model requiring square dimensions. ???
2614,clean_units,"def clean_units(pred_str: str):
    

    def convert_pi_to_number(code_string):
        code_string = code_string.replace(""\\pi"", ""π"")
        # Replace \pi or π not preceded by a digit or } with 3.14
        code_string = re.sub(r""(?<![\d}])\\?π"", ""3.14"", code_string)
        # Replace instances where π is preceded by a digit but without a multiplication symbol, e.g., ""3π"" -> ""3*3.14""
        code_string = re.sub(r""(\d)(\\?π)"", r""\1*3.14"", code_string)
        # Handle cases where π is within braces or followed by a multiplication symbol
        # This replaces ""{π}"" with ""3.14"" directly and ""3*π"" with ""3*3.14""
        code_string = re.sub(r""\{(\\?π)\}"", ""3.14"", code_string)
        code_string = re.sub(r""\*(\\?π)"", ""*3.14"", code_string)
        return code_string

    pred_str = convert_pi_to_number(pred_str)
    pred_str = pred_str.replace(""%"", ""/100"")
    pred_str = pred_str.replace(""$"", """")
    pred_str = pred_str.replace(""¥"", """")
    pred_str = pred_str.replace(""°C"", """")
    pred_str = pred_str.replace("" C"", """")
    pred_str = pred_str.replace(""°"", """")
    return pred_str",Clean the units in the number.,???Convert mathematical and currency symbols in a string to standardized numeric representations.???
2615,get_im_features,"def get_im_features(self, im):
        
        assert isinstance(self.imgsz, (tuple, list)) and self.imgsz[0] == self.imgsz[1], (
            f""SAM 2 models only support square image size, but got {self.imgsz}.""
        )
        self.model.set_imgsz(self.imgsz)

        backbone_out = self.model.forward_image(im)
        _, vision_feats, _, _ = self.model._prepare_backbone_features(backbone_out)
        if self.model.directly_add_no_mem_embed:
            vision_feats[-1] = vision_feats[-1] + self.model.no_mem_embed
        feats = [
            feat.permute(1, 2, 0).view(1, -1, *feat_size)
            for feat, feat_size in zip(vision_feats[::-1], self._bb_feat_sizes[::-1])
        ][::-1]
        return {""image_embed"": feats[-1], ""high_res_feats"": feats[:-1]}",Extracts image features from the SAM image encoder for subsequent processing.,???Extracts and processes image features for model embedding and high-resolution analysis.???
2616,get_series_by_id,"def get_series_by_id(api_url: str, api_key: str, api_timeout: int, series_id: int) -> Optional[Dict[str, Any]]:
    
    try:
        endpoint = f""{api_url}/api/v3/series/{series_id}""
        response = requests.get(endpoint, headers={""X-Api-Key"": api_key}, timeout=api_timeout)
        response.raise_for_status()
        series_data = response.json()
        sonarr_logger.debug(f""Fetched details for Sonarr series ID: {series_id}"")
        return series_data
    except requests.exceptions.RequestException as e:
        sonarr_logger.error(f""Error getting Sonarr series details for ID {series_id}: {e}"")
        return None
    except Exception as e:
        sonarr_logger.error(f""An unexpected error occurred while getting Sonarr series details: {e}"")
        return None",Get series details by ID from Sonarr.,???Fetch series details from API using ID with error handling and logging???
2617,get_extra_params,"def get_extra_params(self) -> Dict[str, Any]:
        
        standard_fields = {""model"", ""input"", ""voice"", ""response_format"", ""speed""}
        return {k: v for k, v in self.model_dump().items() if k not in standard_fields}",Get all extra parameters that aren't part of the standard OpenAI API.,???Extract non-standard parameters from model data dictionary???
2618,get_video_info,"def get_video_info(video_path):
    
    # Read video tensor (T, C, H, W)
    video_tensor, _, info = torchvision.io.read_video(str(video_path),
                                                      output_format=""TCHW"",
                                                      pts_unit=""sec"")

    num_frames = video_tensor.shape[0]
    height = video_tensor.shape[2]
    width = video_tensor.shape[3]
    fps = info.get(""video_fps"", 0)
    duration = num_frames / fps if fps > 0 else 0

    # Extract name
    _, _, videos_dir, video_name = str(video_path).split(""/"")

    return {
        ""path"": str(video_name),
        ""resolution"": {
            ""width"": width,
            ""height"": height
        },
        ""size"": os.path.getsize(video_path),
        ""fps"": fps,
        ""duration"": duration,
        ""num_frames"": num_frames
    }",Get video information using torchvision.,"???Extract and return video metadata including resolution, size, fps, and duration.???"
2619,debug_log_object,"def debug_log_object(self, obj: Any, name: str = ""object"") -> None:
        
        try:
            if hasattr(obj, ""__dict__""):
                logging.info(f""{name} attributes: {list(obj.__dict__.keys())}"")
            elif isinstance(obj, dict):
                logging.info(f""{name} keys: {list(obj.keys())}"")
            elif isinstance(obj, list):
                logging.info(f""{name} is a list with {len(obj)} items"")
                if len(obj) > 0:
                    self.debug_log_object(obj[0], f""{name}[0]"")
            else:
                logging.info(f""{name} type: {type(obj)}"")
        except Exception as e:
            logging.error(f""Error logging {name}: {str(e)}"")",Log object details for debugging,"???  
Logs detailed information about an object's structure and attributes for debugging purposes.  
???"
2620,llm,"def llm(self) -> BaseLLM:
        
        # print(f""class:{self.__class__.__name__}({self.name}), llm: {self._llm}, llm_config: {self._llm_config}"")
        if not self.private_llm:
      
            self.private_llm = self.context.llm_with_cost_manager_from_llm_config(self.config.llm)
        return self.private_llm","Role llm: if not existed, init from role.config",???Retrieve or initialize a private language model using configuration settings.???
2621,load_template_config,"def load_template_config(template_dir: pathlib.Path) -> dict[str, Any]:
    
    config_file = template_dir / TEMPLATE_CONFIG_FILE
    if not config_file.exists():
        return {}

    try:
        with open(config_file) as f:
            config = yaml.safe_load(f)
            return config if config else {}
    except Exception as e:
        logging.error(f""Error loading template config: {e}"")
        return {}",Read .templateconfig.yaml file to get agent configuration.,"???Load and parse YAML configuration from a specified directory, handling errors.???"
2622,find_writable_stats_dir,"def find_writable_stats_dir():
    
    for dir_path in STATS_DIRS:
        try:
            os.makedirs(dir_path, exist_ok=True)
            test_file = os.path.join(dir_path, ""write_test"")
            with open(test_file, 'w') as f:
                f.write(""test"")
            os.remove(test_file)
            logger.info(f""Using stats directory: {dir_path}"")
            return dir_path
        except (IOError, OSError) as e:
            logger.warning(f""Directory {dir_path} is not writable: {e}"")
            continue
    
    # Fallback to current directory
    fallback_dir = os.path.join(os.getcwd(), ""tally"")
    try:
        os.makedirs(fallback_dir, exist_ok=True)
        logger.info(f""Falling back to current directory for stats: {fallback_dir}"")
        return fallback_dir
    except Exception as e:
        logger.error(f""Failed to create fallback stats directory: {e}"")
        return None",Find a writable directory for stats from the list of candidates,"???Determine a writable directory for statistics, with fallback to current directory.???"
2623,register_reasoner,"def register_reasoner(self, reasoner: BaseReasoner, name: str) -> None:
        
        try:
            self.plugin_manager.reasoners[name] = reasoner.__class__
        except Exception as e:
            logger.error(f""Failed to register reasoner {name}: {e}"")
            raise",Register a new reasoner dynamically at runtime.,"???Register a reasoning module by name, handling errors if unsuccessful.???"
2624,generation_cluster_separate,"def generation_cluster_separate():
    
    cluster = _create_ray_virtual_cluster_for_test(
        ""vllm-test-generation-cluster-separate""
    )
    yield cluster
    try:
        cluster.shutdown()
    except Exception as e:
        print(f""Error during generation_cluster_separate shutdown: {e}"")","Create a virtual cluster for the VllmGeneration policy, using 1 GPU.",???Create and manage a test virtual cluster for generation tasks???
2625,format_msg,"def format_msg(self, messages: Union[str, Message, list[dict], list[Message], list[str]]) -> list[dict]:
        
        from Core.Schema.Message import Message

        if not isinstance(messages, list):
            messages = [messages]

        processed_messages = []
        for msg in messages:
            if isinstance(msg, str):
                processed_messages.append({""role"": ""user"", ""content"": msg})
            elif isinstance(msg, dict):
                assert set(msg.keys()) == set([""role"", ""content""])
                processed_messages.append(msg)
            elif isinstance(msg, Message):
                processed_messages.append(msg.to_dict())
            else:
                raise ValueError(
                    f""Only support message type are: str, Message, dict, but got {type(messages).__name__}!""
                )
        return processed_messages",convert messages to list[dict].,???Converts various message formats into a standardized list of dictionaries.???
2626,get_instruction_args,"def get_instruction_args(self):
        
        return {
            ""original_paragraph"": self._original_paragraph,
            ""low"": self._low,
            ""high"": self._high,
        }",Returns the keyward args of `build_description`.,???Returns a dictionary of paragraph and range attributes.???
2627,remove_all_connected_accounts,"def remove_all_connected_accounts():
    
    print(""\n=== Removing All Connected Accounts ==="")
    accounts = toolset.client.connected_accounts.get()
    for account in accounts:
        try:
            print(f""Removing {account.appUniqueId} (ID: {account.id})..."")
            toolset.client.connected_accounts.delete(account.id)
            print(f""✓ Successfully removed {account.appUniqueId}"")
        except Exception as e:
            print(f""✗ Error removing {account.appUniqueId}: {str(e)}"")",Remove all connected accounts.,???Remove all linked accounts and handle potential errors during deletion???
2628,reset_to_default,"def reset_to_default(self) -> None:  # pragma: no cover
        
        self.current_project = self.default_project  # pragma: no cover
        logger.info(f""Reset project context to default: {self.default_project}"")",Reset current project back to the default project.,???Reset project context to default settings and log the action.???
2629,query_api,"def query_api(prompt, kb_id):
    
    try:
        response = requests.post(
            f'{API_URL}/query', json={'query': prompt, 'kb_id': kb_id}, timeout=30
        )
        response.raise_for_status()  # Raise an exception for HTTP errors
        return response.json()['messages']
    except requests.exceptions.RequestException as e:
        st.error(f'API Error: {str(e)}')
        return [{'content': f'Error communicating with the API: {str(e)}'}]",Send a query to the FastAPI server and get the response.,???Send a query to an API and handle potential request errors.???
2630,version_callback,"def version_callback(value: bool) -> None:
    
    if value:  # pragma: no cover
        import basic_memory
        from basic_memory.config import config

        typer.echo(f""Basic Memory version: {basic_memory.__version__}"")
        typer.echo(f""Current project: {config.project}"")
        typer.echo(f""Project path: {config.home}"")
        raise typer.Exit()",Show version and exit.,???Display version and project details if the flag is true.???
2631,mask_to_rle_pytorch,"def mask_to_rle_pytorch(tensor: torch.Tensor) -> List[Dict[str, Any]]:
    
    # Put in fortran order and flatten h,w
    b, h, w = tensor.shape
    tensor = tensor.permute(0, 2, 1).flatten(1)

    # Compute change indices
    diff = tensor[:, 1:] ^ tensor[:, :-1]
    change_indices = diff.nonzero()

    # Encode run length
    out = []
    for i in range(b):
        cur_idxs = change_indices[change_indices[:, 0] == i, 1]
        cur_idxs = torch.cat(
            [
                torch.tensor([0], dtype=cur_idxs.dtype, device=cur_idxs.device),
                cur_idxs + 1,
                torch.tensor([h * w], dtype=cur_idxs.dtype, device=cur_idxs.device),
            ]
        )
        btw_idxs = cur_idxs[1:] - cur_idxs[:-1]
        counts = [] if tensor[i, 0] == 0 else [0]
        counts.extend(btw_idxs.detach().cpu().tolist())
        out.append({""size"": [h, w], ""counts"": counts})
    return out","Encodes masks to an uncompressed RLE, in the format expected by pycoco tools.",???Convert binary mask tensor to run-length encoding format in PyTorch.???
2632,_create_event_data,"def _create_event_data(self, event_message: EventMessage) -> dict[str, str]:
        
        event_data = {
            ""event"": ""message"",
            ""data"": event_message.message.model_dump_json(
                by_alias=True, exclude_none=True
            ),
        }

        # If an event ID was provided, include it
        if event_message.event_id:
            event_data[""id""] = event_message.event_id

        return event_data",Create event data dictionary from an EventMessage.,???Generate event data dictionary from message object with optional ID inclusion.???
2633,client_shorthands_to_paths,"def client_shorthands_to_paths(shorthands: list[str]):
    
    paths = []
    if any(not re.match(r""^[A-z0-9_-]+$"", shorthand) for shorthand in shorthands):
        return shorthands

    for shorthand in shorthands:
        if shorthand in CLIENT_PATHS:
            paths.extend(CLIENT_PATHS[shorthand])
        else:
            raise ValueError(f""{shorthand} is not a valid client shorthand"")
    return paths",Converts a list of client shorthands to a list of paths.,???Convert client shorthand identifiers into corresponding file paths.???
2634,pretty_print_diff,"def pretty_print_diff(diff: str):
    
    rich.print(
        Panel(
            Markdown(
                f,
                code_theme=""monokai"",
            ),
            title=""[bold green]Diff"",
            border_style=""green"",
            box=box.ROUNDED,
            padding=(1, 2),
        )
    )
    rich.print()",Pretty print diff in a panel.,???Formats and displays a code difference with styled output using rich library.???
2635,delete_audio,"def delete_audio(self, audio_id):
        
        if not audio_id:
            raise ValueError(""Audio ID is required to delete a audio."")
        try:
            audio = self.collection.get_audio(audio_id)
            if not audio:
                raise ValueError(
                    f""Audio with ID {audio_id} not found in collection {self.collection.id}.""
                )

            audio.delete()
            return {
                ""success"": True,
                ""message"": f""Video {audio.id} deleted successfully"",
            }
        except ValueError as ve:
            logging.error(f""ValueError while deleting video: {ve}"")
            raise ve
        except Exception as e:
            logging.exception(
                f""Unexpected error occurred while deleting video {audio_id}""
            )
            raise Exception(
                ""An unexpected error occurred while deleting the video. Please try again later.""
            )",Delete a specific audio by its ID.,"???Delete audio by ID, handling errors and confirming success.???"
2636,generate_columns_section,"def generate_columns_section(md: List[str], meta: dict) -> None:
    
    md.append(""#### Columns\n"")
    md.append(""| Column Name | Data Type | Nullable? | Primary Key? |\n"")
    md.append(""|-------------|-----------|-----------|--------------|\n"")
    for col in meta[""columns""]:
        pk = ""Yes"" if col[""name""] in meta[""primary_keys""] else ""No""
        md.append(f""| `{col['name']}` | {col['type']} | {'Yes' if col['nullable'] else 'No'} | {pk} |\n"")
    md.append(""\n"")",Generate columns table section.,???Generate a markdown table detailing database column attributes from metadata.???
2637,train,"def train():
    
    try:
        instance.train(
            n_iterations=int(sys.argv[1]), 
            filename=sys.argv[2], 
            inputs=agentstack.get_inputs(), 
        )
    except Exception as e:
        raise Exception(f""An error occurred while training the crew: {e}"")",Train the crew for a given number of iterations.,"???  
Initiates training process with specified iterations and input data, handling errors.  
???"
2638,mock_openai_mappings,"def mock_openai_mappings():
    
    with patch(
        ""api.src.routers.openai_compatible._openai_mappings"",
        {
            ""models"": {""tts-1"": ""kokoro-v1_0"", ""tts-1-hd"": ""kokoro-v1_0""},
            ""voices"": {""alloy"": ""am_adam"", ""nova"": ""bf_isabella""},
        },
    ):
        yield",Mock OpenAI mappings for testing.,???Mock OpenAI API mappings for models and voices in a test context.???
2639,_calculate_volume_indicators,"def _calculate_volume_indicators(self, df: pd.DataFrame) -> Dict[str, TechnicalSignal]:
        
        signals = {}
        
        try:
            # On-Balance Volume
            df['obv'] = ta.volume.on_balance_volume(df['close'], df['volume'])
            
            # Money Flow Index
            df['mfi'] = ta.volume.money_flow_index(df['high'], df['low'], df['close'], df['volume'])
            
            # Volume Price Trend
            df['vpt'] = ta.volume.volume_price_trend(df['close'], df['volume'])
            
            # Ease of Movement
            df['eom'] = ta.volume.ease_of_movement(df['high'], df['low'], df['volume'])
            
            # Volume-Weighted Average Price
            df['vwap'] = self._calculate_vwap(df)
            
            # Volume Signals
            signals['obv'] = self._analyze_obv(df)
            signals['mfi'] = self._analyze_mfi(df)
            signals['vpt'] = self._analyze_vpt(df)
            signals['vwap'] = self._analyze_vwap(df)
            
            return signals
            
        except Exception as e:
            logger.error(f""Error calculating volume indicators: {e}"")
            raise",Calculate comprehensive volume indicators.,???Calculate and analyze volume-based technical indicators for financial data.???
2640,_get_valid_env_vars,"def _get_valid_env_vars(prefix: str, config_obj: Any) -> set[str]:
        
        valid_vars = set()
        if not hasattr(config_obj, 'model_fields'):
            return valid_vars

        for field_name, _ in config_obj.__class__.model_fields.items():
            full_env_var = f""{ENV_VAR_PREFIX}{prefix}_{field_name}"".upper() if prefix else f""{ENV_VAR_PREFIX}{field_name}"".upper()
            valid_vars.add(full_env_var)

            field_value = getattr(config_obj, field_name)
            if field_value is not None and hasattr(field_value, 'model_fields'):
                nested_prefix = f""{prefix}_{field_name}"" if prefix else field_name
                valid_vars.update(_get_valid_env_vars(nested_prefix, field_value))

        return valid_vars",Recursively collect all valid environment variable names,???Extracts valid environment variable names from a configuration object with nested fields.???
2641,sync_to_commit,"def sync_to_commit(self, target_commit: GitCommit) -> None:
        
        origin_commit = self.ctx.synced_commit
        if origin_commit.hexsha == target_commit.hexsha:
            logger.info(f""Codebase is already synced to {target_commit.hexsha}. Skipping sync_to_commit."")
            return
        if not self.ctx.config.sync_enabled:
            logger.info(f""Syncing codebase is disabled for repo {self._op.repo_name}. Skipping sync_to_commit."")
            return

        logger.info(f""Syncing {self._op.repo_name} to {target_commit.hexsha}"")
        diff_index = origin_commit.diff(target_commit)
        diff_lites = []
        for diff in diff_index:
            diff_lites.append(DiffLite.from_git_diff(diff))
        self.ctx.apply_diffs(diff_lites)
        self.ctx.save_commit(target_commit)",Updates the current base to a new commit.,???Synchronize codebase to a specified commit if syncing is enabled and necessary.???
2642,solve_sokoban,"def solve_sokoban(env, saved_animation_path):
    
    actions = get_shortest_action_path(env.room_fixed, env.room_state)
    print(f""Found {len(actions)} actions: {actions}"")
    imgs = []
    img_before_action = env.render('rgb_array')
    imgs.append(img_before_action)
    for action in actions:
        env.step(action)
        img_after_action = env.render('rgb_array')
        imgs.append(img_after_action)
    ani = plot_animation(imgs)
    ani.save(saved_animation_path)",Solve the given sokoban environment and save the animation,??? Generate and save an animation of the shortest solution path for a Sokoban puzzle. ???
2643,list,"def list(verbose=False):
    
    profiles = profile_config_manager.list_profiles()
    if not profiles:
        console.print(""\n[yellow]No profiles found.[/]\n"")
        return
    console.print(f""\n[green]Found {len(profiles)} profile(s)[/]\n"")
    table = Table(show_header=True, header_style=""bold"")
    table.add_column(""Name"", style=""cyan"")
    table.add_column(""Servers"", overflow=""fold"")
    if verbose:
        table.add_column(""Server Details"", overflow=""fold"")
    for profile_name, configs in profiles.items():
        server_names = [config.name for config in configs]
        row = [profile_name, "", "".join(server_names)]
        if verbose:
            details = []
            for config in configs:
                if isinstance(config, STDIOServerConfig):
                    details.append(f""{config.name}: {config.command} {' '.join(config.args)}"")
                else:
                    details.append(f""{config.name}: {config.url}"")
            row.append(""\n"".join(details))
        table.add_row(*row)
    console.print(table)",List all MCPM profiles.,"???Display profiles and server details in a formatted table, optionally verbose.???"
2644,_generate_payload,"def _generate_payload(self, chunk: Chunk) -> dict:
        
        return {
            ""text"": chunk.text,
            ""start_index"": chunk.start_index,
            ""end_index"": chunk.end_index,
            ""token_count"": chunk.token_count,
        }",Generate the payload for the chunk.,"???  
Create a dictionary payload from a text chunk's attributes.  
???"
2645,build_judge,"def build_judge(model, api_type):
    
    if api_type == 'mit':
        api_key = os.environ.get('MIT_SPIDER_TOKEN', '')
        api_base = os.environ.get('MIT_SPIDER_URL', '')
        return OpenAIWrapper(model, api_base, api_key)
    elif api_type == 'dash':
        api_key = os.environ.get('CHATGPT_DASHSCOPE_API_KEY', '')
        api_base = os.environ.get('DASHSCOPE_API_BASE', '')
        return DashScopeWrapper(model, api_base, api_key)
    else:
        raise ValueError(f""Unsupported API type: {api_type}"")",Build a judge model for evaluation.,???Configure API wrapper based on specified model and API type???
2646,get_financial_metrics,"def get_financial_metrics(
    ticker: str,
    end_date: str,
    period: str = ""ttm"",
    limit: int = 10,
) -> list[FinancialMetrics]:
    
    # Create a cache key that includes all parameters to ensure exact matches
    cache_key = f""{ticker}_{period}_{end_date}_{limit}""
    
    # Check cache first - simple exact match
    if cached_data := _cache.get_financial_metrics(cache_key):
        return [FinancialMetrics(**metric) for metric in cached_data]

    # If not in cache, fetch from API
    headers = {}
    if api_key := os.environ.get(""FINANCIAL_DATASETS_API_KEY""):
        headers[""X-API-KEY""] = api_key

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f""Error fetching data: {ticker} - {response.status_code} - {response.text}"")

    # Parse response with Pydantic model
    metrics_response = FinancialMetricsResponse(**response.json())
    financial_metrics = metrics_response.financial_metrics

    if not financial_metrics:
        return []

    # Cache the results as dicts using the comprehensive cache key
    _cache.set_financial_metrics(cache_key, [m.model_dump() for m in financial_metrics])
    return financial_metrics",Fetch financial metrics from cache or API.,???Fetch and cache financial metrics for a given stock ticker and date period.???
2647,generate_agent_response,"def generate_agent_response(orchestrator: AdaptiveOrchestrator, user_input: str):
    
    if orchestrator.streaming.enabled:
        streaming_handler = StreamingIteratorCallbackHandler()
        orchestrator.run(
            input_data={""input"": user_input, ""user_id"": ""1"", ""session_id"": ""1""},
            config=RunnableConfig(callbacks=[streaming_handler]),
        )

        response_text = """"

        for chunk in streaming_handler:
            content = chunk.data
            if content:
                response_text += "" "" + content.get(""choices"", [{}])[0].get(""delta"", {}).get(""content"")
                yield content

    else:
        result = orchestrator.run({""input"": user_input})
        response_text = result.output.get(""content"", """")
        yield response_text",Processes the user input using the agent.,???Generate adaptive responses based on user input with streaming support???
2648,write_embeddings,"def write_embeddings(
    embeddings: dict[str, np.ndarray],
    output_dir: os.PathLike[str] | str,
    name: str | None = None,
) -> None:
  
  prefix = f'{name}_' if name is not None else ''

  with open(os.path.join(output_dir, f'{prefix}embeddings.npz'), 'wb') as f:
    np.savez_compressed(f, **embeddings)",Writes embeddings to a directory.,???Save dictionary of embeddings to a compressed file in specified directory.???
2649,mock_request_with_messages,"def mock_request_with_messages(mock_request):
    

    async def get_json():
        return {
            ""messages"": [
                {""role"": ""user"", ""content"": ""Hello""},
                {""role"": ""system"", ""content"": ""Test message""},
            ]
        }

    mock_request.json = get_json
    return mock_request",Create a mock request with configurable message content,???Mock a request to return predefined user and system messages.???
2650,create_model,"def create_model(self, model_args):
        
        payload = {
            ""config"": {
                ""batchSize"": model_args.get(""batch"", -1),
                ""epochs"": model_args.get(""epochs"", 300),
                ""imageSize"": model_args.get(""imgsz"", 640),
                ""patience"": model_args.get(""patience"", 100),
                ""device"": str(model_args.get(""device"", """")),  # convert None to string
                ""cache"": str(model_args.get(""cache"", ""ram"")),  # convert True, False, None to string
            },
            ""dataset"": {""name"": model_args.get(""data"")},
            ""lineage"": {
                ""architecture"": {""name"": self.filename.replace("".pt"", """").replace("".yaml"", """")},
                ""parent"": {},
            },
            ""meta"": {""name"": self.filename},
        }

        if self.filename.endswith("".pt""):
            payload[""lineage""][""parent""][""name""] = self.filename

        self.model.create_model(payload)

        # Model could not be created
        # TODO: improve error handling
        if not self.model.id:
            return None

        self.model_url = f""{HUB_WEB_ROOT}/models/{self.model.id}""

        # Start heartbeats for HUB to monitor agent
        self.model.start_heartbeat(self.rate_limits[""heartbeat""])

        LOGGER.info(f""{PREFIX}View model at {self.model_url} 🚀"")",Initializes a HUB training session with the specified model identifier.,???Initialize and configure a machine learning model with specified parameters and start monitoring.???
2651,deploy,"def deploy(self):
        
        print(f""Deploying Kubernetes configurations in namespace: {self.namespace}"")
        self.kubectl.apply_configs(self.namespace, self.k8s_deploy_path)
        self.kubectl.wait_for_ready(self.namespace)",Deploy the Kubernetes configurations.,???Deploy Kubernetes configurations and ensure readiness in specified namespace???
2652,save_pretrained,"def save_pretrained(self, save_directory):
        
        os.makedirs(save_directory, exist_ok=True)
        
        model_path = os.path.join(save_directory, ""decoder_model.pt"")
        save_dict = {
            'model_state_dict': self.state_dict(),
            'codebook_weights': self.codebook_weights
        }
        torch.save(save_dict, model_path)
        
        config = self._create_config()
        config_path = os.path.join(save_directory, ""config.json"")
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)",Save the model and its configuration to a directory,???Save model state and configuration to specified directory.???
2653,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        result = {""number"": self.number, ""when"": self.format_timestamp(self.when)}

        if self.message:
            result[""message""] = self.message

        if self.by:
            result[""by""] = self.by.display_name

        return result",Convert to simplified dictionary for API response.,???Convert object attributes to a simplified dictionary format.???
2654,guess_mime_type,"def guess_mime_type(filename):
    
    extension = os.path.splitext(filename)[1].lower()
    mime_types = {
        "".jpg"": ""image/jpeg"",
        "".jpeg"": ""image/jpeg"",
        "".png"": ""image/png"",
        "".gif"": ""image/gif"",
        "".bmp"": ""image/bmp"",
        "".webp"": ""image/webp"",
        "".pdf"": ""application/pdf"",
        "".txt"": ""text/plain"",
        "".csv"": ""text/csv"",
        "".json"": ""application/json"",
        "".md"": ""text/markdown"",
    }
    return mime_types.get(extension, ""application/octet-stream"")",Guess the MIME type of a file based on its extension.,???Determine file MIME type based on its extension using a predefined mapping.???
2655,extract_numbers_mmmu,"def extract_numbers_mmmu(string):
    
    # Pattern for numbers with commas
    pattern_commas = r""-?\b\d{1,3}(?:,\d{3})+\b""
    # Pattern for scientific notation
    pattern_scientific = r""-?\d+(?:\.\d+)?[eE][+-]?\d+""
    # Pattern for simple numbers without commas
    pattern_simple = r""-?(?:\d+\.\d+|\.\d+|\d+\b)(?![eE][+-]?\d+)(?![,\d])""

    # Extract numbers with commas
    numbers_with_commas = re.findall(pattern_commas, string)
    # Extract numbers in scientific notation
    numbers_scientific = re.findall(pattern_scientific, string)
    # Extract simple numbers without commas
    numbers_simple = re.findall(pattern_simple, string)

    # Combine all extracted numbers
    all_numbers = numbers_with_commas + numbers_scientific + numbers_simple
    return all_numbers",Exact all forms of numbers from a string with regex.,???Extracts various formatted numbers from text using regex patterns.???
2656,get_task_status,"def get_task_status(task_id):
    
    try:
        status_list = redis_client.lrange(f""task:{task_id}:status"", 0, -1)
        return [json.loads(s.decode(""utf-8"")) for s in status_list]
    except Exception as e:
        logger.error(f""Error getting task status: {e}"")
        return []",Get all task status updates from Redis,"???Retrieve and decode task status from Redis, handling errors gracefully.???"
2657,visit_Assign,"def visit_Assign(self, node):
        
        if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):
            var_name = node.targets[0].id
            value = node.value

            # Handle global variable assignments (e.g., MODEL, DEFAULT_LLM_PARAMS)
            if isinstance(value, ast.Dict):
                self.global_vars[var_name] = {}
                for k, v in zip(value.keys, value.values):
                    if isinstance(k, ast.Constant):
                        key = k.value
                        if isinstance(v, ast.Constant):
                            self.global_vars[var_name][key] = v.value
                        elif isinstance(v, ast.Name) and v.id in self.global_vars:
                            self.global_vars[var_name][key] = self.global_vars[v.id]
                logger.debug(
                    f""Captured global variable '{var_name}' with keys: {list(self.global_vars[var_name].keys())}""
                )

            # Handle simple constant assignments (e.g., MODEL = ""gemini/gemini-2.0-flash"")
            elif isinstance(value, ast.Constant):
                self.global_vars[var_name] = value.value
                logger.debug(f""Captured global constant '{var_name}' with value: {value.value}"")

            # Handle workflow assignments, including parenthesized expressions
            if isinstance(value, ast.Tuple) and len(value.elts) == 1:
                value = value.elts[0]  # Unwrap single-element tuple from parentheses
            if isinstance(value, ast.Call):
                self.process_workflow_expr(value, var_name)

        self.generic_visit(node)",Detect global variable assignments and workflow assignments.,???Extracts and processes global variable assignments and workflows from AST nodes.???
2658,convert,"def convert(self, format):
        
        assert format in _formats, f""Invalid bounding box format: {format}, format must be one of {_formats}""
        if self.format == format:
            return
        elif self.format == ""xyxy"":
            func = xyxy2xywh if format == ""xywh"" else xyxy2ltwh
        elif self.format == ""xywh"":
            func = xywh2xyxy if format == ""xyxy"" else xywh2ltwh
        else:
            func = ltwh2xyxy if format == ""xyxy"" else ltwh2xywh
        self.bboxes = func(self.bboxes)
        self.format = format",Converts bounding box format from one type to another.,???Convert bounding box coordinates between different formats.???
2659,_get_pdb_id_and_chain,"def _get_pdb_id_and_chain(hit: parsers.TemplateHit) -> Tuple[str, str]:
    
    # PDB ID: 4 letters. Chain ID: 1+ alphanumeric letters or ""."" if unknown.
    id_match = re.match(r""[a-zA-Z\d]{4}_[a-zA-Z0-9.]+"", hit.name)
    if not id_match:
        raise ValueError(f""hit.name did not start with PDBID_chain: {hit.name}"")
    pdb_id, chain_id = id_match.group(0).split(""_"")
    return pdb_id.lower(), chain_id",Returns PDB id and chain id for an HHSearch Hit.,???Extracts and returns PDB ID and chain from a template hit name.???
2660,simulate_sequence,"def simulate_sequence(self, metadata: dict, sequence: list[str]) -> int:
        
        state = metadata[""initial_value""]
        current_color = metadata[""initial_state""]

        buttons = {btn[""name""]: btn for btn in metadata[""buttons""]}

        for btn_char in sequence:
            btn = buttons.get(btn_char.upper())
            if not btn:
                continue

            # Check button availability
            if btn[""active_state""] not in [current_color, ""any""]:
                continue

            # Apply operation
            if btn[""type""] == ""add"":
                state += btn[""value""]
            elif btn[""type""] == ""subtract"":
                state -= btn[""value""]
            elif btn[""type""] == ""multiply"":
                state *= btn[""value""]

            # Toggle color state
            current_color = ""green"" if current_color == ""red"" else ""red""

        return state",Simulate button presses to verify solutions,???Simulate state changes based on button sequence and initial metadata???
2661,label_loss_items,"def label_loss_items(self, loss_items=None, prefix=""train""):
        
        keys = [f""{prefix}/{x}"" for x in self.loss_names]
        if loss_items is not None:
            loss_items = [round(float(x), 5) for x in loss_items]  # convert tensors to 5 decimal place floats
            return dict(zip(keys, loss_items))
        else:
            return keys",Returns a loss dict with labelled training loss items tensor.,???Generate labeled dictionary of loss metrics with optional rounding.???
2662,extract_text_from_pdf,"def extract_text_from_pdf(pdf_path: str, custom_logger: Any = None) -> Optional[str]:
    
    # Use custom logger if provided, otherwise use module logger
    log = custom_logger if custom_logger else logger

    try:
        if not os.path.exists(pdf_path):
            log.error(f""PDF file not found: {pdf_path}"")
            return None

        log.info(f""📄 Extracting text from PDF: {pdf_path}"")

        pdf_content = """"
        try:
            # Open the PDF file
            with fitz.open(pdf_path) as doc:
                # Iterate through each page
                for page_num in range(len(doc)):
                    # Get the page
                    page = doc[page_num]
                    # Extract text from the page
                    pdf_content += page.get_text()
                    # Add a separator between pages
                    if page_num < len(doc) - 1:
                        pdf_content += ""\n\n""
        except Exception as e:
            log.error(f""Error extracting text from PDF: {str(e)}"")
            return None

        log.info(f""✅ PDF text extraction successful: {len(pdf_content)} characters"")
        return pdf_content

    except Exception as e:
        log.error(f""❌ Error processing PDF: {str(e)}"")
        return None",Extract text content from a PDF file,"???Extracts text from a PDF file, logging progress and errors.???"
2663,final_eval,"def final_eval(self):
        
        ckpt = {}
        for f in self.last, self.best:
            if f.exists():
                if f is self.last:
                    ckpt = strip_optimizer(f)
                elif f is self.best:
                    k = ""train_results""  # update best.pt train_metrics from last.pt
                    strip_optimizer(f, updates={k: ckpt[k]} if k in ckpt else None)
                    LOGGER.info(f""\nValidating {f}..."")
                    self.validator.args.plots = self.args.plots
                    self.metrics = self.validator(model=f)
                    self.metrics.pop(""fitness"", None)
                    self.run_callbacks(""on_fit_epoch_end"")",Performs final evaluation and validation for object detection YOLO model.,"???Evaluate and update model checkpoints, logging validation metrics and executing callbacks.???"
2664,prepare_openinstructmath2_dataset,"def prepare_openinstructmath2_dataset(
    split: str = ""train_1M"",
    seed: int = 42,
    test_size: float = 0.05,
    output_key: str = ""expected_answer"",
) -> dict[str, Dataset | None]:
    
    print(
        ""WARNING: For reproducible experiments, preprocess the dataset once and define your own HfDataset subclass that directly uses the preprocessed datasets.""
    )

    # Load the original dataset
    original_ds = load_dataset(""nvidia/OpenMathInstruct-2"", split=split)

    # Split into train and validation sets using HF's train_test_split
    split_ds = original_ds.train_test_split(test_size=test_size, seed=seed)

    # Format the examples, removing original columns
    train_formatted = split_ds[""train""].map(
        format_math,
        remove_columns=split_ds[""train""].column_names,
        fn_kwargs={""output_key"": output_key},
    )
    val_formatted = split_ds[""test""].map(
        format_math,
        remove_columns=split_ds[""test""].column_names,
        fn_kwargs={""output_key"": output_key},
    )

    return {
        ""train"": train_formatted,
        ""validation"": val_formatted,
    }",Load and split the OpenMathInstruct-2 dataset into train and validation sets using HF's train_test_split.,???Prepare and format a math dataset for training and validation splits.???
2665,_register_actions_with_wallet,"def _register_actions_with_wallet(self) -> None:
        
        self.actions = {}  # Clear existing actions
        self._action_registry = {}  # Clear existing registry

        tools = get_tools(self._wallet_client, list(self._plugins.values()))  # type: ignore

        for tool in tools:
            action_parameters = self._convert_pydantic_to_action_parameters(
                tool.parameters
            )

            self.actions[tool.name] = Action(  # type: ignore
                name=tool.name,
                description=tool.description,
                parameters=action_parameters,
            )
            self._action_registry[tool.name] = tool

            register_action(tool.name)(
                lambda agent, tool_name=tool.name, **kwargs: self.perform_action(
                    tool_name, kwargs
                )
            )",Register actions with the current wallet client,???Initialize and register tools as actions within a wallet system.???
2666,_prepare_retrieve_chunks_request,"def _prepare_retrieve_chunks_request(
        self,
        query: str,
        filters: Optional[Dict[str, Any]],
        k: int,
        min_score: float,
        use_colpali: bool,
        folder_name: Optional[Union[str, List[str]]],
        end_user_id: Optional[str],
    ) -> Dict[str, Any]:
        
        request = {
            ""query"": query,
            ""filters"": filters,
            ""k"": k,
            ""min_score"": min_score,
            ""use_colpali"": use_colpali,
        }
        if folder_name:
            request[""folder_name""] = folder_name
        if end_user_id:
            request[""end_user_id""] = end_user_id
        return request",Prepare request for retrieve_chunks endpoint,???Constructs a dictionary for retrieving data chunks based on query parameters and optional filters.???
2667,get_model_response,"def get_model_response(self, question: str) -> str:
        
        # Build a ""chat"" prompt if developer_prompt is available
        chat = []
        if self.developer_prompt:
            chat.append({""role"": self.developer_role, ""content"": self.developer_prompt})
        chat.append({""role"": ""user"", ""content"": question})

        # Some Hugging Face chat-friendly models use a convenience method like below:
        prompt = self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

        response = self.llm.generate(prompt, self.sampling_params, use_tqdm=False)
        # Extract the text from the response
        response = response[0].outputs[0].text

        if self.verbose:
            print(f""[Prompt]\n{question}\n[Response]\n{response}\n{'-'*60}"")

        return response",Generates a single response to the given question and returns the raw text of that response.,???Generate a response from a language model using a chat-based prompt template.???
2668,track_cache_storage_for_origin,"def track_cache_storage_for_origin(
    origin: str,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""origin""] = origin
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Storage.trackCacheStorageForOrigin"",
        ""params"": params,
    }
    json = yield cmd_dict",Registers origin to be notified when an update occurs to its cache storage list.,???Generate a command to monitor cache storage for a specified origin.???
2669,_prepare_input,"def _prepare_input(
        self,
        query: Optional[str] = None,
        tool: Optional[str] = None,
        tool_arguments: Optional[Dict[str, Any]] = None,
        raw_data_only: bool = False,
    ) -> Dict[str, Any]:
        
        if not query and not tool:
            raise ValueError(""Either query or tool must be provided"")

        input_data = {""raw_data_only"": raw_data_only}
        if query:
            input_data[""query""] = query
        if tool:
            input_data[""tool""] = tool
        if tool_arguments:
            input_data[""tool_arguments""] = tool_arguments
        return input_data",Prepare input data for both sync and async requests.,???Prepare and validate input data for processing based on provided parameters.???
2670,execute_modules,"def execute_modules(self, answer: str) -> Tuple[bool, str]:
        
        feedback = """"
        success = True
        blocks = None
        if answer.startswith(""```""):
            answer = ""I will execute:\n"" + answer # there should always be a text before blocks for the function that display answer

        self.success = True
        for name, tool in self.tools.items():
            feedback = """"
            blocks, save_path = tool.load_exec_block(answer)

            if blocks != None:
                pretty_print(f""Executing {len(blocks)} {name} blocks..."", color=""status"")
                for block in blocks:
                    self.show_block(block)
                    output = tool.execute([block])
                    feedback = tool.interpreter_feedback(output) # tool interpreter feedback
                    success = not tool.execution_failure_check(output)
                    self.blocks_result.append(executorResult(block, feedback, success, name))
                    if not success:
                        self.success = False
                        self.memory.push('user', feedback)
                        return False, feedback
                self.memory.push('user', feedback)
                if save_path != None:
                    tool.save_block(blocks, save_path)
        return True, feedback",Execute all the tools the agent has and return the result.,"???Execute code blocks from tools, providing feedback and handling failures.???"
2671,get_recent_transactions,"def get_recent_transactions(request, filter_type=None):
    
    # Get search term from query params
    search_term = request.GET.get(""q"", """").strip()

    # Base queryset with selected fields
    queryset = (
        Transaction.objects.filter(deleted=False)
        .select_related(""account"", ""category"")
        .order_by(""-created_at"")
    )

    if filter_type:
        if filter_type == ""expenses"":
            queryset = queryset.filter(type=Transaction.Type.EXPENSE)
        elif filter_type == ""income"":
            queryset = queryset.filter(type=Transaction.Type.INCOME)

    # Apply search if provided
    if search_term:
        queryset = queryset.filter(
            Q(description__icontains=search_term)
            | Q(notes__icontains=search_term)
            | Q(internal_note__icontains=search_term)
            | Q(tags__name__icontains=search_term)
            | Q(category__name__icontains=search_term)
        )

    # Prepare data for JSON response
    data = []
    for t in queryset:
        data.append({""text"": str(t), ""value"": str(t.id)})

    return JsonResponse(data, safe=False)",Return the 100 most recent non-deleted transactions with optional search.,???Fetch and filter recent transactions based on type and search criteria for JSON response.???
2672,search_tool,"def search_tool(query: str, max_length: int) -> str:
            
            self.mock(query, max_length)
            return f""LangChain Integration, max_length: {max_length}""",Look up things online.,???Implements a search tool with LangChain integration and length constraint.???
2673,_update_cache,"def _update_cache(self, owner: str, repo: str, sha: str, repo_path: Path) -> str:
        

        if repo_path.exists():
            if not self.quiet:
                print(f""Updating cached repository: {repo_path}"")
            try:
                # Fetch latest changes
                subprocess.run([""git"", ""-C"", str(repo_path), ""fetch"", ""origin""], check=True, capture_output=True)

                # Try to checkout the target SHA
                self._checkout_sha(repo_path, sha)

                # Update cache timestamp
                repo_path.touch()

                return str(repo_path)

            except subprocess.CalledProcessError as e:
                if not self.quiet:
                    print(f""Failed to update cache, re-cloning: {e}"")
                # Remove corrupted cache and re-clone
                shutil.rmtree(repo_path)

        # Fresh clone
        if not self.quiet:
            print(f""Cloning repository to cache: {repo_path}"")
        repo_path.parent.mkdir(parents=True, exist_ok=True)

        subprocess.run([""git"", ""clone"", ""--depth"", ""50"", repo_url, str(repo_path)], check=True, capture_output=True)

        self._checkout_sha(repo_path, sha)
        return str(repo_path)",Clone or update the cached repository.,???Update or clone a Git repository cache based on its existence and target commit.???
2674,add_search_results,"def add_search_results(self, results: Sequence[Dict[str, Any]], *, query: str) -> None:
        
        if not results:
            return
        blob = [f""## Semantic search for: {query}""]
        for i, res in enumerate(results, 1):
            code = res.get(""code"") or res.get(""snippet"") or """"
            file = res.get(""file"", f""result_{i}"")
            blob.append(f""### {file}\n```\n{code}\n```"")
        self._sections.append(""\n"".join(blob))",Append semantic search matches to the context.,???Appends formatted search results to a document section based on a query.???
2675,_iteratively_init_population,"def _iteratively_init_population(self):
        
        while self._population.generation == 0:
            try:
                # get a new func using i1
                prompt = EoHPrompt.get_prompt_i1(self._task_description_str, self._function_to_evolve)
                self._sample_evaluate_register(prompt)
                if self._tot_sample_nums >= self._initial_sample_nums_max:
                    # print(f'Warning: Initialization not accomplished in {self._initial_sample_nums_max} samples !!!')
                    print(f'Note: During initialization, EoH gets {len(self._population) + len(self._population._next_gen_pop)} algorithms '
                          f'after {self._initial_sample_nums_max} trails.')
                    break
            except Exception:
                if self._debug_mode:
                    traceback.print_exc()
                    exit()
                continue",Let a thread repeat {sample -> evaluate -> register to population} to initialize a population.,???Initialize population iteratively until generation advances or sample limit reached.???
2676,process_math_data,"def process_math_data(file_path: str) -> List[Dict]:
    
    if not file_path:
        return []

    raw_data = load_jsonl(file_path)
    processed = []

    for item in raw_data:
        processed.append(
            {
                ""task"": ""math"",
                ""query_id"": str(item[""query_id""]),
                ""prompt"": item[""prompt""],
                ""solutions"": item.get(""solutions"", []),
            }
        )

    return processed",Process math dataset from JSON/JSONL file,???Process and structure math data from a JSONL file.???
2677,do_r,"def do_r(self, elm):
        
        _str = []
        for s in elm.findtext(""./{0}t"".format(OMML_NS)):
            # s = s if isinstance(s,unicode) else unicode(s,'utf-8')
            _str.append(self._t_dict.get(s, s))
        return escape_latex(BLANK.join(_str))","Get text from 'r' element,And try convert them to latex symbols",??? Convert XML text elements to LaTeX format using a dictionary mapping. ???
2678,search_functions,"def search_functions(
    db_session: Session,
    public_only: bool,
    active_only: bool,
    app_names: list[str] | None,
    intent_embedding: list[float] | None,
    limit: int,
    offset: int,
) -> list[Function]:
    
    statement = select(Function).join(App, Function.app_id == App.id)

    # filter out all functions of inactive apps and all inactive functions
    # (where app is active buy specific functions can be inactive)
    if active_only:
        statement = statement.filter(App.active).filter(Function.active)
    # if the corresponding project (api key belongs to) can only access public apps and functions,
    # filter out all functions of private apps and all private functions (where app is public but specific function is private)
    if public_only:
        statement = statement.filter(App.visibility == Visibility.PUBLIC).filter(
            Function.visibility == Visibility.PUBLIC
        )
    # filter out functions that are not in the specified apps
    if app_names is not None:
        statement = statement.filter(App.name.in_(app_names))

    if intent_embedding is not None:
        similarity_score = Function.embedding.cosine_distance(intent_embedding)
        statement = statement.order_by(similarity_score)

    statement = statement.offset(offset).limit(limit)
    logger.debug(f""Executing statement: {statement}"")

    return list(db_session.execute(statement).scalars().all())",Get a list of functions with optional filtering by app names and sorting by vector similarity to intent.,"???Query functions from a database with filters for activity, visibility, and similarity.???"
2679,setup_logging,"def setup_logging(level: str = ""INFO"") -> logging.Logger:
    
    levels = {
        ""DEBUG"": logging.DEBUG,
        ""INFO"": logging.INFO,
        ""WARNING"": logging.WARNING,
        ""ERROR"": logging.ERROR,
        ""CRITICAL"": logging.CRITICAL,
    }

    logger = logging.getLogger(""test_repo"")
    logger.setLevel(levels.get(level.upper(), logging.INFO))

    handler = logging.StreamHandler()
    formatter = logging.Formatter(""%(asctime)s - %(name)s - %(levelname)s - %(message)s"")
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    return logger",Set up and return a configured logger,"???  
Configure and return a logger with customizable severity levels.  
???"
2680,generate_biography,"def generate_biography(self) -> bool:
        
        try:
            # Mark step as in progress
            self.progress.mark_step_status(ProcessStep.GENERATE_BIOGRAPHY, Status.IN_PROGRESS)
            logger.info(""Starting biography generation..."")

            # Generate L1 data and biography
            logger.info(""Generating L1 data and biography..."")
            l1_data = generate_l1_from_l0()
            logger.info(""Successfully generated L1 data and biography"")

            # Store L1 data
            with DatabaseSession.session() as session:
                store_l1_data(session, l1_data)

            # Mark step as completed
            self.progress.mark_step_status(ProcessStep.GENERATE_BIOGRAPHY, Status.COMPLETED)
            logger.info(""Biography generation completed successfully"")
            return True

        except Exception as e:
            logger.error(f""Biography generation failed: {str(e)}"")
            self.progress.mark_step_status(ProcessStep.GENERATE_BIOGRAPHY, Status.FAILED)
            return False",Generate biography using L1 data,"???The code manages the process of generating and storing biography data, updating progress status accordingly.???"
2681,cancel_task,"def cancel_task(task_id):
    
    try:
        # Mark the task as cancelled in Redis
        store_task_status(
            task_id,
            {
                ""status"": ProgressState.CANCELLED,
                ""error"": ""Task cancelled by user"",
                ""timestamp"": time.time(),
            },
        )

        # Try to revoke the Celery task if it hasn't started yet
        celery_app.control.revoke(task_id, terminate=True, signal=""SIGTERM"")

        # Log cancellation to history
        _log_task_to_history(task_id, ""CANCELLED"", ""Task cancelled by user"")

        # Schedule deletion of task data after 30 seconds
        delayed_delete_task_data.apply_async(
            args=[task_id, ""Task cancelled by user and auto-cleaned.""], countdown=30
        )
        logger.info(
            f""Task {task_id} cancelled by user. Data scheduled for deletion in 30s.""
        )

        return {""status"": ""cancelled"", ""task_id"": task_id}
    except Exception as e:
        logger.error(f""Error cancelling task {task_id}: {e}"")
        return {""status"": ""error"", ""message"": str(e)}",Cancel a task by its ID,"???Cancel a task, update status, revoke execution, and schedule data deletion???"
2682,mock_components,"def mock_components():
    
    return [
        {""id"": ""10000"", ""name"": ""Component One""},
        {""id"": ""10001"", ""name"": ""Component Two""},
    ]",Fixture to return mock project components.,???Generate a list of mock component dictionaries with IDs and names.???
2683,get_construct_descriptions,"def get_construct_descriptions() -> Dict[str, str]:
    
    return {
        # Agent-related constructs
        'Agent_creation': 'Create and configure Bedrock Agents with foundation models, instructions, and optional features',
        'Agent_actiongroups': 'Define custom functions for Bedrock Agents to call via Lambda and OpenAPI schemas',
        'Agent_alias': 'Create versioned aliases for Bedrock Agents to manage deployment and integration',
        'Agent_collaboration': 'Configure multiple Bedrock Agents to work together on complex tasks',
        'Agent_custom_orchestration': 'Override default agent orchestration flow with custom Lambda functions',
        'Agent_prompt_override': 'Customize prompts and LLM configurations for different agent processing steps',
        # Knowledge Base constructs
        'Knowledgebases_kendra': 'Create knowledge bases from Amazon Kendra GenAI indexes for RAG applications',
        'Knowledgebases_datasources': 'Configure data sources for Bedrock Knowledge Bases including S3, web crawlers, and more',
        'Knowledgebases_parsing': 'Define strategies for processing and interpreting document contents in knowledge bases',
        'Knowledgebases_transformation': 'Apply custom processing steps to documents during knowledge base ingestion',
        'Knowledgebases_chunking': 'Configure document chunking strategies for optimal knowledge base performance',
        'Knowledgebases_vector_opensearch': 'Use OpenSearch Serverless as a vector store (vector database) for Bedrock Knowledge Bases',
        'Knowledgebases_vector_aurora': 'Use Amazon RDS Aurora PostgreSQL as a vector store (vector database) for Bedrock Knowledge Bases',
        'Knowledgebases_vector_pinecone': 'Use Pinecone as a vector store (vector database) for Bedrock Knowledge Bases',
        'Knowledgebases_vector_creation': 'Create and configure vector stores (vector databases) for Bedrock Knowledge Bases',
        # Other Bedrock constructs
        'Bedrockguardrails': 'Configure content filtering and safety guardrails for Bedrock foundation models',
        'Profiles': 'Create and manage inference profiles for tracking usage and costs across regions',
        # OpenSearch constructs
        'Opensearchserverless_overview': 'Create and configure Amazon OpenSearch Serverless for vector search applications',
        'Opensearch_vectorindex_overview': 'Configure vector indexes in Amazon OpenSearch for semantic search',
    }",Get a dictionary mapping construct names to their descriptions.,"???Defines descriptions for various constructs related to Bedrock Agents, Knowledge Bases, and OpenSearch.???"
2684,install_crd,"def install_crd(self):
        
        crd_url = self.helm_operator_config.get(""CRD"")
        if not crd_url:
            raise ValueError(""CRD URL is not specified in the configuration."")

        print(""Installing CRDs..."")
        command = f""kubectl create -f {crd_url}""
        self.kubectl.exec_command(command)",Install the Custom Resource Definitions (CRDs) for TiDB Operator.,???Install custom resource definitions using a specified URL from configuration.???
2685,check_health,"def check_health():
    
    try:
        if client.check_health():
            console.print(""[green]Server is healthy[/]"")
        else:
            console.print(""[red]Server is not responding correctly[/]"")
            raise typer.Exit(1)
    except Exception as e:
        console.print(f""[red]Error connecting to server: {e}[/]"")
        raise typer.Exit(1)",Check server connection and health status.,??? Check server health status and handle connection errors. ???
2686,_should_filter_line_content,"def _should_filter_line_content(line: str, allowed_priorities: set) -> bool:
    
    line_lower = line.strip().lower()

    # Skip empty lines and non-content lines
    if not line_lower or line_lower.startswith(""#""):
        return False

    # Only filter lines that explicitly mention specific priority levels
    for priority in [""high"", ""medium"", ""low""]:
        if priority not in allowed_priorities:
            # Look for explicit priority mentions, not just related words
            priority_patterns = [
                f""{priority} priority"",
                f""**{priority}**:"",
                f""- **{priority}**:"",
                ""**immediate**:"" if priority == ""high"" else None,
                ""**before merge**:"" if priority == ""medium"" else None,
                ""**follow-up**:"" if priority == ""low"" else None,
            ]

            # Filter None values and check for matches
            patterns = [p for p in priority_patterns if p is not None]
            if any(pattern in line_lower for pattern in patterns):
                return True

    return False",Determine if a line should be filtered out based on priority references.,???Determine if a text line should be filtered based on specified priority levels.???
2687,mapping_model_to_model_cost,"def mapping_model_to_model_cost(self) -> dict:
        
        return {
            ""claude-3-5-sonnet-latest"": ""claude-3-5-sonnet-20241022"",
            ""claude-3-5-haiku-latest"": ""claude-3-5-haiku-20241022"",
            ""claude-3-opus-latest"": ""claude-3-opus-20240229"",
        }",Maps the model name to the model cost name.,"???  
Map model names to corresponding cost identifiers.  
???"
2688,patch_service,"def patch_service(self, name, namespace, body):
        
        try:
            api_response = self.core_v1_api.patch_namespaced_service(
                name, namespace, body
            )
            return api_response
        except ApiException as e:
            print(f""Exception when patching service: {e}\n"")
            return None",Patch a Kubernetes service in a specified namespace.,??? Update Kubernetes service configuration and handle exceptions. ???
2689,symbolic_to_event,"def symbolic_to_event(self, symbolic_event: EventSymbolic) -> Event:
        
        event_dict = symbolic_event.symbolic_info.dict()
        if '## Rows Constraints:\n' in symbolic_event.policies_constraints:
            policies_constraints = symbolic_event.policies_constraints.split('## Rows Constraints:\n')[1]
        else:
            policies_constraints = ''
        res = self.agent.invoke(rows_to_generate=event_dict['tables_rows'], rows_generated=[],
                                event_description=event_dict['enriched_scenario'], variables_definitions='{}',
                                cur_restrictions=None, dataset={},
                                all_restrictions=policies_constraints)

        event = Event(description=symbolic_event.description,
                      database=res['dataset'], scenario=res['final_response_scenario'],
                      relevant_rows=res['final_response_table_rows'])
        return event",Generate an event based on the given symbolic_event.,???Convert symbolic event data into a structured event using agent processing???
2690,edit_message,"def edit_message(st: Any, button_idx: int, message_type: str) -> None:
        
        button_id = f""edit_box_{button_idx}""
        if message_type == ""human"":
            messages = st.session_state.user_chats[st.session_state[""session_id""]][
                ""messages""
            ]
            st.session_state.user_chats[st.session_state[""session_id""]][
                ""messages""
            ] = messages[:button_idx]
            st.session_state.modified_prompt = st.session_state[button_id]
        else:
            st.session_state.user_chats[st.session_state[""session_id""]][""messages""][
                button_idx
            ][""content""] = st.session_state[button_id]",Edit a message in the chat history.,???Edit chat messages based on user interaction and message type???
2691,sample_ddl_queries,"def sample_ddl_queries() -> dict[str, str]:
    
    return {
        ""create_table"": ""CREATE TABLE users (id SERIAL PRIMARY KEY, name TEXT, email TEXT UNIQUE)"",
        ""create_table_with_schema"": ""CREATE TABLE public.users (id SERIAL PRIMARY KEY, name TEXT, email TEXT UNIQUE)"",
        ""create_table_custom_schema"": ""CREATE TABLE app.users (id SERIAL PRIMARY KEY, name TEXT, email TEXT UNIQUE)"",
        ""alter_table"": ""ALTER TABLE users ADD COLUMN active BOOLEAN DEFAULT false"",
        ""drop_table"": ""DROP TABLE users"",
        ""truncate_table"": ""TRUNCATE TABLE users"",
        ""create_index"": ""CREATE INDEX idx_user_email ON users (email)"",
    }",Return a dictionary of sample DDL queries for testing.,??? Provides a dictionary of SQL Data Definition Language (DDL) query templates. ???
2692,set_show_scroll_snap_overlays,"def set_show_scroll_snap_overlays(
    scroll_snap_highlight_configs: typing.List[ScrollSnapHighlightConfig],
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""scrollSnapHighlightConfigs""] = [
        i.to_json() for i in scroll_snap_highlight_configs
    ]
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Overlay.setShowScrollSnapOverlays"",
        ""params"": params,
    }
    json = yield cmd_dict",:param scroll_snap_highlight_configs: An array of node identifiers and descriptors for the highlight appearance.,??? Configure and send scroll snap overlay settings for visual debugging. ???
2693,get_room_info,"def get_room_info(self) -> Dict[str, Any]:
        
        try:
            url = f""{self.api_url}/api/rooms""
            response = self._make_request(""GET"", url)
            room_info = next((room for room in response.get(""rooms"", []) if room[""id""] == self.room), None)
            if not room_info:
                raise EchochambersAPIError(f""Room '{self.room}' not found"")

            return {
                ""id"": room_info[""id""],
                ""name"": room_info[""name""],
                ""topic"": room_info.get(""topic"", ""General Discussion""),
                ""tags"": room_info[""tags""],
                ""messageCount"": room_info[""messageCount""]
            }
        except Exception as e:
            self._handle_error(""Failed to get room info"", e)
            raise",Get information about the current room by listing all rooms and finding ours,???Retrieve and return detailed information about a specific chat room from an API.???
2694,get_episodes_for_series,"def get_episodes_for_series(series_id: int) -> Optional[List[Dict]]:
    
    if APP_TYPE != ""sonarr"":
        logger.error(""get_episodes_for_series() called but APP_TYPE is not sonarr"")
        return None
    
    return arr_request(f""episode?seriesId={series_id}"", method=""GET"")",Get all episodes for a specific series,"???Fetch series episodes if app type is 'sonarr', else log error.???"
2695,load,"def load(cls, filepath: str):
        
        with open(filepath, 'r') as f:
            data = json.load(f)
        
        # Create new graph
        cache_dir = os.path.dirname(filepath)
        graph = cls(cache_dir)
        graph.timestamp = data.get(""timestamp"", graph.timestamp)
        
        # Create all nodes first
        nodes = {}
        for node_hash, node_data in data[""nodes""].items():
            node = PromptNode(
                name=node_data[""name""],
                text=node_data[""text""],
                metadata=node_data.get(""metadata"", {})
            )
            node.response = node_data.get(""response"")
            nodes[node_hash] = node
            graph.nodes_by_hash[node_hash] = node
        
        # Set up connections
        for node_hash, node_data in data[""nodes""].items():
            node = nodes[node_hash]
            for child_hash in node_data.get(""children"", []):
                if child_hash in nodes:
                    node.kids.append(nodes[child_hash])
        
        # Set root nodes
        for root_hash in data.get(""root_nodes"", []):
            if root_hash in nodes:
                graph.root_nodes.append(nodes[root_hash])
        
        return graph",Load a graph from a JSON file,???Load and reconstruct a graph structure from a JSON file.???
2696,load_selected_conversation,"def load_selected_conversation(self, messages):
        
        global conversation_messages
        
        self.clear_chat()
        
        # Reset conversation messages, ensuring system prompt is present
        conversation_messages = []
        has_system = False
        
        for msg in messages:
            if msg[""role""] == ""system"":
                has_system = True
            conversation_messages.append(msg)
            # Only display user and assistant messages in UI
            if msg[""role""] in [""user"", ""assistant""]:
                if msg[""role""] == ""assistant"":
                    self.add_message(msg[""content""], role=msg[""role""], engine=msg.get(""model"", MODEL_ENGINE))
                else:
                    self.add_message(msg[""content""], role=msg[""role""])
        
        # Add system prompt if not present
        if not has_system:
            conversation_messages.insert(0, {""role"": ""system"", ""content"": SYSTEM_PROMPT})
        
        # Close sidebar after selection
        self.sidebar_visible = False
        self.history_sidebar.hide_sidebar()
        
        # Scroll to see latest messages
        self.scroll_to_bottom()",Handle clicking on a conversation in the sidebar,"???Load and display a conversation, ensuring system prompt inclusion and UI update.???"
2697,_get_name_node,"def _get_name_node(self) -> TSNode:
        
        for child in self.ts_node.children:
            # =====[ Identifier ]=====
            # Just `@dataclass` etc.
            if child.type == ""identifier"":
                return child

            # =====[ Attribute ]=====
            # e.g. `@a.b`
            elif child.type == ""attribute"":
                return child

            # =====[ Call ]=====
            # e.g. `@a.b()`
            elif child.type == ""call"":
                func = child.child_by_field_name(""function"")
                return func

        msg = f""Could not find decorator name within {self.source}""
        raise ValueError(msg)",Returns the name of the decorator.,???Extracts decorator name node from syntax tree children or raises error if not found.???
2698,cfg2task,"def cfg2task(cfg):
        
        m = cfg[""head""][-1][-2].lower()  # output module name
        if m in {""classify"", ""classifier"", ""cls"", ""fc""}:
            return ""classify""
        if ""detect"" in m:
            return ""detect""
        if ""segment"" in m:
            return ""segment""
        if m == ""pose"":
            return ""pose""
        if m == ""obb"":
            return ""obb""",Guess from YAML dictionary.,???Determine task type from configuration module name.???
2699,list_dict_to_dict_list,"def list_dict_to_dict_list(list_dict: list[dict[K, V]]) -> dict[K, list[V]]:
    
    if len(list_dict) == 0:
        return {}

    keys = list_dict[0].keys()
    if any(d.keys() != keys for d in list_dict):
        raise ValueError(""All dicts must have the same keys"")

    return {k: [d[k] for d in list_dict] for k in keys}","Converts a list of dicts that contain the same keys to a dict of lists, where each list contains an ordered list of values of the corresponding dict.","???Transforms a list of dictionaries into a dictionary of lists, ensuring consistent keys.???"
2700,get_collection_info,"def get_collection_info(self) -> Dict[str, Any]:
        
        # vecs collections have various properties we can inspect
        return {
            ""name"": self.collection.name,
            ""dimension"": self.collection.dimension,
            # Add more collection info as available from vecs
        }",Get information about the collection.,???Retrieve metadata details of a vector collection as a dictionary.???
2701,get_memory_client,"def get_memory_client(self, memory_name: str | MemoryRef) -> MemoryEditor:
        
        if memory_name not in self._memory_clients:
            raise ValueError(f""Memory `{memory_name}` not found"")

        return self._memory_clients[memory_name].instance",Return the instantiated memory client for the given name.,"???Retrieve a memory editor instance by memory name or reference, raising an error if not found.???"
2702,match,"def match(response, correct_answer) -> int:
        
        if not isinstance(response, str) or isinstance(correct_answer, str):
            return 0
        resp = pipe(f""[CLS] {correct_answer.strip()} [SEP] {response.strip()} [SEP]"")
        return 1 if resp[0][""label""] == ""ENTAILMENT"" else 0",Return whether the response and correct answer agree with each other.,???Determine if a response semantically matches a correct answer using a language model.???
2703,sol_balance,"def sol_balance(agent, **kwargs):
    
    agent.logger.info(""\n💰 CHECKING BALANCE"")
    try:
        result = agent.connection_manager.perform_action(
            connection_name=""solana"",
            action_name=""get-balance"",
            params=[kwargs.get('token_address', None)]
        )
        agent.logger.info(f""Balance: {result}"")
        return result
    except Exception as e:
        agent.logger.error(f""❌ Balance check failed: {str(e)}"")
        return None",Check SOL or token balance,???Check and log the balance of a Solana token using an agent's connection manager.???
2704,fetch_latent,"def fetch_latent(fp, data_backend_id: str):
    
    debug_log(
        f"" -> pull latents for fp {fp} from cache via data backend {data_backend_id}""
    )
    latent = StateTracker.get_vaecache(id=data_backend_id).retrieve_from_cache(fp)

    # Move to CPU and pin memory if it's not on the GPU
    if not torch.backends.mps.is_available():
        debug_log("" -> push latents to GPU via pinned memory"")
        latent = latent.to(""cpu"").pin_memory()
    return latent",Worker method to fetch latent for a single image.,???Retrieve cached latent data and manage memory allocation based on GPU availability.???
2705,get_system_info,"def get_system_info() -> dict:
    
    return {
        ""platform"": platform.system(),
        ""platform_version"": platform.version(),
        ""python_version"": sys.version,
        ""python_path"": sys.executable,
    }","Get system information including Python version, OS, etc.","???  
Retrieve and return system and Python environment details as a dictionary.  
???"
2706,start_server,"def start_server(self):
        
        logger.info(""Starting vLLM server..."")
        # Command to start the vLLM server
        is_awq = ""awq"" in self.model_name.lower()
        command = [
            ""vllm"",
            ""serve"",
            self.model_name.replace(""hosted_vllm/"", """"),
            ""--host"",
            self.host,
            ""--port"",
            str(self.port),
            ""--dtype"",
            ""bfloat16"" if not is_awq else ""float16"",
            ""--limit-mm-per-prompt"",
            f""image={self.max_num_imgs},video=0"",
            ""--served-model-name"",
            self.model_name.replace(""hosted_vllm/"", """"),
            ""--max-model-len"",
            str(self.max_model_len),
            ""--gpu-memory-utilization"",
            str(self.gpu_memory_utilization),
            ""--enforce-eager"",
            ""--disable-log-stats"",  # disable log stats
        ]
        if is_awq:
            command.extend([""--quantization"", ""awq""])

        # Start the server as a subprocess
        self.server_process = subprocess.Popen(command)",Start the vLLM server in a background thread.,???Initialize and launch a vLLM server with specified configurations and model parameters.???
2707,handle_unordered_list,"def handle_unordered_list(self, element: lxml_html.HtmlElement) -> str:
        
        self.list_state.append({""type"": ""ul"", ""index"": 0})
        content = self.process_children(element, True)
        self.list_state.pop()
        return f""\n{content}\n""",Handle unordered list elements,???Process HTML unordered list elements into formatted string content.???
2708,convert_plaintext_links_to_html,"def convert_plaintext_links_to_html(content):
    
    soup = BeautifulSoup(content, ""html.parser"")

    # Find the main content area (adjust this selector based on your HTML structure)
    main_content = soup.find(""main"") or soup.find(""div"", class_=""md-content"")
    if not main_content:
        return content  # Return original content if main content area not found

    modified = False
    for paragraph in main_content.find_all([""p"", ""li""]):  # Focus on paragraphs and list items
        for text_node in paragraph.find_all(string=True, recursive=False):
            if text_node.parent.name not in {""a"", ""code""}:  # Ignore links and code blocks
                new_text = re.sub(
                    r'<a href=""\1"">\1</a>',
                    str(text_node),
                )
                if ""<a href="" in new_text:
                    # Parse the new text with BeautifulSoup to handle HTML properly
                    new_soup = BeautifulSoup(new_text, ""html.parser"")
                    text_node.replace_with(new_soup)
                    modified = True

    return str(soup) if modified else content",Convert plaintext links to HTML hyperlinks in the main content area only.,???Transform plaintext URLs in HTML content into clickable links within main sections.???
2709,add_reply,"def add_reply(self, reply_data: dict):
        
        logger.debug(f""Adding new reply to queue: {reply_data['tweet_id']}"")
        data = self.read_data()
        # Use tweet_id from the API response
        reply_data[""message_id""] = reply_data[""tweet_id""]  # Store tweet_id as message_id
        data[""pending_replies""].append(reply_data)  # Append to maintain order
        self.write_data(data)",Add new reply to queue,???Queue a new reply using tweet ID as message ID for processing.???
2710,generate_code_solution,"def generate_code_solution(context: dict[str, Any], **kwargs):
        

        logger.info(""CODE GENERATION"")

        messages = context.get(""messages"")

        if context.get(""reiterate"", False):
            messages += [Message(role=""user"", content=""Generate code again taking into account errors. {}"")]

        code_solution = code_llm(messages)
        context[""solution""] = code_solution

        context[""messages""] += [
            Message(
                role=""assistant"",
                content=f""\n Imports: {code_solution.get('libraries')} \n Code: {code_solution.get('code')}"",
            )
        ]

        context[""iterations_num""] += 1

        return {""result"": code_solution, **context}",Generate a code solution,"???Generate code solutions iteratively, updating context with results and messages.???"
2711,_cache_response,"def _cache_response(
        self,
        messages: list[dict[str, Any]],
        model_name: str,
        response: str,
    ):
        
        hash_messages = hashlib.sha256(str(messages).encode()).hexdigest()
        cache_file = os.path.join(
            self.cache_dir,
            f""{model_name.replace('/', '_')}_{hash_messages}.json"",
        )
        with open(cache_file, ""w"") as f:
            json.dump(response, f)",Cache the response for the given messages and model name.,???Cache AI model responses using hashed message identifiers.???
2712,enhance_span_attributes,"def enhance_span_attributes(span, trace_data):
    
    common_attributes = {
        ""trace.step_number"": trace_data.get(""step_number"", 0),
        ""trace.component_type"": trace_data.get(""type"", ""unknown""),
        ""trace.timestamp"": datetime.now().isoformat(),
    }

    if ""metadata"" in trace_data and ""usage"" in trace_data[""metadata""]:
        usage = trace_data[""metadata""][""usage""]
        common_attributes.update(
            {
                ""llm.token_count.input"": usage.get(""inputTokens"", 0),
                ""llm.token_count.output"": usage.get(""outputTokens"", 0),
                ""llm.token_count.total"": usage.get(""inputTokens"", 0)
                + usage.get(""outputTokens"", 0),
            }
        )

    if ""duration"" in trace_data:
        common_attributes[""trace.duration""] = trace_data[""duration""]

    if ""metadata"" in trace_data:
        common_attributes[""trace.metadata""] = json.dumps(
            trace_data[""metadata""], cls=DateTimeEncoder
        )

    set_span_attributes(span, common_attributes)",Enhances span with comprehensive attributes from trace data,???Augment span with trace and metadata attributes for logging???
2713,lookup_user_profile,"def lookup_user_profile(input_string: str) -> str:
    
    # Actually just returns canned data
    if ""1234"" in input_string:
        return json.dumps(
            {""name"": ""John Doe"", ""age"": 30, ""email"": ""john.doe@example.com""}
        )
    else:
        return json.dumps({""error"": ""No user with that ID was found""})",Look up a user profile by ID.,???Simulates user profile retrieval based on input string ID.???
2714,render_content,"def render_content(content: str, inputs: Dict[str, Any]) -> str:
        
        try:
            template = Template(content)
            rendered_content = template.render(inputs)
        except TemplateError as e:
            raise ValueError(f""Template rendering error: {e}"")

        return rendered_content",Render the Prompty content using Jinja2 with the provided inputs.,"??? Render a template with dynamic inputs, handling errors gracefully. ???"
2715,_format_user_preview,"def _format_user_preview(self, user: Dict[str, Any]) -> Dict[str, Any]:
        
        return {
            ""id"": f""user_{user.get('id', '')}"",
            ""title"": user.get(""login"", """"),
            ""link"": user.get(""html_url"", """"),
            ""snippet"": user.get(""bio"", ""No bio provided""),
            ""name"": user.get(""name"", """"),
            ""followers"": user.get(""followers"", 0),
            ""public_repos"": user.get(""public_repos"", 0),
            ""location"": user.get(""location"", """"),
            ""search_type"": ""user"",
            ""user_login"": user.get(""login"", """"),
        }",Format user search result as preview,"???  
Transforms user data into a structured preview dictionary for display.  
???"
2716,parse_insertion_point,"def parse_insertion_point(position: Optional[str] = None) -> Optional[InsertionPoint]:
    
    if position is None:
        return None  # defer assumptions

    valid_positions = {x.value for x in InsertionPoint}
    if position not in valid_positions:
        raise ValueError(f""Position must be one of {','.join(valid_positions)}."")

    return next(x for x in InsertionPoint if x.value == position)",Parse an insertion point CLI argument into an InsertionPoint enum.,???Validate and convert a position string to an InsertionPoint object.???
2717,task_reflect_block_around_dot,"def task_reflect_block_around_dot(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    dot_color = 2

    dot_pos = rng.randint(0, size - 1)
    block_size = rng.randint(1, size - 1)
    block_pos = rng.randint(0, size - block_size)
    block_end = block_pos + block_size - 1

    # Check if block is strictly to left or right of dot
    strictly_left = block_end < dot_pos
    strictly_right = block_pos > dot_pos

    if not (strictly_left or strictly_right):
        return None

    block_color = rng.randint(3, 9)  # Different from dot color
    block = [block_color] * block_size

    # Calculate reflection bounds
    min_reflect = 2 * dot_pos - block_end
    max_reflect = 2 * dot_pos - block_pos
    if min_reflect < 0 or max_reflect >= size:
        return None

    question = gen_field(size)
    question = write_block(block_pos, block, question)
    question[dot_pos] = dot_color

    answer = gen_field(size)
    answer[dot_pos] = dot_color
    for i in range(block_size):
        reflect_idx = 2 * dot_pos - (block_pos + i)
        answer[reflect_idx] = block[i]

    return {""input"": question, ""output"": answer}",Generate a task where a block is reflected around a dot.,???Generate a reflected block pattern around a dot within a bounded array.???
2718,_validate_connection,"def _validate_connection(self):
        
        try:
            # Make a minimal test query
            response = self._make_request(""test"")

            # Check if we got a valid response
            if response.get(""error""):
                error_msg = response[""error""].get(""message"", ""Unknown error"")
                raise ValueError(f""Google PSE API error: {error_msg}"")

            # If we get here, the connection is valid
            logger.info(""Google PSE connection validated successfully"")
            return True

        except Exception as e:
            # Log the error and re-raise
            logger.error(f""Error validating Google PSE connection: {str(e)}"")
            raise",Test the connection to ensure API key and Search Engine ID are valid,???Validate Google PSE API connection by testing and handling errors???
2719,create_httpx_client,"def create_httpx_client(self, settings: Settings) -> httpx.AsyncClient:
        
        headers = {
            ""Authorization"": f""Bearer {settings.supabase_access_token}"",
            ""Content-Type"": ""application/json"",
        }

        return httpx.AsyncClient(
            base_url=settings.supabase_api_url,
            headers=headers,
            timeout=30.0,
        )",Create and configure an httpx client for API requests.,"???  
Initialize an asynchronous HTTP client with authorization headers.  
???"
2720,_get_user_timezone,"def _get_user_timezone():
    
    try:
        from src.primary.utils.timezone_utils import get_user_timezone
        return get_user_timezone()
    except Exception as e:
        stateful_logger.warning(f""Could not get user timezone, defaulting to UTC: {e}"")
        import pytz
        return pytz.UTC",Get the user's selected timezone from general settings,"???  
Retrieve user timezone, default to UTC on failure.  
???"
2721,edge_case_support_case_data,"def edge_case_support_case_data() -> Dict[str, Any]:
    
    return {
        ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
        ""displayId"": ""12345678910"",
        ""subject"": ""EC2 instance not starting"" * 50,  # Very long subject
        ""status"": ""opened"",
        ""serviceCode"": ""amazon-elastic-compute-cloud-linux"",
        ""categoryCode"": ""using-aws"",
        ""severityCode"": ""urgent"",
        ""submittedBy"": ""user@example.com"",
        ""timeCreated"": ""2023-01-01T12:00:00Z"",
        ""recentCommunications"": {
            ""communications"": [
                {
                    ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
                    ""body"": ""My EC2 instance i-1234567890abcdef0 is not starting."",
                    ""submittedBy"": ""user@example.com"",
                    ""timeCreated"": ""2023-01-01T12:00:00Z"",
                }
            ],
            ""nextToken"": None,
        },
        ""ccEmailAddresses"": [""team@example.com""],
        ""language"": ""en"",
        ""nextToken"": None,
    }",Return a dictionary with edge case support case data.,???Generate mock support case data with edge-case attributes for testing.???
2722,generate,"def generate(self, messages, temperature=0.0, max_new_tokens=None, **kwargs):
        
        if self.thinking:
            full_response = self.llm_client.messages.create(
                system=messages[0][""content""][0][""text""],
                model=self.model,
                messages=messages[1:],
                max_tokens=8192,
                thinking={""type"": ""enabled"", ""budget_tokens"": 4096},
                **kwargs,
            )

            thoughts = full_response.content[0].thinking
            print(""CLAUDE 3.7 THOUGHTS:"", thoughts)
            return full_response.content[1].text

        return (
            self.llm_client.messages.create(
                system=messages[0][""content""][0][""text""],
                model=self.model,
                messages=messages[1:],
                max_tokens=max_new_tokens if max_new_tokens else 4096,
                temperature=temperature,
                **kwargs,
            )
            .content[0]
            .text
        )",Generate the next message based on previous messages,???Generate AI responses with optional thought processing and token limits.???
2723,register,"def register(self, app: typer.Typer, run_command_func: Callable) -> None:
        
        # Default implementation - override in subclasses if needed
        # Pass help text explicitly to the command decorator
        @app.command(self.name, help=self.help)
        def _command_wrapper(
            config_file: str = typer.Option(""server_config.json"", help=""Configuration file path""),
            server: Optional[str] = typer.Option(None, help=""Server to connect to""),
            provider: str = typer.Option(""openai"", help=""LLM provider name""),
            model: Optional[str] = typer.Option(None, help=""Model name""),
            disable_filesystem: bool = typer.Option(False, help=""Disable filesystem access""),
        ) -> None:  # ← REMOVED **kwargs - this was causing the error
            
            servers, _, server_names = process_options(
                server, disable_filesystem, provider, model, config_file
            )
            
            extra_params = {
                ""provider"": provider,
                ""model"": model,
                ""server_names"": server_names,
            }
            
            run_command_func(
                self.wrapped_execute,
                config_file,
                servers,
                extra_params=extra_params
            )
        
        # Explicitly set the wrapper's docstring to match the help text
        _command_wrapper.__doc__ = self.help",Register this command with the Typer app.,???Registers a command with options to a Typer app for execution.???
2724,perform_action,"def perform_action(self, connection: str, action: str, params: Optional[List[str]] = None) -> Dict[str, Any]:
        
        data = {
            ""connection"": connection,
            ""action"": action,
            ""params"": params or []
        }
        return self._make_request(""POST"", ""/agent/action"", json=data)",Execute an agent action,???Send a POST request to execute a specified action with parameters.???
2725,_send_execute_request,"def _send_execute_request(self, code: str) -> str:
        
        import uuid

        # Generate a unique message ID
        msg_id = str(uuid.uuid4())

        # Create execute request
        execute_request = {
            ""header"": {
                ""msg_id"": msg_id,
                ""username"": ""anonymous"",
                ""session"": str(uuid.uuid4()),
                ""msg_type"": ""execute_request"",
                ""version"": ""5.0"",
            },
            ""parent_header"": {},
            ""metadata"": {},
            ""content"": {
                ""code"": code,
                ""silent"": False,
                ""store_history"": True,
                ""user_expressions"": {},
                ""allow_stdin"": False,
            },
        }

        self.ws.send(json.dumps(execute_request))
        return msg_id",Send code execution request to kernel.,???Send code execution request with unique ID via WebSocket???
2726,_generate_dot_file,"def _generate_dot_file(self, graph: Dict[str, Dict[str, Any]]):
        
        dot_lines = [""digraph TerraformDependencies {"", '    rankdir=""LR"";', ""    node [shape=box];""]

        # Add nodes with categories as colors
        for node_id, data in graph.items():
            color = '""#1f77b4""'  # Default blue
            if data.get(""category"") == ""resource"":
                color = '""#ff7f0e""'  # Orange
            elif data.get(""category"") == ""module"":
                color = '""#2ca02c""'  # Green
            elif data.get(""category"") == ""data"":
                color = '""#9467bd""'  # Purple

            # Include file path in the label
            file_path = data.get(""path"", """")
            path_to_display = file_path if file_path else ""unknown file""
            label = f'""{node_id}\n({data.get(""type"", ""unknown"")})\n[{path_to_display}]""'
            dot_lines.append(f'    ""{node_id}"" [label={label}, style=""filled"", fillcolor={color}];')

        # Add edges
        for node_id, data in graph.items():
            if ""dependencies"" in data:
                for dep in data[""dependencies""]:
                    if dep in graph:  # Only add edges to nodes that exist
                        dot_lines.append(f'    ""{node_id}"" -> ""{dep}"";')

        dot_lines.append(""}"")

        return ""\n"".join(dot_lines)",Generate a DOT file for visualization with Graphviz.,???Generate a DOT file representing a graph with categorized nodes and dependencies.???
2727,on_stream_complete,"def on_stream_complete(completion_data):
        
        try:
            # Join collected chunks to create the complete answer
            answer_text = """".join(completion_data[""chunks""])

            # Update root span with the complete response
            if root_span:
                root_span.set_attribute(SpanAttributes.LLM_COMPLETIONS, answer_text)
                root_span.set_attribute(
                    ""streaming.chunks"", len(completion_data[""chunks""])
                )
                root_span.set_attribute(""streaming.completed"", True)

                # Clean up spans
                from .agent import span_manager

                span_manager.reset()

                # Set end time
                end_timestamp, end_time_iso = (
                    time.time(),
                    datetime.now(timezone.utc).replace(tzinfo=None).isoformat(),
                )
                root_span.set_attribute(SpanAttributes.LANGFUSE_END_TIME, end_time_iso)

                # Set final status
                root_span.set_status(Status(StatusCode.OK))

        except Exception as e:
            logger.error(f""Error in stream complete callback: {str(e)}"", exc_info=True)",Callback when stream is complete.,"???Handle completion of data stream, update attributes, and manage error logging.???"
2728,_build_scrape_data,"def _build_scrape_data(self, url: str) -> dict:
        
        base_data = {
            ""url"": url,
            ""formats"": self.formats,
            ""onlyMainContent"": self.only_main_content,
        }

        conditional_fields = {
            ""includeTags"": self.include_tags,
            ""excludeTags"": self.exclude_tags,
            ""headers"": self.headers,
            ""waitFor"": self.wait_for if self.wait_for > 0 else None,
            ""mobile"": self.mobile if self.mobile else None,
            ""skipTlsVerification"": self.skip_tls_verification if self.skip_tls_verification else None,
            ""timeout"": self.timeout if self.timeout != 30000 else None,
            ""removeBase64Images"": self.remove_base64_images if self.remove_base64_images else None,
            ""proxy"": self.proxy,
        }

        if not self.block_ads:
            conditional_fields[""blockAds""] = False

        if self.json_options:
            conditional_fields[""jsonOptions""] = self.json_options.model_dump(exclude_none=True, by_alias=True)
        if self.actions:
            conditional_fields[""actions""] = [action.model_dump(exclude_none=True) for action in self.actions]
        if self.location:
            conditional_fields[""location""] = self.location.model_dump(exclude_none=True)

        # Filter out None values and merge with base data
        filtered_fields = {k: v for k, v in conditional_fields.items() if v is not None}
        return {**base_data, **filtered_fields}",Build the request payload for the Firecrawl API.,???Constructs a comprehensive web scraping configuration dictionary based on specified parameters.???
2729,chunk_by_symbols,"def chunk_by_symbols(
    path: str = typer.Argument(..., help=""Path to the local repository.""),
    file_path: str = typer.Argument(..., help=""Relative path to the file within the repository.""),
    output: Optional[str] = typer.Option(None, ""--output"", ""-o"", help=""Output to JSON file instead of stdout.""),
):
    
    from kit import Repository

    try:
        repo = Repository(path)
        chunks = repo.chunk_file_by_symbols(file_path)

        if output:
            Path(output).write_text(json.dumps(chunks, indent=2))
            typer.echo(f""Symbol chunks written to {output}"")
        else:
            for chunk in chunks:
                typer.echo(f""--- {chunk.get('type', 'Symbol')}: {chunk.get('name', 'N/A')} ---"")
                typer.echo(chunk.get(""code"", """"))
                typer.echo()
    except Exception as e:
        typer.secho(f""Error: {e}"", fg=typer.colors.RED)
        raise typer.Exit(code=1)","Chunk a file's content by symbols (functions, classes).",???Parse and output code file symbols from a repository to JSON or console.???
2730,set_async_call_stack_depth,"def set_async_call_stack_depth(
    max_depth: int,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""maxDepth""] = max_depth
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Runtime.setAsyncCallStackDepth"",
        ""params"": params,
    }
    json = yield cmd_dict",Enables or disables async call stacks tracking.,???Configure asynchronous call stack depth for runtime debugging???
2731,get_thumbnail_path,"def get_thumbnail_path(file_hash):
    
    thumbnail_dir = os.path.join(os.getcwd(), '.thumbnails')
    # Check if PNG version exists first
    png_path = os.path.join(thumbnail_dir, f""{file_hash}.png"")
    if os.path.exists(png_path):
        return png_path
    # Otherwise use JPEG path
    return os.path.join(thumbnail_dir, f""{file_hash}.jpg"")",Get the full path to a cached thumbnail.,???Determine the file path for a thumbnail image based on its hash.???
2732,optimize_training_args,"def optimize_training_args(self, training_args):
        
        if not training_args:
            return None
            
        # Get optimal configuration based on hardware
        config = self.get_optimal_training_config()
        
        # Apply configurations to training arguments
        if not getattr(training_args, ""fp16"", False) and not getattr(training_args, ""bf16"", False):
            training_args.fp16 = config[""fp16""]
            training_args.bf16 = config[""bf16""]
        
        if not getattr(training_args, ""gradient_checkpointing"", False):
            training_args.gradient_checkpointing = config[""gradient_checkpointing""]
        
        if training_args.gradient_accumulation_steps == 1:
            training_args.gradient_accumulation_steps = config[""gradient_accumulation_steps""]
        
        logger.info(""Training configuration optimized for memory efficiency:"")
        logger.info(f""  Mixed precision: FP16={training_args.fp16}, BF16={training_args.bf16}"")
        logger.info(f""  Gradient checkpointing: {training_args.gradient_checkpointing}"")
        logger.info(f""  Gradient accumulation steps: {training_args.gradient_accumulation_steps}"")
        
        return training_args",Configure training arguments for efficient memory usage.,???Optimize training parameters for memory efficiency based on hardware configuration.???
2733,_load_or_create_vector_store,"def _load_or_create_vector_store(self):
        
        vector_store_path = self._get_vector_store_path()

        # Check if vector store exists and is up to date
        if vector_store_path.exists() and not self._check_folders_modified():
            logger.info(f""Loading existing vector store from {vector_store_path}"")
            try:
                vector_store = FAISS.load_local(
                    str(vector_store_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True,
                    normalize_L2=True,
                )

                # Add this code to show document count
                doc_count = len(vector_store.index_to_docstore_id)
                logger.info(f""Loaded index with {doc_count} document chunks"")

                return vector_store
            except Exception:
                logger.exception(""Error loading vector store"")
                logger.info(""Will create a new vector store"")

        # Create a new vector store
        return self._create_vector_store()",Load the vector store from disk or create it if needed,"???Load or create a vector store, checking for updates and handling errors???"
2734,get_value,"def get_value(self, frame_index: int) -> Union[float, int]:
        
        if frame_index < 0 or frame_index >= self.frame_count:
            raise ValueError(f""Frame index {frame_index} out of bounds [0, {self.frame_count})"")
        return float(self._sequence[frame_index])",Get the parameter value for a specific frame,???Retrieve numeric value from sequence by frame index with bounds check.???
2735,init_context,"def init_context(self, problem_desc: str, instructions: str, apis: str):
        

        self.shell_api = self._filter_dict(apis, lambda k, _: ""exec_shell"" in k)
        self.submit_api = self._filter_dict(apis, lambda k, _: ""submit"" in k)
        stringify_apis = lambda apis: ""\n\n"".join(
            [f""{k}\n{v}"" for k, v in apis.items()]
        )

        self.system_message = DOCS_SHELL_ONLY.format(
            prob_desc=problem_desc,
            shell_api=stringify_apis(self.shell_api),
            submit_api=stringify_apis(self.submit_api),
        )

        self.task_message = instructions

        self.history.append({""role"": ""system"", ""content"": self.system_message})
        self.history.append({""role"": ""user"", ""content"": self.task_message})",Initialize the context for the agent.,"???Initialize context with problem description, instructions, and filtered API details???"
2736,type,"def type(self) -> type:
        

        def method_stub(name: str):
            def not_implemented(*args, **kwargs):
                raise NotImplementedError(
                    f""Method '{name}' is configured in config.json for tool '{self.name}'""
                    f""but has not been implemented in the tool module ({self.module_name}).""
                )

            return not_implemented

        # fmt: off
        type_ = type(f'{snake_to_camel(self.name)}Module', (Protocol,), {  # type: ignore[arg-type]
            method_name: method_stub(method_name) for method_name in self.tools
        },)
        # fmt: on
        return runtime_checkable(type_)",Dynamically generate a type for the tool module.,???Dynamically creates a protocol class with unimplemented methods for tool configuration.???
2737,task_gravity,"def task_gravity(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    density = 0.5
    question = [rng.randint(1, 9) if rng.random() < density else 0 for _ in range(size)]

    non_zero = [x for x in question if x != 0]
    answer = non_zero + [0] * (size - len(non_zero))

    return {""input"": question, ""output"": answer}",Generate a task where all non-zero elements are attracted to the left.,???Generate a list with random integers and shift non-zeros left???
2738,generate_notes,"def generate_notes(directory: Path, git_hash: GitHash) -> int:
    
    # Detect package type
    path = directory.resolve(strict=True)
    release = gen_version()

    click.echo(f'# Release: {release}')
    click.echo('')
    click.echo('## Updated packages')
    for package in find_changed_packages(path, git_hash):
        name = package.package_name()
        version = package.package_version()
        click.echo(f'- {name}@{version}')
    click.echo('')

    return 0",Generates detailed release notes.,???Generate release notes for updated packages in a specified directory.???
2739,_get_context_path,"def _get_context_path(self, user_id: str) -> Path:
        
        sanitized_user_id = user_id.replace(""/"", ""_"").replace(""\\"", ""_"")
        return self.storage_dir / f""{sanitized_user_id}.json""",Get the file path for a specific user context,"???  
Generate a sanitized file path for user data storage.  
???"
2740,setup_metadata_files,"def setup_metadata_files(self, temp_dir):
        
        # Create directory structure and metadata files
        templates_data = [
            (""chatbot-assistant"", ""1.0.0"", {
                ""title"": ""Chatbot Assistant"",
                ""description"": ""Helpful conversational assistant"",
                ""category"": ""conversation"",
                ""tags"": [""chatbot"", ""assistant""],
            }),
            (""data-analyst"", ""1.0.0"", {
                ""title"": ""Data Analyst"",
                ""description"": ""Data analysis and insights specialist"",
                ""category"": ""analysis"",
                ""tags"": [""data"", ""analysis""],
            }),
            (""code-reviewer"", ""2.0.0"", {
                ""title"": ""Code Reviewer"",
                ""description"": ""Automated code review and suggestions"",
                ""category"": ""development"",
                ""tags"": [""code"", ""review""],
                ""requires_structured_outputs"": True,
            }),
        ]
        
        for slug, version, metadata in templates_data:
            template_dir = temp_dir / slug.replace("" "", ""_"").lower() / f""v{version.replace('.', '_')}""
            template_dir.mkdir(parents=True, exist_ok=True)
            
            metadata_file = template_dir / ""metadata.json""
            with open(metadata_file, ""w"") as f:
                json.dump(metadata, f)",Create metadata files for test templates.,???Create directory structure and metadata files for various software templates in a temporary directory.???
2741,_redact_language_label,"def _redact_language_label(language_label: str) -> str:
  
  return re.sub(r'gl-python/', '{LANGUAGE_LABEL}/', language_label)",Removed because replay requests are used for all languages.,???Replaces 'gl-python/' with '{LANGUAGE_LABEL}/' in a given language label string.???
2742,save_token,"def save_token(self, token: str) -> None:
        
        try:
            with open(self.token_file, ""w"") as f:
                json.dump({""token"": token}, f)

            # Secure the file permissions (read/write for owner only)
            os.chmod(self.token_file, 0o600)
        except Exception as e:
            print(f""Error saving token: {e!s}"")
            raise",Save api token to disk.,???Store and secure a token in a file with restricted access permissions.???
2743,handle_compress,"def handle_compress(args: argparse.Namespace) -> int:
    
    try: result = compress(args.string); print(f""Compressed string: {result}""); return 0
    except Exception as e: logger.error(f""Error compressing: {e}""); print(f""Error: {e}""); return 1",Handle the compress command.,"???Function compresses input string, logs errors, and returns status code.???"
2744,_parse_rule,"def _parse_rule(self, rule_dict: Dict[str, Any]) -> BaseRule:
        
        rule_type = rule_dict.get(""type"")
        stage = rule_dict.get(""stage"")

        if not stage:
            # Handle missing stage - default to post_parsing for backward compatibility
            logger.warning(f""Rule is missing 'stage' field, defaulting to 'post_parsing': {rule_dict}"")
            rule_dict[""stage""] = ""post_parsing""
            stage = ""post_parsing""

        if stage not in [""post_parsing"", ""post_chunking""]:
            raise ValueError(f""Invalid stage '{stage}' in rule: {rule_dict}"")

        logger.debug(f""Parsing rule of type: {rule_type}, stage: {stage}"")

        if rule_type == ""metadata_extraction"":
            # Default use_images to False if not present
            if ""use_images"" not in rule_dict:
                rule_dict[""use_images""] = False
            return MetadataExtractionRule(**rule_dict)
        elif rule_type == ""natural_language"":
            return NaturalLanguageRule(**rule_dict)
        else:
            raise ValueError(f""Unknown rule type: {rule_type}"")",Parse a rule dictionary into a rule object,"???Parse and validate rule configurations, defaulting missing fields and instantiating specific rule objects.???"
2745,_validate_sequence,"def _validate_sequence(self, code: str) -> List[str]:
        
        errors = []
        lines = code.strip().split('\n')
        
        # Check if starts with sequenceDiagram
        if not lines[0].strip() == 'sequenceDiagram':
            errors.append(""Sequence diagram must start with 'sequenceDiagram' keyword"")
            
        # Track participants
        participants = set()
        
        for line in lines[1:]:
            line = line.strip()
            if not line:
                continue
                
            # Check participant definitions
            if line.startswith('participant'):
                participant = line.split()[1]
                participants.add(participant)
                
            # Check message syntax
            elif any(op in line for op in ['->>','-->>','->','-->']):
                parts = re.split(r'[-]+>>|[-]+>', line)
                if len(parts) == 2:
                    sender = parts[0].strip()
                    receiver = parts[1].split(':')[0].strip()
                    
                    if sender not in participants:
                        errors.append(f""Undefined participant '{sender}' in message"")
                    if receiver not in participants:
                        errors.append(f""Undefined participant '{receiver}' in message"")
                else:
                    errors.append(f""Invalid message syntax in line: {line}"")
        
        return errors",Validate sequence diagram syntax.,???Validate sequence diagram syntax and participant definitions in code???
2746,set_cpu_throttling_rate,"def set_cpu_throttling_rate(
    rate: float,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""rate""] = rate
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Emulation.setCPUThrottlingRate"",
        ""params"": params,
    }
    json = yield cmd_dict",Enables CPU throttling to emulate slow CPUs.,???Configure CPU throttling rate for emulation via JSON command.???
2747,status_message,"def status_message(self) -> str:
        
        if self._state.status == MigrationStatus.IN_PROGRESS:
            progress = (
                f"" ({self._state.projects_migrated}/{self._state.projects_total})""
                if self._state.projects_total > 0
                else """"
            )
            return f""🔄 File sync in progress{progress}: {self._state.message}. Use sync_status() tool for details.""
        elif self._state.status == MigrationStatus.FAILED:
            return f""❌ File sync failed: {self._state.error or 'Unknown error'}. Use sync_status() tool for details.""
        elif self._state.status == MigrationStatus.COMPLETED:
            return ""✅ File sync completed successfully""
        else:
            return ""✅ System ready""",Get a user-friendly status message.,???Generate status messages based on file synchronization progress and outcomes.???
2748,generate,"def generate(self, prompt: str, **kwargs) -> str:
                            
                            url = f""{self.base_url}/api/generate""
                            data = {""model"": self.model, ""prompt"": prompt, ""stream"": False, **kwargs}
                            response = self.session.post(url, json=data)
                            response.raise_for_status()
                            return response.json().get(""response"", """")",Generate text using Ollama's API.,???Send a prompt to an API to generate and return a text response.???
2749,perform_action,"def perform_action(self, action_name: str, kwargs) -> Any:
        
        if action_name not in self.actions:
            raise KeyError(f""Unknown action: {action_name}"")

        load_dotenv()
        
        if not self.is_configured(verbose=True):
            raise SonicConnectionError(""Sonic is not properly configured"")

        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f""Invalid parameters: {', '.join(errors)}"")

        method_name = action_name.replace('-', '_')
        method = getattr(self, method_name)
        return method(**kwargs)",Execute a Sonic action with validation,???Execute validated action by invoking corresponding method with parameters.???
2750,_build_headers,"def _build_headers(self) -> Dict[str, str]:
        
        headers = {
            ""User-Agent"": random.choice(USER_AGENTS),
            ""Accept"": ""application/json"",
            ""Accept-Language"": ""en-US,en;q=0.5"",
            ""DNT"": ""1"",
        }
        logger.debug(f""Built headers: {headers}"")
        return headers",Build request headers with random User-Agent,???Generate HTTP headers with randomized user-agent for requests.???
2751,set_show_fps_counter,"def set_show_fps_counter(
    show: bool,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""show""] = show
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Overlay.setShowFPSCounter"",
        ""params"": params,
    }
    json = yield cmd_dict",Requests that backend shows the FPS counter :param show: True for showing the FPS counter,???Enable or disable the FPS counter overlay via a command dictionary.???
2752,_prepare_inputs,"def _prepare_inputs(self, inputs: dict) -> dict:
        
        initial_node_names = self.graph.find_initial_nodes()
        initial_node_required_inputs = set()
        for initial_node_name in initial_node_names:
            initial_node = self.graph.get_node(initial_node_name)
            if initial_node.inputs:
                initial_node_required_inputs.update([inp.name for inp in initial_node.inputs if inp.required])
        if ""goal"" in initial_node_required_inputs and ""goal"" not in inputs:
            inputs.update({""goal"": self.graph.goal})
            
        return inputs",Prepare the inputs for the workflow execution.,"???  
Ensure required initial node inputs are present in the input dictionary.  
???"
2753,codebase,"def codebase(context_mock, tmpdir):
    
    # language=python
    content = 
    with get_codebase_session(tmpdir=tmpdir, files={""src/main.py"": content}, verify_output=False) as codebase:
        yield codebase",Create a simple codebase for testing.,???Create a temporary codebase session for testing Python code.???
2754,format_diff_message,"def format_diff_message(optim_text: str, incr_text: str) -> str:
    
    diff: list[str] = list(difflib.ndiff(optim_text.splitlines(), incr_text.splitlines()))

    # Collect differences
    only_in_optim: list[str] = []
    only_in_incr: list[str] = []

    for line in diff:
        if line.startswith(""- ""):
            only_in_optim.append(line[2:])
        elif line.startswith(""+ ""):
            only_in_incr.append(line[2:])

    message: list[str] = []
    if only_in_optim:
        message.append(""\nOnly in optimized prompt:"")
        message.extend(f""  {line}"" for line in only_in_optim)

    if only_in_incr:
        message.append(""\nOnly in incremental prompt:"")
        message.extend(f""  {line}"" for line in only_in_incr)

    return ""\n"".join(message)",Creates a detailed diff message between two texts.,???Generate a formatted message highlighting differences between two text versions.???
2755,send_conversation,"def send_conversation(self, conversation: ""Conversation"", **kwargs) -> ""Message"":
        
        from ..models import Message

        messages = [
            {""role"": msg.role, ""content"": msg.text} for msg in conversation.messages
        ]

        request_kwargs = {
            **self.DEFAULT_KWARGS,
            **kwargs,
            ""model"": conversation.llm_model or self.DEFAULT_MODEL,
            ""messages"": messages,
        }

        response = self.client.chat.completions.create(**request_kwargs)
        assistant_message = response.choices[0].message

        # Create and return a properly formatted Message instance
        return Message(
            role=""assistant"",
            text=assistant_message.content or """",
            raw=response,
            llm_model=conversation.llm_model or self.DEFAULT_MODEL,
            llm_provider=self.NAME,
        )",Send a conversation to the Ollama API.,???Send conversation data to a chat API and return a formatted response message.???
2756,simplify_grasp_labels,"def simplify_grasp_labels(root, save_path):
    
    obj_names = list(range(88))
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    for i in obj_names:
        print('\nsimplifying object {}:'.format(i))
        label = np.load(os.path.join(root, 'grasp_label', '{}_labels.npz'.format(str(i).zfill(3))))
        # point_num = len(label['points'])
        print('original shape:               ', label['points'].shape, label['offsets'].shape, label['scores'].shape)
        # if point_num > 4820:
        #     idxs = np.random.choice(point_num, 4820, False)
        #     points = label['points'][idxs]
        #     offsets = label['offsets'][idxs]
        #     scores = label['scores'][idxs]
        #     print('Warning!!!  down sample object {}'.format(i))
        # else:
        points = label['points']
        scores = label['scores']
        offsets = label['offsets']
        width = offsets[:, :, :, :, 2]
        print('after simplify, offset shape: ', points.shape, scores.shape, width.shape)
        np.savez(os.path.join(save_path, '{}_labels.npz'.format(str(i).zfill(3))),
                 points=points, scores=scores, width=width)","original dataset grasp_label files have redundant data,  We can significantly save the memory cost",???Simplify and save grasp labels for objects by processing and storing relevant data.???
2757,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""WebAudio.disable"",
    }
    json = yield cmd_dict",Disables the WebAudio domain.,???Disable WebAudio functionality by sending a command dictionary.???
2758,_setup_environment,"def _setup_environment(self, attn_backend: str, sparse_backend: str, spconv_algo: str, smooth_k: bool):
        
        # Try attention
        success = set_attention_backend(attn_backend)
        if not success:
            self.logger.warning(f""Failed to set {attn_backend} or not installed, fallback to sdpa."")

        # Try sparse
        success2 = set_sparse_backend(sparse_backend, spconv_algo)
        if not success2:
            self.logger.warning(f""Failed to set {sparse_backend} or not installed, fallback to default."")

        # If user wants SageAttn smooth_k, we set environment var (if they'd want that):
        os.environ['SAGEATTN_SMOOTH_K'] = '1' if smooth_k else '0'",Set up environment variables and backends lazily.,???Configure system backends and environment variables for attention and sparsity settings.???
2759,run_simple_agent_image_workflow,"def run_simple_agent_image_workflow():
    
    llm = setup_llm(model_provider=""gemini"", model_name=""gemini-2.0-flash"", temperature=1)
    agent = SimpleAgent(
        name=""SimpleImageAgent"",
        id=""simple_image_agent"",
        llm=llm,
    )

    tracing = TracingCallbackHandler()
    wf = Workflow(flow=Flow(nodes=[agent]))

    result = wf.run(
        input_data={""input"": ""What can you tell me about this artwork?"", ""images"": [IMAGE_URL]},
        config=RunnableConfig(callbacks=[tracing]),
    )

    agent_output = result.output[agent.id][""output""][""content""]
    print(""SimpleAgent image workflow response:"", agent_output)

    return agent_output, tracing.runs",Example workflow with image URL using SimpleAgent,???Execute an image analysis workflow using a language model agent and trace the process.???
2760,evaluate_mmmu,"def evaluate_mmmu(samples):
    
    pred_correct = 0
    judge_dict = dict()
    for sample in samples:
        gold_i = sample[""answer""]
        pred_list = sample[""parsed_pred""]
        correct = False
        for pred_i in pred_list:
            if sample[""question_type""] == ""multiple-choice"":
                correct = eval_multi_choice(gold_i, pred_i)
            else:  # open question
                correct = eval_open(gold_i, pred_i)

            if correct:
                judge_dict[sample[""id""]] = ""Correct""
                pred_correct += 1
                break

        if not correct:
            judge_dict[sample[""id""]] = ""Wrong""

    if len(samples) == 0:
        return {""acc"": 0}
    return judge_dict, {""acc"": pred_correct / len(samples)}",Batch evaluation for multiple choice and open questions.,???Evaluate prediction accuracy for multiple-choice and open questions.???
2761,_get_line_starts,"def _get_line_starts(self) -> list[Editable]:
        
        line_start_nodes = super()._get_line_starts()
        if len(line_start_nodes) >= 3 and line_start_nodes[0].source == ""{"" and line_start_nodes[-1].source == ""}"":
            # Remove the first and last line of the code block as they are opening and closing braces.
            return line_start_nodes[1:-1]
        return line_start_nodes",Returns an ordered list of first Editable for each non-empty line within the code block,"???Extracts line starts, excluding surrounding braces if present.???"
2762,track_cpu_usage,"def track_cpu_usage(self, interval: float) -> Optional[float]:
        
        try:
            return psutil.cpu_percent(interval=interval)
        except Exception as e:
            logger.warning(f""Failed to track CPU usage: {str(e)}"")
            return None",Track CPU usage percentage,"???  
Monitors CPU usage over a specified interval, logging errors if tracking fails.  
???"
2763,delete_agent_alias,"def delete_agent_alias(self, agent_id, alias_id, alias_name):
        
        try:
            print(f""Deleting agent alias: {alias_name} ({alias_id})"")
            self.bedrock_agent.delete_agent_alias(
                agentId=agent_id,
                agentAliasId=alias_id
            )
            return True
        except Exception as e:
            print(f""Error deleting agent alias {alias_name} ({alias_id}): {str(e)}"")
            return False",Delete a specific agent alias,"???Attempt to remove an agent alias, logging success or error.???"
2764,validate_and_set_input,"def validate_and_set_input(user_input: Union[str, int, float], type_map: Dict, attribute_name: str):
    
    if isinstance(user_input, float):
        user_input = int(user_input)  # Convert float to int

    # Handle string input
    if isinstance(user_input, str):
        user_input = user_input.strip().lower()
        if user_input in type_map:
            return int(type_map[user_input])
        else:
            raise ValueError(
                f""Invalid {attribute_name} type '{user_input}'. Must be one of {list(type_map.keys())}."")
    elif isinstance(user_input, int) and user_input in type_map.values():
        return int(user_input)
    else:
        raise ValueError(
            f""{attribute_name} must be a string or an integer and must be a valid type."")",Validate an input strinbg or int from a map of strings to integers.,"???  
Validate and convert user input based on a type mapping dictionary.  
???"
2765,get_file_content,"def get_file_content(repo_id: str, file_path: str):
    
    try:
        repo = registry.get_repo(repo_id)
    except KeyError:
        raise HTTPException(status_code=404, detail=""Repo not found"")
    try:
        content = repo.get_file_content(file_path)
        from fastapi.responses import PlainTextResponse

        return PlainTextResponse(content=content)
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail=f""File not found: {file_path}"")
    except IOError as e:
        raise HTTPException(status_code=500, detail=f""Error reading file: {e!s}"")",Get the content of a specific file in the repository.,"???Fetch and return file content from a repository, handling errors gracefully.???"
2766,to_langchain,"def to_langchain(self) -> ChatOpenAI:
        
        kwargs = self.kwargs
        if self.json:
            kwargs[""response_format""] = {""type"": ""json_object""}

        return ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature or 0.5,
            base_url=os.environ.get(
            ),
            max_tokens=self.max_tokens,
            model_kwargs=kwargs,
            streaming=self.streaming,
            api_key=SecretStr(os.environ.get(""OPENROUTER_API_KEY"", ""openrouter"")),
            top_p=self.top_p,
        )",Convert the language model to a LangChain chat model for Open Router.,???Convert configuration to ChatOpenAI instance with JSON support and environment-based settings.???
2767,norm_layer,"def norm_layer(norm_type: str, *args, **kwargs) -> nn.Module:
    
    if norm_type == ""group"":
        return GroupNorm32(32, *args, **kwargs)
    elif norm_type == ""layer"":
        return ChannelLayerNorm32(*args, **kwargs)
    else:
        raise ValueError(f""Invalid norm type {norm_type}"")",Return a normalization layer.,"???  
Selects and returns a normalization layer based on the specified type.  
???"
2768,parse,"def parse(self, input: dict, response: str) -> dict:
        
        return {
            ""recipe"": response,
            ""cuisine"": input[""cuisine""],
        }",Parse the model response along with the input to the model into the desired output format..,"???  
Transforms input data and response into a structured recipe dictionary.  
???"
2769,check_mcp_cli_installation,"def check_mcp_cli_installation():
    
    print(""\n2. MCP CLI Installation"")
    print(""-"" * 24)
    
    # Check if mcp-cli command exists
    try:
        result = subprocess.run(['which', 'mcp-cli'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f""   ✅ mcp-cli command found: {result.stdout.strip()}"")
        else:
            print(""   ❌ mcp-cli command not found in PATH"")
    except:
        print(""   ❌ Cannot check mcp-cli command"")
    
    # Check if we can import mcp_cli
    try:
        import mcp_cli
        print(f""   ✅ mcp_cli module: {mcp_cli.__file__}"")
        
        # Check for main components
        from mcp_cli.tools.manager import ToolManager
        print(""   ✅ ToolManager importable"")
        
        from mcp_cli.run_command import run_command
        print(""   ✅ run_command importable"")
        
    except ImportError as e:
        print(f""   ❌ Cannot import mcp_cli: {e}"")
        return False
    
    return True",Check MCP CLI installation.,???Verify MCP CLI installation by checking command presence and module importability.???
2770,save,"def save(self):
        
        try:
            temp = self.data.copy()
            if self.data.get(""cipher_enabled"") and self.data.get(""hashed_password""):
                temp[""hashed_password""] = base64.b64encode(
                    temp[""hashed_password""]
                ).decode(""utf-8"")
            with open(self.file_name, ""w"") as f:
                json.dump(temp, f, indent=4)
        except Exception as e:
            logging.error(f""Failed to save data: {e}"")",Save data to file,??? Serialize and save encrypted data to a file with error logging. ???
2771,get_output_dir,"def get_output_dir(base_dir: str) -> str:
    
    # Get the directory of the input file
    
    base_name = f""output_metrics_{datetime.now().strftime('%Y%m%d')}""
    counter = 1
    while True:
        dir_name = os.path.join(base_dir, f""{base_name}_{counter}"")
        if not os.path.exists(dir_name):
            os.makedirs(dir_name)
            return dir_name
        counter += 1",Create and return the output directory name with date and sequence number in the same directory as the input file,???Create a unique timestamped output directory within a base directory.???
2772,structured_response,"def structured_response(
        self,
        prompt: str,
        response_model: Type[T],
        *,
        llm_model: str | None = None,
        image_url: str | None = None,
        **kwargs,
    ) -> T:
        
        # Ensure messages are provided in kwargs
        messages = [
            {""role"": ""user"", ""content"": [{""type"": ""text"", ""text"": prompt}]},
        ]

        
        if image_url:
            messages[0][""content""].append(
                {""type"": ""image_url"", ""image_url"": {""url"": image_url}}
            )

        response = self.structured_client.chat.completions.create(
            messages=messages,
            model=llm_model or self.DEFAULT_MODEL,
            response_model=response_model,
            **{**self.DEFAULT_KWARGS, **kwargs},
        )
        return response_model.model_validate(response)",Get a structured response from the OpenAI API.,???Generate structured AI responses using text and optional image input.???
2773,get_trace_info,"def get_trace_info(data_part):
        
        if 'modelInvocationInput' in data_part:
            return {
                'trace_id': data_part['modelInvocationInput'].get('traceId', ''),
                'type': 'POST_PROCESSING'
            }
        elif 'modelInvocationOutput' in data_part:
            return {
                'trace_id': data_part['modelInvocationOutput'].get('traceId', ''),
                'type': 'POST_PROCESSING'
            }
        return None",Extract trace ID from post-processing data,???Extract trace information from input or output data for post-processing???
2774,_extract_arguments_type_schema,"def _extract_arguments_type_schema(func: DecoratedFunction) -> dict | None:
    
    try:
        spec = importlib.util.spec_from_file_location(""module"", func.filepath)
        module = importlib.util.module_from_spec(spec)

        fn_arguments_param_type = None
        for p in func.parameters:
            if p[0] == ""arguments"":
                fn_arguments_param_type = p[1]

        if fn_arguments_param_type is not None:
            spec.loader.exec_module(module)

            schema = getattr(module, fn_arguments_param_type).model_json_schema()
            return schema
        return None
    except Exception as e:
        print(f""Error parsing {func.filepath}, could not introspect for arguments parameter"")
        print(e)
        return None",Extracts the arguments type schema from a DecoratedFunction object.,???Extracts JSON schema for function arguments from a specified module file.???
2775,find_and_click_btn,"def find_and_click_btn(self, btn_type: str = 'login', timeout: int = 5) -> bool:
        
        buttons = self.get_buttons_xpath()
        if not buttons:
            self.logger.warning(""No visible buttons found"")
            return False

        for button_text, xpath in buttons:
            if btn_type.lower() in button_text.lower() or btn_type.lower() in xpath.lower():
                try:
                    wait = WebDriverWait(self.driver, timeout)
                    element = wait.until(
                        EC.element_to_be_clickable((By.XPATH, xpath)),
                        message=f""Button with XPath '{xpath}' not clickable within {timeout} seconds""
                    )
                    if self.click_element(xpath):
                        self.logger.info(f""Clicked button '{button_text}' at XPath: {xpath}"")
                        return True
                    else:
                        self.logger.warning(f""Button '{button_text}' at XPath: {xpath} not clickable"")
                        return False
                except TimeoutException:
                    self.logger.warning(f""Timeout waiting for '{button_text}' button at XPath: {xpath}"")
                    return False
                except Exception as e:
                    self.logger.error(f""Error clicking button '{button_text}' at XPath: {xpath} - {str(e)}"")
                    return False
        self.logger.warning(f""No button matching '{btn_type}' found"")
        return False",Find and click a submit button matching the specified type.,"???Locate and click a specified button type within a timeout period, logging outcomes.???"
2776,broadcast,"def broadcast(self, input_: torch.Tensor, src: int = 0):
        
        assert src < self.world_size, f""Invalid src rank ({src})""

        # Bypass the function if we are using only 1 GPU.
        if self.world_size == 1:
            return input_
        # Broadcast.
        torch.distributed.broadcast(input_,
                                    src=self.ranks[src],
                                    group=self.device_group)
        return input_",Broadcast the input tensor.,???Broadcast tensor across multiple GPUs if more than one is available.???
2777,task_paint_biggest_block,"def task_paint_biggest_block(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    target_color = 1
    initial_color = rng.randint(2, 9)

    # Generate random blocks
    question = gen_field(size)
    blocks = []
    pos = 0

    while pos < size:
        if rng.random() < 0.4 and size - pos >= 2:
            block_size = rng.randint(2, min(size - pos, 6))
            blocks.append((pos, block_size))
            for i in range(block_size):
                question[pos + i] = initial_color
            pos += block_size + 1
        else:
            pos += 1

    if len(blocks) < 2:
        return None

    # Find biggest block
    biggest_pos, biggest_size = max(blocks, key=lambda x: x[1])

    # Check if there are multiple blocks of the same size
    biggest_count = sum(1 for _, size in blocks if size == biggest_size)
    if biggest_count > 1:
        return None

    answer = question.copy()
    for i in range(biggest_size):
        answer[biggest_pos + i] = target_color

    return {""input"": question, ""output"": answer}",Generate a task where the largest block is painted a different color.,"???Randomly generate and paint the largest block in a sequence, returning the modified sequence.???"
2778,swap_sonic,"def swap_sonic(agent, **kwargs):
    
    try:
        token_in = kwargs.get(""token_in"")
        token_out = kwargs.get(""token_out"") 
        amount = float(kwargs.get(""amount""))
        slippage = float(kwargs.get(""slippage"", 0.5))

        # Direct passthrough to connection method - add your logic before/after this call!
        agent.connection_manager.connections[""sonic""].swap(
            token_in=token_in,
            token_out=token_out,
            amount=amount,
            slippage=slippage
        )
        return 

    except Exception as e:
        logger.error(f""Failed to swap tokens: {str(e)}"")
        return None",Swap tokens on Sonic chain.,???Facilitates token exchange via a connection manager with error handling.???
2779,get_avatar,"def get_avatar(load_name):
    
    try:
        # Use LoadService to get avatar
        avatar_data, error, status_code = LoadService.get_avatar(load_name)

        if error:
            return APIResponse.error(code=status_code, message=error)

        return APIResponse.success(
            data={""avatar_data"": avatar_data},
            message=""Avatar retrieved successfully""
        )

    except Exception as e:
        logger.error(f""An error occurred while getting avatar: {str(e)}"", exc_info=True)
        return APIResponse.error(code=500, message=""Internal server error"")",Get load's avatar data,"???Retrieve user avatar data, handle errors, and return API response.???"
2780,initialize_postgres,"def initialize_postgres():
    

    try:
        import psycopg2

    except ImportError:
        raise ImportError(""Please install psycopg2 library to use PostgreSQL."")

    conn = psycopg2.connect(
        dbname=os.getenv(""POSTGRES_DB"", ""postgres""),
        user=os.getenv(""POSTGRES_USER"", ""postgres""),
        password=os.getenv(""POSTGRES_PASSWORD"", ""postgres""),
        host=os.getenv(""POSTGRES_HOST"", ""localhost""),
        port=os.getenv(""POSTGRES_PORT"", ""5432""),
    )
    cursor = conn.cursor()

    try:
        cursor.execute(CREATE_SESSIONS_TABLE)
        cursor.execute(CREATE_CONVERSATIONS_TABLE)
        cursor.execute(CREATE_CONTEXT_MESSAGES_TABLE)
        conn.commit()
        logger.info(""PostgreSQL tables created successfully"")
    except Exception as e:
        logger.exception(f""Error creating PostgreSQL tables: {e}"")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()",Initialize the PostgreSQL database by creating the necessary tables.,"???Establishes PostgreSQL connection, creates tables, and handles errors.???"
2781,log_move_and_thought,"def log_move_and_thought(move, thought, latency):
    
    log_file_path = os.path.join(CACHE_DIR, ""sokoban_moves.log"")
    
    log_entry = f""[{time.strftime('%Y-%m-%d %H:%M:%S')}] Move: {move}, Thought: {thought}, Latency: {latency:.2f} sec\n""
    
    try:
        with open(log_file_path, ""a"") as log_file:
            log_file.write(log_entry)
    except Exception as e:
        print(f""[ERROR] Failed to write log entry: {e}"")",Logs the move and thought process into a log file inside the cache directory.,"???Logs game moves, player thoughts, and latency to a file with timestamp.???"
2782,empty_resources_template_file,"def empty_resources_template_file(self, tmp_path):
        
        template = {""Resources"": {}}

        template_file = tmp_path / ""empty_resources_template.json""
        template_file.write_text(json.dumps(template))

        return template_file",Create a CloudFormation template file with empty Resources section.,"???Create an empty JSON template file with a ""Resources"" key at a temporary path.???"
2783,is_certificate_valid,"def is_certificate_valid(cert_path: str) -> bool:
            
            try:
                with open(cert_path, ""rb"") as cert_file:
                    cert_data = cert_file.read()  # Read the certificate file
                    cert = x509.load_pem_x509_certificate(cert_data, default_backend())

                    # Use timezone-aware expiration date
                    expiration_date = cert.not_valid_after_utc
                    current_time = datetime.now(timezone.utc)
                    return expiration_date > current_time
            except Exception as e:
                logger.error(f""Failed to validate certificate {cert_path}: {e}"")
                return False",Check if a certificate is valid (not expired) using cryptography.,???Check if a certificate file is valid by comparing its expiration date to the current time.???
2784,validate_workflow_structure,"def validate_workflow_structure(structure: WorkflowStructure, nodes: Dict[str, NodeDefinition],
                              is_main: bool = False) -> List[NodeError]:
    
    issues: List[NodeError] = []

    if is_main and not structure.start:
        issues.append(NodeError(node_name=None, description=""Main workflow is missing a start node""))
    elif structure.start and structure.start not in nodes:
        issues.append(NodeError(node_name=structure.start, description=""Start node is not defined in nodes""))

    for trans in structure.transitions:
        if trans.from_node not in nodes:
            issues.append(NodeError(node_name=trans.from_node, description=""Transition from undefined node""))

        to_nodes: List[Union[str, BranchCondition]] = [trans.to_node] if isinstance(trans.to_node, str) else trans.to_node
        for to_node in to_nodes:
            target_node = to_node if isinstance(to_node, str) else to_node.to_node
            if target_node not in nodes:
                issues.append(NodeError(node_name=target_node, description=f""Transition to undefined node from '{trans.from_node}'""))
            if isinstance(to_node, BranchCondition) and to_node.condition:
                try:
                    compile(to_node.condition, ""<string>"", ""eval"")
                except SyntaxError:
                    issues.append(NodeError(node_name=trans.from_node, description=f""Invalid branch condition syntax: {to_node.condition}""))

        if trans.condition and isinstance(trans.to_node, str):
            try:
                compile(trans.condition, ""<string>"", ""eval"")
            except SyntaxError:
                issues.append(NodeError(node_name=trans.from_node, description=f""Invalid condition syntax in transition: {trans.condition}""))

    return issues","Validate a WorkflowStructure for consistency, including branch and converge support.","???Validate workflow structure for node and transition errors, returning a list of identified issues.???"
2785,show_problems,"def show_problems(dataset):
    
    for inst, entry in dataset.items():
        problem = entry.problem_statement.splitlines()[0]
        print(f""{inst}: {problem}"")",Print out all the instance_id and problem_descriptions.,???Extracts and prints the first line of problem statements from a dataset.???
2786,load_cat_behaviors_for_prompt,"def load_cat_behaviors_for_prompt(filepath: str) -> str:
    
    behaviors_description = [
        ""\n\nHere is a detailed list of behaviors you, as a cat, can use and what they generally mean:""
    ]

    try:
        with open(filepath, ""r"", encoding=""utf-8"") as f:
            behaviors = json.load(f)  # <<< one big load
            for behavior_data in behaviors:
                behaviors_description.append(
                    f""- **{behavior_data['behavior']}**: {behavior_data['description']}""
                )
        return ""\n"".join(behaviors_description)
    except FileNotFoundError:
        return (
            ""\n\nWarning: Cat behaviors file not found at '{filepath}'. ""
            ""You'll have to rely on your basic cat instincts (meow, hiss, purr, hairball, silence).""
        )
    except json.JSONDecodeError as e:
        return (
            f""\n\nWarning: Error decoding cat behaviors file '{filepath}'. ""
            f""Please ensure it's valid JSONL. Error: {e}. Rely on basic instincts.""
        )",Loads cat behaviors from a JSONL file and formats them for the system prompt.,"???Load and format cat behavior descriptions from a file, handling errors gracefully.???"
2787,_format_simple_blocks,"def _format_simple_blocks(self, block_content: dict, block_type: str) -> str:
        
        if block_type in [""embed"", ""bookmark""]:
            url = block_content.get(""url"", """")
            caption = self._extract_rich_text_plain(block_content.get(""caption"", []))
            content = f""[{block_type.title()}]({url})""
            if caption:
                content += f""\n*{caption}*""
            return content
        elif block_type == ""equation"":
            expression = block_content.get(""expression"", """")
            return f""$$\n{expression}\n$$""
        elif block_type == ""divider"":
            return ""---""
        else:
            return ""**Table of Contents**""","Format simple blocks like embed, bookmark, equation, divider.",???Format content blocks into markdown based on their type and attributes.???
2788,get_pr_files,"def get_pr_files(self, owner: str, repo: str, pr_number: int) -> list[Dict[str, Any]]:
        
        url = f""{self.config.github.base_url}/repos/{owner}/{repo}/pulls/{pr_number}/files""
        response = self.github_session.get(url)
        response.raise_for_status()
        return response.json()",Get list of files changed in the PR.,???Fetches file details from a specific GitHub pull request using API.???
2789,get_acc_with_contion,"def get_acc_with_contion(self, res_pd, key, value):
        
        total_pd = res_pd[res_pd[key] == value]

        correct_pd = total_pd[total_pd[""true_false""] == True]
        acc = ""{:.2f}"".format(len(correct_pd) / len(total_pd) * 100) if len(total_pd) > 0 else ""0.00""
        return len(correct_pd), len(total_pd), acc",Calculate the accuracy of predictions with a specific condition,???Calculate accuracy based on a condition in a DataFrame.???
2790,evaluate_final_output,"def evaluate_final_output(
    final_output: str,
    ground_truth_answer: GroundTruthAnswer,
) -> EvaluationResult:
    
    ground_truth_text = str(ground_truth_answer[""value""])

    # Check for exact match (case-insensitive)
    exact_match = final_output.strip().lower() == ground_truth_text.strip().lower()

    # Calculate F1 score for partial matching
    f1_score = _calculate_f1_score(final_output, ground_truth_text)

    return EvaluationResult(
        passed=exact_match,
        reason=f""Partial Match (F1) score is {round(f1_score, 2)}"",
        criteria=""Is the answer a direct match?"",
        points=1,
    )",Compare answers using simple string matching and F1 score.,???Evaluate output accuracy using exact match and F1 score comparison.???
2791,cuda_single_eval_wrapper,"def cuda_single_eval_wrapper(curr_work: WorkArgs, configs: dict, dataset, run_dir: str):
    

    with mp.Pool(1) as pool:
        try:
            result = pool.apply_async(
                evaluate_single_sample,
                args=(curr_work, configs, dataset, run_dir),
            ).get(timeout=configs.timeout)
        except KeyboardInterrupt:
            print(
                ""\n [Terminate] Caught KeyboardInterrupt, terminating workers...""
            )
            pool.terminate()
            pool.join()
            raise
        except mp.TimeoutError as e:
            print(f""[WARNING] Evaluation TIMED OUT for Problem ID: {curr_work.problem_id}, Sample ID: {curr_work.sample_id}"")

        print(f""[Eval Result] Problem ID: {curr_work.problem_id}, Sample ID: {curr_work.sample_id}: {result}"")
        return result",Wrapper to handle timeout and keyboard interrupt,???Evaluate a single dataset sample using multiprocessing with timeout handling.???
2792,convert_monomer_features,"def convert_monomer_features(monomer_features: FeatureDict) -> FeatureDict:
    
    converted = {}
    unnecessary_leading_dim_feats = {
        ""sequence"",
        ""domain_name"",
        ""num_alignments"",
        ""seq_length"",
    }
    for feature_name, feature in monomer_features.items():
        if feature_name in unnecessary_leading_dim_feats:
            # asarray ensures it's a np.ndarray.
            feature = np.asarray(feature[0], dtype=feature.dtype)
        elif feature_name == ""aatype"":
            # The multimer model performs the one-hot operation itself.
            feature = np.argmax(feature, axis=-1).astype(np.int32)
        converted[feature_name] = feature
    return converted",Reshapes and modifies monomer features for multimer models.,??? Convert and optimize monomer feature data for multimer model processing. ???
2793,create_new_configmap,"def create_new_configmap(self, name, namespace, data):
        
        config_map = client.V1ConfigMap(
            api_version=""v1"",
            kind=""ConfigMap"",
            metadata=client.V1ObjectMeta(name=name),
            data=data,
        )
        try:
            return self.core_v1_api.create_namespaced_config_map(namespace, config_map)
        except ApiException as e:
            print(f""Exception when creating configmap: {e}\n"")
            return None",Create a new configmap.,"???Create a Kubernetes ConfigMap with specified name, namespace, and data.???"
2794,add_missing_columns,"def add_missing_columns(self):
        
        self.logger.info(""Checking for missing columns in requests table..."")

        if self.db_type == 'sqlite':
            query = ""PRAGMA table_info(requests);""
            existing_columns = {row[1] for row in self.execute_query(query)}
        elif self.db_type == 'postgres':
            query = 
            existing_columns = {row[0] for row in self.execute_query(query)}
        elif self.db_type in ['mysql', 'mariadb']:
            query = ""SHOW COLUMNS FROM requests;""
            existing_columns = {row[0] for row in self.execute_query(query)}
        else:
            self.logger.warning(f""Unsupported DB type: {self.db_type}"")
            return

        if 'user_id' not in existing_columns:
            self.logger.info(""Adding column user_id..."")
            self.execute_query(""ALTER TABLE requests ADD COLUMN user_id TEXT;"", commit=True)",Check and add missing columns to the requests table.,???Ensure 'user_id' column exists in requests table across different databases.???
2795,delete_item,"def delete_item(self, item_id):
        
        def delete():
            try:
                subprocess.run(
                    [""cliphist"", ""delete"", item_id],
                    check=True
                )
                self._pending_updates = True
                if not self._loading:
                    GLib.idle_add(self._load_clipboard_items_thread)
            except subprocess.CalledProcessError as e:
                print(f""Error deleting clipboard item: {e}"", file=sys.stderr)
            return False
        GLib.idle_add(delete)",Delete the selected clipboard item (GLib.idle_add),???Schedule clipboard item deletion and handle potential errors???
2796,get_config_values,"def get_config_values():
    
    logger.debug(""Resolving default values for configuration"")
    default_values = get_default_values()
    resolved_values = {key: value() if callable(value) else value for key, value in default_values.items()}
    logger.debug(f""Resolved configuration values: {resolved_values}"")
    return resolved_values",Executes the lambdas and returns the actual values for JSON serialization.,"???Retrieve and resolve default configuration values, logging the process.???"
2797,write_data,"def write_data(self, data: Dict):
        
        try:
            with self.file_path.open(""w"") as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logger.error(f""Error writing reply history: {str(e)}"")",Write data to file,"???  
Safely writes dictionary data to a JSON file, logging errors if encountered.  
???"
2798,step,"def step(self, action):
        
        action_is_valid = action in self.get_available_actions() or (""search[<content>]"" in self.get_available_actions() and action.startswith('search[') and action.endswith(']'))
        last_observation = self.observation
        state, reward, done, info = WebAgentTextEnv.step(self, action)
        self.prepare_render_cache(self.observation)
            
        info = (info or {}).copy()
        info.update({
            ""reward"": reward,
            ""action_is_effective"": self.observation != last_observation,
            ""action_is_valid"": action_is_valid,
            ""success"": 1 if reward == 1 else 0,
            ""success_purchase"": 1 if done else 0,
            ""success_find"": 1 if reward == 1 else 0,
            ""end_of_page"": 1 if tuple(self.get_available_actions()) == ('click[back to search]', 'click[< prev]') else 0,
        })
        return self.observation, reward, done, info","Take an action in the environment and return the next observation, reward, done, and info.","???Validate and execute action in web environment, updating state and info.???"
2799,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""WebAuthn.disable"",
    }
    json = yield cmd_dict",Disable the WebAuthn domain.,???Disables WebAuthn by sending a command dictionary via a generator.???
2800,check_model,"def check_model(self, model: str, **kwargs) -> bool:
        
        try:
            client = self._get_client()
            try:
                client.models.retrieve(model=model)
                return True
            except Exception:
                return False
        except Exception as e:
            raise EternalAIAPIError(f""Model check failed: {e}"")",Check if a specific model is available,"???Verify model existence using client retrieval, handling exceptions.???"
2801,on_train_epoch_end,"def on_train_epoch_end(trainer):
    
    if mlflow:
        mlflow.log_metrics(
            metrics={
                **sanitize_dict(trainer.lr),
                **sanitize_dict(trainer.label_loss_items(trainer.tloss, prefix=""train"")),
            },
            step=trainer.epoch,
        )",Log training metrics at the end of each train epoch to MLflow.,???Log sanitized training metrics to MLflow at epoch end???
2802,extract_schema_from_response_format,"def extract_schema_from_response_format(response_format: Dict[str, Any]) -> Optional[str]:
    
    try:
        if not response_format:
            return None
            
        # Check if it's the OpenAI format
        if isinstance(response_format, dict):
            if response_format.get(""type"") == ""json_schema"":
                schema_data = response_format.get(""json_schema"", {})
                if isinstance(schema_data, dict) and ""schema"" in schema_data:
                    return json.dumps(schema_data[""schema""])
                return json.dumps(schema_data)
                
        logger.warning(f""Could not extract valid schema from response_format"")
        return None
    except Exception as e:
        logger.error(f""Error extracting schema from response_format: {str(e)}"")
        return None",Extract schema from response_format field.,"???Extracts JSON schema from a given response format dictionary, handling errors.???"
2803,task_move_n_pix,"def task_move_n_pix(rng: Random, size: int, move_pix: int, solid: bool) -> Optional[dict[str, list[int]]]:
    
    if size <= move_pix + 1:
        return None

    block_size = rng.randint(1, size - move_pix - 1)
    block_pos = rng.randint(0, size - block_size - move_pix)

    if solid:
        color = rng.randint(1, 9)
        block = [color] * block_size
    else:
        block = [rng.randint(1, 9) for _ in range(block_size)]

    question = write_block(block_pos, block, gen_field(size))
    answer = write_block(block_pos + move_pix, block, gen_field(size))

    return {""input"": question, ""output"": answer}",Generate a task where a block is moved to the right by move_pix pixels.,???Generate a block movement task with randomized parameters and return input-output mapping.???
2804,_format_unique_id_property,"def _format_unique_id_property(self, value: dict) -> str:
        
        prefix = value.get(""prefix"", """")
        number = value.get(""number"", """")
        return f""{prefix}{number}"" if prefix else str(number)",Format unique_id property values.,"???  
Formats a unique identifier by combining prefix and number from a dictionary.  
???"
2805,disable_2fa_with_password_and_otp,"def disable_2fa_with_password_and_otp(username: str, password: str, otp_code: str) -> bool:
    
    user_data = get_user_data() # Assuming this gets data for the logged-in user implicitly
    
    # 1. Verify Password
    if not verify_password(user_data.get(""password"", """"), password):
        logger.warning(f""Failed to disable 2FA for '{username}': Invalid password provided."")
        return False
        
    # 2. Verify OTP Code against permanent secret
    perm_secret = user_data.get(""2fa_secret"")
    if not user_data.get(""2fa_enabled"") or not perm_secret:
        logger.error(f""Failed to disable 2FA for '{username}': 2FA is not enabled or secret missing."")
        # Should ideally not happen if called from the correct UI state, but good to check
        return False 
        
    totp = pyotp.TOTP(perm_secret)
    if not totp.verify(otp_code):
        logger.warning(f""Failed to disable 2FA for '{username}': Invalid OTP code provided."")
        return False
        
    # 3. Both verified, proceed to disable
    user_data[""2fa_enabled""] = False
    user_data[""2fa_secret""] = None
    if save_user_data(user_data):
        logger.info(f""2FA disabled successfully for '{username}' after verifying password and OTP."")
        return True
    else:
        logger.error(f""Failed to save user data after disabling 2FA for '{username}'."")
        return False","Disable 2FA for the specified user, requiring both password and OTP code.",???Disable two-factor authentication by verifying user password and OTP code.???
2806,web_research,"def web_research(state: SummaryState):
    

    # Search the web
    if config.search_api == SearchAPI.TAVILY:
        search_results = tavily_search(state.search_query, include_raw_content=True, max_results=1)
        search_str = deduplicate_and_format_sources(
            search_results, max_tokens_per_source=1000, include_raw_content=True
        )
    elif config.search_api == SearchAPI.PERPLEXITY:
        search_results = perplexity_search(state.search_query, state.research_loop_count)
        search_str = deduplicate_and_format_sources(
            search_results, max_tokens_per_source=1000, include_raw_content=False
        )
    elif config.search_api == SearchAPI.DUCKDUCKGO:
        search_results = duckduckgo_search(state.search_query, max_results=3, fetch_full_page=config.fetch_full_page)
        search_str = deduplicate_and_format_sources(
            search_results, max_tokens_per_source=1000, include_raw_content=True
        )
    else:
        raise ValueError(f""Unsupported search API: {config.search_api}"")

    return {
        ""sources_gathered"": [format_sources(search_results)],
        ""research_loop_count"": state.research_loop_count + 1,
        ""web_research_results"": [search_str],
    }",Gather information from the web,???Conduct web searches using specified APIs and format results for research purposes.???
2807,process_msa,"def process_msa(
    path: Path,
    outdir: str,
    max_seqs: int,
    resource: Resource,
) -> None:
    
    outdir = Path(outdir)
    out_path = outdir / f""{path.stem}.npz""
    if not out_path.exists():
        msa = parse_a3m(path, resource, max_seqs)
        np.savez_compressed(out_path, **asdict(msa))",Run processing in a worker thread.,???Convert MSA file to compressed format if not already processed.???
2808,do_g,"def do_g(self, gray: PDFStackT) -> None:
        
        gray_f = safe_float(gray)

        if gray_f is None:
            log.warning(
                f""Cannot set gray level because {gray!r} is an invalid float value""
            )
        else:
            self.graphicstate.ncolor = gray_f
            self.ncs = self.csmap[""DeviceGray""]",Set gray level for nonstroking operations,???Set gray color level in graphics state if valid float???
2809,get_coding_environment,"def get_coding_environment() -> str:
    
    logger.debug(""Retrieving coding environment details."")
    result = (
        f""{get_environment()}""
        ""\n\n""
        ""<codebase_first_level>\n""
        f""{git_ls(directory_path=os.getcwd(), recursive=False, max_depth=1)}""
        ""\n</codebase_first_level>\n""
    )
    logger.debug(f""Coding environment details:\n{result}"")
    return result",Retrieve coding environment details.,???Retrieve and log current coding environment and codebase structure.???
2810,__next__,"def __next__(self):
        
        im0 = np.asarray(self.sct.grab(self.monitor))[:, :, :3]  # BGRA to BGR
        s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: ""

        self.frame += 1
        return [str(self.screen)], [im0], [s]",Captures and returns the next screenshot as a numpy array using the mss library.,???Capture and return screen image data with metadata.???
2811,config_file_with_format,"def config_file_with_format(tmp_path: Path) -> Path:
    
    config_file = tmp_path / ""config.yaml""
    with open(config_file, ""w"") as f:
        yaml.dump({""log_format"": ""TEXT"", ""log_level"": ""DEBUG""}, f)
    return config_file",Create a config file with log format.,???Create a YAML config file with logging settings in a temporary directory.???
2812,log_generations_to_swanlab,"def log_generations_to_swanlab(self, samples, step, _type='val'):
        
        import swanlab

        swanlab_text_list = []
        for i, sample in enumerate(samples):
            row_text = f
            swanlab_text_list.append(swanlab.Text(row_text, caption=f""sample {i+1}""))

        # Log to swanlab
        swanlab.log({f""{_type}/generations"": swanlab_text_list}, step=step)",Log samples to swanlab as text,???Log sample generations to Swanlab with captions and step tracking.???
2813,ask_for_approval,"def ask_for_approval(
    purpose: str, amount: float, tool_context: ToolContext
) -> dict[str, Any]:
  
  return {
      'status': 'pending',
      'amount': amount,
      'ticketId': 'reimbursement-ticket-001',
  }",Ask for approval for the reimbursement.,???Function initiates approval request for financial reimbursement.???
2814,_save_pretrained,"def _save_pretrained(self, save_directory: Path) -> None:
        

        model_path = save_directory / ""bigvgan_generator.pt""
        torch.save({""generator"": self.state_dict()}, model_path)

        config_path = save_directory / ""config.json""
        with open(config_path, ""w"") as config_file:
            json.dump(self.h, config_file, indent=4)",Save weights and config.json from a Pytorch model to a local directory.,???Save model state and configuration to specified directory.???
2815,_execute_modification,"def _execute_modification(
        self,
        query: str,
        params: Dict[str, Any],
        query_type: QueryType
    ) -> str:
        
        with self._engine.begin() as conn:
            try:
                if query_type in [QueryType.CREATE, QueryType.ALTER, QueryType.DROP]:
                    self._validate_schema_operation(query)
                
                result = conn.execute(text(query), params)
                row_count = result.rowcount
                
                operation = query_type.value.capitalize()
                message = f""**{operation} Operation Successful**\n""
                
                if row_count >= 0:  # Not all operations return a row count
                    message += f""Affected rows: `{row_count}`""
                
                logger.info(f""{operation} operation completed successfully"")
                return message

            except Exception as e:
                logger.error(f""Modification operation failed: {str(e)}"")
                raise",Execute a database modification operation within a transaction.,???Execute and log database schema modification operations with error handling.???
2816,dataset,"def dataset(tmp_path):
    
    content = {""sampled_files"": [""file1.txt"", ""file2.txt"", ""file3.txt""]}
    return create_json_file(tmp_path, content)",Fixture to create a default dataset file.,???Create a JSON file with sample file data at a specified path.???
2817,log_move_and_thought,"def log_move_and_thought(move, thought, latency, session_dir):
    
    log_file_path = os.path.join(session_dir, ""moves.log"")
    
    log_entry = f""[{time.strftime('%Y-%m-%d %H:%M:%S')}] Move: {move}, Thought: {thought}, Latency: {latency:.2f} sec\n""
    
    try:
        with open(log_file_path, ""a"") as log_file:
            log_file.write(log_entry)
    except Exception as e:
        print(f""[ERROR] Failed to write log entry: {e}"")",Logs the move and thought process into a log file inside the session directory.,???Log player actions and reflections with timestamps to a session file.???
2818,generate_stream_text,"def generate_stream_text(self, prompt: str, **kwargs) -> Iterator[str]:
        
        kwargs.pop(""llm_model"", None)
        try:
            response = self.client.generate_content(prompt, stream=True, **kwargs)
            for chunk in response:
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            raise RuntimeError(
                f""Failed to generate streaming text with Gemini API: {e}""
            ) from e",Generate streaming text using the Gemini API.,"???  
Stream text generation using Gemini API with error handling.  
???"
2819,update_dataset,"def update_dataset(dataset_jsonl, rejected_tests, dry_run=True):
    
    temp_file = dataset_jsonl + "".temp""
    removed_count = 0

    try:
        with open(dataset_jsonl, ""r"") as source, open(temp_file, ""w"") as target:
            for line in source:
                if not line.strip():
                    continue

                try:
                    test = json.loads(line)
                    test_id = test.get(""id"")

                    if test_id in rejected_tests:
                        removed_count += 1
                    else:
                        target.write(line)
                except json.JSONDecodeError:
                    continue
    except FileNotFoundError:
        print(f""Error: Dataset file {dataset_jsonl} not found."")
        sys.exit(1)

    if not dry_run:
        os.replace(temp_file, dataset_jsonl)
    else:
        os.remove(temp_file)

    return removed_count",Create a new dataset.jsonl without the rejected tests.,"???  
Filter and update a dataset by removing entries with rejected IDs, optionally simulating changes.  
???"
2820,undo,"def undo() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DOM.undo"",
    }
    json = yield cmd_dict",Undoes the last performed action.,???Generate a command to undo the last DOM action in a JSON format.???
2821,get_documents_from_csv,"def get_documents_from_csv(csv_file_path, site):
    
    documents = []
    with open(csv_file_path, ""r"", encoding=""utf-8"") as file:
        for line in file:
            if line.strip():
                try:
                    docs = documentsFromCSVLine(line, site)
                    documents.extend(docs)
                except ValueError as e:
                    print(f""Skipping row due to error: {str(e)}"")
    return documents",Reads and parses documents from a CSV-style text file,"???Extract documents from CSV file, handling errors gracefully.???"
2822,mock_aliases_registry,"def mock_aliases_registry(self):
        
        mock_aliases = {
            '/h': '/help',
            '/m': '/memory',
            '/a': '/agent'
        }
        
        with patch('cai.repl.commands.help.COMMAND_ALIASES', mock_aliases):
            yield mock_aliases",Create a mock aliases registry for testing.,???Mock command aliases for testing in a command-line interface context.???
2823,get_images,"def get_images(self):
        
        images = self.collection.get_images()
        return [
            {
                ""id"": image.id,
                ""collection_id"": image.collection_id,
                ""name"": image.name,
                ""url"": image.url,
                ""type"": ""image"",
            }
            for image in images
        ]",Get all images in a collection.,???Retrieve and format image metadata from a collection for output???
2824,format_tool_calls,"def format_tool_calls(content):
    
    tool_calls = []
    for tool_request in content:
        if ""toolUse"" in tool_request:
            tool = tool_request[""toolUse""]

            tool_calls.append(
                ChatCompletionMessageToolCall(
                    id=tool[""toolUseId""],
                    function={
                        ""name"": tool[""name""],
                        ""arguments"": json.dumps(tool[""input""]),
                    },
                    type=""function"",
                )
            )
    return tool_calls",Converts Converse API response tool calls to AG2 format,???Extracts and formats tool usage data into structured function call objects.???
2825,list_memos,"def list_memos(self):
        
        print(colored(""LIST OF MEMOS"", ""light_green""))
        for uid, text in self.uid_text_dict.items():
            input_text, output_text = text
            print(
                colored(
                    f""  ID: {uid}\n    INPUT TEXT: {input_text}\n    OUTPUT TEXT: {output_text}"",
                    ""light_green"",
                )
            )",Prints the contents of MemoStore.,???Display formatted memo entries with unique identifiers and text content.???
2826,sync,"def sync(
    verbose: bool = typer.Option(
        False,
        ""--verbose"",
        ""-v"",
        help=""Show detailed sync information."",
    ),
) -> None:
    
    try:
        # Show which project we're syncing
        typer.echo(f""Syncing project: {config.project}"")
        typer.echo(f""Project path: {config.home}"")

        # Run sync
        asyncio.run(run_sync(verbose=verbose))

    except Exception as e:  # pragma: no cover
        if not isinstance(e, typer.Exit):
            logger.exception(
                ""Sync command failed"",
                f""project={config.project},""
                f""error={str(e)},""
                f""error_type={type(e).__name__},""
                f""directory={str(config.home)}"",
            )
            typer.echo(f""Error during sync: {e}"", err=True)
            raise typer.Exit(1)
        raise",Sync knowledge files with the database.,???Synchronize project data with optional verbosity and error handling.???
2827,install,"def install(self) -> None:
        
        spinner = Spinner(""dots2"", text=self.ui.create_loading_text(InstallationStage.PREPARING, 0), style=""green"")

        with Live(spinner, console=self.ui.console, refresh_per_second=30) as live:
            try:
                process = self.run_pip_install()

                while True:
                    output_line = process.stdout.readline()
                    if output_line == """" and process.poll() is not None:
                        break

                    stage, progress = self.parse_pip_output(output_line)
                    spinner.text = self.ui.create_loading_text(stage, progress)

                # Show completion
                spinner.text = self.ui.create_loading_text(InstallationStage.COMPLETE, 1.0)

                if process.poll() == 0:
                    live.update(self.ui.create_success_text())
                else:
                    error = process.stderr.read()
                    error_text = Text(error, style=""red"")
                    live.update(error_text)
                    sys.exit(1)

            except Exception as e:
                error_text = Text(f""Error: {str(e)}"", style=""red"")
                live.update(error_text)
                sys.exit(1)

        self.ui.console.print()",Execute the installation with progress tracking and UI updates.,???Displays a loading spinner while executing a pip installation process.???
2828,add_automation,"def add_automation(self, name: str, description: str) -> dict:
        
        if name in self.automations:
            raise ValueError(f""Automation '{name}' already exists"")
        
        self.automations[name] = {
            ""description"": description,
            ""agents"": []
        }
        return gr.update(choices=list(self.automations.keys()))",Add a new automation flow,"???Add new automation entry if unique, updating available choices.???"
2829,safe_kuzu_execute,"def safe_kuzu_execute(conn, query, error_message=""Kuzu query failed""):
    
    try:
        result = conn.execute(query)
        return result, None
    except Exception as e:
        error = f""{error_message}: {str(e)}""
        print(error)
        return None, error",Execute a Kuzu query with proper error handling,"???  
Executes a database query safely, returning results or an error message.  
???"
2830,state_dict,"def state_dict(self, *args, **kwargs):
        
        if not self.is_peft_model:
            pretrained_model_state_dict = self.pretrained_model.state_dict(*args, **kwargs)
        else:
            # if it is a peft model, only save the v_head
            pretrained_model_state_dict = {}

        v_head_state_dict = self.v_head.state_dict(*args, **kwargs)
        for k, v in v_head_state_dict.items():
            pretrained_model_state_dict[f""v_head.{k}""] = v
        return pretrained_model_state_dict",Returns the state dictionary of the model.,"???  
Generate a state dictionary, conditionally including model components based on model type.  
???"
2831,lm_studio_fn,"def lm_studio_fn(self, history, verbose=False):
        
        thought = """"
        route_start = f""{self.server_ip}/v1/chat/completions""
        payload = {
            ""messages"": history,
            ""temperature"": 0.7,
            ""max_tokens"": 4096,
            ""model"": self.model
        }
        try:
            response = requests.post(route_start, json=payload)
            result = response.json()
            if verbose:
                print(""Response from LM Studio:"", result)
            return result.get(""choices"", [{}])[0].get(""message"", {}).get(""content"", """")
        except requests.exceptions.RequestException as e:
            raise Exception(f""HTTP request failed: {str(e)}"") from e
        except Exception as e:
            raise Exception(f""An error occurred: {str(e)}"") from e
        return thought",Use local lm-studio server to generate text.,???Send chat history to language model API and return generated response???
2832,build_s3_zipped_step_path,"def build_s3_zipped_step_path(
    *,
    model_name: str,
    project: str,
    step: int,
    s3_bucket: str | None = None,
    prefix: str | None = None,
) -> str:
    
    base_path = build_s3_path(
        model_name=model_name,
        project=project,
        s3_bucket=s3_bucket,
        prefix=prefix,
    )
    return f""{base_path}/zipped-steps/{step:04d}.zip""",Return the fully-qualified S3 URI for a zipped step in a model directory.,???Constructs an S3 path for a specific zipped step in a project.???
2833,inject_network_loss,"def inject_network_loss(self, microservices: List[str], duration: str = ""200s""):
        
        chaos_experiment = {
            ""apiVersion"": ""chaos-mesh.org/v1alpha1"",
            ""kind"": ""NetworkChaos"",
            ""metadata"": {""name"": ""loss"", ""namespace"": self.namespace},
            ""spec"": {
                ""action"": ""loss"",
                ""mode"": ""one"",
                ""duration"": duration,
                ""selector"": {
                    ""namespaces"": [self.namespace],
                    ""labelSelectors"": {""io.kompose.service"": "", "".join(microservices)},
                },
                ""loss"": {""loss"": ""99"", ""correlation"": ""100""},
            },
        }

        self.create_chaos_experiment(chaos_experiment, ""network-loss"")",Inject a network loss fault.,???Simulate network packet loss in specified microservices for a defined duration.???
2834,from_string,"def from_string(cls, s):
        
        try:
            if s is None:
                return cls(""unsupported"")
            return cls(s.lower())
        except KeyError as exc:
            raise ValueError(f""Invalid metric type: {s}"") from exc",Initialize the aggregation type from a string.,"???  
Convert string input to a class instance, handling unsupported or invalid types.  
???"
2835,_convert_to_parameter_location,"def _convert_to_parameter_location(self, param_in: str) -> ParameterLocation:
        
        if param_in in [""path"", ""query"", ""header"", ""cookie""]:
            return param_in  # type: ignore[return-value]  # Safe cast since we checked values
        logger.warning(f""Unknown parameter location: {param_in}, defaulting to 'query'"")
        return ""query""",Convert string parameter location to our ParameterLocation type.,"???Map input string to parameter location, defaulting to 'query' if unknown.???"
2836,uninstall_toolbox,"def uninstall_toolbox(
    toolbox_name: str = typer.Argument(..., help=""Name of the toolbox or package to uninstall"")
) -> None:
    
    # Core global uninstall (pip uninstall + global config save)
    messages = uninstall_toolbox_core(toolbox_name)
    for msg in messages:
        console.print(msg)
    # Disable in project config
    project_cfg = load_project_config()
    enabled = project_cfg.get(""enabled_toolboxes"", [])
    if toolbox_name in enabled:
        enabled.remove(toolbox_name)
        project_cfg[""enabled_toolboxes""] = enabled
        save_project_config(project_cfg)
        console.print(f""[green]Toolbox '{toolbox_name}' disabled in project config.[/green]"")",Uninstall a toolbox and update both global and project configs.,???Uninstall and disable a specified toolbox from global and project configurations.???
2837,simulate_llm_conversation,"def simulate_llm_conversation(self, context: str, num_turns: int = 3) -> str:
        
        conversation_log = []

        def get_response(personality: str, previous_messages: str) -> str:
            prompt = (
                f""{personality}. You are participating in a brief group discussion ""
                f""about the following context:\n{context}\n\n""
                f""Previous messages:\n{previous_messages}\n\n""
                ""Provide a short, focused response (1-2 sentences) that builds on ""
                ""the discussion. Be creative but stay on topic.""
            )

            temp_conv = sm.create_conversation(
                llm_model=self.llm_model, llm_provider=self.llm_provider
            )
            temp_conv.add_message(role=""user"", text=prompt)
            response = temp_conv.send()
            return response.text.strip()

        # Select random personalities for this conversation
        selected_personalities = random.sample(
            self.llm_personalities, min(num_turns, len(self.llm_personalities))
        )

        with ThreadPoolExecutor() as executor:
            for i, personality in enumerate(selected_personalities, 1):
                previous = ""\n"".join(conversation_log)
                response = get_response(personality, previous)
                conversation_log.append(f""Speaker {i}: {response}"")

        return ""\n\n"".join(conversation_log)",Simulate a conversation between multiple LLM personalities about the context,???Simulate a multi-turn conversation using AI personalities based on given context.???
2838,action_to_dict,"def action_to_dict(self, curr_action):
        
        actions = [
            {
                ""dof_pos_target"": {
                    joint_name: curr_action[i, index]
                    for index, joint_name in enumerate(sorted(self.scenario.robots[0].joint_limits.keys()))
                }
            }
            for i in range(self.num_envs)
        ]
        return actions",Converts action tensor to dict with joint keys,"???  
Convert current action data into a structured dictionary for each environment.  
???"
2839,instantiate_cond_stage,"def instantiate_cond_stage(self, config: Dict[str, Any]):
        
        logger.info(""creating cond stage"")
        model = instantiate_from_config(config)
        self.cond_stage_model = model.eval()
        for param in self.cond_stage_model.parameters():
            param.requires_grad = False
        self.components.append(Component.COND_STAGE_MODEL.value)
        self.cond_stage_model_path = config.get(""ckpt_path"", f""{Component.COND_STAGE_MODEL.value}.ckpt"")
        logger.info(f""self.cond_stage_model_path: {self.cond_stage_model_path}"")",Instantiates the conditional stage model of the generative process.,???Initialize and configure a conditional stage model from a given configuration.???
2840,generate,"def generate(self, prompt: str, **kwargs) -> str:
                    
                    url = f""{self.base_url}/api/generate""
                    data = {""model"": self.model, ""prompt"": prompt, ""stream"": False, **kwargs}
                    response = self.session.post(url, json=data)
                    response.raise_for_status()
                    return response.json().get(""response"", """")",Generate text using Ollama's API.,???Function sends a prompt to an API to generate and return a response.???
2841,find_model,"def find_model(model_name):
    
    if model_name in pretrained_models:  # Find/download our pre-trained G.pt checkpoints
        return download_model(model_name)

    # Load a custom Sana checkpoint:
    model_name = hf_download_or_fpath(model_name)
    assert os.path.isfile(model_name), f""Could not find Sana checkpoint at {model_name}""
    print(colored(f""[Sana] Loading model from {model_name}"", attrs=[""bold""]))
    return torch.load(model_name, map_location=lambda storage, loc: storage)","Finds a pre-trained G.pt model, downloading it if necessary.","???  
Retrieve or load a specified machine learning model checkpoint.  
???"
2842,read_timeline,"def read_timeline(self, cursor: Optional[int] = None, limit: Optional[int] = 100) -> IterableCastsResult:
        
        logger.debug(f""Reading timeline, cursor: {cursor}, limit: {limit}"")
        return self._client.get_recent_casts(cursor, limit)",Read all recent casts,???Fetch recent timeline entries with optional cursor and limit parameters.???
2843,delete_iam_roles_and_policies,"def delete_iam_roles_and_policies(self):
        
        fm_policy_arn = f""arn:aws:iam::{self.account_number}:policy/{self.fm_policy_name}""
        s3_policy_arn = f""arn:aws:iam::{self.account_number}:policy/{self.s3_policy_name}""
        oss_policy_arn = f""arn:aws:iam::{self.account_number}:policy/{self.oss_policy_name}""
        self.iam_client.detach_role_policy(
            RoleName=self.kb_execution_role_name,
            PolicyArn=s3_policy_arn
        )
        self.iam_client.detach_role_policy(
            RoleName=self.kb_execution_role_name,
            PolicyArn=fm_policy_arn
        )
        self.iam_client.detach_role_policy(
            RoleName=self.kb_execution_role_name,
            PolicyArn=oss_policy_arn
        )
        self.iam_client.delete_role(RoleName=self.kb_execution_role_name)
        self.iam_client.delete_policy(PolicyArn=s3_policy_arn)
        self.iam_client.delete_policy(PolicyArn=fm_policy_arn)
        self.iam_client.delete_policy(PolicyArn=oss_policy_arn)
        return 0",Delete IAM Roles and policies used by the Knowledge Base,"???  
Remove specified IAM roles and associated policies in AWS account.  
???"
2844,remove_playlist_from_watch,"def remove_playlist_from_watch(playlist_spotify_id: str):
    
    table_name = f""playlist_{playlist_spotify_id.replace('-', '_')}""
    try:
        with _get_playlists_db_connection() as conn:  # Use playlists connection
            cursor = conn.cursor()
            cursor.execute(
                ""DELETE FROM watched_playlists WHERE spotify_id = ?"",
                (playlist_spotify_id,),
            )
            cursor.execute(f""DROP TABLE IF EXISTS {table_name}"")
            conn.commit()
            logger.info(
                f""Playlist {playlist_spotify_id} removed from watchlist and its table '{table_name}' dropped in {PLAYLISTS_DB_PATH}.""
            )
    except sqlite3.Error as e:
        logger.error(
            f""Error removing playlist {playlist_spotify_id} from watchlist in {PLAYLISTS_DB_PATH}: {e}"",
            exc_info=True,
        )
        raise",Removes a playlist from watched_playlists and drops its tracks table in playlists.db.,???Remove a playlist from the watchlist and delete its associated database table.???
2845,show_kill_confirmation_menu,"def show_kill_confirmation_menu(self, button, session_name):
        
        menu = Gtk.Menu()
        
        # Confirmation message as a disabled menu item
        msg_item = Gtk.MenuItem(label=f""Kill session '{session_name}'?"")
        msg_item.set_sensitive(False)
        menu.append(msg_item)
        
        # Separator
        menu.append(Gtk.SeparatorMenuItem())
        
        # Confirm option
        confirm_item = Gtk.MenuItem(label=""Confirm"")
        confirm_item.connect(""activate"", lambda _: self.kill_session(session_name))
        menu.append(confirm_item)
        
        # Cancel option
        cancel_item = Gtk.MenuItem(label=""Cancel"")
        # Close notch on cancel
        cancel_item.connect(""activate"", lambda _: self.close_manager())
        menu.append(cancel_item)
        
        menu.show_all()
        
        # Show the menu positioned at the button
        menu.popup_at_widget(
            button,
            Gdk.Gravity.SOUTH_WEST,
            Gdk.Gravity.NORTH_WEST,
            None
        )",Show a confirmation menu for killing a session,???Display a confirmation menu for terminating a session with confirm and cancel options.???
2846,_add_exception_to_messages,"def _add_exception_to_messages(
        self,
        messages: list[MessageDeprecated],
        response: str | None,
        e: Exception,
    ) -> list[MessageDeprecated]:
        

        return [
            *messages,
            MessageDeprecated(
                role=MessageDeprecated.Role.ASSISTANT,
                content=response or ""EMPTY MESSAGE"",
            ),
            MessageDeprecated(
                role=MessageDeprecated.Role.USER,
                content=f""Your previous response was invalid with error `{e}`.\nPlease retry"",
            ),
        ]",Add an exception message to the messages list so it can be retried,???Append error message to conversation history with assistant and user roles.???
2847,validate_config,"def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        
        required_fields = [""timeline_read_count"", ""tweet_interval""]
        missing_fields = [field for field in required_fields if field not in config]
        
        if missing_fields:
            raise ValueError(f""Missing required configuration fields: {', '.join(missing_fields)}"")
            
        if not isinstance(config[""timeline_read_count""], int) or config[""timeline_read_count""] <= 0:
            raise ValueError(""timeline_read_count must be a positive integer"")

        if not isinstance(config[""tweet_interval""], int) or config[""tweet_interval""] <= 0:
            raise ValueError(""tweet_interval must be a positive integer"")
            
        return config",Validate Twitter configuration from JSON,???Validate configuration dictionary for required fields and positive integer values???
2848,_loop_operation,"def _loop_operation(self, **kwargs):
        
        while self._loop_switch:
            for _ in range(self._interval_time):
                if self._loop_switch:
                    time.sleep(1)
                else:
                    break

            if self._loop_switch:
                if self._operation_status_run:
                    continue

                self._operation_status_run = True

                if len(self.target_names) > 1:
                    self.logger.warning(""current version is not stable under target_names.size > 1!"")

                for target_name in self.target_names:
                    try:
                        self.run_operation(target_name=target_name, **kwargs)
                    except Exception as e:
                        self.logger.exception(f""op_name={self.name} target_name={target_name} encounter exception. ""
                                              f""args={e.args}"")

                self._operation_status_run = False","Loops until _loop_switch is False, sleeping for 1 second in each interval.","???  
Execute periodic operations on targets with error handling and logging.  
???"
2849,format_size,"def format_size(size_bytes: int) -> str:
    
    for unit in [""B"", ""KB"", ""MB"", ""GB""]:
        if size_bytes < 1024:
            return f""{size_bytes:.1f}{unit}""
        size_bytes /= 1024
    return f""{size_bytes:.1f}TB""",Format file size in human readable format,???Converts byte size into human-readable format with appropriate units.???
2850,get_github_stars,"def get_github_stars(github_repos: Dict[str, str]) -> Dict[str, int]:
    
    if not github_repos:
        return {}

    repo_count = len(github_repos)
    status_message(f""Fetching GitHub stars for {repo_count} repositories..."")

    # Convert dict values to list for batch processing
    repo_urls = list(github_repos.values())

    # Fetch stars
    url_to_stars = fetch_github_stars_batch(repo_urls)

    # Map server names to star counts
    server_stars = {}
    for server_name, repo_url in github_repos.items():
        if repo_url in url_to_stars:
            server_stars[server_name] = url_to_stars[repo_url]

    return server_stars",Fetch GitHub stars for all repositories,???Fetch GitHub star counts for a list of repositories and map them to server names.???
2851,get_tools,"def get_tools():
    
    auth_header = request.headers.get('Authorization')
    if not auth_header:
        return jsonify({'error': 'No authorization header provided'}), 401

    user_data = decode_token(auth_header)
    if not user_data:
        return jsonify({'error': 'Invalid token'}), 401

    access_level = user_data.get('access', 'basic')
    available_tools = get_available_tools()
    
    # Filter tools based on access level
    allowed_tools = {
        tool_id: {
            'name': tool['config']['actionGroupName'],
            'description': tool['config'].get('description', '')
        }
        for tool_id, tool in available_tools.items()
        if access_level.lower() in tool['access_level']
    }
    
    return jsonify(allowed_tools)",Endpoint to get available tools based on user's access level,???Authorize user and filter tools based on access level for response???
2852,valid_spec_for_lifecycle,"def valid_spec_for_lifecycle():
    
    # This should now directly match the ToolSpecification model structure
    return {
        ""name"": ""TestGreeter"",
        ""description"": ""A simple test greeter tool"",
        ""purpose"": ""To greet a user with a personalized message."",
        ""input_parameters"": [
            {
                ""name"": ""user_name"",
                ""type"": ""string"",  # Ensure 'type' alias is handled if needed, or use 'type_'
                ""description"": ""The name of the user to greet."",
                ""required"": True,
            }
        ],
        ""output_format"": ""string"",  # This was 'output_schema.type' before
    }",Provides a valid tool specification as a dictionary for lifecycle tests.,"???  
Define a tool specification for a personalized greeting lifecycle model.  
???"
2853,start_arq_worker,"def start_arq_worker():
    
    global worker_process
    try:
        logging.info(""Starting ARQ worker..."")

        # Ensure logs directory exists
        log_dir = os.path.join(os.getcwd(), ""logs"")
        os.makedirs(log_dir, exist_ok=True)

        # Worker log file paths
        worker_log_path = os.path.join(log_dir, ""worker.log"")

        # Open log files
        worker_log = open(worker_log_path, ""a"")

        # Add timestamp to log
        timestamp = subprocess.check_output([""date""]).decode().strip()
        worker_log.write(f""\n\n--- Worker started at {timestamp} ---\n\n"")
        worker_log.flush()

        # Use sys.executable to ensure the same Python environment is used
        worker_cmd = [sys.executable, ""-m"", ""arq"", ""core.workers.ingestion_worker.WorkerSettings""]

        # Start the worker with output redirected to log files
        worker_process = subprocess.Popen(
            worker_cmd,
            stdout=worker_log,
            stderr=worker_log,
            env=dict(os.environ, PYTHONUNBUFFERED=""1""),  # Ensure unbuffered output
        )
        logging.info(f""ARQ worker started with PID: {worker_process.pid}"")
        logging.info(f""Worker logs available at: {worker_log_path}"")
    except Exception as e:
        logging.error(f""Failed to start ARQ worker: {e}"")
        sys.exit(1)",Start the ARQ worker as a subprocess.,???Initialize and log the execution of an ARQ worker process with error handling.???
2854,_handle_execution_error,"def _handle_execution_error(
        attempt: ExecutionAttempt, context_vars: ContextVariables, context: ReliableToolContext, e: Exception
    ) -> ReplyResult:
        
        err_msg = f""{type(e).__name__}: {e}""
        logger.error(  # Log the error from the wrapped function
            ""Wrapped function '%s' execution error: %s"",
            getattr(tool_function, ""__name__"", ""unknown_func""),
            err_msg,
            exc_info=True,  # Include traceback for wrapped function error
        )
        attempt.error = err_msg
        if attempt not in context.attempts:
            context.attempts.append(attempt)

        _set_reliable_tool_context(context_vars, context_variables_key, context)

        # Go to runner in this scenario because an error can just be handled by the runner again
        return ReplyResult(
            context_variables=context_vars,
            target=AgentTarget(runner),
            message=f""Function execution failed with error: {err_msg}."",
        )",Shared logic to handle tool_function execution error.,"???Handle execution errors by logging, updating context, and returning a failure response.???"
2855,step,"def step(self, action: str):
        
        valid_action = check_format(action, self.ACTION_LOOKUP.values())
        
        if not valid_action:
            return f""Invalid action format: {action}"", 0, False, {""action_is_effective"": False, ""action_is_valid"": False, ""success"": False}
        
        obs, rewards, dones, infos = self.alfred_env.step([action])  # BatchEnv expects a list of commands
        
        observation = obs[0]
        self.render_cache = observation
        base_reward = rewards[0]
        done = dones[0]
        info = {""action_is_effective"": True, ""action_is_valid"": True, ""success"": done}
        
        reward = self.compute_score(base_reward, valid_action, done)
        
        return self.render_cache, reward, done, info",Take a step in the environment using the provided action string.,"???Validate and execute action in environment, returning observation, reward, completion status, and info.???"
2856,_fetch_tp_shard_tensor_gate_up,"def _fetch_tp_shard_tensor_gate_up(tensor, gate_name, up_name) -> torch.Tensor:
        
        nonlocal state_dict
        nonlocal mp_group
        tp_rank = mpu.get_tensor_model_parallel_rank()
        tp_size = mpu.get_tensor_model_parallel_world_size()
        if gate_name in state_dict and up_name in state_dict:
            gate_weight = state_dict[gate_name]
            up_weight = state_dict[up_name]
            new_gate_up_weight = torch.empty(config.intermediate_size * 2, config.hidden_size, dtype=params_dtype, device=torch.cuda.current_device())
            for i in range(tp_size):
                gate_weight_tp = gate_weight[i * intermediate_size_tp : (i + 1) * intermediate_size_tp]
                up_weight_tp = up_weight[i * intermediate_size_tp : (i + 1) * intermediate_size_tp]
                new_gate_up_weight[intermediate_size_tp * 2 * i : intermediate_size_tp * 2 * (i + 1)].copy_(torch.cat([gate_weight_tp, up_weight_tp], dim=0))

            tensor_chunk = torch.chunk(new_gate_up_weight, tp_size, dim=0)
            if tensor is not None:
                tensor = tensor.data.copy_(tensor_chunk[tp_rank], non_blocking=True)
        else:
            print(f""tp_shard tensor:[{gate_name}, {up_name}] not in state_dict, skip loading"")",fetch gate_up tensor in tp shards,???Fetch and combine tensor shards based on gate and update weights for parallel processing.???
2857,get_open_set_messages_embeddings,"def get_open_set_messages_embeddings(self):
        
        save_path = f'../model/model_saved/{self.dataset_name}/open_set/'
        num_blocks = 21  # Use 2012-style processing for all datasets
        
        for i in range(num_blocks):
            block = i + 1
            print('\n\n====================================================')
            print('block: ', block)

            SBERT_embedding_path = f'{save_path}{block}/SBERT_embeddings.pkl'

            if not exists(SBERT_embedding_path):
                df_np = np.load(f'{save_path}{block}/{block}.npy', allow_pickle=True)
                
                df = pd.DataFrame(data=df_np, columns=self.columns + ['original_index', 'date'])
                print(""Dataframe loaded."")

                df['processed_text'] = [preprocess_sentence(s) for s in df['text']]
                print('message text contents preprocessed.')

                embeddings = SBERT_embed(df['processed_text'].tolist(), language=self.language)

                with open(SBERT_embedding_path, 'wb') as fp:
                    pickle.dump(embeddings, fp)
                print('SBERT embeddings stored.')
        return",Get SBERT embeddings for open set messages,???Generate and save SBERT embeddings for dataset message blocks if not already present.???
2858,_get_theme,"def _get_theme(self, theme: str) -> tuple[List[str], str]:
        
        if theme in DARK_THEMES:
            return DARK_THEMES[theme], TEXT_COLOR_DARK
        elif theme in LIGHT_THEMES:
            return LIGHT_THEMES[theme], TEXT_COLOR_LIGHT
        else:
            raise ValueError(f""Invalid theme: {theme}"")",Get the theme from the theme name.,"???Determine color scheme based on theme type, raising error if invalid.???"
2859,_direct_capture,"def _direct_capture(self, event: BaseTelemetryEvent) -> None:
		
		if self._posthog_client is None:
			return

		try:
			self._posthog_client.capture(
				self.user_id,
				event.name,
				{**event.properties, **POSTHOG_EVENT_SETTINGS},
			)
		except Exception as e:
			logger.error(f'Failed to send telemetry event {event.name}: {e}')",Should not be thread blocking because posthog magically handles it,???Capture and log telemetry events using a PostHog client with error handling.???
2860,_get_type_info,"def _get_type_info(self, type_hint: Type) -> dict:
        
        if isinstance(type_hint, type) and issubclass(type_hint, BaseModel):
            return type_hint.model_json_schema()

        return {
            ""type"": self.type_mapping.get(type_hint, ""string""),
            ""description"": f""Value of type {getattr(type_hint, '__name__', 'any')}"",
        }",Get type information for a single type.,???Determine JSON schema or default type info for a given type hint???
2861,is_empty,"def is_empty(self) -> bool:
        
        try:
            return self.vector_store.count_documents() == 0
        except Exception as e:
            raise PineconeError(f""Error checking if Pinecone index is empty: {e}"") from e",Checks if the Pinecone index is empty.,"???Check if the document storage is empty, handling potential errors.???"
2862,_update_layout,"def _update_layout(self, fig: go.Figure, symbol: str, title: str) -> None:
        
        fig.update_layout(
            title=f""{symbol} - {title}"",
            xaxis_title=""Date"",
            yaxis_title=""Price"",
            template=""plotly_dark"",
            height=500,
            showlegend=True,
            legend=dict(yanchor=""top"", y=0.99, xanchor=""left"", x=0.01),
        )",Update plot layout with consistent styling,"???Update plot layout with title, axis labels, and legend settings.???"
2863,get_sorting,"def get_sorting(request: HttpRequest):
        

        profile = request.user.profile

        sorting_dict = {
            'Newest': '-date_joined',
            'Oldest': 'date_joined',
            'Email_asc': Lower('email'),
            'Email_desc': Lower('email').desc(),
        }

        return sorting_dict[profile.user_sorting]",Get the sorting of the overview page.,???Determine sorting order based on user profile preferences.???
2864,generate_user_actions,"def generate_user_actions(n_users, posts_per_user):
    
    actions = []

    for user_id in range(1, n_users + 1):
        # Add sign up action for each user
        user_message = (
            ""username"" + str(user_id),
            ""name"" + str(user_id),
            ""No descrption."",
        )
        actions.append((user_id, user_message, ""sign_up""))

        if user_id <= users_per_group:
            # This group of users sends m posts each
            for post_num in range(1, posts_per_user + 1):
                actions.append((
                    user_id,
                    f""This is post {post_num} from User{user_id}"",
                    ""create_post"",
                ))
        elif user_id <= 2 * users_per_group:
            # This group of users sends 1 post each
            actions.append(
                (user_id, f""This is post 1 from User{user_id}"", ""create_post""))
        # The last group does not send any posts

    return actions",Generate a list of user actions for n users with different posting behaviors.,???Simulate user actions by generating sign-ups and posts based on user groups.???
2865,validate_and_print,"def validate_and_print(wav_path: str, category: str):
    
    if not os.path.exists(wav_path):
        print(f""Warning: WAV file not found at {wav_path}"")
        return

    print(f""\n=== Validating {category} Audio ==="")
    result = validate_tts(wav_path)

    if ""error"" in result:
        print(f""Error: {result['error']}"")
    else:
        print(f""Duration: {result['duration']}"")
        print(f""Sample Rate: {result['sample_rate']} Hz"")
        print(f""Peak Amplitude: {result['peak_amplitude']}"")
        print(f""RMS Level: {result['rms_level']}"")

        if result[""issues""]:
            print(""\nIssues Found:"")
            for issue in result[""issues""]:
                print(f""- {issue}"")
        else:
            print(""\nNo issues found"")",Validate a WAV file and print results.,???Validate and report audio file properties and issues for a given category.???
2866,register_types,"def register_types(
        cls,
        types: Union[str, List[str]],
        embeddings_cls: Type[BaseEmbeddings]
    ) -> None:
        
        if not issubclass(embeddings_cls, BaseEmbeddings):
            raise ValueError(f""{embeddings_cls} must be a subclass of BaseEmbeddings"")

        if isinstance(types, str):
            cls.type_registry[types] = embeddings_cls
        elif isinstance(types, list):
            for type in types:
                cls.type_registry[type] = embeddings_cls
        else:
            raise ValueError(f""Invalid types: {types}"")",Register a new type.,???Register embedding classes to type registry ensuring subclass validation???
2867,as_service,"def as_service(self, port: int, host: str = ""0.0.0.0""):
        
        from dapr_agents.service.fastapi import FastAPIServerBase

        self._http_server = FastAPIServerBase(
            service_name=self.name,
            service_port=port,
            service_host=host,
        )

        # Register built-in routes
        self.app.add_api_route(""/status"", lambda: {""ok"": True})
        self.app.add_api_route(
            ""/start-workflow"", self.run_workflow_from_request, methods=[""POST""]
        )

        # Allow subclass to register additional routes
        self.register_routes()

        return self",Enables FastAPI-based service mode for the agent by initializing a FastAPI server instance.,???Initialize and configure a FastAPI server with default and custom routes.???
2868,create_task,"def create_task(
        self,
        agent_id: str,
        query: Optional[str] = None,
        tool: Optional[str] = None,
        tool_arguments: Optional[Dict[str, Any]] = None,
        raw_data_only: bool = False,
        agent_type: str = ""AGENT"",
    ) -> MeshTaskResponse:
        
        input_data = self._prepare_input(query, tool, tool_arguments, raw_data_only)
        payload = self._prepare_payload(
            agent_id=agent_id,
            agent_type=agent_type,
            task_details=input_data,
        )

        response = self.client.post(f""{self.base_url}/mesh_task_create"", json=payload)
        response.raise_for_status()
        return MeshTaskResponse(**response.json())",Create a new asynchronous task.,???Create and send a task request to a server using specified parameters and return the response.???
2869,mock_issue_types,"def mock_issue_types():
    
    return [
        {""id"": ""10000"", ""name"": ""Bug"", ""description"": ""A bug""},
        {""id"": ""10001"", ""name"": ""Task"", ""description"": ""A task""},
    ]",Fixture to return mock issue types.,"???Function returns a list of mock issue types with IDs, names, and descriptions.???"
2870,_pv_exists,"def _pv_exists(self, pv_name: str) -> bool:
        
        command = f""kubectl get pv {pv_name}""
        try:
            result = KubeCtl().exec_command(command)
            if ""No resources found"" in result or ""Error"" in result:
                return False
        except CalledProcessError as e:
            return False
        return True",Check if the PersistentVolume exists.,???Check if a Kubernetes persistent volume exists using kubectl command???
2871,_init_prompt_blocks,"def _init_prompt_blocks(self):
        
        super()._init_prompt_blocks()
        self._prompt_blocks.update(
            {
                ""plan"": self._get_next_state_prompt(),
                ""assign"": self._get_actions_prompt(),
                ""handle_input"": self._get_graph_handle_input_prompt(),
            }
        )","Initialize the prompt blocks with finding next state, actions and final answer prompts.","???  
Initialize and update prompt blocks with state, actions, and input handling prompts.  
???"
2872,args_as_list,"def args_as_list(args: dict[str, str]) -> list[str]:
		
		return [f'--{key.lstrip(""-"")}={value}' if value else f'--{key.lstrip(""-"")}' for key, value in args.items()]",Return the extra launch CLI args as a list of strings.,"???  
Convert dictionary of arguments into a list of command-line options.  
???"
2873,create_namespace,"def create_namespace(self):
        
        result = self.kubectl.exec_command(f""kubectl get namespace {self.namespace}"")
        if ""notfound"" in result.lower():
            print(f""Namespace {self.namespace} not found. Creating namespace."")
            create_namespace_command = f""kubectl create namespace {self.namespace}""
            create_result = self.kubectl.exec_command(create_namespace_command)
            print(f""Namespace {self.namespace} created successfully: {create_result}"")
        else:
            print(f""Namespace {self.namespace} already exists."")",Create the namespace for the application if it doesn't exist.,???Check and create a Kubernetes namespace if it doesn't exist.???
2874,search_episode,"def search_episode(api_url: str, api_key: str, api_timeout: int, episode_ids: List[int]) -> Optional[Union[int, str]]:
    
    if not episode_ids:
        sonarr_logger.warning(""No episode IDs provided for search."")
        return None
    try:
        endpoint = f""{api_url}/api/v3/command""
        payload = {
            ""name"": ""EpisodeSearch"",
            ""episodeIds"": episode_ids
        }
        response = requests.post(endpoint, headers={""X-Api-Key"": api_key}, json=payload, timeout=api_timeout)
        response.raise_for_status()
        command_id = response.json().get('id')
        sonarr_logger.info(f""Triggered Sonarr search for episode IDs: {episode_ids}. Command ID: {command_id}"")
        return command_id
    except requests.exceptions.RequestException as e:
        sonarr_logger.error(f""Error triggering Sonarr search for episode IDs {episode_ids}: {e}"")
        return None
    except Exception as e:
        sonarr_logger.error(f""An unexpected error occurred while triggering Sonarr search: {e}"")
        return None",Trigger a search for specific episodes in Sonarr.,"???Initiates episode search via Sonarr API, handling errors and logging outcomes.???"
2875,platform_must_be_valid,"def platform_must_be_valid(cls, v):
        
        try:
            Platform(v)
            return v
        except ValueError:
            supported = ', '.join([p.value for p in Platform])
            raise ValueError(f""Invalid platform '{v}'. Must be one of: {supported}"")",Validate that the platform is one of the supported platforms,"???  
Validate platform input against predefined options, raising error if invalid.  
???"
2876,init_student_metrics,"def init_student_metrics(self):
        
        self.student_metrics = {
            ""overall_accuracy"": self.profile.get(""current_avg_score"", 0) / 100,
            ""topic_accuracies"": {},
        }

        # Set initial proficiency values from profile
        for topic in self.profile.get(""topics"", []):
            topic_name = topic.get(""name"")
            proficiency = topic.get(""proficiency"", 0.5)
            self.student_metrics[""topic_accuracies""][topic_name] = proficiency",Initialize student metrics from profile.,???Initialize student performance metrics from profile data for accuracy tracking.???
2877,get_provider_config_env,"def get_provider_config_env(key: str, index: int, default: str | None = None):
    
    env_var = f""{key}_{index}"" if index > 0 else key
    if default:
        return os.environ.get(env_var, default)
    try:
        return os.environ[env_var]
    except KeyError:
        raise MissingEnvVariablesError([env_var])",Retrieve a value for a given env.,???Retrieve environment variable for provider configuration with optional default.???
2878,create_summary_panel,"def create_summary_panel(events: list[Event]) -> Panel:
    

    text = Text()

    # Count various event types
    chatting = 0
    tool_calls = 0
    mcps = set()

    for event in events:
        if event.type == ""info"":
            if ""mcp_connection_manager"" in event.namespace:
                message = event.message
                if "": "" in message:
                    mcp_name = message.split("": "")[0]
                    mcps.add(mcp_name)

        progress_event = convert_log_event(event)
        if progress_event:
            if progress_event.action == ProgressAction.CHATTING:
                chatting += 1
            elif progress_event.action == ProgressAction.CALLING_TOOL:
                tool_calls += 1

    text.append(""Summary:\n\n"", style=""bold"")
    text.append(""MCPs: "", style=""bold"")
    text.append(f""{', '.join(sorted(mcps))}\n"", style=""green"")
    text.append(""Chat Turns: "", style=""bold"")
    text.append(f""{chatting}\n"", style=""blue"")
    text.append(""Tool Calls: "", style=""bold"")
    text.append(f""{tool_calls}\n"", style=""magenta"")

    return Panel(text, title=""Event Statistics"")",Create a summary panel with stats.,"???Generate a summary panel displaying event statistics including MCPs, chat turns, and tool calls.???"
2879,get_meeting_transcriptions,"def get_meeting_transcriptions(meeting_id, start_time=None, end_time=None):
        
        session = get_session()
        try:
            query = session.query(Transcription).filter_by(meeting_id=meeting_id)
            
            if start_time:
                query = query.filter(Transcription.timestamp >= start_time)
            if end_time:
                query = query.filter(Transcription.timestamp <= end_time)
                
            query = query.order_by(Transcription.timestamp)
            transcriptions = query.all()
            
            result = []
            for t in transcriptions:
                result.append({
                    ""id"": t.id,
                    ""speaker"": t.speaker,
                    ""content"": t.content,
                    ""timestamp"": t.timestamp.isoformat(),
                    ""confidence"": t.confidence
                })
            
            return result
        except SQLAlchemyError as e:
            logger.error(f""Error retrieving transcriptions: {e}"")
            raise
        finally:
            session.close()","Get all transcriptions for a meeting, optionally filtered by time range",???Retrieve and format meeting transcriptions within optional time bounds from a database.???
2880,validate_directory_structure,"def validate_directory_structure(root_dir: Path) -> bool:
    
    if not root_dir.exists():
        print(f""Error: Directory {root_dir} does not exist"")
        return False
    
    acts = [d for d in root_dir.iterdir() if d.is_dir()]
    if not acts:
        print(f""Error: No acts found in {root_dir}"")
        return False
    
    return True",Validate that the directory structure matches expected format,???Check if the directory exists and contains subdirectories???
2881,parallel_weight_loader,"def parallel_weight_loader(self, param: torch.Tensor, loaded_weight: torch.Tensor) -> None:
    
    assert param.size() == loaded_weight.size(
    ), 'the parameter size is not align with the loaded weight size, param size: {}, loaded_weight size: {}'.format(
        param.size(), loaded_weight.size())
    assert param.data.dtype == loaded_weight.data.dtype, ""if we want to shared weights, the data type should also be the same""

    param.data = loaded_weight.data",Parallel Linear weight loader.,???Ensure parameter and weight alignment before updating model weights.???
2882,stop_audio,"def stop_audio():
    
    global audio_player

    if audio_player is None:
        return JSONResponse({""error"": ""Audio player not initialized""}, status_code=500)

    try:
        audio_player.stop()
        return {""status"": ""stopped""}
    except Exception as e:
        return JSONResponse(
            {""error"": f""Failed to stop audio: {str(e)}""}, status_code=500
        )",Stop any currently playing audio.,???Handle stopping audio playback with error management.???
2883,format_currency_from_cents,"def format_currency_from_cents(amount_cents: Optional[int], currency: str = ""USD"") -> Optional[str]:
    
    if amount_cents is None:
        return None
    amount_dollars = amount_cents / 100
    if currency == ""USD"":
        return f""${amount_dollars:,.2f}""
    return f""{amount_dollars:,.2f} {currency}""",Convert cents to formatted currency string.,???Convert cents to formatted currency string with optional currency code.???
2884,_get_neighbors,"def _get_neighbors(
        self, project_id: str, node_ids: List[str]
    ) -> Optional[List[Dict[str, Any]]]:
        
        query = 
        with self.neo4j_driver.session() as session:
            result = session.run(query, project_id=project_id, node_ids=node_ids)
            record = result.single()

            if not record:
                return None
            return record[""neighbors""]",Retrieve neighbors from Neo4j within 2 hops in either direction.,???Retrieve node neighbors from a database using project and node identifiers.???
2885,_format_metrics,"def _format_metrics(self, metrics: Dict[str, float]) -> str:
        
        # Use safe formatting to handle mixed numeric and string values
        formatted_parts = []
        for name, value in metrics.items():
            if isinstance(value, (int, float)):
                try:
                    formatted_parts.append(f""- {name}: {value:.4f}"")
                except (ValueError, TypeError):
                    formatted_parts.append(f""- {name}: {value}"")
            else:
                formatted_parts.append(f""- {name}: {value}"")
        return ""\n"".join(formatted_parts)",Format metrics for the prompt using safe formatting,???Formats a dictionary of metrics into a readable string with precision handling.???
2886,get_agent_tools,"def get_agent_tools(self, agent_name: str) -> ast.List:
        
        method = asttools.find_method(self.get_agent_methods(), agent_name)
        if method is None:
            raise ValidationError(f""Method `{agent_name}` does not exist in {ENTRYPOINT}"")

        agent_class = asttools.find_class_instantiation(method, 'FunctionAgent')
        if agent_class is None:
            raise ValidationError(f""Method `{agent_name}` does not call `FunctionAgent` in {ENTRYPOINT}"")

        tools_kwarg = asttools.find_kwarg_in_method_call(agent_class, 'tools')
        if not tools_kwarg:
            raise ValidationError(f""`FunctionAgent` does not have a kwarg `tools` in {ENTRYPOINT}"")

        if not isinstance(tools_kwarg.value, ast.List):
            raise ValidationError(f""`FunctionAgent` must define a list for kwarg `tools` in {ENTRYPOINT}"")

        return tools_kwarg.value",Get the list of tools used by an agent as an AST List node.,???Extracts and validates a list of tools from a specified agent method.???
2887,read_text_file,"def read_text_file(file_path):
    
    max_chars = 3000  # Limit processing time
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            text = file.read(max_chars)
        return text
    except Exception as e:
        print(f""Error reading text file {file_path}: {e}"")
        return None",Read text content from a text file.,"???Read a text file up to 3000 characters, handling errors gracefully.???"
2888,_create_execution_container,"def _create_execution_container(self, volume_name: str) -> docker.models.containers.Container:
        
        # TODO: Add a name to the container and volume so that they are unique for a each row
        return self.client.containers.create(
            self.PYTHON_IMAGE,
            command=[  # noqa: E501
                f""bash -c python {self.WORKSPACE_DIR}/program.py < {self.WORKSPACE_DIR}/input.txt > {self.WORKSPACE_DIR}/output.txt 2> {self.WORKSPACE_DIR}/error.txt; echo $? > {self.WORKSPACE_DIR}/exit_code.txt""  # noqa: E501
            ],
            volumes={volume_name: {""bind"": self.WORKSPACE_DIR, ""mode"": ""rw""}},
            working_dir=self.WORKSPACE_DIR,
            entrypoint=[""/bin/sh"", ""-c""],
            user=""root"",
        )",Create the main execution container.,???Create a Docker container to execute a Python script with input/output redirection.???
2889,_eval_ast,"def _eval_ast(node: ast.AST) -> Decimal:
    
    if isinstance(node, ast.BinOp):
        left: Decimal = _eval_ast(node.left)
        right: Decimal = _eval_ast(node.right)
        if isinstance(node.op, ast.Add):
            return left + right
        elif isinstance(node.op, ast.Sub):
            return left - right
        elif isinstance(node.op, ast.Mult):
            return left * right
        elif isinstance(node.op, ast.Div):
            return left / right
        else:
            raise ValueError(f""Unsupported operator: {node.op}"")
    elif isinstance(node, ast.UnaryOp):
        operand: Decimal = _eval_ast(node.operand)
        if isinstance(node.op, ast.UAdd):
            return operand
        elif isinstance(node.op, ast.USub):
            return -operand
        else:
            raise ValueError(f""Unsupported unary operator: {node.op}"")
    elif isinstance(node, ast.Constant):  # For Python 3.8+
        return Decimal(str(node.value))
    elif isinstance(node, ast.Num):  # For older Python versions
        return Decimal(str(node.n))
    else:
        raise ValueError(f""Unsupported expression component: {node}"")",Recursively evaluate an AST node using Decimal arithmetic.,???Evaluate arithmetic expressions in an abstract syntax tree using decimal precision.???
2890,db_path,"def db_path():
    
    current_test_dir = Path(__file__).parent
    db_filepath = current_test_dir / f""codegate_test_{uuid.uuid4()}.db""
    db_fullpath = db_filepath.absolute()
    connection.init_db_sync(str(db_fullpath))
    yield db_fullpath
    if db_fullpath.is_file():
        db_fullpath.unlink()",Creates a temporary database file path.,"???  
Generate and manage a temporary database file for testing purposes.  
???"
2891,regenerate_system_prompt,"def regenerate_system_prompt(self) -> None:
        
        system_prompt = generate_system_prompt(self.internal_tools)
        if self.conversation_history and self.conversation_history[0].get(""role"") == ""system"":
            self.conversation_history[0][""content""] = system_prompt
        else:
            self.conversation_history.insert(0, {""role"": ""system"", ""content"": system_prompt})",Regenerate system prompt with current tools.,???Update or insert system prompt in conversation history based on role check.???
2892,validate_config,"def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        
        required_fields = [""rpc""]
        missing_fields = [field for field in required_fields if field not in config]
        if missing_fields:
            raise ValueError(
                f""Missing required configuration fields: {', '.join(missing_fields)}""
            )

        if not isinstance(config[""rpc""], str):
            raise ValueError(""rpc must be a positive integer"")

        return config",Validate Solana configuration from JSON,???Ensure configuration contains required fields and validate their types???
2893,get_audio,"def get_audio(self, audio_id):
        
        audio = self.collection.get_audio(audio_id)
        return {
            ""id"": audio.id,
            ""name"": audio.name,
            ""collection_id"": audio.collection_id,
            ""length"": audio.length,
            ""url"": audio.generate_url(),
        }",Get an audio by ID.,"???  
Retrieve and return audio metadata and URL by audio ID.  
???"
2894,filter_on_length,"def filter_on_length(
    data: Dataset, max_length: int, tokenizer: PreTrainedTokenizer
) -> Dataset:
    

    def check_length(x):
        return (
            len(
                tokenizer.apply_chat_template(
                    x[""prompt""], tokenize=True, add_generation_prompt=True
                )
            )
            <= max_length
        )

    len_before = len(data)
    data = data.filter(check_length)
    print(
        f""Filtered dataset: {len_before} -> {len(data)} samples based on max_length {max_length}""
    )
    return data",Filters dataset entries based on tokenized prompt length.,???Filter dataset entries by tokenized prompt length using a specified maximum limit.???
2895,filter_timestamp,"def filter_timestamp(self, regex_filter: str) -> Dict[str, datetime]:
        
        pattern = re.compile(regex_filter)
        timestamps_snapshot = self._timestamps.copy()
        return {key: ts for key, ts in timestamps_snapshot.items() if pattern.search(key)}",Retrieve timestamps whose keys match the regex filter.,???Filter timestamps by matching keys against a regex pattern.???
2896,cpu_parallelism_group,"def cpu_parallelism_group():
    
    if _model_name is None:
        raise RuntimeError(""Global constant `model_name` is accessed before set."")
    if _cpu_pgroups.get(_model_name, None) is None:
        raise RuntimeError(f""Parallelism group for model {_model_name} is not set."")
    return _cpu_pgroups[_model_name]",Returns the GLOO 3D parallelism group of a specific model.,???Ensure CPU parallelism group is set for the model???
2897,format_tool_for_display,"def format_tool_for_display(tool: ToolInfo, show_details: bool = False) -> Dict[str, str]:
    
    display_data = {
        ""name"": tool.name,
        ""server"": tool.namespace,
        ""description"": tool.description or ""No description""
    }
    
    if show_details and tool.parameters:
        # Format parameters
        params = []
        if ""properties"" in tool.parameters:
            for name, details in tool.parameters[""properties""].items():
                param_type = details.get(""type"", ""any"")
                required = name in tool.parameters.get(""required"", [])
                params.append(f""{name}{' (required)' if required else ''}: {param_type}"")
        
        display_data[""parameters""] = ""\n"".join(params) if params else ""None""
    
    return display_data",Format a tool for display in UI.,???Format tool information for display with optional detailed parameters???
2898,_format_moves,"def _format_moves(self, moves: list[tuple[str, str, str]]) -> str:
        
        if not moves:
            return ""No""
        return json.dumps([f""{color},{start},{end}"" for color, start, end in moves])",Format the solution moves as a string.,???Format chess moves into JSON string or return 'No' if empty???
2899,collect_memory_usage,"def collect_memory_usage(self) -> Dict[str, float]:
        
        vm = psutil.virtual_memory()
        return {
            'total': vm.total,
            'available': vm.available,
            'percent': vm.percent,
            'used': vm.used,
            'free': vm.free
        }",Collect memory usage statistics,???Gathers and returns system memory statistics as a dictionary.???
2900,py_annotation,"def py_annotation(self) -> str:
        
        if self.items:
            if self.items.ref:
                py_ref = ref_to_python_domain(self.items.ref, self.domain)
                ann = ""typing.List[{}]"".format(py_ref)
            else:
                ann = ""typing.List[{}]"".format(
                    CdpPrimitiveType.get_annotation(self.items.type)
                )
        else:
            if self.ref:
                py_ref = ref_to_python_domain(self.ref, self.domain)
                ann = py_ref
            else:
                ann = CdpPrimitiveType.get_annotation(typing.cast(str, self.type))
        if self.optional:
            ann = f""typing.Optional[{ann}]""
        return ann",This property's Python type annotation.,???Generate Python type annotations based on item references and optionality???
2901,chat_component,"def chat_component():
    
    st.subheader(f""{t('Welcome to ')} Agent RAG"")

    # Setup chat container
    his_container = st.container(height=500)
    
    with his_container:
        # Display chat history
        for (query, response) in st.session_state.agent_rag_history:
            with st.chat_message(name=""user"", avatar=""user""):
                st.markdown(query, unsafe_allow_html=True)
            with st.chat_message(name=""assistant"", avatar=""assistant""):
                st.markdown(response, unsafe_allow_html=True)
        container = st.empty()
        container_a = st.empty()
        
    # Handle user input
    chat_input_container = st.container()
    with chat_input_container:
        if query := st.chat_input(t(""Please input your question"")):
            container.chat_message(""user"").markdown(query, unsafe_allow_html=True)
            response_with_extra_info, pure_response = asyncio.run(listen(query,container_a))
            
            # Update chat history
            st.session_state.agent_rag_messages.append({""role"": ""user"", ""content"": query})
            st.session_state.agent_rag_messages.append({""role"": ""assistant"", ""content"": pure_response})
            st.session_state.agent_rag_history.append((query, response_with_extra_info))
            st.rerun()",Display the chat interface component.,???Initialize and manage a chat interface with user input handling and session-based message history.???
2902,generate_agent_response,"def generate_agent_response(agent: SimpleAgent, user_input: str):
    
    response_text = """"
    if agent.streaming.enabled:
        streaming_handler = StreamingIteratorCallbackHandler()
        agent.run(
            input_data={""input"": user_input, ""user_id"": ""1"", ""session_id"": ""1""},
            config=RunnableConfig(callbacks=[streaming_handler]),
        )

        for chunk in streaming_handler:
            content = chunk.data.get(""choices"", [{}])[0].get(""delta"", {}).get(""content"", """")
            if content:
                print(content)
                response_text += "" "" + content
                yield content
    else:
        result = agent.run({""input"": user_input})
        response_text = result.output.get(""content"", """")
        yield response_text",Processes the user input using the agent.,???Generate and stream or return agent's response based on configuration???
2903,update_env_var,"def update_env_var(name, value):
    
    os.environ[name] = value
    vertex_log('info', f""Updated environment variable: {name}"")",Update environment variable in memory.,???Update an environment variable and log the change.???
2904,stream_decode,"def stream_decode(self, text: str) -> Optional[Dict[str, Any]]:
        
        if not self.enable_thinking:
            return {""delta_content"": text}

        return self._parse_stream_response(text)",Parse tool calls from model output.,"???Decodes text based on thinking mode, returning parsed or raw content.???"
2905,cleanup_cache,"def cleanup_cache(self, max_size_gb: Optional[float] = None) -> None:
        
        if not self.cache_dir.exists():
            return

        # Get cache size
        total_size = sum(f.stat().st_size for f in self.cache_dir.rglob(""*"") if f.is_file()) / (
            1024**3
        )  # Convert to GB

        print(f""Cache size: {total_size:.2f} GB"")

        if max_size_gb and total_size > max_size_gb:
            print(f""Cache exceeds {max_size_gb} GB, cleaning up..."")

            # Get all repo directories with their last modified times
            repos = []
            for owner_dir in self.cache_dir.iterdir():
                if owner_dir.is_dir():
                    for repo_dir in owner_dir.iterdir():
                        if repo_dir.is_dir():
                            repos.append((repo_dir.stat().st_mtime, repo_dir))

            # Sort by last modified (oldest first)
            repos.sort()

            # Remove oldest repos until we're under the limit
            for _, repo_dir in repos:
                if total_size <= max_size_gb:
                    break

                print(f""Removing old cache: {repo_dir}"")
                repo_size = sum(f.stat().st_size for f in repo_dir.rglob(""*"") if f.is_file()) / (1024**3)

                shutil.rmtree(repo_dir)
                total_size -= repo_size",Clean up old cache entries.,???Remove oldest cache directories if total size exceeds specified limit.???
2906,clean_url,"def clean_url(self, url:str) -> str:
        
        clean = url.split('#')[0]
        parts = clean.split('?', 1)
        base_url = parts[0]
        if len(parts) > 1:
            query = parts[1]
            essential_params = []
            for param in query.split('&'):
                if param.startswith('_skw=') or param.startswith('q=') or param.startswith('s='):
                    essential_params.append(param)
                elif param.startswith('_') or param.startswith('hash=') or param.startswith('itmmeta='):
                    break
            if essential_params:
                return f""{base_url}?{'&'.join(essential_params)}""
        return base_url",Clean URL to keep only the part needed for navigation to the page,"???  
Extracts and retains essential query parameters from a URL, discarding fragments and non-essential parts.  
???"
2907,collect_sample_data,"def collect_sample_data(
    engine,
    tables: List[str],
    table_metadata: Dict[str, dict],
    sample_data: Dict[str, list],
    sampled_ids: Dict[str, list],
) -> None:
    
    for table in tables:
        with engine.connect() as conn:
            # Get parent samples
            result = conn.execute(text(f""SELECT * FROM {table} LIMIT 5""))
            samples = [dict(row._mapping) for row in result]
            sample_data[table] = samples

            # Store IDs for child sampling
            if samples and table_metadata[table][""primary_keys""]:
                pk_col = table_metadata[table][""primary_keys""][0]
                sampled_ids[table] = [row[pk_col] for row in samples]",Collect sample data while maintaining referential integrity.,???Extract sample records and primary keys from database tables for analysis.???
2908,replay,"def replay():
    
    try:
        JobpostingCrew().crew().replay(task_id=sys.argv[1])

    except Exception as e:
        raise Exception(f""An error occurred while replaying the crew: {e}"")",Replay the crew execution from a specific task.,"??? 
Executes a task replay for a job posting crew using a specified task ID.
???"
2909,parse_json_blob,"def parse_json_blob(json_blob: str) -> tuple[dict[str, str], str]:
    ""Extracts the JSON blob from the input and returns the JSON data and the rest of the input.""
    try:
        first_accolade_index = json_blob.find(""{"")
        last_accolade_index = [a.start() for a in list(re.finditer(""}"", json_blob))][-1]
        json_data = json_blob[first_accolade_index : last_accolade_index + 1]
        json_data = json.loads(json_data, strict=False)
        return json_data, json_blob[:first_accolade_index]
    except IndexError:
        raise ValueError(""The model output does not contain any JSON blob."")
    except json.JSONDecodeError as e:
        place = e.pos
        if json_blob[place - 1 : place + 2] == ""},\n"":
            raise ValueError(
                ""JSON is invalid: you probably tried to provide multiple tool calls in one action. PROVIDE ONLY ONE TOOL CALL.""
            )
        raise ValueError(
            f""The JSON blob you used is invalid due to the following error: {e}.\n""
            f""JSON blob was: {json_blob}, decoding failed on that specific part of the blob:\n""
            f""'{json_blob[place - 4 : place + 5]}'.""
        )",Extracts the JSON blob from the input and returns the JSON data and the rest of the input.,"???Extracts and parses JSON data from a string, returning the JSON and remaining text.???"
2910,set_group_attributes,"def set_group_attributes(group_name: str, attributes: HueAttributes) -> dict[str, Any]:
    
    if not (bridge := _get_bridge()):
        return {""error"": ""Bridge not connected"", ""success"": False}

    if not isinstance(attributes, dict) or not attributes:
        return {
            ""error"": ""Attributes must be a non-empty dictionary"",
            ""success"": False,
            ""group"": group_name,
        }

    try:
        result = bridge.set_group(group_name, dict(attributes))
        return {
            ""group"": group_name,
            ""set_attributes"": attributes,
            ""success"": True,
            ""phue2_result"": result,
        }
    except (KeyError, PhueException, ValueError, Exception) as e:
        return handle_phue_error(group_name, ""set_group_attributes"", e)",Sets multiple attributes for all lights within a specific group.,"???Set lighting group attributes, handling errors and connection status???"
2911,get_balance,"def get_balance(self, address: Optional[str] = None, token_address: Optional[str] = None) -> float:
        
        try:
            if not address:
                private_key = os.getenv('SONIC_PRIVATE_KEY')
                if not private_key:
                    raise SonicConnectionError(""No wallet configured"")
                account = self._web3.eth.account.from_key(private_key)
                address = account.address

            if token_address:
                contract = self._web3.eth.contract(
                    address=Web3.to_checksum_address(token_address),
                    abi=self.ERC20_ABI
                )
                balance = contract.functions.balanceOf(address).call()
                decimals = contract.functions.decimals().call()
                return balance / (10 ** decimals)
            else:
                balance = self._web3.eth.get_balance(address)
                return self._web3.from_wei(balance, 'ether')

        except Exception as e:
            logger.error(f""Failed to get balance: {e}"")
            raise",Get balance for an address or the configured wallet,???Retrieve cryptocurrency balance for a given address or token using Web3.???
2912,get_train_dataloader,"def get_train_dataloader(self) -> DataLoader:
        

        if self.precompute_ref_log_probs and not self._precomputed_train_ref_log_probs:
            dataloader_params = {
                ""batch_size"": self.args.per_device_train_batch_size,
                ""collate_fn"": self.data_collator,
                ""num_workers"": self.args.dataloader_num_workers,
                ""pin_memory"": self.args.dataloader_pin_memory,
                ""shuffle"": False,
            }

            # prepare dataloader
            data_loader = self.accelerator.prepare(DataLoader(self.train_dataset, **dataloader_params))
            reference_completion_logps = []

            for padded_batch in tqdm(iterable=data_loader, desc=""Train dataset reference log probs""):
                reference_completion_logp = self.compute_reference_log_probs(padded_batch)

                reference_completion_logp = self.accelerator.gather_for_metrics(reference_completion_logp)
                reference_completion_logps.append(reference_completion_logp.cpu())

            self.train_dataset = self.train_dataset.add_column(
                name=""reference_logps"", column=torch.cat(reference_completion_logps).float().numpy()
            )

            self._precomputed_train_ref_log_probs = True

        return super().get_train_dataloader()",Returns the training [`~torch.utils.data.DataLoader`].,???Precompute and append reference log probabilities to training dataset.???
2913,google_fn,"def google_fn(self, history, verbose=False):
        
        base_url = self.server_ip
        if self.is_local:
            raise Exception(""Google Gemini is not available for local use. Change config.ini"")

        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=history,
            )
            if response is None:
                raise Exception(""Google response is empty."")
            thought = response.choices[0].message.content
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""GOOGLE API error: {str(e)}"") from e",Use google gemini to generate text.,"???Function queries Google API for chat completion, handling errors and optionally printing the response.???"
2914,update_struc_data,"def update_struc_data(self):
        
        self.struc_data = {}  # {comm1: [vol1, cut1, community_node_SE, leaf_nodes_SE], comm2:[vol2, cut2, community_node_SE, leaf_nodes_SE]，... }
        for vname in self.division.keys():
            comm = self.division[vname]
            volume = self.get_volume(comm)
            cut = self.get_cut(comm)
            if volume == 0:
                vSE = 0
            else:
                vSE = - (cut / self.vol) * math.log2(volume / self.vol)
            vnodeSE = 0
            for node in comm:
                d = self.graph.degree(node, weight='weight')
                if d != 0:
                    vnodeSE -= (d / self.vol) * math.log2(d / volume)
            self.struc_data[vname] = [volume, cut, vSE, vnodeSE]","calculate the volume, cut, communitiy mode SE, and leaf nodes SE of each cummunity, then store them into self.struc_data",???Calculate and store structural metrics for each community in a graph???
2915,open_url,"def open_url(
    dialog_id: str, account_index: int, account_url_type: AccountUrlType
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""dialogId""] = dialog_id
    params[""accountIndex""] = account_index
    params[""accountUrlType""] = account_url_type.to_json()
    cmd_dict: T_JSON_DICT = {
        ""method"": ""FedCm.openUrl"",
        ""params"": params,
    }
    json = yield cmd_dict",:param dialog_id: :param account_index: :param account_url_type:,???Generate command to open a URL with specified dialog and account parameters.???
2916,list_actions,"def list_actions(self, input_list: List[str]) -> None:
        
        if len(input_list) < 2:
            logger.info(""\nPlease specify a connection."")
            logger.info(""Format: list-actions {connection}"")
            logger.info(""Use 'list-connections' to see available connections."")
            return

        self.agent.connection_manager.list_actions(connection_name=input_list[1])",Handle list actions command,???Log instructions or list actions based on input list length.???
2917,generate_json,"def generate_json(self, prompt: str, schema: str) -> Dict[str, Any]:
        
        try:
            # Create JSON generator with the schema
            generator = generate.json(self.model, schema)
            logger.info(""Created JSON generator with schema"")

            # Generate JSON response
            result = generator(prompt)
            logger.info(""Successfully generated JSON response"")
            return result

        except Exception as e:
            logger.error(f""Error generating JSON: {str(e)}"")
            raise",Generate JSON based on the provided schema and prompt.,???Generate JSON output from prompt using a specified schema???
2918,get_choice_delta_text,"def get_choice_delta_text(self, rsp: dict) -> str:
        
        return rsp.get(""choices"", [{}])[0].get(""delta"", {}).get(""content"", """")",Required to provide the first text of stream choice,???Extracts content from the first choice's delta in a response dictionary.???
2919,load_all_datasets,"def load_all_datasets(
    data_config: DataConfig,
    split: str,
    tokenizer: PreTrainedTokenizer,
    rank: int,
    world_size: int,
) -> InterleaveDataset:
    

    if data_config.split_by_data_rank and (
        data_config.data_rank is not None and data_config.data_world_size is not None
    ):
        split_rank = data_config.data_rank * world_size + rank
        split_world_size = data_config.data_world_size * world_size
    else:
        split_rank = rank
        split_world_size = world_size


    get_logger().info(""Loading Train dataset(s)"")

    ds = _load_datasets(
        dataset_names=data_config.dataset_name_or_paths,
        split=split,
        data_rank=split_rank,
        data_world_size=split_world_size,
        probabilities=_get_probabilities(data_config),
        reverse_data_files=data_config.reverse_data_files,
        tokenizer=tokenizer,
    )

    get_logger().info(f""Train dataset: {ds}"")

    return ds",Load all datasets and interleave them,???Load and configure datasets for distributed training with tokenizer support.???
2920,transfer,"def transfer(self, to_address: str, amount: float, token_address: Optional[str] = None) -> str:
        
        try:
            current_balance = self.get_balance(token_address=token_address)
            if current_balance < amount:
                raise ValueError(f""Insufficient balance. Required: {amount}, Available: {current_balance}"")
            tx = self._prepare_transfer_tx(to_address, amount, token_address)
            private_key = os.getenv('EVM_PRIVATE_KEY') or os.getenv('ETH_PRIVATE_KEY')
            account = self._web3.eth.account.from_key(private_key)
            signed = account.sign_transaction(tx)
            tx_hash = self._web3.eth.send_raw_transaction(signed.rawTransaction)
            tx_url = self._get_explorer_link(tx_hash.hex())
            return tx_url

        except Exception as e:
            logger.error(f""Transfer failed: {str(e)}"")
            raise",Transfer ETH or tokens with balance validation,"???Facilitates cryptocurrency transfer by preparing, signing, and sending a transaction.???"
2921,_initialize_model,"def _initialize_model(self):
        
        self.model = BiCodec.load_from_checkpoint(f""{self.model_dir}/BiCodec"")
        self.processor = Wav2Vec2FeatureExtractor.from_pretrained(
            f""{self.model_dir}/wav2vec2-large-xlsr-53""
        )
        self.feature_extractor = Wav2Vec2Model.from_pretrained(
            f""{self.model_dir}/wav2vec2-large-xlsr-53""
        )
        self.feature_extractor.config.output_hidden_states = True",Load and initialize the BiCodec model and Wav2Vec2 feature extractor.,???Initialize audio processing model and feature extractor from pre-trained checkpoints.???
2922,reconstruct,"def reconstruct(self) -> bytes:
        
        headers = ""\r\n"".join(self.headers)
        request_line = f""{self.method} /{self.path} {self.version}\r\n""
        header_block = f""{request_line}{headers}\r\n\r\n""

        # Convert header block to bytes and combine with body
        result = header_block.encode(""utf-8"")
        if self.body:
            result += self.body

        return result",Reconstruct HTTP request from stored details,"???Constructs and encodes an HTTP request from method, path, headers, and body.???"
2923,_validate_action,"def _validate_action(action: str) -> None:
    
    if action not in ACTIONS:
        valid_actions = "", "".join(ACTIONS.keys())
        raise ValueError(f""Invalid action '{action}'. Valid actions: {valid_actions}"")",Validate that the action is supported.,"???This function checks if a given action is valid, raising an error if not.???"
2924,log_tool_usage_sync,"def log_tool_usage_sync(func):
    
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        tool_name = func.__name__
        logging.info(f""🔧 TOOL TRIGGERED: {tool_name}"")
        try:
            # Sanitize arguments to avoid logging sensitive info
            safe_args = sanitize_args(args)
            safe_kwargs = {k: sanitize_value(v) for k, v in kwargs.items()}
            logging.info(f""🔍 TOOL ARGS: {tool_name} called with {len(safe_kwargs)} parameters"")
            
            result = func(*args, **kwargs)
            
            # Log completion but not the actual result content (might be large or sensitive)
            logging.info(f""✅ TOOL COMPLETED: {tool_name}"")
            return result
        except Exception as e:
            logging.error(f""❌ TOOL ERROR: {tool_name} - {str(e)}"")
            raise
    return wrapper",Decorator to log when a synchronous tool is being used.,"???Decorator logs tool usage, sanitizes inputs, and handles exceptions for functions.???"
2925,get_random_headers,"def get_random_headers():
    
    return {
        ""accept"": ""application/json, text/plain, */*"",
        ""accept-language"": ""en-US,en;q=0.9"",
        ""content-type"": ""application/json"",
        ""sec-ch-ua"": '""Chromium"";v=""122"", ""Not(A:Brand"";v=""24"", ""Google Chrome"";v=""122""',
        ""sec-ch-ua-mobile"": ""?0"",
        ""sec-ch-ua-platform"": '""Windows""',
        ""sec-fetch-dest"": ""empty"",
        ""sec-fetch-mode"": ""cors"",
        ""sec-fetch-site"": ""same-site"",
        ""user-agent"": random.choice(USER_AGENTS),
        ""x-client-platform"": ""web"",
    }",Generate random headers for requests,??? Generate a dictionary of HTTP headers with randomized user-agent. ???
2926,exec,"def exec(self, inputs):
        
        print(""🔎 Searching for relevant documents..."")
        query_embedding, index, texts = inputs
        
        # Search for the most similar document
        distances, indices = index.search(query_embedding, k=1)
        
        # Get the index of the most similar document
        best_idx = indices[0][0]
        distance = distances[0][0]
        
        # Get the corresponding text
        most_relevant_text = texts[best_idx]
        
        return {
            ""text"": most_relevant_text,
            ""index"": best_idx,
            ""distance"": distance
        }",Search the index for similar documents,???Retrieve the most relevant document based on query similarity.???
2927,run_python_node_with_random_import,"def run_python_node_with_random_import():
    
    python_code_with_random = 

    python_node = Python(code=python_code_with_random, model_config=ConfigDict())
    input_data = {""min"": 1, ""max"": 10}
    config = None
    result = python_node.run(input_data, config)
    print(""\nResult with random import:"")
    print(result)",Basic example of running Python node with random import.,??? Execute Python code with random import and print results. ???
2928,mock_env,"def mock_env():
    
    original_env = dict(os.environ)
    test_env = {'AWS_PROFILE': 'test-profile', 'AWS_REGION': 'us-west-2'}
    os.environ.update(test_env)
    yield test_env
    os.environ.clear()
    os.environ.update(original_env)",Create a mock environment with test AWS credentials.,???Temporarily set and restore environment variables for testing.???
2929,store_embedding,"def store_embedding(self, message_data: MessageData) -> None:
        
        try:
            with self.conn.cursor() as cur:
                cur.execute(
                    f,
                    (
                        message_data.message,
                        message_data.embedding,
                        message_data.timestamp,
                        message_data.message_type,
                        message_data.chat_id,
                        message_data.source_interface,
                        message_data.original_query,
                        message_data.original_embedding,
                        message_data.response_type,
                        message_data.key_topics,
                        message_data.tool_call,
                    ),
                )
            self.conn.commit()
            logger.info(""Successfully stored message with metadata in database"")
        except Exception as e:
            logger.error(f""Failed to store message: {str(e)}"")
            raise",Store a message and its embedding in PostgreSQL,???Store message metadata and embedding in database with error handling???
2930,security_issue_tool_spec,"def security_issue_tool_spec(self):
        
        class ToolSpec:
            def __init__(self):
                self.name = ""security_issue_tool""
                self.description = ""This tool will have security issues in its implementation""
                self.input_params = [
                    {""name"": ""command"", ""type"": ""string"", ""description"": ""Command to execute"", ""required"": True}
                ]
                self.output_format = ""Command output""
                self.constraints = [""Be secure"", ""Validate inputs""]
        
        return ToolSpec()",Create a specification for a tool that will trigger security issues.,???Define a tool specification class for a security-focused command execution tool.???
2931,get_optimal_training_config,"def get_optimal_training_config(self) -> Dict[str, Any]:
        
        # Default configs that rely on PyTorch's automatic memory management
        config = {
            ""device_map"": ""auto"",
            ""fp16"": False,
            ""bf16"": False,
            ""gradient_checkpointing"": True,
            ""gradient_accumulation_steps"": 1,
        }
        
        # Enable mixed precision based on hardware support
        if self.cuda_available:
            capability = torch.cuda.get_device_capability()
            if capability[0] >= 8:  # Ampere or newer (supports BF16)
                config[""bf16""] = True
            elif capability[0] >= 7:  # Volta or newer (supports FP16)
                config[""fp16""] = True
                
            # Adjust accumulation steps based on available memory
            vram_gb = self.get_memory_info().get(""vram_total_gb"", 0)
            if vram_gb < 8:  # Small GPUs
                config[""gradient_accumulation_steps""] = 4
            elif vram_gb < 16:  # Medium GPUs
                config[""gradient_accumulation_steps""] = 2
        
        return config",Get recommended configurations for model training based on hardware capabilities.,???Determine optimal training configuration based on GPU capabilities and memory.???
2932,replace_macrons_with_latex_overline,"def replace_macrons_with_latex_overline(text: str) -> str:
    
    result = []
    for char in text:
        if char.isalpha():
            decomposed = unicodedata.normalize(""NFD"", char)
            if len(decomposed) > 1 and decomposed[1] == ""\u0304"":  # Macron accent
                result.append(f""\\overline{{{decomposed[0]}}}"")
            else:
                result.append(char)
        elif char != ""\u0304"":
            result.append(char)
        else:
            result[-1] = f""\\overline{{{result[-1]}}}""

    return """".join(result)",Replace letters with macrons with the LaTeX bar.,???Convert macron-accented characters in text to LaTeX overline format.???
2933,parse_directory,"def parse_directory(self, directory: Path) -> list[ParsedComponent]:
        
        components = []

        for file_path in directory.glob(""**/*.py""):
            # Skip __pycache__ and other hidden directories
            if ""__pycache__"" in file_path.parts or any(
                part.startswith(""."") for part in file_path.parts
            ):
                continue

            try:
                file_components = self.parse_file(file_path)
                components.extend(file_components)
            except Exception as e:
                relative_path = file_path.relative_to(self.project_root)
                console.print(
                    f""[bold red]Error parsing {relative_path}:[/bold red] {e}""
                )

        return components",Parse all Python files in a directory recursively.,"???Recursively parse Python files in a directory, collecting components.???"
2934,get_dom_counters_for_leak_detection,"def get_dom_counters_for_leak_detection() -> typing.Generator[
    T_JSON_DICT, T_JSON_DICT, typing.List[DOMCounter]
]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Memory.getDOMCountersForLeakDetection"",
    }
    json = yield cmd_dict
    return [DOMCounter.from_json(i) for i in json[""counters""]]",Retruns DOM object counters after preparing renderer for leak detection.,???Generate DOM counters for memory leak detection in a structured format.???
2935,fix_color_format,"def fix_color_format(color: str) -> str:
    
    if not color:
        return """"
    
    color = color.strip().upper()
    if not color.startswith('#'):
        color = f""#{color}""
        
    color = color[1:]
    if len(color) == 3:
        r, g, b = color[0], color[1], color[2]
        return f""#{r}{r}{g}{g}{b}{b}""
    elif len(color) < 6:
        raise ValueError(f""Invalid color format: {color}"")
    elif len(color) > 6:
        color = color[:6]
        
    return f""#{color}""",Fix color format to valid hex code,"???Normalize and validate hex color codes, expanding shorthand and ensuring correct length.???"
2936,set_ignore_certificate_errors,"def set_ignore_certificate_errors(
    ignore: bool,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""ignore""] = ignore
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Security.setIgnoreCertificateErrors"",
        ""params"": params,
    }
    json = yield cmd_dict",Enable/disable whether all certificate errors should be ignored.,???Configure security to bypass certificate errors via a command generator.???
2937,get_mutation_prompt,"def get_mutation_prompt(self, task_description: str, order: Literal[""zero-order"", ""first-order""], **kwargs) -> str:
        
        if order == ""zero-order"":
            mutation_prompt = self.generate_mutation_prompt(task_description=task_description)
        elif order == ""first-order"":
            mutation_prompt = random.choice(mutation_prompts)
        else:
            raise ValueError(f""Invalid order: {order}. The order should be either 'zero-order' or 'first-order'."")
        return mutation_prompt",Get the mutation prompt for optimization.,???Determine mutation prompt based on task description and specified order.???
2938,add_commands,"def add_commands(
    subparsers: argparse._SubParsersAction, command_names: typing.List[str] = None
):
    
    all_cmds = Command.list_available()
    if command_names is None:
        logger.warn(""invalid command_names, will add all available commands."")
        command_names = all_cmds
    for cmd in command_names:
        if cmd not in all_cmds:
            raise ValueError(f""command {cmd} not in available commands {all_cmds}"")
        # Command Subclasses doesn't accept init args, so just pass subclass name is OK.
        cls = Command.from_config(cmd)
        cls.add_to_parser(subparsers)",add commands to subparsers,???Add specified or all available commands to a parser using configuration.???
2939,mock_comment_response,"def mock_comment_response():
        
        return MagicMock(
            json=lambda: [
                {
                    ""body"": ""This is a comment."",
                }
            ]
        )",Create a mock response for GitHub issue comments.,???Simulate a mock API response returning a list with a single comment.???
2940,sample_server_config,"def sample_server_config(self):
        
        return STDIOServerConfig(
            name=""sample-server"",
            command=""npx"",
            args=[""-y"", ""@modelcontextprotocol/sample-server""],
            env={""API_KEY"": ""sample-key""},
        )",Create a sample ServerConfig for testing,"???Creates a server configuration with predefined command, arguments, and environment variables.???"
2941,get_inference,"def get_inference(self, topic_id: int) -> Dict[str, Any]:
        
        try:
            response = self._make_request('get_inference_by_topic_id', topic_id)
            return {
                ""topic_id"": topic_id,
                ""inference"": response.inference_data.network_inference_normalized
            }
        except Exception as e:
            raise AlloraAPIError(f""Failed to get inference: {str(e)}"")",Get inference from Allora Network for a specific topic,???Retrieve and return normalized inference data for a given topic ID.???
2942,get_available_actions,"def get_available_actions(self):
        
        html_obj = self._parse_html()

        # Collect search bar, buttons, links, and options as clickables
        search_bar = html_obj.find(id='search_input')
        has_search_bar = True if search_bar is not None else False
        buttons = html_obj.find_all(class_='btn')
        product_links  = html_obj.find_all(class_='product-link')
        buying_options = html_obj.select('input[type=""radio""]')

        self.text_to_clickable = {
            f'{b.get_text()}'.lower(): b
            for b in buttons + product_links
        }
        for opt in buying_options:
            opt_value = opt.get('value')
            self.text_to_clickable[f'{opt_value}'] = opt
        return dict(
            has_search_bar=has_search_bar,
            clickables=list(self.text_to_clickable.keys()),
        )",Returns list of available actions at the current step,???Extracts interactive elements from HTML for user actions mapping.???
2943,_parse_range,"def _parse_range(self, range_str: str) -> Tuple[int, int]:
        
        try:
            if "","" in range_str:
                start, count = range_str.split("","")
                return int(start), int(count)
            return int(range_str), 1
        except ValueError:
            raise ValueError(f""Invalid range format: {range_str}"")","Parse a range string (e.g., 'start,length') into start and count.","???Parse a string to extract numeric range values, handling errors.???"
2944,convert_lora_from_diffusion_pipe_or_something,"def convert_lora_from_diffusion_pipe_or_something(lora_sd: dict[str, torch.Tensor], prefix: str) -> dict[str, torch.Tensor]:
    
    # convert from diffusers(?) to default LoRA
    # Diffusers format: {""diffusion_model.module.name.lora_A.weight"": weight, ""diffusion_model.module.name.lora_B.weight"": weight, ...}
    # default LoRA format: {""prefix_module_name.lora_down.weight"": weight, ""prefix_module_name.lora_up.weight"": weight, ...}

    # note: Diffusers has no alpha, so alpha is set to rank
    new_weights_sd = {}
    lora_dims = {}
    for key, weight in lora_sd.items():
        diffusers_prefix, key_body = key.split(""."", 1)
        if diffusers_prefix != ""diffusion_model"" and diffusers_prefix != ""transformer"":
            print(f""unexpected key: {key} in diffusers format"")
            continue

        new_key = f""{prefix}{key_body}"".replace(""."", ""_"").replace(""_lora_A_"", "".lora_down."").replace(""_lora_B_"", "".lora_up."")
        new_weights_sd[new_key] = weight

        lora_name = new_key.split(""."")[0]  # before first dot
        if lora_name not in lora_dims and ""lora_down"" in new_key:
            lora_dims[lora_name] = weight.shape[0]

    # add alpha with rank
    for lora_name, dim in lora_dims.items():
        new_weights_sd[f""{lora_name}.alpha""] = torch.tensor(dim)

    return new_weights_sd",Convert LoRA weights to the format used by the diffusion pipeline to Musubi Tuner.,???Convert diffusion model weights to default LoRA format with alpha adjustment.???
2945,ask_for_confirmation,"def ask_for_confirmation(message: str) -> bool:
    
    y = (""y"", ""yes"", ""1"")
    n = (""n"", ""no"", ""0"", """")
    full_message = f""{message} (y/n) ""
    while True:
        answer = input(full_message).lower()
        if answer in y:
            return True
        if answer in n:
            return False
        print(f""Invalid input. Must be one of: yes/no/y/n or empty for no"")",Ask user for confirmation with Y/N prompt.,"???Function prompts user for yes/no confirmation, returning True or False.???"
2946,insert_after_symbol,"def insert_after_symbol(
        self,
        name_path: str,
        relative_file_path: str,
        body: str,
        *,
        use_same_indentation: bool = True,
        at_new_line: bool = True,
    ) -> None:
        
        symbol_candidates = self.find_by_name(name_path, within_relative_path=relative_file_path)
        if len(symbol_candidates) == 0:
            raise ValueError(f""No symbol with name {name_path} found in file {relative_file_path}"")
        if len(symbol_candidates) > 1:
            raise ValueError(
                f""Found multiple {len(symbol_candidates)} symbols with name {name_path} in file {relative_file_path}. ""
                f""May be an overwritten variable, in which case you can ignore this error. Proceeding with the last one. ""
                f""Found symbols at locations: \n"" + json.dumps([s.location.to_dict() for s in symbol_candidates], indent=2)
            )
        symbol = symbol_candidates[-1]
        return self.insert_after_symbol_at_location(
            symbol.location, body, at_new_line=at_new_line, use_same_indentation=use_same_indentation
        )",Inserts content after the symbol with the given name in the given file.,"???Insert code snippet after specified symbol in a file, handling multiple matches???"
2947,_handle_execution_result,"def _handle_execution_result(self, container, status_code: int) -> CodeExecutionOutput:
        
        stdout = self._get_container_file(container, f""{self.WORKSPACE_DIR}/output.txt"")
        stderr = self._get_container_file(container, f""{self.WORKSPACE_DIR}/error.txt"")
        files = self._get_created_files(container, self.WORKSPACE_DIR)

        if status_code == 0:
            return CodeExecutionOutput(
                message=""success"",
                stdout=stdout,
                stderr=stderr,
                files=files,
            )

        # Create a more descriptive error message
        error_message = f""Program exited with status code {status_code}""
        if stderr:
            error_message = f""{error_message}\n\nError details:\n{stderr}""

        return CodeExecutionOutput(
            message=""error"",
            error=error_message,
            stdout=stdout,
            stderr=stderr,
            files=files,
        )",Handle the execution result and create appropriate output.,"???Process execution results, returning success or detailed error output.???"
2948,perform_action,"def perform_action(self, action_name: str, kwargs) -> Any:
        
        if action_name not in self.actions:
            raise KeyError(f""Unknown action: {action_name}"")

        # Explicitly reload environment variables
        load_dotenv()
        
        if not self.is_configured(verbose=True):
            raise GroqConfigurationError(""Groq is not properly configured"")

        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f""Invalid parameters: {', '.join(errors)}"")

        # Call the appropriate method based on action name
        method_name = action_name.replace('-', '_')
        method = getattr(self, method_name)
        return method(**kwargs)",Execute a Groq action with validation,???Execute validated action by invoking corresponding method with parameters.???
2949,_preferred_temporality,"def _preferred_temporality(self) -> Dict:
        
        return {
            ""counter"": AggregationTemporality.CUMULATIVE,
            ""up_down_counter"": AggregationTemporality.CUMULATIVE,
            ""observable_counter"": AggregationTemporality.CUMULATIVE,
            ""observable_up_down_counter"": AggregationTemporality.CUMULATIVE,
            ""histogram"": AggregationTemporality.CUMULATIVE,
            ""observable_gauge"": AggregationTemporality.CUMULATIVE,
        }",Returns the preferred temporality for each instrument kind.,"???  
Define default cumulative aggregation temporality for various metric types.  
???"
2950,insert_data,"def insert_data(self, documents: List[dict]):
        
        from azure.core.credentials import AzureKeyCredential
        from azure.search.documents import SearchClient

        search_client = SearchClient(
            endpoint=self.endpoint,
            index_name=self.index_name,
            credential=AzureKeyCredential(self.api_key),
        )

        actions = [
            {
                ""@search.action"": ""upload"" if doc.get(""id"") else ""merge"",
                ""id"": doc.get(""id"", str(uuid.uuid4())),
                ""content"": doc[""text""],
                ""content_vector"": doc[""vector""],
            }
            for doc in documents
        ]

        result = search_client.upload_documents(actions)
        return [x.succeeded for x in result]",Batch insert documents with vector embeddings,???Upload or merge documents into Azure Search index using unique identifiers.???
2951,read_frames,"def read_frames(self, mp4_path: str) -> None:
        
        if not os.path.exists(mp4_path):
            print(f""{mp4_path} doesn't exist."")
            return None
        else:
            # decode the video
            frames = decode_video(mp4_path)
            print(f""There are {len(frames)} frames decoded from {mp4_path} (24fps)."")

            # downsample the frames to align with the annotations
            frames = frames[:: self.annot_sample_rate]
            print(
                f""Videos are annotated every {self.annot_sample_rate} frames. ""
                ""To align with the annotations, ""
                f""downsample the video to {len(frames)} frames.""
            )
            return frames",Read the frames and downsample them to align with the annotations.,"???Check video file existence, decode frames, and downsample to match annotations.???"
2952,_ensure_id_cache,"def _ensure_id_cache(self, ts: TensorState):
        
        if hasattr(self, ""_robot_joint_ids""):
            return

        mjm = self._mj_model
        mjx_m = self._mjx_model

        # ----- robots ----------------------------------------------------
        self._robot_joint_ids, self._robot_act_ids = {}, {}
        for rname in ts.robots:
            full_jnames = [f""{rname}/{jn}"" for jn in self._get_jnames(rname, sort=True)]
            j_ids = [mujoco.mj_name2id(mjm, mujoco.mjtObj.mjOBJ_JOINT, n) for n in full_jnames]
            a_ids = sorted_actuator_ids(mjx_m, f""{rname}/"")
            self._robot_joint_ids[rname] = jnp.asarray(j_ids, dtype=jnp.int32)
            self._robot_act_ids[rname] = jnp.asarray(a_ids, dtype=jnp.int32)

        # ----- objects ---------------------------------------------------
        self._object_joint_ids, self._object_act_ids = {}, {}
        for oname in ts.objects:
            full_jnames = [f""{oname}/{jn}"" for jn in self._get_jnames(oname, sort=True)]
            j_ids = [mujoco.mj_name2id(mjm, mujoco.mjtObj.mjOBJ_JOINT, n) for n in full_jnames]
            a_ids = sorted_actuator_ids(mjx_m, f""{oname}/"")
            self._object_joint_ids[oname] = jnp.asarray(j_ids, dtype=jnp.int32)
            self._object_act_ids[oname] = jnp.asarray(a_ids, dtype=jnp.int32)",Build joint-/actuator-ID lookup tables (one-time per handler).,???Initialize and cache joint and actuator IDs for robots and objects if not already cached.???
2953,_remove_none_values,"def _remove_none_values(d: dict) -> dict:
    
    return {k: v for k, v in d.items() if v is not None}",Remove all None values from a dictionary.,"???  
Filter out dictionary entries with null values.  
???"
2954,inject_gallery_html,"def inject_gallery_html(notebooks_md_path: Path, metadata_yml_path: Path) -> None:
    
    with open(notebooks_md_path, encoding=""utf-8"") as f:
        content = f.read()

    gallery_html = render_gallery_html(metadata_yml_path)

    updated_content = content.replace(""{{ render_gallery(gallery_items) }}"", gallery_html)
    with open(notebooks_md_path, ""w"", encoding=""utf-8"") as f:
        f.write(updated_content)",Generate the index.html file for the notebooks section.,???Replace placeholder in markdown with rendered HTML from metadata.???
2955,validate_path,"def validate_path(self, command: str, path: Path):
        
        # Check if its an absolute path
        if not path.is_absolute():
            suggested_path = Path("""") / path
            raise ToolError(
                f""The path {path} is not an absolute path, it should start with `/`. Maybe you meant {suggested_path}?""
            )
        # Check if path exists
        if not path.exists() and command != ""create"":
            raise ToolError(
                f""The path {path} does not exist. Please provide a valid path.""
            )
        if path.exists() and command == ""create"":
            raise ToolError(
                f""File already exists at: {path}. Cannot overwrite files using command `create`.""
            )
        # Check if the path points to a directory
        if path.is_dir():
            if command != ""view"":
                raise ToolError(
                    f""The path {path} is a directory and only the `view` command can be used on directories""
                )",Check that the path/command combination is valid.,"???Validate file path based on command, ensuring existence and correct usage.???"
2956,parse_relation,"def parse_relation(token: Token) -> Dict[str, Any] | None:
    
    # Remove bullet point if present
    content = token.content.strip()

    # Extract [[target]]
    target = None
    rel_type = ""relates_to""  # default
    context = None

    start = content.find(""[["")
    end = content.find(""]]"")

    if start != -1 and end != -1:
        # Get text before link as relation type
        before = content[:start].strip()
        if before:
            rel_type = before

        # Get target
        target = content[start + 2 : end].strip()

        # Look for context after
        after = content[end + 2 :].strip()
        if after.startswith(""("") and after.endswith("")""):
            context = after[1:-1].strip() or None

    if not target:  # pragma: no cover
        return None

    return {""type"": rel_type, ""target"": target, ""context"": context}",Extract relation parts from token.,"???Parse structured text to extract relationship type, target, and context.???"
2957,_validate_interval,"def _validate_interval(self, interval: str, start_date: datetime) -> str:
        
        if interval not in self.INTERVAL_LIMITS:
            logger.warning(f""Invalid interval: {interval}. Using default: 4h"")
            return ""4h""

        limit = self.INTERVAL_LIMITS[interval]
        if limit != 'max':
            limit_days = int(''.join(filter(str.isdigit, limit)))
            if 'y' in limit:
                limit_days *= 365
            
            date_diff = (datetime.now() - start_date).days
            if date_diff > limit_days:
                logger.warning(f""Interval {interval} only supports {limit} of historical data. Adjusting interval..."")
                return self._get_appropriate_interval(date_diff)
        
        return interval",Validate and adjust the interval based on date range.,???Validate and adjust time interval based on historical data limits???
2958,generate_to_json,"def generate_to_json(self, dict_: str, use_self: bool = True) -> str:
        
        self_ref = ""self."" if use_self else """"
        assign = f""{dict_}['{self.name}'] = ""
        if self.items:
            if self.items.ref:
                assign += f""[i.to_json() for i in {self_ref}{self.py_name}]""
            else:
                assign += f""[i for i in {self_ref}{self.py_name}]""
        else:
            if self.ref:
                assign += f""{self_ref}{self.py_name}.to_json()""
            else:
                assign += f""{self_ref}{self.py_name}""
        if self.optional:
            code = dedent(
                f
            )
        else:
            code = assign
        return code",Generate the code that exports this property to the specified JSON dict.,???Convert object attributes to JSON format with optional self-reference.???
2959,add_project,"def add_project(self, name: str, path: str) -> ProjectConfig:
        
        if name in self.config.projects:  # pragma: no cover
            raise ValueError(f""Project '{name}' already exists"")

        # Ensure the path exists
        project_path = Path(path)
        project_path.mkdir(parents=True, exist_ok=True)  # pragma: no cover

        self.config.projects[name] = str(project_path)
        self.save_config(self.config)
        return ProjectConfig(name=name, home=project_path)",Add a new project to the configuration.,"???Add a new project to configuration, ensuring unique name and valid path.???"
2960,_adjust_head_parameters,"def _adjust_head_parameters(self, cur_state_dict, pretrain_state_dict):
        
        # List of parameters to adjust
        if (
            pretrain_state_dict[""decoder.denoising_class_embed.weight""].size()
            != cur_state_dict[""decoder.denoising_class_embed.weight""].size()
        ):
            del pretrain_state_dict[""decoder.denoising_class_embed.weight""]

        head_param_names = [""decoder.enc_score_head.weight"", ""decoder.enc_score_head.bias""]
        for i in range(8):
            head_param_names.append(f""decoder.dec_score_head.{i}.weight"")
            head_param_names.append(f""decoder.dec_score_head.{i}.bias"")

        adjusted_params = []

        for param_name in head_param_names:
            if param_name in cur_state_dict and param_name in pretrain_state_dict:
                cur_tensor = cur_state_dict[param_name]
                pretrain_tensor = pretrain_state_dict[param_name]
                adjusted_tensor = self.map_class_weights(cur_tensor, pretrain_tensor)
                if adjusted_tensor is not None:
                    pretrain_state_dict[param_name] = adjusted_tensor
                    adjusted_params.append(param_name)
                else:
                    print(f""Cannot adjust parameter '{param_name}' due to size mismatch."")

        return pretrain_state_dict",Adjust head parameters between datasets.,??? Adjusts model parameters by mapping current weights to pretrained weights if compatible. ???
2961,init_communicator,"def init_communicator(self):
        
        # Get the tensor parallel size from the server
        response = requests.get(url)
        if response.status_code == 200:
            tensor_parallel_size = response.json()[""tensor_parallel_size""]
        else:
            raise Exception(f""Request failed: {response.status_code}, {response.text}"")

        world_size = tensor_parallel_size + 1
        self.rank = tensor_parallel_size  # The client's rank is the last process

        # Initialize weight update group
        # In the server side, the host is set to 0.0.0.0
        response = self.session.post(url, json={""host"": ""0.0.0.0"", ""port"": self.group_port, ""world_size"": world_size})
        if response.status_code != 200:
            raise Exception(f""Request failed: {response.status_code}, {response.text}"")

        # Set up the communication group for weight broadcasting
        pg = StatelessProcessGroup.create(host=self.host, port=self.group_port, rank=self.rank, world_size=world_size)
        self.pynccl_comm = PyNcclCommunicator(pg, device=""cuda:0"")",Initializes the weight update group in a distributed setup for model synchronization.,???Initialize distributed communication for tensor parallel processing.???
2962,_load_trajectory,"def _load_trajectory(self, filename: str) -> dict:
        
        test_dir = os.path.dirname(os.path.abspath(__file__))
        traj_path = os.path.join(test_dir, filename)
        
        try:
            with open(traj_path, 'r') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            print_warning(f""Failed to load trajectory file {filename}: {str(e)}"")
            return {""messages"": []}",Load trajectory from JSON file,"???Load trajectory data from a JSON file, handling errors gracefully.???"
2963,_build_request_body,"def _build_request_body(self, input_data: JinaScrapeInputSchema) -> dict[str, Any]:
        
        url = input_data.url or self.url
        if not url:
            raise ToolExecutionException(
                ""No URL provided. Please provide a URL either during node initialization or execution."",
                recoverable=True,
            )

        return {""url"": url}",Build request body according to Jina Reader API specification.,???Constructs a request payload with a validated URL from input data.???
2964,verify_vertex_connection,"def verify_vertex_connection(
    project_id: str,
    location: str = ""us-central1"",
) -> None:
    
    credentials, _ = google.auth.default()
    client = PredictionServiceClient(
        credentials=credentials,
        client_options=ClientOptions(
            api_endpoint=f""{location}-aiplatform.googleapis.com""
        ),
        client_info=get_client_info(),
        transport=initializer.global_config._api_transport,
    )
    request = get_dummy_request(project_id=project_id)
    client.count_tokens(request=request)",Verifies Vertex AI connection with a test Gemini request.,???Establishes a connection to Google AI Platform to verify token count for a project.???
2965,generate_document_embeddings,"def generate_document_embeddings(self) -> bool:
        
        try:
            # Mark step as in progress
            self.progress.mark_step_status(ProcessStep.GENERATE_DOCUMENT_EMBEDDINGS, Status.IN_PROGRESS)
            documents = self.list_documents() 
            for doc in documents:
                doc_id = doc.get(""id"")

                # Directly call document service instead of API
                embedding = document_service.process_document_embedding(doc_id)
                if embedding is None:
                    logger.error(
                        f""Generate document embeddings failed for doc_id: {doc_id}""
                    )
                    self.progress.mark_step_status(ProcessStep.GENERATE_DOCUMENT_EMBEDDINGS, Status.FAILED)
                    return False
                self.progress.mark_step_status(ProcessStep.GENERATE_DOCUMENT_EMBEDDINGS, Status.COMPLETED)
                logger.info(f""Successfully generated embedding for document {doc_id}"") 
            return True
        except Exception as e:
            logger.error(f""Generate document embeddings failed: {str(e)}"")
            self.progress.mark_step_status(ProcessStep.GENERATE_DOCUMENT_EMBEDDINGS, Status.FAILED)
            return False",Process embeddings for all documents,"???Generate embeddings for documents, updating progress and handling errors.???"
2966,write,"def write(self, prefix: str, message: str) -> None:
        
        # Ensure spinner is cleared before printing new lines
        print(""\r"" + "" "" * 80 + ""\r"", end="""") # Clear line heuristically
        print(f""\n{prefix}{message}"")",Print a message with a prefix.,???Clear console line and print formatted message with prefix.???
2967,_load_config,"def _load_config(self) -> None:
        
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, ""r"", encoding=""utf-8"") as f:
                    self._config = json.load(f)
            except json.JSONDecodeError:
                logger.error(f""Error parsing config file: {self.config_path}"")
                self._config = self._default_config()
        else:
            self._config = self._default_config()
            self._save_config()",Load configuration from file or create default,???Load configuration from file or initialize default settings if file is missing or invalid.???
2968,merge_lora,"def merge_lora(self):
        
        print(f""Loading LoRA checkpoint from {self.lora_name_or_path}..."")
        # Load the LoRA config and model
        config = PeftConfig.from_pretrained(self.lora_name_or_path)
        self.model = PeftModel.from_pretrained(self.model, self.lora_name_or_path)
        
        self.model = self.model.merge_and_unload()",Merge the LoRA model with the base model.,???Load and merge a LoRA model checkpoint into the existing model.???
2969,get_similar_strategies,"def get_similar_strategies(self, query: str, n: int = 5) -> List[Tuple[Strategy, float]]:
        
        if not self.strategies:
            return []
        
        # Extract strategy texts and update vectorizer
        strategy_texts = [s.strategy_text for s in self.strategies]
        if self.vectors is None or len(self.vectors.shape) == 0 or self.vectors.shape[0] != len(strategy_texts):
            try:
                self.vectors = self.vectorizer.fit_transform(strategy_texts)
            except Exception as e:
                logger.error(f""Error creating strategy vectors: {str(e)}"")
                return []
        
        # Convert query to vector and find similarities
        try:
            query_vector = self.vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.vectors).flatten()
            
            # Get top strategies with their similarity scores
            sorted_indices = similarities.argsort()[::-1]
            return [(self.strategies[i], float(similarities[i])) for i in sorted_indices[:n]]
        except Exception as e:
            logger.error(f""Error finding similar strategies: {str(e)}"")
            return []",Find strategies similar to a query using TF-IDF similarity.,???Retrieve top matching strategies based on text similarity to a given query.???
2970,compute_concurrency_distribution,"def compute_concurrency_distribution(roots: list[ConcurrencyCallNode]) -> dict[int, float]:
    
    all_nodes = flatten_calls(roots)
    if not all_nodes:
        return {}

    events = []
    for n in all_nodes:
        if n.start_time <= n.end_time:
            events.append((n.start_time, +1))
            events.append((n.end_time, -1))

    events.sort(key=lambda x: x[0])
    dist_map: dict[int, float] = {}
    curr_conc = 0
    prev_time = events[0][0]

    for (time_val, delta) in events:
        if time_val > prev_time:
            length = time_val - prev_time
            dist_map[curr_conc] = dist_map.get(curr_conc, 0.0) + length
        curr_conc += delta
        prev_time = time_val

    return dist_map","Flatten calls, produce (start, +1)/(end, -1), accumulate total time at each concurrency level.",???Calculate time distribution of concurrent events from call nodes.???
2971,remove_tool_for_llm,"def remove_tool_for_llm(self, tool: Tool) -> None:
        
        try:
            self._register_for_llm(tool=tool, api_style=""tool"", is_remove=True)
            self._tools.remove(tool)
        except ValueError:
            raise ValueError(f""Tool {tool} not found in collection"")",Remove a tool (register for LLM tool),"???Remove a tool from the collection, handling errors if not found.???"
2972,fetch,"def fetch(datadir: Path, max_file_size: Optional[int] = None) -> list[PDB]:
    
    data = []
    excluded = 0
    for file in datadir.rglob(""*.cif*""):
        # The clustering file is annotated by pdb_entity id
        pdb_id = str(file.stem).lower()

        # Check file size and skip if too large
        if max_file_size is not None and (file.stat().st_size > max_file_size):
            excluded += 1
            continue

        # Create the target
        target = PDB(id=pdb_id, path=str(file))
        data.append(target)

    print(f""Excluded {excluded} files due to size."")  # noqa: T201
    return data",Fetch the PDB files.,"??? 
Retrieve and filter PDB files by size from a directory, returning valid entries. 
???"
2973,remove,"def remove(package: str):
    
    # If `package` has been provided with a version, it will be stripped.
    requirement = Requirement(package)

    # TODO it may be worth considering removing unused sub-dependencies as well
    def on_progress(line: str):
        if RE_UV_PROGRESS.match(line):
            log.info(line.strip())

    def on_error(line: str):
        log.error(f""uv: [error]\n {line.strip()}"")

    log.info(f""Uninstalling {requirement.name}"")
    _wrap_command_with_callbacks(
        [get_uv_bin(), 'remove', '--python', _python_executable, requirement.name],
        on_progress=on_progress,
        on_error=on_error,
    )",Uninstall a package with `uv`.,???Uninstall a specified package while logging progress and errors.???
2974,get_document_chunks,"def get_document_chunks(document_id: int):
    
    try:
        logger.info(f""Attempting to retrieve chunks for document_id: {document_id}"")

        chunks = document_service.get_document_chunks(document_id)

        if not chunks:
            logger.warning(f""No chunks found for document_id: {document_id}"")
            return jsonify(
                APIResponse.error(message=f""No chunks found for document {document_id}"")
            )

        return jsonify(
            APIResponse.success(
                data={
                    ""document_id"": document_id,
                    ""total_chunks"": len(chunks),
                    ""chunks"": chunks,
                }
            )
        )

    except Exception as e:
        logger.error(
            f""Error getting document chunks for document_id {document_id}: {str(e)}"",
            exc_info=True,
        )
        return jsonify(
            APIResponse.error(
                message=f""Error getting document chunks for document_id {document_id}: {str(e)}""
            )
        )",Get chunks for the specified document,"???Retrieve and return document chunks, handling errors and logging outcomes.???"
2975,search_books,"def search_books(api_url: str, api_key: str, book_ids: List[int], api_timeout: int = 120) -> Optional[Dict]:
    
    endpoint = f""{api_url}/api/v1/command"" # This uses the full URL, not arr_request
    headers = {'X-Api-Key': api_key}
    payload = {
        'name': 'BookSearch',
        'bookIds': book_ids
    }
    try:
        # This uses requests.post directly, not arr_request. It's already correct.
        response = requests.post(endpoint, headers=headers, json=payload, timeout=api_timeout)
        response.raise_for_status()
        command_data = response.json()
        command_id = command_data.get('id')
        logger.info(f""Successfully triggered BookSearch command for book IDs: {book_ids}. Command ID: {command_id}"")
        return command_data # Return the full command object which includes the ID
    except requests.exceptions.RequestException as e:
        logger.error(f""Error triggering BookSearch command for book IDs {book_ids} via {endpoint}: {e}"")
        return None
    except Exception as e:
        logger.error(f""An unexpected error occurred triggering BookSearch for book IDs {book_ids}: {e}"")
        return None",Triggers a search for specific book IDs in Readarr.,???Send book search requests to an API and handle responses or errors.???
2976,calculate_bleu_scores,"def calculate_bleu_scores(prediction: str, reference: str) -> Dict[str, float]:
    
    pred_tokens = nltk.word_tokenize(prediction.lower())
    ref_tokens = [nltk.word_tokenize(reference.lower())]
    
    weights_list = [(1, 0, 0, 0), (0.5, 0.5, 0, 0), (0.33, 0.33, 0.33, 0), (0.25, 0.25, 0.25, 0.25)]
    smooth = SmoothingFunction().method1
    
    scores = {}
    for n, weights in enumerate(weights_list, start=1):
        try:
            score = sentence_bleu(ref_tokens, pred_tokens, weights=weights, smoothing_function=smooth)
        except Exception:
            print(f""Error calculating BLEU score: {e}"")
            score = 0.0
        scores[f'bleu{n}'] = score
    
    return scores",Calculate BLEU scores with different n-gram settings.,???Calculate BLEU scores for a prediction against a reference using various n-gram weights.???
2977,load_custom_model,"def load_custom_model(
    model_custom_src: str, context: dict, build_directory: str = None
) -> nn.Module:
    
    if build_directory:
        context[""BUILD_DIRECTORY""] = build_directory
        # Add import at the start of the source code
        model_custom_src = (
            ""import os\n"" f""os.environ['TORCH_EXTENSIONS_DIR'] = '{build_directory}'\n""
        ) + model_custom_src

    try:
        compile(model_custom_src, ""<string>"", ""exec"")
        exec(model_custom_src, context)
        # DANGER: need to delete refernece from global namespace
    except SyntaxError as e:
        print(f""Syntax Error in custom generated code or Compilation Error {e}"")
        return None

    ModelNew = context.get(""ModelNew"")
    return ModelNew",Load class from custom NN.module pytorch code this is the code output by LLM with calls to custom cuda kernels,???Load and execute custom model code within a specified context and directory???
2978,add_transcription,"def add_transcription(meeting_id, content, speaker=None, confidence=None):
        
        session = get_session()
        try:
            # Check if meeting exists
            meeting = session.query(Meeting).filter_by(id=meeting_id).first()
            if not meeting:
                logger.warning(f""Meeting not found: {meeting_id}, cannot add transcription"")
                return None
            
            # Create transcription
            transcription = Transcription(
                meeting_id=meeting_id,
                speaker=speaker,
                content=content,
                timestamp=datetime.utcnow(),
                confidence=confidence
            )
            session.add(transcription)
            session.commit()
            logger.info(f""Added transcription to meeting: {meeting_id}"")
            return transcription
        except SQLAlchemyError as e:
            session.rollback()
            logger.error(f""Error adding transcription: {e}"")
            raise
        finally:
            session.close()",Add a transcription entry to a meeting,"???Add transcription to a meeting record in the database, handling errors.???"
2979,_calculate_answer_consistency,"def _calculate_answer_consistency(self, answers: List[str]) -> float:
        
        if len(answers) < 2:
            return 0.5
        
        # Normalize answers for comparison
        normalized_answers = []
        for answer in answers:
            # Remove common variations and normalize
            norm_answer = re.sub(r'[^\w\s]', '', answer.lower().strip())
            norm_answer = re.sub(r'\s+', ' ', norm_answer)
            normalized_answers.append(norm_answer)
        
        # Count occurrences
        answer_counts = Counter(normalized_answers)
        most_common_count = answer_counts.most_common(1)[0][1]
        total_answers = len(answers)
        
        # Calculate agreement ratio
        agreement_ratio = most_common_count / total_answers
        
        logger.debug(f""Answer distribution: {dict(answer_counts)}"")
        logger.debug(f""Agreement ratio: {agreement_ratio:.3f} ({most_common_count}/{total_answers})"")
        
        # Also consider semantic similarity for non-identical answers
        max_similarity = 0.0
        for i, ans1 in enumerate(normalized_answers):
            for j, ans2 in enumerate(normalized_answers[i+1:], i+1):
                similarity = SequenceMatcher(None, ans1, ans2).ratio()
                max_similarity = max(max_similarity, similarity)
        
        # Combine exact matches and semantic similarity
        consistency = max(agreement_ratio, max_similarity)
        
        return min(consistency, 1.0)",Calculate consistency of final answers.,???Calculate consistency of answers by combining exact match ratio and semantic similarity.???
2980,get_random_color,"def get_random_color(existing_colors_hex: set) -> str:
    
    attempts = 0
    while attempts < 100:
        r = random.randint(0, 255)
        g = random.randint(0, 255)
        b = random.randint(0, 255)
        hex_color = f'#{r:02x}{g:02x}{b:02x}'
        if hex_color not in existing_colors_hex:
            return hex_color
        attempts += 1
    return f'#{random.randint(0,255):02x}{random.randint(0,255):02x}{random.randint(0,255):02x}'","Generates a random hex color string, trying to avoid existing ones.","???Generate a unique random hex color not in the given set, with retry logic.???"
2981,get_token_data_by_ticker,"def get_token_data_by_ticker(agent, **kwargs):
    
    agent.logger.info(""\n🔍 FETCHING TOKEN DATA BY TICKER"")
    try:
        result = agent.connection_manager.perform_action(
            connection_name=""solana"",
            action_name=""get-token-by-ticker"",
            params=[kwargs.get('ticker')]
        )
        agent.logger.info(""✅ Token data retrieved!"")
        return result
    except Exception as e:
        agent.logger.error(f""❌ Token data fetch failed: {str(e)}"")
        return None",Get token data by ticker,???Retrieve cryptocurrency token data using a specified ticker symbol.???
2982,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""CSS.disable"",
    }
    json = yield cmd_dict",Disables the CSS agent for the given page.,???Disables CSS tracking by sending a command to the server.???
2983,evaluate_llm_judge,"def evaluate_llm_judge(question, gold_answer, generated_answer):
    
    response = client.chat.completions.create(
        model=""gpt-4o-mini"",
        messages=[
            {
                ""role"": ""user"",
                ""content"": ACCURACY_PROMPT.format(
                    question=question,
                    gold_answer=gold_answer,
                    generated_answer=generated_answer,
                ),
            }
        ],
        response_format={""type"": ""json_object""},
        temperature=0.0,
    )
    label = json.loads(response.choices[0].message.content)[""label""]
    return 1 if label == ""CORRECT"" else 0",Evaluate the generated answer against the gold answer using an LLM judge.,???Evaluate AI-generated answers against a reference using a language model for accuracy assessment.???
2984,eternai_list_models,"def eternai_list_models(agent, **kwargs):
    
    agent.logger.info(""\n📋 LISTING AVAILABLE MODELS"")
    try:
        result = agent.connection_manager.perform_action(
            connection_name=""eternalai"",
            action_name=""list-models"",
            params=[]
        )
        agent.logger.info(""✅ Models listed successfully!"")
        return result
    except Exception as e:
        agent.logger.error(f""❌ Model listing failed: {str(e)}"")
        return None",List all available EternalAI models,???Function lists available AI models via a connection manager.???
2985,create_configuration_override,"def create_configuration_override(mcp: FastMCP, mq_client_getter: BOTO3_CLIENT_GETTER, _: str):
    

    @mcp.tool()
    def create_configuration(
        region: str, authentication_strategy: str, engine_type: str, engine_version: str, name: str
    ):
        
        create_params = {
            'AuthenticationStrategy': authentication_strategy,
            'EngineType': engine_type,
            'EngineVersion': engine_version,
            'Name': name,
            'Tags': {
                'mcp_server_version': MCP_SERVER_VERSION,
            },
        }
        mq_client = mq_client_getter(region)
        response = mq_client.create_configuration(**create_params)
        return response",Create configuration for AmazonMQ broker.,???Define a tool to create a configuration with specified parameters using a message queue client.???
2986,get_audio_duration,"def get_audio_duration(audio_file: str) -> float:
    
    cmd = ['ffmpeg', '-i', audio_file]
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    _, stderr = process.communicate()
    output = stderr.decode('utf-8', errors='ignore')
    
    try:
        duration_str = [line for line in output.split('\n') if 'Duration' in line][0]
        duration_parts = duration_str.split('Duration: ')[1].split(',')[0].split(':')
        duration = float(duration_parts[0])*3600 + float(duration_parts[1])*60 + float(duration_parts[2])
    except Exception as e:
        print(f""[red]❌ Error: Failed to get audio duration: {e}[/red]"")
        duration = 0
    return duration",Get the duration of an audio file using ffmpeg.,???Extracts audio file duration using FFmpeg command-line tool.???
2987,prepare_final_report,"def prepare_final_report(agent_state: ""AgentState"", title, sections, conclusion, citations):
    
    import json

    report = {
        ""title"": title,
        ""sections"": [section.model_dump() for section in sections],
        ""conclusion"": conclusion,
        ""citations"": citations,
    }
    agent_state.memory.update_block_value(label=""report_outline"", value=json.dumps(report, indent=2))

    return ""Your report has been successfully stored inside of memory section final_report. Next step: return the completed report to the user using send_message so they can review it. You final report should use proper markdown formatting (make sure to use markdown headings/subheadings, lists, bold, italics, etc., as well as links for citations).""",Prepare a top-level (summarized) draft report based on the research process you have completed.,???Generate and store a structured report outline in memory for user review.???
2988,identify_reasoning_steps,"def identify_reasoning_steps(self, text: str) -> List[Dict]:
        
        try:
            prompt = self.reasoning_identification_template.format(
                start_token=self.reasoning_start_token,
                end_token=self.reasoning_end_token,
                text=text,
            )

            if self.together_model:
                response = self.together_model.generate_insight(
                    prompt, self.reasoning_step_identification_response_model
                )
                print(""\nDEBUG - Raw response:"")
                print(response)

                # Clean up the response by removing markdown code blocks
                cleaned_response = response.replace(""```json"", """").replace(""```"", """").strip()

                try:
                    parsed = json.loads(cleaned_response)
                    print(""\nDEBUG - Parsed JSON:"")
                    print(json.dumps(parsed, indent=2))
                    return parsed.get(""reasoning_steps"", [])
                except json.JSONDecodeError as e:
                    print(f""\nDEBUG - JSON parsing error: {e}"")
                    print(""Cleaned response:"", cleaned_response)
                    return []

        except Exception as e:
            print(f""Error in identify_reasoning_steps: {str(e)}"")
            return []",Use the insight model to identify reasoning steps,???Extract reasoning steps from text using a model-generated prompt and parse the JSON response.???
2989,set_config,"def set_config(self, key: str, value: Any, update: bool = False) -> None:
        
        if key is None:
            raise ValueError(""Key must be provided."")

        if update:
            current_value = getattr(self.config, key, {})
            if not isinstance(current_value, dict):
                raise TypeError(
                    f""Expected a dict for key '{key}', got {type(current_value).__name__} instead.""
                )
            current_value.update(value)
        else:
            setattr(self.config, key, value)",Set or update the configuration for a given key.,???Configure or update settings with validation and type checking???
2990,write,"def write(self) -> None:
        
        with open(conf.PATH / self._filename, 'a') as f:
            for key, value in self._new_variables.items():
                if value is None:
                    f.write(f'\n#{key}=""""')  # comment-out empty values
                else:
                    f.write(f'\n{key}={value}')",Append new variables to the end of the file.,"???Append new variables to a file, commenting out empty values.???"
2991,load_index,"def load_index(self, config):
        
        index_path = os.path.join(config.index_path, f'{config.retrieval_method}_Flat.index')
        index = faiss.read_index(index_path)

        if self.config.faiss_gpu:

            # Apply FAISS GPU settings
            co = faiss.GpuMultipleClonerOptions()
            co.useFloat16 = True  # Reduce memory footprint
            co.shard = True  # Distribute index across all GPUs
            
            print(""[FAISSIndexServer] Moving FAISS index to all GPUs with sharding enabled..."")
            index = faiss.index_cpu_to_all_gpus(index, co=co)

            print(""[FAISSIndexServer] FAISS index successfully moved to GPUs."")
        return index",Loads the FAISS index into GPU memory with sharding.,???Load and optionally distribute a FAISS index across GPUs based on configuration settings.???
2992,remove_observer,"def remove_observer(self, observer: Callable):
        
        try:
            self._observers = [obs for obs in self._observers if obs[0] != observer]
            self.react_agent._observers = [obs for obs in self.react_agent._observers if obs[0] != observer]
        except Exception as e:
            logger.error(f""Failed to remove observer: {e}"")
            raise",Remove an observer from both the Agent and its internal react_agent.,"???Remove specified observer from both local and agent observer lists, handling errors.???"
2993,_load_json_settings,"def _load_json_settings(path: Path, settings: Settings) -> Settings:
    
    try:
        import json

        with open(path) as f:
            config_data = json.load(f)

        # Update settings from config data
        for key, value in config_data.items():
            if hasattr(settings, key):
                setattr(settings, key, value)

        return settings
    except Exception as e:
        console.print(
            f""[bold red]Error loading JSON config from {path}: {e}[/bold red]""
        )
        return settings",Load settings from a JSON file.,"???Load and update configuration settings from a JSON file, handling errors gracefully.???"
2994,invalid_schema_json_file,"def invalid_schema_json_file(tmp_path):
    
    file_path = tmp_path / ""invalid_schema.json""
    # Missing 'task_description'
    invalid_data = {""inputs"": {""data"": ""string""}}
    with open(file_path, ""w"") as f:
        json.dump(invalid_data, f)
    return file_path",Creates a temporary JSON file with valid JSON but invalid schema.,???Create a temporary JSON file with an invalid schema for testing purposes.???
2995,get_size,"def get_size(path: str, fs: Optional[AbstractFileSystem] = None) -> int:
    

    fs = fs or get_fs(path)

    if not exists(path, fs=fs):
        raise ValueError(f""Path {path} does not exist"")
    if is_dir(path, fs=fs):
        raise ValueError(f""Path {path} is a directory"")

    return fs.info(path)[""size""]",Get the size of a file,"???Retrieve file size from path, raising errors for non-existence or directory.???"
2996,extract_text,"def extract_text(path, backend=""aryn""):
    
    print(f""Extracting text from {path} using {backend} backend"")
    text = []
    if backend == ""aryn"":
        from aryn_sdk.partition import partition_file

        with open(path, ""rb"") as f:
            data = partition_file(f)

        elements = data[""elements""]
        for element in elements:
            try:
                text.append(element[""text_representation""])
            except KeyError:
                continue
    elif backend == ""local"":
        import pdfplumber

        with pdfplumber.open(path) as pdf:
            for page in pdf.pages:
                text.append(page.extract_text() or """")
    else:
        raise ValueError(f""Invalid OCR backend: {backend}"")
    return """".join(text)",Extract text from a PDF file using the specified backend.,???Extracts text from a file using specified OCR backend???
2997,manager,"def manager(monkeypatch):
    
    tm = ToolManager(config_file=""dummy"", servers=[])

    # Provide predictable data
    dummy = DummyRegistry([(""ns1"", ""t1""), (""ns2"", ""t2""), (""default"", ""t1"")])
    dummy._meta[(""ns1"", ""t1"")] = DummyMeta(
        ""d1"",
        {""properties"": {""a"": {""type"": ""int""}}, ""required"": [""a""]},
        is_async=True,
        tags={""x""},
    )
    dummy._meta[(""ns2"", ""t2"")] = DummyMeta(""d2"", {}, is_async=False, tags=set())

    # Monkey‑patch in the dummy registry
    monkeypatch.setattr(tm, ""_registry"", dummy)
    return tm",Return a ToolManager instance whose registry is replaced by DummyRegistry.,???Initialize a ToolManager with a dummy registry for testing purposes.???
2998,chat_example,"def chat_example(client):
    
    print(""\n=== Chat Example ==="")

    # Single message example
    messages = [
        {""role"": ""system"", ""content"": ""You are a helpful AI assistant.""},
        {""role"": ""user"", ""content"": ""What is the capital of France?""},
    ]

    try:
        responses, usage, done_reasons = client.chat(messages)

        print(f""Response: {responses[0]}"")
        print(f""Usage: {usage}"")
        print(f""Done reason: {done_reasons[0]}"")
    except Exception as e:
        print(f""Error during chat: {e}"")",Example of using the chat API.,???Simulates a chat interaction with an AI assistant using predefined messages.???
2999,load_proxies,"def load_proxies() -> list:
    
    proxy_all = []
    proxy_file = [""http_proxy.txt"", ""socks5_proxy.txt"", ""socks4_proxy.txt""]
    for fn in proxy_file:
        f_obj = pathlib.Path(path, fn)
        if f_obj.exists():
            proxy_lst = pathlib.Path(path, fn).read_text(
                encoding=""utf8"").split(""\n"")
            if not proxy_lst:
                continue
            if fn == ""http_proxy.txt"":
                for proxy in proxy_lst:
                    if proxy:
            elif fn == ""socks5_proxy.txt"":
                for proxy in proxy_lst:
                    if proxy:
            elif fn == ""socks4_proxy.txt"":
                for proxy in proxy_lst:
                    if proxy:
        else:
            f_obj.touch()
    logger.success(f""代理列表加载完成,代理数:{len(proxy_all)}"")
    return proxy_all",load proxies for files :return: proxies list,???Load and return a list of proxies from specified text files.???
3000,_get_execution_order,"def _get_execution_order(self) -> list[str]:
        
        self._validate_dependencies()

        visited = set()
        order = []

        def visit(atom_name: str):
            if atom_name in visited:
                return

            for dep in self.atoms[atom_name].dependencies:
                visit(dep)

            visited.add(atom_name)
            order.append(atom_name)

        for atom_name in self.atoms:
            visit(atom_name)

        logger.info(f""[DAG] Computed atom execution order: {order}"")

        return order",Returns a valid execution order for atoms based on dependencies.,???Determine execution sequence for dependency graph nodes???
3001,join_task_statement,"def join_task_statement(intent: str, input_schema: Type[BaseModel], output_schema: Type[BaseModel]) -> str:
    
    problem_statement: str = (
        ""# Problem Statement""
        ""\n\n""
        f""{intent}""
        ""\n\n""
        ""# Expected Model Input Schema""
        ""\n\n""
        f""{json.dumps(convert_schema_to_type_dict(input_schema), indent=4, default=str)}""
        ""\n\n""
        ""# Expected Model Output Schema""
        ""\n\n""
        f""{json.dumps(convert_schema_to_type_dict(output_schema), indent=4, default=str)}""
    )
    return problem_statement",Join the problem statement into a single string.,"???  
Generate a formatted problem statement with input and output schemas.  
???"
3002,group_chat_reply,"def group_chat_reply(
            agent: ""ConversableAgent"",
            messages: Optional[list[dict[str, Any]]] = None,
            sender: Optional[""Agent""] = None,
            config: Optional[Any] = None,
        ) -> tuple[bool, Optional[dict[str, Any]]]:
            
            # Get the configuration stored directly on the agent
            group_config = agent._group_chat_config  # type: ignore[attr-defined]

            # Pull through the second last message from the outer chat (the last message will be the handoff message)
            # This may need work to make sure we get the right message(s) from the outer chat
            message = (
                messages[-2][""content""]
                if messages and len(messages) >= 2 and ""content"" in messages[-2]
                else ""No message to pass through.""
            )

            try:
                # Run the group chat with direct agent references from the config
                result, _, _ = initiate_group_chat(
                    pattern=group_config.pattern,
                    messages=message,
                    max_rounds=group_config.max_rounds,
                )

                # Return the summary from the chat result summary
                return True, {""content"": result.summary}

            except Exception as e:
                # Handle any errors during execution
                return True, {""content"": f""Error running group chat: {str(e)}""}",Run the inner group chat and return its results as a reply.,???Facilitates group chat interaction using agent configuration and message history.???
3003,kwargs_for_launch_persistent_context,"def kwargs_for_launch_persistent_context(self) -> BrowserLaunchPersistentContextArgs:
		
		return BrowserLaunchPersistentContextArgs(**self.model_dump(exclude={'args'}), args=self.get_args())",Return the kwargs for BrowserType.launch().,???Returns configuration for launching a persistent browser context.???
3004,get_default_swaparr_stats,"def get_default_swaparr_stats() -> Dict[str, int]:
    
    return {
        ""processed"": 0,
        ""strikes"": 0,
        ""removals"": 0,
        ""ignored"": 0
    }",Get the default Swaparr stats structure,"??? 
Initialize a dictionary with default statistics for swap operations. 
???"
3005,format_prompt,"def format_prompt(user_query: str) -> str:
    
    input_str =  + INSTRUCTION
    input_str += 
    input_str += user_query + 
    
    return [
        {""role"": ""system"", ""content"": ""You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.""},
        {""role"": ""user"", ""content"": input_str}
    ]",Format the prompt for the model using the same template as make_prefix.,???Formats user query into structured assistant prompt for response generation.???
3006,__del__,"def __del__(self):
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                loop.create_task(self.cleanup())
            else:
                loop.run_until_complete(self.cleanup())
        except Exception as e:
            logger.error(f""Cleanup failed | Agent: {self.agent_name} | Error: {str(e)}"")",Destructor to ensure cleanup of resources,"???Ensure asynchronous cleanup on object deletion, logging errors if encountered.???"
3007,get_current_version,"def get_current_version(default_version: str = ""0.0.0"") -> str:
    
    version_file = VERSION_FILE_PATH
    try:
        with version_file.open('r', encoding='utf-8') as f:
            version = f.read().strip()
        if not version:
            helper_logger.warning(f""VERSION file ('{version_file}') is empty. Using default version '{default_version}'."")
            return default_version
        return version
    except FileNotFoundError:
        helper_logger.warning(f""VERSION file not found at '{version_file}'. Using default version '{default_version}'."")
        return default_version
    except IOError as e:
        helper_logger.error(f""Error reading VERSION file ('{version_file}'): {e}. Using default version '{default_version}'."")
        return default_version",Reads the current version from the VERSION file.,"???  
Retrieve software version from file, defaulting if unavailable or unreadable.  
???"
3008,parse_action_response,"def parse_action_response(response: str) -> Tuple[str, str]:
        
        try:
            root = etree.fromstring(response, parser=XMLResultHandler._parser)
            if root is None:
                raise ValueError(""Empty XML document"")
            
            # Log XML parsing warnings
            if XMLResultHandler._parser.error_log:
                for error in XMLResultHandler._parser.error_log:
                    logger.warning(f""XML parse warning: {error.message} (line {error.line})"")
            
            return (
                root.findtext(""Thought"") or """",
                root.findtext(""Code"") or """"
            )
        except etree.XMLSyntaxError as e:
            logger.error(f""Critical XML parsing error: {e}"")
            raise ValueError(f""Malformed XML structure: {e}"") from e",Parse XML response to extract thought and code with robust error handling.,???Parse XML response to extract 'Thought' and 'Code' elements???
3009,create_configuration,"def create_configuration(
        region: str, authentication_strategy: str, engine_type: str, engine_version: str, name: str
    ):
        
        create_params = {
            'AuthenticationStrategy': authentication_strategy,
            'EngineType': engine_type,
            'EngineVersion': engine_version,
            'Name': name,
            'Tags': {
                'mcp_server_version': MCP_SERVER_VERSION,
            },
        }
        mq_client = mq_client_getter(region)
        response = mq_client.create_configuration(**create_params)
        return response",Create configuration for AmazonMQ broker.,???Creates a configuration for a message queue client using specified parameters.???
3010,_trajectory_log,"def _trajectory_log(self, trajectory: Trajectory) -> str:
        
        header = f""reward: {trajectory.reward} {' '.join(f'{k}: {v}' for k, v in trajectory.metrics.items())}\n\n""
        formatted_messages = []
        for message_or_choice in trajectory.messages_and_choices:
            if isinstance(message_or_choice, dict):
                message = message_or_choice
            else:
                message = cast(Message, message_or_choice.message.model_dump())
            formatted_messages.append(format_message(message))
        return header + ""\n"".join(formatted_messages)",Format a trajectory into a readable log string.,"???  
Format and log trajectory details including rewards and messages.  
???"
3011,increment_island_generation,"def increment_island_generation(self, island_idx: Optional[int] = None) -> None:
        
        idx = island_idx if island_idx is not None else self.current_island
        self.island_generations[idx] += 1
        logger.debug(f""Island {idx} generation incremented to {self.island_generations[idx]}"")",Increment generation counter for an island,???Increment specified or current island's generation count and log the update.???
3012,get_tools,"def get_tools(self):
        
        all_tools = []
        for server in self.servers:
            try:
                tools = server.get_tools()
                all_tools.extend(tools)
                logger.info(f""Added {len(tools)} tools from MCP server"")
            except Exception as e:
                logger.error(f""Error getting tools from MCP server: {str(e)}"")
        return all_tools",Return a flattened list of all tools across all servers,"???Aggregate tools from multiple servers, logging successes and errors.???"
3013,update_snapshot,"def update_snapshot(self, repo_app_id: int, diff: str) -> bool:
        
        try:
            self._make_request(self.update_diff_url, {""repo_app_id"": repo_app_id, ""new_diff"": diff})
            return True
        except Exception:
            return False",Send snapshot data to external DB via API,"???  
Attempts to update a repository snapshot with a new diff and returns success status.  
???"
3014,run_task_tests,"def run_task_tests(task_list: List[str]):
    
    import pytest

    package_root = find_test_root(start_path=pathlib.Path(__file__))
    task_string = "" or "".join(task_list)
    args = [
        f""{package_root}/tests/test_version_stable.py"",
        f""--rootdir={package_root}"",
        ""-k"",
        f""{task_string}"",
    ]
    sys.path.append(str(package_root))
    pytest_return_val = pytest.main(args)
    if pytest_return_val:
        raise ValueError(f""Not all tests for the specified tasks ({task_list}) ran successfully! Error code: {pytest_return_val}"")",Find the package root and run the tests for the given tasks,???Execute specified task tests using pytest and handle failures with error reporting.???
3015,template_engine,"def template_engine(self):
        
        mock_engine = MagicMock()
        mock_engine.render.side_effect = lambda template_name, data: f""# Generated Tool: {data['name']}\n\n{data['implementation']}""
        return mock_engine",Create a mock template engine.,???Creates a mock template engine to simulate rendering with dynamic data.???
3016,_restore_sigint_handler,"def _restore_sigint_handler(self) -> None:
        
        try:
            if self._prev_sigint_handler:
                try:
                    signal.signal(signal.SIGINT, self._prev_sigint_handler)
                    self._prev_sigint_handler = None
                except Exception as e:
                    log.warning(f""Error restoring signal handler: {e}"")
        except Exception as exc:
            log.error(f""Error in _restore_sigint_handler: {exc}"")",Restore the original SIGINT handler with error handling.,???Restore previous interrupt signal handler with error logging.???
3017,untrack_cache_storage_for_storage_key,"def untrack_cache_storage_for_storage_key(
    storage_key: str,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""storageKey""] = storage_key
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Storage.untrackCacheStorageForStorageKey"",
        ""params"": params,
    }
    json = yield cmd_dict",Unregisters storage key from receiving notifications for cache storage.,???Generate command to untrack cache storage by storage key.???
3018,process_action,"def process_action(self, action_chunk, obs):
        
        action_chunk = [a.to(self.device) for a in action_chunk]
        for a in action_chunk:
            assert a.shape == (self.num_envs, self.policy_cfg.action_config.action_dim), (
                f""Expected num_envs X action_dim : {self.num_envs} X {self.policy_cfg.action_config.action_dim}, got {a.shape} instead""
            )
        if self.policy_cfg.action_config.action_type == ""joint_pos"":
            qpos_action_chunk = action_chunk
        elif self.policy_cfg.action_config.action_type == ""ee"":
            qpos_action_chunk = []
            robot_ee_state = obs[""robot_ee_state""].to(self.device)
            robot_root_state = obs[""robot_root_state""].to(self.device)
            robot_pos, robot_quat = robot_root_state[:, 0:3], robot_root_state[:, 3:7]
            curr_ee_pos, curr_ee_quat = robot_ee_state[:, 0:3], robot_ee_state[:, 3:7]
            curr_ee_pos_local = transforms.quaternion_apply(
                transforms.quaternion_invert(robot_quat), curr_ee_pos - robot_pos
            )
            curr_ee_quat_local = transforms.quaternion_multiply(transforms.quaternion_invert(robot_quat), curr_ee_quat)
            curr_robot_q = obs[""joint_qpos""].to(self.device)
            for action in action_chunk:
                target_qpos = self._solve_ik(action, curr_ee_pos_local, curr_ee_quat_local, curr_robot_q)
                qpos_action_chunk.append(target_qpos)

        if self.policy_cfg.action_config.interpolate_chunk:
            return self._interpolate_chunk(obs[""joint_qpos""].to(self.device), qpos_action_chunk)
        else:
            return qpos_action_chunk",Processes a chunk of actions into joint positions.,"???Process and validate action data, compute joint positions, and optionally interpolate results.???"
3019,_process_llm_gen_span,"def _process_llm_gen_span(self, span: Dict[str, Any]) -> None:
        
        model = span.get(""data"", {}).get(""model"", ""unknown"")
        input_tokens = span.get(""data"", {}).get(""input_tokens"", 0)
        output_tokens = span.get(""data"", {}).get(""output_tokens"", 0)
        
        logger.debug(f""LLM Generation: {model}, Input tokens: {input_tokens}, Output tokens: {output_tokens}"")",Process an LLM generation span,???Log language model generation details including model type and token counts.???
3020,insert_before_symbol,"def insert_before_symbol(
        self,
        name_path: str,
        relative_file_path: str,
        body: str,
        *,
        at_new_line: bool = True,
        use_same_indentation: bool = True,
    ) -> None:
        
        symbol_candidates = self.find_by_name(name_path, within_relative_path=relative_file_path)
        if len(symbol_candidates) == 0:
            raise ValueError(f""No symbol with name {name_path} found in file {relative_file_path}"")
        if len(symbol_candidates) > 1:
            raise ValueError(
                f""Found multiple {len(symbol_candidates)} symbols with name {name_path} in file {relative_file_path}. ""
                f""May be an overwritten variable, in which case you can ignore this error. Proceeding with the first one. ""
                f""Found symbols at locations: \n"" + json.dumps([s.location.to_dict() for s in symbol_candidates], indent=2)
            )
        symbol = symbol_candidates[0]
        self.insert_before_symbol_at_location(symbol.location, body, at_new_line=at_new_line, use_same_indentation=use_same_indentation)",Inserts content before the symbol with the given name in the given file.,"???Insert code snippet before a specified symbol in a file, handling multiple matches???"
3021,prompt,"def prompt(self, input):
        
        return [
            {""role"": ""system"", ""content"": SKY_T1_SYSTEM_PROMPT},
            {""role"": ""user"", ""content"": input[""problem""]},
        ]",Create a prompt for the LLM to reason about the problem.,"???  
Formats input into a structured prompt for a system-user interaction.  
???"
3022,get_bot_status,"def get_bot_status(self, user_id):
        
        try:
            pod_list = self.core_v1.list_namespaced_pod(
                namespace=self.namespace,
                label_selector=f""app=bot,user-id={user_id}""
            )
            
            result = []
            for pod in pod_list.items:
                meeting_id = pod.metadata.labels.get(""meeting-id"", ""unknown"")
                status = pod.status.phase
                result.append({
                    ""pod_name"": pod.metadata.name,
                    ""user_id"": user_id,
                    ""meeting_id"": meeting_id,
                    ""status"": status,
                    ""creation_time"": pod.metadata.creation_timestamp.isoformat() if pod.metadata.creation_timestamp else None
                })
            
            return result
        except client.rest.ApiException as e:
            logger.error(f""Error getting pod status: {e}"")
            raise",Get status of all bot pods for a user,???Retrieve and return the status of bot pods for a specific user in a namespace.???
3023,parse_size_string_to_bytes,"def parse_size_string_to_bytes(size_string):
    
    if not size_string:
        return 25 * 1024 * 1024 * 1024  # Default 25GB
    
    # Extract the numeric part and unit
    unit = """"
    for i in range(len(size_string) - 1, -1, -1):
        if not size_string[i].isalpha():
            value = float(size_string[:i+1])
            unit = size_string[i+1:].upper()
            break
    else:
        swaparr_logger.error(f""Invalid size string: {size_string}, using default 25GB"")
        return 25 * 1024 * 1024 * 1024
    
    # Convert to bytes based on unit
    if unit == 'B':
        return int(value)
    elif unit == 'KB':
        return int(value * 1024)
    elif unit == 'MB':
        return int(value * 1024 * 1024)
    elif unit == 'GB':
        return int(value * 1024 * 1024 * 1024)
    elif unit == 'TB':
        return int(value * 1024 * 1024 * 1024 * 1024)
    else:
        swaparr_logger.error(f""Unknown size unit in: {size_string}, using default 25GB"")
        return 25 * 1024 * 1024 * 1024","Parse a size string like '25GB', '1TB' to bytes","???Convert a size string to bytes, defaulting to 25GB if invalid.???"
3024,plot_results,"def plot_results(results, title, filename):
    
    df = pd.DataFrame(results)
    df = df[df['elapsed'].notnull()]
    if df.empty:
        print(f""No successful results to plot for {title}."")
        return
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    df.boxplot(column='elapsed', by=['provider'], ax=ax)
    plt.title(title)
    plt.suptitle("""")
    plt.ylabel('Elapsed Time (s)')
    plt.xlabel('Provider')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(filename)
    plt.show()",Plot results and save to file.,???Visualize and save elapsed time data by provider in a boxplot.???
3025,generate_span_labels,"def generate_span_labels(text: str, entities: List[Tuple[str, int, int, str]]) -> str:
    
    if not isinstance(text, str) or pd.isna(text) or not entities:
        return """"
    
    spans = []
    for entity_type, start, end, _ in entities:
        spans.append(f""{start}:{end}:{entity_type}"")
    
    return ""|"".join(spans)",Generate span labels in format: start:end:entity_type|start:end:entity_type,??? Convert entity data into formatted span labels for text annotation. ???
3026,compare_answers,"def compare_answers(correct_answer: str, predicted_answer: Optional[str]) -> bool:
    
    logger.debug(f""Comparing answers - Correct: {repr(correct_answer)}, Predicted: {repr(predicted_answer)}"")
    
    if predicted_answer is None:
        logger.debug(""Predicted answer is None"")
        return False
    
    # Try numerical comparison first
    if numerically_equal(correct_answer, predicted_answer):
        return True
        
    normalized_correct = normalize_answer(correct_answer)
    normalized_predicted = normalize_answer(predicted_answer)
    
    logger.debug(f""Normalized answers - Correct: {repr(normalized_correct)}, Predicted: {repr(normalized_predicted)}"")
    
    # If either normalization returns None or empty string, answers don't match
    if not normalized_correct or not normalized_predicted:
        logger.debug(""One or both normalized answers are None or empty"")
        return False
        
    # If both answers became empty strings, they don't match
    if normalized_correct == """" and normalized_predicted == """":
        logger.debug(""Both answers normalized to empty strings"")
        return False
    
    # For intervals, they must match exactly (including brackets)
    if ('\\left[' in normalized_correct or '\\left(' in normalized_correct) and \
       ('\\left[' in normalized_predicted or '\\left(' in normalized_predicted):
        result = normalized_correct == normalized_predicted
        logger.debug(f""Interval comparison result: {result}"")
        return result
    
    result = normalized_correct == normalized_predicted
    logger.debug(f""Comparison result: {result}"")
    return result",Compare the correct answer with the predicted answer.,"???Compares predicted and correct answers for equivalence, considering numerical and normalized forms.???"
3027,check_binary_exists,"def check_binary_exists(path: str, name: str) -> None:
  
  if not os.path.exists(path):
    raise RuntimeError(f'{name} binary not found at {path}')",Checks if a binary exists on the given path and raises otherwise.,"???Check if a binary file exists at a specified path, raising an error if not found.???"
3028,update_build_triggers,"def update_build_triggers(tf_dir: Path) -> None:
    
    build_triggers_path = tf_dir / ""build_triggers.tf""
    if build_triggers_path.exists():
        with open(build_triggers_path) as f:
            content = f.read()

        # Add repository dependency to all trigger resources
        modified_content = content.replace(
            ""depends_on = [resource.google_project_service.cicd_services, resource.google_project_service.shared_services]"",
            ""depends_on = [resource.google_project_service.cicd_services, resource.google_project_service.shared_services, google_cloudbuildv2_repository.repo]"",
        )

        # Update repository reference in all triggers
        modified_content = modified_content.replace(
            'repository = ""projects/${var.cicd_runner_project_id}/locations/${var.region}/connections/${var.host_connection_name}/repositories/${var.repository_name}""',
            ""repository = google_cloudbuildv2_repository.repo.id"",
        )

        with open(build_triggers_path, ""w"") as f:
            f.write(modified_content)

        console.print(""✅ Updated build triggers with repository dependency"")",Update build triggers configuration.,???Enhances build triggers by adding repository dependencies and updating repository references.???
3029,get_task_info,"def get_task_info(task_id):
    
    try:
        task_info = redis_client.get(f""task:{task_id}:info"")
        if task_info:
            return json.loads(task_info.decode(""utf-8""))
        return {}
    except Exception as e:
        logger.error(f""Error getting task info: {e}"")
        return {}",Get task information from Redis,"???Retrieve and decode task information from Redis, handling errors gracefully.???"
3030,_show_progress,"def _show_progress(self) -> None:
        
        elapsed_time = time.time() - self.start_time
        if elapsed_time < 0.01: elapsed_time = 0.01 # Avoid division by zero

        items_per_second = self.processed_items / elapsed_time
        percent_complete = (self.processed_items / max(1, self.total_items)) * 100 # Avoid division by zero total_items
        remaining_items = self.total_items - self.processed_items
        eta_seconds = remaining_items / items_per_second if items_per_second > 0 else 0

        # Format ETA nicely
        if eta_seconds > 3600:
             eta_str = f""{eta_seconds/3600:.1f}h""
        elif eta_seconds > 60:
             eta_str = f""{eta_seconds/60:.1f}m""
        else:
             eta_str = f""{eta_seconds:.1f}s""

        # Use a fixed width for progress numbers to prevent line jitter
        progress_str = f""{self.processed_items:>{len(str(self.total_items))}}/{self.total_items}""

        print(
            f""Progress: {progress_str} ({percent_complete:6.1f}%) | ""
            f""{items_per_second:6.1f} items/s | ""
            f""ETA: {eta_str:<6}"", # Left align ETA string in a fixed width
            end=""\r"",
            flush=True # Ensure it updates immediately
        )",Show progress information to stdout.,"???Display processing progress with percentage, speed, and estimated time remaining.???"
3031,extract_domain_name,"def extract_domain_name(url: str) -> str:
    
    try:
        # Remove protocol and www
        domain = url.lower()
            if domain.startswith(prefix):
                domain = domain[len(prefix):]
        
        # Get the main domain part (before first slash or query)
        domain = domain.split('/')[0].split('?')[0]
        
        # Extract the main part (e.g., 'tavily' from 'tavily.com')
        parts = domain.split('.')
        if len(parts) >= 2:
            main_name = parts[0]
            # Capitalize the name
            return main_name.capitalize()
        return domain.capitalize()
    except Exception as e:
        logger.error(f""Error extracting domain name from {url}: {e}"")
        return ""Website""",Extract a readable website name from a URL.,???Extracts and capitalizes the main domain name from a given URL string???
3032,list_models,"def list_models(search: Optional[str] = None):
    
    console = Console()
    all_models = get_all_models()

    if search:
        # Perform fuzzy matching
        matched_models = process.extractBests(search, all_models, limit=None, score_cutoff=70)
        models = [model for model, score in matched_models]
    else:
        models = all_models

    console.print(Panel(f""Total Models: {len(models)} "" f""({len(all_models)} total)"", title=""Supported LiteLLM Models""))

    if not models:
        console.print(f""[yellow]No models found matching '[bold]{search}[/bold]'[/yellow]"")
        return

    for model in sorted(models):
        console.print(f""- {model}"")",List supported LiteLLM models with optional fuzzy search.,???Display and filter available models with optional fuzzy search functionality.???
3033,save_cache,"def save_cache(self, enforce_constraints: bool = False):
        
        # Prune any buckets that have fewer samples than batch_size
        if enforce_constraints:
            self._enforce_min_bucket_size()
        if self.read_only:
            logger.debug(""Skipping cache update on storage backend, read-only mode."")
            return
        # Convert any non-strings into strings as we save the index.
        aspect_ratio_bucket_indices_str = {
            key: [str(path) for path in value]
            for key, value in self.aspect_ratio_bucket_indices.items()
        }
        # Encode the cache as JSON.
        cache_data = {
            ""config"": StateTracker.get_data_backend_config(
                data_backend_id=self.data_backend.id
            ),
            ""aspect_ratio_bucket_indices"": aspect_ratio_bucket_indices_str,
        }
        logger.debug(f""save_cache has config to write: {cache_data['config']}"")
        cache_data_str = json.dumps(cache_data)
        # Use our DataBackend to write the cache file.
        self.data_backend.write(self.cache_file, cache_data_str)",Save cache data to file.,???Save cache data with optional constraints and convert indices to strings.???
3034,parse_time_string_to_seconds,"def parse_time_string_to_seconds(time_string):
    
    if not time_string:
        return 7200  # Default 2 hours
    
    unit = time_string[-1].lower()
    try:
        value = int(time_string[:-1])
    except ValueError:
        swaparr_logger.error(f""Invalid time string: {time_string}, using default 2 hours"")
        return 7200
    
    if unit == 'd':
        return value * 86400  # Days to seconds
    elif unit == 'h':
        return value * 3600   # Hours to seconds
    elif unit == 'm':
        return value * 60     # Minutes to seconds
    else:
        swaparr_logger.error(f""Unknown time unit in: {time_string}, using default 2 hours"")
        return 7200","Parse a time string like '2h', '30m', '1d' to seconds",???Convert time string to seconds with error handling and defaults.???
3035,exec,"def exec(self, prep_res):
        
        if not prep_res:
            return ""Empty text""
        prompt = f""Summarize this text in 10 words: {prep_res}""
        summary = call_llm(prompt)  # might fail
        return summary",Execute the summarization using LLM.,???Generate a 10-word summary of input text using a language model.???
3036,check_cuda_available,"def check_cuda_available():
    
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        cuda_info = {}
        
        if cuda_available:
            cuda_info = {
                ""device_count"": torch.cuda.device_count(),
                ""current_device"": torch.cuda.current_device(),
                ""device_name"": torch.cuda.get_device_name(0)
            }
        
        return jsonify(APIResponse.success(
            data={
                ""cuda_available"": cuda_available,
                ""cuda_info"": cuda_info
            },
            message=""CUDA availability check completed""
        ))
    except Exception as e:
        error_msg = f""Error checking CUDA availability: {str(e)}""
        logger.error(error_msg)
        return jsonify(APIResponse.error(message=error_msg, code=500))",Check if CUDA is available for model training/inference,???Check and return CUDA availability and device details using PyTorch???
3037,extract_first_heading,"def extract_first_heading(file_path):
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
            # Remove frontmatter
            content = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)
            
            # Find first heading
            heading_match = re.search(r'#\s+(.+)', content)
            if heading_match:
                return heading_match.group(1).strip()
    except Exception as e:
        print(f""Error extracting heading from {file_path}: {e}"")
    
    # Fallback to filename if no heading found
    return Path(file_path).stem.replace('_', ' ').title()",Extract the first heading from markdown content,"???Extracts the first heading from a text file, defaulting to filename if absent.???"
3038,uv_pip_install,"def uv_pip_install(repo_dir: Path, args: List[str]) -> None:
    
    cmd = [""uv"", ""pip"", ""install""] + args
    try:
        subprocess.run(cmd, cwd=repo_dir, check=True)
    except Exception as e:
        logging.error(f""Failed to run uv pip install {args}: {e}"")",Run 'uv pip install ...' in the specified repo_dir.,"???  
Executes a package installation command in a specified directory with error logging.  
???"
3039,to_string,"def to_string(self) -> str:
        
        parts = []
        for op, param in zip(self.operations, self.parameters):
            if op == Operation.ADD:
                parts.append(f""add {param}"")
            elif op == Operation.MULTIPLY:
                parts.append(f""multiply by {param}"")
            elif op == Operation.SQUARE:
                parts.append(""square"")
            elif op == Operation.DOUBLE:
                parts.append(""double"")
            elif op == Operation.HALF:
                parts.append(""halve"")
            elif op == Operation.PREV_PLUS:
                parts.append(""add previous"")
        return "" then "".join(parts)",Convert rule to human-readable string,???Convert operations and parameters into a descriptive string sequence.???
3040,run_web_server,"def run_web_server(port, static_dir):
    
    import http.server
    import socket
    import socketserver

    class Handler(http.server.SimpleHTTPRequestHandler):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, directory=static_dir, **kwargs)

    # Try ports in sequence until one works
    max_port_attempts = 10
    current_port = port

    for attempt in range(max_port_attempts):
        try:
            with socketserver.TCPServer(("""", current_port), Handler) as httpd:
                httpd.serve_forever()
                break
        except socket.error:
            if attempt < max_port_attempts - 1:
                print(f""Port {current_port} is in use, trying port {current_port + 1}"")
                current_port += 1
            else:
                raise Exception(
                    f""Could not find an available port after {max_port_attempts} attempts""
                )",Run a simple HTTP server for the frontend,"???Start a web server on a specified port, serving files from a directory, with port fallback.???"
3041,init_agentops,"def init_agentops(use_agentops) -> bool:
    
    if not use_agentops:
        return False
        
    import agentops
    agentops_api_key = os.getenv(""AGENTOPS_API_KEY"")
    if agentops_api_key:
        agentops.init(agentops_api_key, default_tags=[""Stock Analysis Agent using owl""])
        print(f""{Fore.GREEN}AgentOps tracking enabled{Style.RESET_ALL}"")
        return True
    else:
        print(f""{Fore.YELLOW}Warning: AGENTOPS_API_KEY not set, AgentOps tracking disabled{Style.RESET_ALL}"")
        return False",Initialize AgentOps tracking (if enabled),"???  
Initialize and configure AgentOps tracking based on environment API key presence.  
???"
3042,gd_outputs,"def gd_outputs(gd):
    
    name_list, input_list = [], []
    for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef
        name_list.append(node.name)
        input_list.extend(node.input)
    return sorted(f""{x}:0"" for x in list(set(name_list) - set(input_list)) if not x.startswith(""NoOp""))",TensorFlow GraphDef model output node names.,???Identify and return sorted unique output node names from a graph definition???
3043,image_to_video,"def image_to_video(
        self,
        image_url: str,
        save_at: str,
        duration: float,
        config: dict,
        prompt: Optional[str] = None,
    ):
        
        try:
            model_name = config.get(""model_name"", ""fal-ai/fast-svd-lcm"")
            arguments = {""image_url"": image_url, ""duration"": duration}

            if model_name == ""fal-ai/haiper-video/v2/image-to-video"":
                arguments[""duration""] = 6

            if prompt:
                arguments[""prompt""] = prompt

            res = fal_client.run(
                model_name,
                arguments=arguments,
            )

            video_url = res[""video""][""url""]

            with open(save_at, ""wb"") as f:
                f.write(requests.get(video_url).content)
        except Exception as e:
            raise Exception(f""Error generating video: {type(e).__name__}: {str(e)}"")",Generate video from an image URL.,???Convert an image to a video using a specified model and save it locally.???
3044,update_refs,"def update_refs(obj, defs_keys, prop_name):
            
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key == ""$ref"" and isinstance(value, str) and value.startswith(""#/$defs/""):
                        ref_key = value[len(""#/$defs/"") :]
                        if ref_key in defs_keys:
                            obj[key] = f""#/properties/{prop_name}/$defs/{ref_key}""
                    else:
                        update_refs(value, defs_keys, prop_name)
            elif isinstance(obj, list):
                for item in obj:
                    update_refs(item, defs_keys, prop_name)","Recursively update $ref values that start with ""#/$defs/"".",???Recursively update JSON references to new property paths based on definitions.???
3045,lookup_stock_price,"def lookup_stock_price(company_name: str, **kwargs) -> str:
            

            stock_symbol = search_for_stock_symbol(company_name)
            logger.info(f""Looking up stock price for {stock_symbol}"")

            context[""current_task""] = """"
            context[""task_result""] = f""{stock_symbol} is currently trading at $100.00""

            return f""Symbol {stock_symbol} is currently trading at $100.00""",Useful for looking up a stock price .,???Retrieve and log stock price for a given company name.???
3046,log_message,"def log_message(self,message, mode):
        
        # Add HTML styling based on the mode
        if mode == 'debug':
            styled_entry = f'<span style=""color:green;"">{message}</span>'
        elif mode == 'info':
            styled_entry = f'<span style=""color:#003366;"">{message}</span>'
        elif mode == 'warning':
            styled_entry = f'<span style=""color:orange;"">{message}</span>'
        elif mode == 'error':
            styled_entry = f'<span style=""color:red;"">{message}</span>'
        elif mode == 'table':
            df = pd.read_json(message)
            html_table = df.to_html(classes='dataframe', index=False, escape=False)
            styled_entry = f""<div style='color:lightgreen;'>{html_table}</div>""  # Styling for the table
        else:
            styled_entry = message  # Default for other modes

        self.log_messages.append(styled_entry)",Logs a message with the specified mode.,???Format and log messages with HTML styling based on severity level.???
3047,send_conversation,"def send_conversation(self, conversation: ""Conversation"") -> ""Message"":
        
        from ..models import Message

        # Convert messages to Gemini's format
        chat = self.client.start_chat()

        # Send all previous messages to establish context
        for msg in conversation.messages[:-1]:  # All messages except the last one
            chat.send_message(msg.text)

        # Send the final message and get response
        try:
            response = chat.send_message(conversation.messages[-1].text)
        except Exception as e:
            raise RuntimeError(f""Failed to send conversation to Gemini API: {e}"") from e

        # Create and return a properly formatted Message instance
        return Message(
            role=""assistant"",
            text=response.text,
            raw=response,
            llm_model=self.model_name,
            llm_provider=self.NAME,
        )",Send a conversation to the Gemini API.,???Send conversation to Gemini API and return formatted response message.???
3048,_process_image,"def _process_image(self, image_path: str) -> Optional[str]:
        
        try:
            if not os.path.exists(image_path):
                self.logger.error(f""Image file not found: {image_path}"")
                return None

            # Get the MIME type
            mime_type, _ = mimetypes.guess_type(image_path)
            if not mime_type:
                mime_type = ""application/octet-stream""

            # Read and encode the image
            with open(image_path, ""rb"") as img_file:
                img_data = base64.b64encode(img_file.read()).decode(""utf-8"")

            self.logger.info(f""✅ Image processed successfully: {image_path}"")
            return img_data

        except Exception as e:
            self.logger.error(f""❌ Error processing image: {str(e)}"")
            return None",Process an image file and return base64 encoded data or None if processing fails,"???Process and encode an image file to base64, logging success or errors.???"
3049,step,"def step(self, message: str, details: str = """"):
        
        if not self.enabled:
            return

        elapsed = time.time() - self.start_time

        if details:
            self.console.print(f""[dim]🔄 {message}: {details} ({elapsed:.1f}s)[/dim]"")
        else:
            self.console.print(f""[dim]🔄 {message} ({elapsed:.1f}s)[/dim]"")",Report a workflow step,???Log elapsed time for a message if logging is enabled.???
3050,_interrupt_now,"def _interrupt_now(self) -> None:
        
        try:
            # cancel running asyncio tasks via ToolProcessor
            tp = getattr(self.context, ""tool_processor"", None)
            if tp:
                try:
                    tp.cancel_running_tasks()
                except Exception as tp_exc:
                    log.error(f""Error cancelling tool processor tasks: {tp_exc}"")
            
            try:
                self.stop_tool_calls()
            except Exception as stop_exc:
                log.error(f""Error stopping tool calls: {stop_exc}"")
        except Exception as exc:
            log.error(f""Error in _interrupt_now: {exc}"")",Called on first Ctrl-C or `/interrupt`.,"???  
Handles task interruption by canceling ongoing processes and logging errors.  
???"
3051,git_repo_with_untracked,"def git_repo_with_untracked(sample_git_repo):
    
    # Create untracked files
    untracked_files = [""untracked.txt"", ""src/untracked.py"", ""docs/draft.md""]

    for file_path in untracked_files:
        full_path = sample_git_repo / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text(f""Untracked content of {file_path}"")

    return sample_git_repo",Create a git repository with both tracked and untracked files.,???Simulate a Git repository with specified untracked files.???
3052,_format_messages,"def _format_messages(self, messages: list):
        
        formatted_messages = []

        for message in messages:
            if message[""role""] == ""assistant"" and message.get(""tool_calls""):
                formatted_messages.append(
                    {
                        ""role"": message[""role""],
                        ""content"": message[""content""],
                        ""tool_calls"": [
                            {
                                ""id"": tool_call[""id""],
                                ""function"": {
                                    ""name"": tool_call[""tool""][""name""],
                                    ""arguments"": json.dumps(
                                        tool_call[""tool""][""arguments""]
                                    ),
                                },
                                ""type"": tool_call[""type""],
                            }
                            for tool_call in message[""tool_calls""]
                        ],
                    }
                )
            else:
                formatted_messages.append(message)

        return formatted_messages",Format the messages to the format that Google Gemini expects.,???Format assistant messages with tool call details for structured output???
3053,_load_model_weights,"def _load_model_weights(self, model_path):
        
        try:
            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
            # print(f""Model loaded successfully from {model_path}"")
            self.logger.info(f""Model loaded successfully from {model_path}"")
        except Exception as e:
            # print(f""Error loading model: {e}"")
            self.logger.error(f""Error loading model: {e}"")
            raise e",Load pre-trained model weights.,"???Load and log model weights from a specified file path, handling errors.???"
3054,_get_file_path,"def _get_file_path(self, file_type: Literal[""client_info"", ""tokens""]) -> Path:
        
        key = self.get_cache_key()
        return self.cache_dir / f""{key}_{file_type}.json""",Get the file path for the specified cache file type.,???Generate a file path using a cache key and file type.???
3055,create_s3_bucket,"def create_s3_bucket(self, bucket_name: str):
        
        try:
            self.s3_client.head_bucket(Bucket=bucket_name)
            print(f'Bucket {bucket_name} already exists - retrieving it!')
        except ClientError as e:
            print(f'Creating bucket {bucket_name}')
            if self.region_name == ""us-east-1"":
                self.s3_client.create_bucket(
                    Bucket=bucket_name
                )
            else:
                self.s3_client.create_bucket(
                    Bucket=bucket_name,
                    CreateBucketConfiguration={'LocationConstraint': self.region_name}
                )","Check if bucket exists, and if not create S3 bucket for knowledge base data source","???Check if an S3 bucket exists, create it if not, considering region constraints.???"
3056,eval_json,"def eval_json(self, stats):
        
        if self.args.save_json and self.is_coco and len(self.jdict):
            anno_json = self.data[""path""] / ""annotations/instances_val2017.json""  # annotations
            pred_json = self.save_dir / ""predictions.json""  # predictions
            LOGGER.info(f""\nEvaluating pycocotools mAP using {pred_json} and {anno_json}..."")
                check_requirements(""pycocotools>=2.0.6"")
                from pycocotools.coco import COCO  # noqa
                from pycocotools.cocoeval import COCOeval  # noqa

                for x in anno_json, pred_json:
                    assert x.is_file(), f""{x} file not found""
                anno = COCO(str(anno_json))  # init annotations api
                pred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)
                for i, eval in enumerate([COCOeval(anno, pred, ""bbox""), COCOeval(anno, pred, ""segm"")]):
                    if self.is_coco:
                        eval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # im to eval
                    eval.evaluate()
                    eval.accumulate()
                    eval.summarize()
                    idx = i * 4 + 2
                    stats[self.metrics.keys[idx + 1]], stats[self.metrics.keys[idx]] = eval.stats[
                        :2
                    ]  # update mAP50-95 and mAP50
            except Exception as e:
                LOGGER.warning(f""pycocotools unable to run: {e}"")
        return stats",Return COCO-style object detection evaluation metrics.,???Evaluate and summarize COCO mAP metrics using JSON predictions and annotations.???
3057,observation,"def observation(self):
        
        html = self.state[""html""]
        if self.observation_mode == ""html"":
            return html
        elif self.observation_mode == ""text"":
            return self.convert_html_to_text(html, simple=True)
        elif self.observation_mode == ""text_rich"":
            return self.convert_html_to_text(html, simple=False)
        elif self.observation_mode == ""url"":
            return self.state[""url""]
        else:
            raise ValueError(f""Observation mode {self.observation_mode} not supported."")",Compiles state into either the `html` or `text` observation mode,"???  
Determine output format based on observation mode: HTML, text, rich text, or URL.  
???"
3058,_get_locked,"def _get_locked(self, name: str):
        
        value = ray.get(self._kv_store.get.remote(name))
        if value is None:
            raise NameEntryNotFoundError(f""No such entry: {name}"")
        return value",Get a value with lock already acquired.,"???  
Retrieve a value from a remote store, raising an error if not found.  
???"
3059,set_show_debug_borders,"def set_show_debug_borders(
    show: bool,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""show""] = show
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Overlay.setShowDebugBorders"",
        ""params"": params,
    }
    json = yield cmd_dict",Requests that backend shows debug borders on layers :param show: True for showing debug borders,???Enable or disable debug borders in overlay via command dictionary.???
3060,_convert_tools_for_llm,"def _convert_tools_for_llm(self, tools: List[Dict[str, Any]], tool_manager: ToolManager) -> List[Dict[str, Any]]:
        
        try:
            # Try to use tool manager's conversion if available
            if hasattr(tool_manager, ""convert_to_openai_tools""):
                return tool_manager.convert_to_openai_tools(tools)
            
            # Fallback to basic conversion
            return [
                {
                    ""type"": ""function"",
                    ""function"": {
                        ""name"": tool.get(""name"", ""unknown""),
                        ""description"": tool.get(""description"", """"),
                        ""parameters"": tool.get(""parameters"", {}),
                    },
                }
                for tool in tools
            ]
        except Exception as exc:
            logger.warning(f""Error converting tools: {exc}"")
            return []",Convert tools to LLM format.,"???Convert tool configurations for compatibility with language models, using a tool manager if available.???"
3061,combined_filter,"def combined_filter():
    
    return EvalFilterConfig(
        allowlist=EvalFilterEntryConfig(field={""repo"": [""iproute2"", ""vxlan""]}),  # Keep repos ""iproute2"" and ""vxlan""
        denylist=EvalFilterEntryConfig(field={""instance_id"": [""iproute2_99""]})  # Remove one specific instance
    )",Fixture for a combined allowlist & denylist filter config.,???Configure filters to include specific repositories and exclude a particular instance.???
3062,get_log_entries,"def get_log_entries(entity_id, entry_type=None, limit=10, db_path=""~/npcsh_history.db""):
    
    db_path = os.path.expanduser(db_path)
    with sqlite3.connect(db_path) as conn:
        query = ""SELECT entry_type, content, metadata, timestamp FROM npc_log WHERE entity_id = ?""
        params = [entity_id]
        
        if entry_type:
            query += "" AND entry_type = ?""
            params.append(entry_type)
        
        query += "" ORDER BY timestamp DESC LIMIT ?""
        params.append(limit)
        
        results = conn.execute(query, params).fetchall()
        
        return [
            {
                ""entry_type"": r[0],
                ""content"": json.loads(r[1]),
                ""metadata"": json.loads(r[2]) if r[2] else None,
                ""timestamp"": r[3]
            }
            for r in results
        ]",Get log entries for an NPC or team,???Retrieve and format log entries from a database based on entity and type criteria.???
3063,add_relation,"def add_relation(self, relation: Relation) -> None:
        
        related_nodes = f""{relation.source_id}->{relation.label}->{relation.target_id}""
        if relation.source_id in self.nodes and relation.target_id in self.nodes and related_nodes not in self.existing_relations:
            self.relations.add(relation)
            self.existing_relations.add(related_nodes)",Add a relation to the graph.,???Add unique relationship between nodes if both exist???
3064,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""HeadlessExperimental.disable"",
    }
    json = yield cmd_dict",Disables headless events for the target.,???Disables headless experimental features via command dictionary.???
3065,_prepare_list_documents_request,"def _prepare_list_documents_request(
        self,
        skip: int,
        limit: int,
        filters: Optional[Dict[str, Any]],
        folder_name: Optional[Union[str, List[str]]],
        end_user_id: Optional[str],
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        
        params = {
            ""skip"": skip,
            ""limit"": limit,
        }
        if folder_name:
            params[""folder_name""] = folder_name
        if end_user_id:
            params[""end_user_id""] = end_user_id
        data = filters or {}
        return params, data",Prepare request for list_documents endpoint,???Constructs request parameters for listing documents with optional filters and folder criteria.???
3066,write_pid_file,"def write_pid_file(pid):
    
    try:
        PID_FILE.write_text(str(pid))
        logger.debug(f""PID {pid} written to {PID_FILE}"")
    except IOError as e:
        logger.error(f""Error writing PID file: {e}"")
        sys.exit(1)",write the process id to the pid file,"???Write process ID to file, log success or handle write error.???"
3067,_init_prompt_blocks,"def _init_prompt_blocks(self):
        
        super()._init_prompt_blocks()
        self._prompt_blocks.update(
            {
                ""plan"": self._get_adaptive_plan_prompt(),
                ""final"": self._get_adaptive_final_prompt(),
                ""respond"": self._get_adaptive_respond_prompt(),
                ""reflect"": self._get_adaptive_reflect_prompt(),
                ""handle_input"": self._get_adaptive_handle_input_prompt(),
            }
        )",Initialize the prompt blocks with adaptive plan and final prompts.,??? Initialize and update adaptive prompt blocks for various stages in a process. ???
3068,check_connection,"def check_connection() -> str:
    
    try:
        metadata = make_jsonrpc_request(""get_metadata"")
        return f""Successfully connected to IDA Pro (open file: {metadata['module']})""
    except Exception as e:
        if sys.platform == ""darwin"":
            shortcut = ""Ctrl+Option+M""
        else:
            shortcut = ""Ctrl+Alt+M""
        return f""Failed to connect to IDA Pro! Did you run Edit -> Plugins -> MCP ({shortcut}) to start the server?""",Check if the IDA plugin is running,???Check connection to IDA Pro and provide success or failure feedback with instructions.???
3069,_expand_entities,"def _expand_entities(self, graph: Graph, seed_entities: List[Entity], hop_depth: int) -> List[Entity]:
        
        if hop_depth <= 1:
            return seed_entities

        # Create a set of entity IDs we've seen
        seen_entity_ids = {entity.id for entity in seed_entities}
        all_entities = list(seed_entities)

        # Create a map for fast entity lookup
        entity_map = {entity.id: entity for entity in graph.entities}

        # For each hop
        for _ in range(hop_depth - 1):
            new_entities = []

            # For each entity we've found so far
            for entity in all_entities:
                # Find connected entities through relationships
                connected_ids = self._get_connected_entity_ids(graph.relationships, entity.id, seen_entity_ids)

                # Add new connected entities
                for entity_id in connected_ids:
                    if target_entity := entity_map.get(entity_id):
                        new_entities.append(target_entity)
                        seen_entity_ids.add(entity_id)

            # Add new entities to our list
            all_entities.extend(new_entities)

            # Stop if no new entities found
            if not new_entities:
                break

        return all_entities",Expand entities by traversing relationships.,???Expand entities in a graph by traversing relationships up to a specified depth.???
3070,remove_graph_node,"def remove_graph_node(self, node_config: Union[AgentConfig, TaskConfig]):
        

        # this just removes the node, use `remove_graph_edge` to remove the edges
        def _get_node_name(node: ast.expr) -> str:
            if isinstance(node, ast.Str):
                return node.s
            raise ValidationError(f""Could not determine name of node `{node}` in {ENTRYPOINT}"")

        existing_nodes: list[ast.Call] = self.get_graph_nodes()
        for node in existing_nodes:
            source_node, target_node = node.args
            source = _get_node_name(source_node)
            if source == node_config.name:
                return self.remove_node(node)

        raise ValidationError(f""Node `{node_config.name}` not found in {ENTRYPOINT}"")",Remove a node and it's edges from the graph configuration.,???Remove a specified node from a graph configuration if it exists.???
3071,add_from_config,"def add_from_config(self, config: GuardrailConfig) -> None:
        

        for rule in config.rules:
            pattern = re.compile(rule.pattern)

            def _make_check(
                p: re.Pattern[str], r: GuardrailRule
            ) -> Callable[[str], Awaitable[str]]:
                if r.action is GuardrailAction.REDACT:

                    async def _check(text: str) -> str:
                        return p.sub(""[REDACTED]"", text)

                else:  # DENY or FLAG -> raise error on match

                    async def _check(text: str) -> str:
                        if p.search(text):
                            raise ValueError(f""Policy violation: {r.name}"")
                        return text

                return _check

            self.checks.append(_make_check(pattern, rule))",Add checks from a :class:`GuardrailConfig`.,???Configure text checks based on rules to redact or flag violations.???
3072,load_corpus,"def load_corpus(dataset_name=""corag/kilt-corpus"", split_name=""train""):
    
    print(f""Loading corpus from {dataset_name}..."")
    # corpus = []
    dataset = load_dataset(dataset_name, split=split_name)
    # for item in tqdm(dataset):
    #     corpus.append(item)
    return dataset",Load the corpus from dataset,???Load and return a specified dataset split for processing.???
3073,_create_agent_observation_path,"def _create_agent_observation_path(self, episode_id: int, step_num: int) -> str:
        
        return os.path.join(self.agent_observations_dir, f""env_obs_e{episode_id:03d}_s{step_num:04d}.png"")",Generates a unique file path for an observation image.,"???  
Constructs a file path for saving agent observations using episode and step identifiers.  
???"
3074,from_langchain_doc,"def from_langchain_doc(cls, doc: Document, document_id: str, user_id: str, embedding: List[float]) -> ""ChunkEmbedding"":
        
        # Convert Document to dict format
        doc_dict = {
            ""page_content"": doc.page_content,
            ""metadata"": doc.metadata
        }
        
        return cls(
            id=doc.id,  # Use the LangChain document's ID directly as string
            document_id=document_id,
            user_id=user_id,
            data=json.loads(json.dumps(doc_dict, cls=DateTimeEncoder)),
            embedding=embedding
        )",Create ChunkEmbedding from LangChain Document,???Convert a document to a chunk embedding with metadata and user details.???
3075,_extract_rich_text_markdown,"def _extract_rich_text_markdown(self, rich_text: List[dict]) -> str:
        
        if not rich_text:
            return """"

        result_parts = []
        for text_obj in rich_text:
            text = text_obj.get(""plain_text"", """")
            if not text:
                continue

            annotations = text_obj.get(""annotations"", {})
            href = text_obj.get(""href"")

            # Apply formatting
            if annotations.get(""bold""):
                text = f""**{text}**""
            if annotations.get(""italic""):
                text = f""*{text}*""
            if annotations.get(""strikethrough""):
                text = f""~~{text}~~""
            if annotations.get(""underline""):
                text = f""<u>{text}</u>""
            if annotations.get(""code""):
                text = f""`{text}`""

            # Handle links
            if href:
                text = f""[{text}]({href})""

            result_parts.append(text)

        return """".join(result_parts)",Extract rich text and convert to markdown formatting.,???Convert rich text objects into formatted Markdown string with annotations and links.???
3076,get_extra_context,"def get_extra_context(_) -> dict:  # pragma: no cover
        

        return {
            'page': 'pdf_highlight_overview',
            'get_next_overview_page': 'get_next_pdf_highlight_overview_page',
            'kind': 'highlights',
        }",get further information that needs to be passed to the template.,"???  
Function returns a dictionary with page and navigation context for PDF highlights.  
???"
3077,_overlay_mask,"def _overlay_mask(self, img, mask, output_path):
        
        try:
            mask_stacked = np.stack((mask,) * 3, axis=-1)
            fig, ax = plt.subplots(figsize=(10, 10))
            ax.axis(""off"")
            ax.imshow(img)
            ax.imshow(mask_stacked, alpha=0.4)
            # plt.savefig(""overlayed_plot.png"", bbox_inches=""tight"")
            plt.savefig(output_path, bbox_inches=""tight"")
            logger.info(""Overlayed segmentation mask saved as 'overlayed_plot.png'"")
            # return ""overlayed_plot.png""
            return True
        except Exception as e:
            logger.error(f""Error generating overlay: {e}"")
            raise e",Overlay the segmentation mask on the original image.,???Overlay a segmentation mask on an image and save the result to a file.???
3078,instances_to_coco_json,"def instances_to_coco_json(instances, img_id, output_logits=False):
    
    num_instance = len(instances)
    if num_instance == 0:
        return []

    boxes = instances.pred_boxes.tensor.numpy()
    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)
    boxes = boxes.tolist()
    scores = instances.scores.tolist()
    classes = instances.pred_classes.tolist()
    object_descriptions = instances.pred_object_descriptions.data
    if output_logits:
        logits = instances.logits.tolist()

    results = []
    for k in range(num_instance):
        result = {
            ""image_id"": img_id,
            ""category_id"": classes[k],
            ""bbox"": boxes[k],
            ""score"": scores[k],
            ""object_descriptions"": object_descriptions[k],
        }
        if output_logits:
            result[""logit""] = logits[k]

        results.append(result)
    return results",Add object_descriptions and logit (if applicable) to detectron2's instances_to_coco_json,???Convert object detection instances to COCO JSON format with optional logits inclusion.???
3079,_load_tools_if_needed,"def _load_tools_if_needed(self, tools_xml_path: str):
        
        global _tools_cache
        
        if tools_xml_path not in _tools_cache:
            logger.info(f""Loading tools from {tools_xml_path}"")
            _tools_cache[tools_xml_path] = self._load_tools_from_file(tools_xml_path)
        else:
            logger.debug(f""Using cached tools from {tools_xml_path}"")",Load tools from XML if not already cached.,"??? Load tools from XML if not cached, else use cache ???"
3080,print_dict,"def print_dict(d, logger, delimiter=0):
    
    for k, v in sorted(d.items()):
        if isinstance(v, dict):
            logger.info(""{}{} : "".format(delimiter * "" "", str(k)))
            print_dict(v, logger, delimiter + 4)
        elif isinstance(v, list) and len(v) >= 1 and isinstance(v[0], dict):
            logger.info(""{}{} : "".format(delimiter * "" "", str(k)))
            for value in v:
                print_dict(value, logger, delimiter + 4)
        else:
            logger.info(""{}{} : {}"".format(delimiter * "" "", k, v))",Recursively visualize a dict and indenting acrrording by the relationship of keys.,???Recursively log nested dictionary and list structures with indentation.???
3081,check_model,"def check_model(self, model: str, **kwargs) -> bool:
        
        try:
            client = self._get_client()
            try:
                models = client.models.list()
                for hyperbolic_model in models.data:
                    if hyperbolic_model.id == model:
                        return True
                return False
            except Exception as e:
                raise HyperbolicAPIError(f""Model check failed: {e}"")
                
        except Exception as e:
            raise HyperbolicAPIError(f""Model check failed: {e}"")",Check if a specific model is available,"???  
Verify model existence in client model list, handling exceptions.  
???"
3082,safe_execute,"def safe_execute(func):
    
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logging.error(f""Error in {func.__name__}: {str(e)}"", exc_info=True)
    return wrapper",Decorator to catch exceptions and log them,"???  
Wraps a function to log errors during its execution.  
???"
3083,get_message_attachments,"def get_message_attachments(message_id):
    
    try:
        command_history = CommandHistory(db_path)
        attachments = command_history.get_message_attachments(message_id)
        return jsonify({""attachments"": attachments, ""error"": None})
    except Exception as e:
        return jsonify({""error"": str(e)}), 500",Get all attachments for a message,"???Retrieve and return message attachments by ID, handling errors gracefully.???"
3084,generate_text,"def generate_text(self, prompt: str, **kwargs) -> str:
        
        merged_kwargs = {**self.default_kwargs, **kwargs}
        return generate_text(
            prompt=prompt,
            llm_provider=self.llm_provider,
            llm_model=self.llm_model,
            **merged_kwargs,
        )",Generate text using the session's default provider and model.,"???  
Facilitate text generation using a language model with customizable parameters.  
???"
3085,_get_empty_config,"def _get_empty_config(self) -> Dict[str, Any]:
        
        return {self.configure_key_name: {}}",Get empty config structure for Cline,???Returns a dictionary with a key for configuration initialized to an empty dictionary.???
3086,to_px,"def to_px(val: float) -> str:
	
	s = f'{val:.1f}'.rstrip('0').rstrip('.')
	return f'{s}px'","Convert float to px string, e.g. 42.0 -> '42px'.",???Convert a floating-point number to a pixel string representation.???
3087,retrieve_not_updated_memory,"def retrieve_not_updated_memory(self):
        
        if not self.retrieve_not_updated_top_k:
            return

        filter_dict = {
            ""user_name"": self.user_name,
            ""target_name"": self.target_name,
            ""store_status"": StoreStatusEnum.VALID.value,
            ""memory_type"": [MemoryTypeEnum.OBSERVATION.value, MemoryTypeEnum.OBS_CUSTOMIZED.value],
            ""obs_updated"": 0,
        }
        nodes: List[MemoryNode] = self.memory_store.retrieve_memories(top_k=self.retrieve_not_updated_top_k,
                                                                      filter_dict=filter_dict)
        self.memory_manager.set_memories(NOT_UPDATED_NODES, nodes)",Retrieves top-K not updated memories based on the query and stores them in the memory handler.,???Retrieve and store non-updated memory nodes based on specific criteria.???
3088,default_eval_config,"def default_eval_config(mock_evaluator):
    
    eval_config = EvalConfig()
    eval_config.general.dataset = EvalDatasetJsonConfig()
    eval_config.general.output = EvalOutputConfig()
    eval_config.general.max_concurrency = 1
    eval_config.general.output.dir = Path("".tmp/aiq/examples/mock/"")
    eval_config.evaluators = {""MockEvaluator"": mock_evaluator}

    return eval_config",Fixture for default evaluation configuration.,"???  
Configure evaluation settings with mock evaluator and output directory.  
???"
3089,get_memories,"def get_memories(self) -> str:
        
        entities = self.db.retrieve_recent_entities(
            days=3650
        )  # Retrieve entities from the last 10 years
        if not entities:
            return ""No memories found.""

        memory_parts = [""## All Stored Memories""]

        for entity, total, user_count, llm_count in entities:
            memory_parts.append(
                f""- **{entity}**: {total} mentions (User: {user_count}, AI: {llm_count})""
            )

        return ""\n"".join(memory_parts)",Retrieve and format all stored memories.,???Retrieve and format a decade's worth of entity memories from a database.???
3090,download_pdf,"def download_pdf(url):
    
    print(f'Downloading PDF from {url}...')
    response = requests.get(url)
    response.raise_for_status()  # Raise an exception for HTTP errors
    return response.content",Download PDF from URL and return as bytes.,"???  
Fetch and return PDF content from a specified URL, handling errors.  
???"
3091,stop,"def stop(self):
        
        if not self.is_tracing:
            print(""Tracing is not active."")
            return

        # Write current_trace to a JSON file
        final_traces = {
            ""project_id"": self.user_detail[""project_id""],
            ""trace_id"": str(uuid.uuid4()),
            ""session_id"": None,
            ""trace_type"": ""llamaindex"",
            ""metadata"": self.user_detail[""trace_user_detail""][""metadata""],
            ""pipeline"": self.user_detail[""trace_user_detail""][""pipeline""],
            ""traces"": self.json_event_handler.current_trace,

        }

        with open('new_llamaindex_traces.json', 'w') as f:
            json.dump([final_traces], f, default=str, indent=4)
        
        llamaindex_instrumentation_data = extract_llama_index_data([final_traces])
        converted_back_to_callback = convert_llamaindex_instrumentation_to_callback(llamaindex_instrumentation_data)

         # Just indicate tracing is stopped
        self.is_tracing = False
        print(""Tracing stopped."")
        return converted_back_to_callback",Stop tracing by unregistering handlers.,"???Stop tracing, save trace data to JSON, and convert for callback use.???"
3092,_cache_s3_file,"def _cache_s3_file(s3_path: str, local_cache_dir: str):
    
    bucket, key = parse_s3_path(s3_path)

    # Define the local file path
    local_file_path = os.path.join(local_cache_dir, bucket + ""__"" + key.replace(""/"", ""_""))
    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
    lock_file = f""{local_file_path}.lock""

    # Use a file lock to prevent concurrent writes
    with FileLock(lock_file):
        if not os.path.exists(local_file_path):
            logger.info(f""Downloading {s3_path} to {local_file_path}"")
            s3_client = boto3.client(""s3"", aws_access_key_id=os.getenv(""DS_AWS_ACCESS_KEY_ID""), aws_secret_access_key=os.getenv(""DS_AWS_SECRET_ACCESS_KEY""))
            s3_client.download_file(bucket, key, local_file_path)
        else:
            pass
            # logger.info(f""File {local_file_path} already exists, skipping download."")

    return local_file_path","Downloads an S3 object to a local cache directory, ensuring no two writers corrupt the same file.",???Cache S3 file locally with locking to prevent concurrent downloads.???
3093,_simulate_student_answer,"def _simulate_student_answer(self, question: Dict[str, Any]) -> Tuple[str, bool]:
        
        # Get topic and difficulty
        topic = question.get(""topic"")
        difficulty = question.get(""difficulty"")

        # Get student proficiency for this topic
        proficiency = self.student_metrics[""topic_accuracies""].get(topic, 0.5)

        # Adjust probability of correct answer based on difficulty
        difficulty_factor = {
            ""easy"": 0.3,  # +30% chance of getting it right
            ""medium"": 0.0,  # no adjustment
            ""hard"": -0.2,  # -20% chance of getting it right
        }

        # Calculate probability of correct answer
        correct_prob = proficiency + difficulty_factor.get(difficulty, 0.0)
        correct_prob = max(0.1, min(0.9, correct_prob))  # Clamp between 0.1 and 0.9

        # Determine if answer is correct
        import random

        is_correct = random.random() < correct_prob

        if is_correct:
            # Return correct answer
            return question[""correct_answer""], True
        else:
            # Return random incorrect answer
            options = list(question[""options""].keys())
            options.remove(question[""correct_answer""])
            return random.choice(options), False",Simulate a student answering the question based on their profile.,???Simulates student answer accuracy based on topic proficiency and question difficulty.???
3094,get_auth_headers,"def get_auth_headers(self) -> Dict[str, str]:
        
        headers = {""Content-Type"": ""application/json""}
        if self.auth_token:
            headers[""Authorization""] = f""Bearer {self.auth_token}""
        return headers",Get headers with authorization if token is available.,"???  
Generate HTTP headers with optional authorization token.  
???"
3095,_calculate_pe_ratio,"def _calculate_pe_ratio(self, symbol: str) -> float:
        
        try:
            stock = yf.Ticker(symbol)
            info = stock.info
            pe_ratio = info.get(""trailingPE"", info.get(""forwardPE"", None))
            if pe_ratio is None:
                raise ValueError(""PE ratio data not available"")
            return float(pe_ratio)
        except Exception as e:
            raise ValueError(f""Failed to calculate PE ratio: {str(e)}"")",Calculate Price-to-Earnings ratio for given stock symbol,"???Calculate stock's PE ratio using Yahoo Finance data, handling exceptions.???"
3096,get_diagnostics_report,"def get_diagnostics_report(self) -> Dict[str, Any]:
        
        if not self._enabled:
            return {""diagnostics_enabled"": False}

        # Calculate averages for performance data
        performance_summary = {}
        for func, data in self._performance_data.items():
            count = cast(int, data[""count""])
            if count > 0:
                total_time = cast(float, data[""total_time""])
                avg_time = total_time / count
                performance_summary[func] = {
                    ""count"": count,
                    ""avg_time"": avg_time,
                    ""min_time"": data[""min_time""],
                    ""max_time"": data[""max_time""],
                    ""last_call"": data[""last_call""],
                    ""total_time"": total_time,
                }

        return {
            ""diagnostics_enabled"": True,
            ""uptime"": self.uptime,
            ""start_time"": self._start_time,
            ""performance"": performance_summary,
            ""errors"": dict(self._error_counts),
            ""requests"": dict(self._request_counts),
        }",Get a report of all diagnostics data.,???Generate a diagnostics report summarizing system performance and error metrics.???
3097,get_job,"def get_job(self, job_id: str) -> Optional[Dict[str, Any]]:
        
        return self.jobs.find_one({""job_id"": job_id})",Retrieve a job by ID.,???Retrieve a job record by its identifier from the job database.???
3098,validate_arguments,"def validate_arguments(self, **kwargs) -> bool:
        
        try:
            required_args = [arg.name for arg in self.arguments if arg.required]
            for arg in required_args:
                if arg not in kwargs:
                    raise ValueError(f""Missing required argument: {arg}"")
                    
            if len(kwargs['symbols']) != len(kwargs['asset_types']):
                raise ValueError(""Number of symbols must match number of asset types"")
                
            valid_asset_types = ['crypto', 'stock', 'index']
            invalid_types = set(kwargs['asset_types']) - set(valid_asset_types)
            if invalid_types:
                raise ValueError(f""Invalid asset types: {invalid_types}"")
                
            return True
        except Exception as e:
            logger.error(f""Argument validation error: {e}"")
            return False",Validate the provided arguments.,???Validate input arguments for required presence and correct asset type matching???
3099,parse_get_spreadsheet_response,"def parse_get_spreadsheet_response(api_response: dict) -> dict:
    
    properties = api_response.get(""properties"", {})
    sheets = [parse_sheet(sheet) for sheet in api_response.get(""sheets"", [])]

    return {
        ""title"": properties.get(""title"", """"),
        ""spreadsheetId"": api_response.get(""spreadsheetId"", """"),
        ""spreadsheetUrl"": api_response.get(""spreadsheetUrl"", """"),
        ""sheets"": sheets,
    }",Parse the get spreadsheet Google Sheets API response into a structured dictionary.,???Extracts spreadsheet details and sheets from API response dictionary.???
3100,get_dummy_state_dict,"def get_dummy_state_dict(state_dict, dummy_dict={}):
    
    for k in state_dict.keys():
        if isinstance(state_dict[k], dict):
            dummy_dict[k] = get_dummy_state_dict(state_dict[k], {})
        elif isinstance(state_dict[k], torch.Tensor):
            dummy_dict[k] = torch.randn(state_dict[k].shape)
        else:
            dummy_dict[k] = state_dict[k]
    return dummy_dict",Recursively get the dummy state dict by replacing tensors with random ones of the same shape.,???Generate a dummy state dictionary with random tensors for nested structures.???
3101,find_element_with_text,"def find_element_with_text(text: str) -> str:
    
    print(f""🔍 Finding element with text: '{text}'"")  # Added print statement

    try:
        if element:
            return ""Element found.""
        else:
            return ""Element not found.""
    except selenium.common.exceptions.NoSuchElementException:
        return ""Element not found.""
    except selenium.common.exceptions.ElementNotInteractableException:
        return ""Element not interactable, cannot click.""",Finds an element on the page with the given text.,"???Locate web element by text, handle exceptions for absence or non-interaction.???"
3102,get_pull_safe,"def get_pull_safe(self, number: int) -> PullRequest | None:
        
        try:
            pr = self.repo.get_pull(number)
            return pr
        except UnknownObjectException as e:
            return None
        except Exception as e:
            logger.warning(f""Error getting PR by number: {number}\n\t{e}"")
            return None",Returns a PR by its number,"???Retrieve pull request by number, handle exceptions gracefully.???"
3103,get_original_url,"def get_original_url(pdf_hash: str, db_path: str) -> Optional[str]:
    
    if not pdf_hash:
        return None

    try:
        sqlite_db_path = os.path.expanduser(db_path)
        if not os.path.exists(sqlite_db_path):
            print(f""SQLite database not found at {sqlite_db_path}"")
            return None

        conn = sqlite3.connect(sqlite_db_path)
        cursor = conn.cursor()

        cursor.execute(""SELECT uri FROM pdf_mapping WHERE pdf_hash = ?"", (pdf_hash,))
        result = cursor.fetchone()

        conn.close()

        if result:
            return result[0]
        return None
    except Exception as e:
        print(f""Error looking up URL for PDF hash {pdf_hash}: {e}"")
        return None",Look up the original URL for a PDF hash in the SQLite database.,???Retrieve original URL from database using PDF hash as a lookup key.???
3104,create_summary_agent_prompt,"def create_summary_agent_prompt(agent: ConversableAgent, messages: list[dict[str, Any]]) -> str:
            
            update_ingested_documents()

            documents_to_ingest: list[Ingest] = cast(list[Ingest], agent.context_variables.get(""DocumentsToIngest"", []))
            queries_to_run: list[Query] = cast(list[Query], agent.context_variables.get(""QueriesToRun"", []))

            system_message = (
                ""You are a summary agent and you provide a summary of all completed tasks and the list of queries and their answers. ""
                ""Output two sections: 'Ingestions:' and 'Queries:' with the results of the tasks. Number the ingestions and queries. ""
                ""If there are no ingestions output 'No ingestions', if there are no queries output 'No queries' under their respective sections. ""
                ""Don't add markdown formatting. ""
                ""For each query, there is one answer and, optionally, a list of citations.""
                ""For each citation, it contains two fields: 'text_chunk' and 'file_path'.""
                ""Format the Query and Answers and Citations as 'Query:\nAnswer:\n\nCitations:'. Add a number to each query if more than one. Use the context below:\n""
                ""For each query, output the full citation contents and list them one by one,""
                ""format each citation as '\nSource [X] (chunk file_path here):\n\nChunk X:\n(text_chunk here)' and mark a separator between each citation using '\n#########################\n\n'.""
                ""If there are no citations at all, DON'T INCLUDE ANY mention of citations.\n""
                f""Documents ingested: {documents_to_ingest}\n""
                f""Documents left to ingest: {len(documents_to_ingest)}\n""
                f""Queries left to run: {len(queries_to_run)}\n""
                f""Query and Answers and Citations: {queries_to_run}\n""
            )

            return system_message",Create the summary agent prompt and updates ingested documents,???Generate a summary of completed tasks and queries with results and citations.???
3105,to_debug_report,"def to_debug_report(self) -> str:
        
        return (
            ""Python SDK Debug Report:\n""
            ""OS: {env}\n""
            ""Python Version: {pyversion}\n""
            ""Version of the API: 0.1.6\n""
            ""SDK Package Version: 1.0.0"".format(env=sys.platform, pyversion=sys.version)
        )",Gets the essential information for debugging.,???Generate a formatted debug report with system and version details.???
3106,_log_chat_finished,"def _log_chat_finished(self, model: Optional[str] = None) -> None:
        
        data = {
            ""progress_action"": ProgressAction.READY,
            ""model"": model,
            ""agent_name"": self.name,
        }
        self.logger.debug(""Chat finished"", data=data)",Log a chat finished event,???Log chat completion with model and agent details for debugging???
3107,calculate,"def calculate(expression: str) -> str:
    
    if not all(char in ""0123456789+-*/(). "" for char in expression):
        return ""Error: invalid characters in expression""
    try:
        return str(round(float(eval(expression, {""__builtins__"": None}, {})), 2))
    except Exception as e:
        track_event(ExceptionEvent(exception_type=type(e).__name__,
                                   error_message=str(e)))
        return f""Error: {e}""",Calculate the result of a mathematical expression.,"???  
Evaluates mathematical expressions safely, returning results or error messages.  
???"
3108,typescript_skill_func,"def typescript_skill_func(codebase: CodebaseType):
        
        # for each file in the codebase
        for file in codebase.files:
            # skip files that do not have default exports
            if not file.default_exports:
                continue
            # list to store non-default exported components
            non_default_exported_components = []
            # get the names of the default exports
            default_exports = [export.name for export in file.default_exports]
            # for each function in the file
            for function in file.functions:
                # if the function is a JSX component and is not a default export
                if function.is_jsx and function.name not in default_exports:
                    # add the function to the list of non-default exported components
                    non_default_exported_components.append(function)
            # if there are non-default exported components
            if non_default_exported_components:
                for component in non_default_exported_components:
                    # create a new file in the same directory as the original file
                    component_dir = Path(file.filepath).parent
                    # create a new file path for the component
                    new_file_path = component_dir / f""{component.name}.tsx""
                    # if the file does not exist create it
                    new_file = codebase.create_file(str(new_file_path))
                    # add an import for React
                    new_file.add_import('import React from ""react"";')
                    # move the component to the new file
                    component.move_to_file(new_file)",Moves all JSX components that are not exported by default into a new file that is in the same directory as the original file.,???Refactor non-default JSX components into separate TypeScript files within a codebase.???
3109,setup_bash_state,"def setup_bash_state():
    

    # Update CONFIG immediately
    CONFIG.update(3, 55, 5)

    # Create new BashState with mode
    home_dir = os.path.expanduser(""~"")
    bash_state = BashState(Console(), home_dir, None, None, None, ""wcgw"", False, None)
    server.BASH_STATE = bash_state

    try:
        yield server.BASH_STATE
    finally:
        try:
            bash_state.cleanup()
        except Exception as e:
            print(f""Error during cleanup: {e}"")
        server.BASH_STATE = None",Setup BashState for each test,???Initialize and manage a BashState object with cleanup in a server environment.???
3110,deleteByFields,"def deleteByFields(self, field: str, values: list):
        
        if field not in self.fields:
            raise ValueError(f""field name `{field}` is illegal"")

        if not values:
            return True

        placeHolders = ','.join(['?']*len(values))
        sql = f""DELETE FROM {self.table} WHERE {field} IN ({placeHolders})""
        self.query.prepare(sql)

        for value in values:
            self.query.addBindValue(value)

        return self.query.exec()",delete multi records based on the value of a field,???Delete records from a table based on specified field values.???
3111,_ensure_results_table,"def _ensure_results_table(self, table_name):
        
        db_path = ""~/npcsh_history.db""
        with sqlite3.connect(os.path.expanduser(db_path)) as conn:
            conn.execute(f)
            conn.commit()",Ensure results table exists,???Ensure the existence of a results table in a SQLite database.???
3112,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""LayerTree.enable"",
    }
    json = yield cmd_dict",Enables compositing tree inspection.,???Enable LayerTree feature by sending a command and awaiting response???
3113,create_session,"def create_session(self, session_name):
        
        if not session_name:
            # Get existing session names
            existing_sessions = self.get_tmux_sessions()
            
            # Find the next available number
            counter = 0
            while str(counter) in existing_sessions:
                counter += 1
                
            session_name = str(counter)
            
        try:
            # Clean the session name (replace spaces with underscores)
            clean_name = session_name.strip().replace("" "", ""_"")
            
            # Create session
            subprocess.run(
                [""tmux"", ""new-session"", ""-d"", ""-s"", clean_name],
                check=True
            )
            
            # Refresh the session list
            self.refresh_sessions()
            
            # Clear entry
            self.session_name_entry.set_text("""")
            
            # Launch a terminal and attach to this session
            terminal_cmd = self.get_terminal_command(f""tmux attach-session -t {clean_name}"")
            exec_shell_command_async(terminal_cmd)
            
            # Close manager
            self.close_manager()
            
        except Exception as e:
            print(f""Error creating tmux session: {e}"")",Create a new tmux session,"???Create a new tmux session with a unique name, handling errors and updating the session list.???"
3114,_get_optimal_worker_count,"def _get_optimal_worker_count(self) -> int:
        
        try:
            cpu_cores = cpu_count()
        except Exception as e:
            warnings.warn(
                f""Proceeding with 1 worker. Error calculating optimal worker count: {e}""
            )
            return 1",Get the optimal number of workers for parallel processing.,"???Determine optimal worker count based on available CPU cores, defaulting to one on error.???"
3115,log,"def log(self, names=[], normalizer=1.0, reset=True):
        
        if len(names) == 0:
            names = self.timers.keys()
        assert normalizer > 0.0
        string = ""time (ms)""
        for name in names:
            cnt = self.timers[name].cnt
            if cnt == 0:
                continue
            elapsed_time = self.timers[name].elapsed(reset=reset) * 1000.0 / normalizer
            string += "" | {}: {:.2f} {} {:.2f}"".format(
                name, elapsed_time, cnt, elapsed_time / cnt
            )
        logger.info(string)",Log a group of timers.,"???Log elapsed time for specified timers, normalizing and optionally resetting them.???"
3116,span_context,"def span_context(span):
    
    try:
        yield span
    except Exception as e:
        logger.exception(f""Error in span operation: {e}"")
        if span:
            span.set_status(Status(StatusCode.ERROR))
            span.record_exception(e)
    finally:
        if span and span.is_recording():
            span.end()",Context manager for OpenTelemetry span operations,"???Manage and log exceptions during span operations, ensuring proper status and closure.???"
3117,uninstall_gateway,"def uninstall_gateway(
    server: StdioServer,
) -> StdioServer:
    
    if not is_invariant_installed(server):
        raise MCPServerIsNotGateway()

    assert isinstance(server.args, list), ""args is not a list""
    args, unknown = parser.parse_known_args(server.args[2:])
    if server.env is None:
        new_env = None
    else:
        new_env = {
            k: v
            for k, v in server.env.items()
            if k != ""INVARIANT_API_KEY"" and k != ""INVARIANT_API_URL"" and k != ""GUARDRAILS_API_URL""
        } or None
    assert args.exec is not None, ""exec is None""
    assert args.exec, ""exec is empty""
    return StdioServer(
        command=args.exec[0],
        args=args.exec[1:],
        env=new_env,
    )",Uninstall the gateway for the given server.,???Remove gateway configuration from server and return updated server instance???
3118,_build_rule,"def _build_rule(self, filter_string, **kwargs) -> None:
        
        rule = [{""value"":filter_string }]
        payload = {""add"": rule}
        return self._make_request('post', 'tweets/search/stream/rules', use_bearer=True, json=payload)",Build a rule for the stream,???Constructs and sends a request to add a tweet filter rule.???
3119,extract_frames,"def extract_frames(
    vid_input: str,
    output_temp_dir: str = TEMP_VIDEO_FRAMES_DIR,
    start_number: int = 0,
    clean=True
):
    
    if clean:
        clean_temp_dir(temp_dir=output_temp_dir)

    os.makedirs(output_temp_dir, exist_ok=True)
    output_path = os.path.join(output_temp_dir, ""%05d.jpg"")

    command = [
        'ffmpeg',
        '-loglevel', 'error',
        '-y',  # Enable overwriting
        '-i', vid_input,
        '-qscale:v', '2',
        '-vf', f'scale=iw:ih',
        '-start_number', str(start_number),
        f'{output_path}'
    ]

    try:
        subprocess.run(command, check=True)
        print(f""Video frames extracted to \""{os.path.normpath(output_temp_dir)}\"""")
    except subprocess.CalledProcessError as e:
        print(""Error occurred while extracting frames from the video"")
        raise RuntimeError(f""An error occurred: {str(e)}"")

    return get_frames_from_dir(output_temp_dir)",Extract frames as jpg files and save them into output_temp_dir.,"???Extracts video frames to a specified directory using FFmpeg, optionally cleaning the directory first.???"
3120,mutation_check,"def mutation_check(func):
    

    @wraps(func)
    async def wrapper(*args, **kwargs):
        readonly = os.environ.get('DDB-MCP-READONLY', '').lower()
        if readonly in ('true', '1', 'yes'):  # treat these as true
            return {'error': 'Mutation not allowed: DDB-MCP-READONLY is set to true.'}
        return await func(*args, **kwargs)

    return wrapper",Decorator to block mutations if DDB-MCP-READONLY is set to true.,"???  
Decorator restricts function execution based on environment variable for read-only mode.  
???"
3121,load,"def load(cls, project_root: Path | str) -> Self:
        
        project_root = Path(project_root)
        yaml_path = project_root / cls.rel_path_to_project_yml()
        if not yaml_path.exists():
            raise FileNotFoundError(f""Project configuration file not found: {yaml_path}"")
        with open(yaml_path, encoding=""utf-8"") as f:
            yaml_data = yaml.safe_load(f)
        if ""project_name"" not in yaml_data:
            yaml_data[""project_name""] = project_root.name
        return cls._from_yml_data(yaml_data)",Load a ProjectConfig instance from the path to the project root.,"???Load and parse project configuration from YAML file, handling defaults.???"
3122,_parse_question_response,"def _parse_question_response(self, response: str) -> Dict[str, Any]:
        
        # Try to find JSON content within the response
        try:
            # Extract just the JSON part if there's additional text
            start = response.find(""{"")
            end = response.rfind(""}"") + 1
            if start >= 0 and end > start:
                json_str = response[start:end]
                return json.loads(json_str)
            else:
                return json.loads(response)  # Try parsing the whole response
        except json.JSONDecodeError:
            raise ValueError(""Could not parse JSON from LLM response"")",Parse the LLM response to extract question data.,"???Extract and parse JSON data from a string response, handling errors.???"
3123,get_completion_content,"def get_completion_content(completion) -> str:
    
    if isinstance(completion, str):
        return completion
    elif isinstance(completion, dict):
        if ""content"" in completion:
            return completion[""content""]
        elif isinstance(completion.get(""message"", {}), dict):
            return completion[""message""].get(""content"", """")
    elif isinstance(completion, list) and len(completion) > 0:
        if isinstance(completion[0], dict) and ""content"" in completion[0]:
            return completion[0][""content""]

    logger.warning(f""Could not extract content from completion: {completion}"")
    return str(completion)",Extract content from completion in various formats.,"???Extracts content from various data structures, logging a warning if unsuccessful.???"
3124,set_xhr_breakpoint,"def set_xhr_breakpoint(url: str) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""url""] = url
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DOMDebugger.setXHRBreakpoint"",
        ""params"": params,
    }
    json = yield cmd_dict",Sets breakpoint on XMLHttpRequest.,???Establishes a breakpoint for XMLHttpRequests matching a specified URL.???
3125,_format_tools,"def _format_tools(self, tools: List[FastTool]) -> str:
        
        if not tools:
            return ""No tool information provided.""

        tool_descriptions = []
        for tool in tools:
            desc = f""- {tool.name}: {tool.description}""
            tool_descriptions.append(desc)

        return ""\n"".join(tool_descriptions)",Format a list of tools into a readable string.,??? Formats a list of tools into a descriptive string output. ???
3126,get_logs,"def get_logs():
    
    try:
        # Get the log file path
        log_file = APP_LOG_FILES.get(""whisparr"")
        
        if not log_file or not os.path.exists(log_file):
            return jsonify({""success"": False, ""message"": ""Log file not found""}), 404
            
        # Read the log file (last 200 lines)
        with open(log_file, 'r') as f:
            lines = f.readlines()
            log_content = ''.join(lines[-200:])
            
        return jsonify({""success"": True, ""logs"": log_content})
    except Exception as e:
        error_message = f""Error fetching Whisparr logs: {str(e)}""
        whisparr_logger.error(error_message)
        traceback.print_exc()
        return jsonify({""success"": False, ""message"": error_message}), 500",Get the log file for Whisparr,"???Fetch and return the last 200 lines of a specified log file, handling errors.???"
3127,check_api_quota,"def check_api_quota(api_key, search_engine_id):
    
    params = {
        ""key"": api_key,
        ""cx"": search_engine_id,
        ""q"": ""test"",  # Minimal query
        ""num"": 1,  # Request only 1 result
    }

    try:
        response = requests.get(url, params=params, timeout=10)

        # Check for quota errors specifically
        if response.status_code == 429:
            return (
                False,
                ""API quota exceeded. Google PSE has a limit of 100 requests per day on the free tier."",
            )
        elif response.status_code != 200:
            return False, f""API error: {response.status_code} - {response.text}""

        # If we get here, the API is working
        return True, None

    except Exception as e:
        return False, f""Error checking API: {str(e)}""",Make a direct minimal request to check API quota status,???Check if API quota is exceeded by sending a minimal query request.???
3128,do_GET,"def do_GET(self) -> None:  # noqa: N802
        
        global \
            authorization_code, \
            callback_received, \
            callback_error, \
            authorization_state

        # Parse the query parameters from the URL
        query = urllib.parse.urlparse(self.path).query
        params = urllib.parse.parse_qs(query)

        if ""error"" in params:
            callback_error = params[""error""][0]
            callback_received = True
            self._send_response(f""Authorization failed: {callback_error}"")
            return

        if ""code"" in params:
            authorization_code = params[""code""][0]
            if ""state"" in params:
                authorization_state = params[""state""][0]
            callback_received = True
            self._send_response(
                ""Authorization successful! You can close this window now.""
            )
        else:
            self._send_response(
                ""Invalid callback: Authorization code missing"", status=400
            )",Handle GET requests (OAuth callback).,"???Handle OAuth callback, updating authorization status and responding to client???"
3129,_validate_inference_results,"def _validate_inference_results(
    yolox_results: Any,
    paddle_results: Any,
    valid_arrays: List[Any],
    valid_images: List[str],
) -> Tuple[List[Any], List[Any]]:
    
    if not isinstance(yolox_results, list) or not isinstance(paddle_results, list):
        logger.warning(
            ""Unexpected result types from inference clients: yolox_results=%s, paddle_results=%s. ""
            ""Proceeding with available results."",
            type(yolox_results).__name__,
            type(paddle_results).__name__,
        )
        if not isinstance(yolox_results, list):
            yolox_results = [None] * len(valid_arrays)
        if not isinstance(paddle_results, list):
            paddle_results = [(None, None)] * len(valid_images)

    if len(yolox_results) != len(valid_arrays):
        raise ValueError(f""Expected {len(valid_arrays)} yolox results, got {len(yolox_results)}"")
    if len(paddle_results) != len(valid_images):
        raise ValueError(f""Expected {len(valid_images)} paddle results, got {len(paddle_results)}"")

    return yolox_results, paddle_results",Validate that both inference results are lists and have the expected lengths.,???Validate and align inference results with expected data structures.???
3130,create_lambda_resources,"def create_lambda_resources(
    account_id: str = '',
    region: str = '',
    lambda_info_file: str = os.path.join(RESOURCES_DIR, ""ActionGroups"", ""lambda_functions_info.json"")
) -> Dict:
    
    try:
        # Try different import approaches
        try:
            from ActionGroups.create_action_group import create_all_lambda_functions
        except ImportError:
            logger.error(""Could not import create_action_group module"")
            raise
        
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(lambda_info_file), exist_ok=True)
        
        logger.info(""Starting Lambda functions creation..."")
        lambda_functions = create_all_lambda_functions(
            region=region,
            account_id=account_id,
            output_file=lambda_info_file
        )
        logger.info(""Successfully created Lambda functions"")
        return lambda_functions
    except Exception as e:
        logger.error(f""Error creating Lambda resources: {str(e)}"")
        raise",Create Lambda functions and save their information.,"???Create AWS Lambda functions and log results, handling import and file errors???"
3131,claude_projects_json_content,"def claude_projects_json_content():
    
    return [
        {
            ""uuid"": ""test-uuid"",
            ""name"": ""Test Project"",
            ""created_at"": ""2025-01-05T20:55:32.499880+00:00"",
            ""updated_at"": ""2025-01-05T20:56:39.477600+00:00"",
            ""prompt_template"": ""# Test Prompt\n\nThis is a test prompt."",
            ""docs"": [
                {
                    ""uuid"": ""doc-uuid-1"",
                    ""filename"": ""Test Document"",
                    ""content"": ""# Test Document\n\nThis is test content."",
                    ""created_at"": ""2025-01-05T20:56:39.477600+00:00"",
                },
                {
                    ""uuid"": ""doc-uuid-2"",
                    ""filename"": ""Another Document"",
                    ""content"": ""# Another Document\n\nMore test content."",
                    ""created_at"": ""2025-01-05T20:56:39.477600+00:00"",
                },
            ],
        }
    ]",Sample Claude projects data for testing.,???Generate a JSON structure representing a project with associated documents.???
3132,make_full_prompt,"def make_full_prompt(tasks, additional_instructions, processing_type=""sequential""):
    
    prompt = ''
    if processing_type == 'sequential':
        prompt += 
    elif processing_type == ""allow_parallel"":
        prompt += 

    for task_num, task in enumerate(tasks, 1):
        prompt += f""Task {task_num}. {task}\n""

    prompt += ""\nBefore returning the final answer, review whether you have achieved the expected output for each task.""

    if additional_instructions:
        prompt += f""\n{additional_instructions}""

    return prompt",Build a full prompt from tasks and instructions.,???Generate a task-oriented prompt with optional instructions and processing type.???
3133,commit_changes,"def commit_changes(self, message: str, verify: bool = False) -> bool:
        
        staged_changes = self.git_cli.git.diff(""--staged"")
        if staged_changes:
            if self.bot_commit and (info := self._get_username_email()):
                user, email = info
                message += f""\n\n Co-authored-by: {user} <{email}>""
            commit_args = [""-m"", message]
            if self.bot_commit:
                commit_args.append(f""--author='{CODEGEN_BOT_NAME} <{CODEGEN_BOT_EMAIL}>'"")
            if not verify:
                commit_args.append(""--no-verify"")
            self.git_cli.git.commit(*commit_args)
            return True
        else:
            logger.info(""No changes to commit. Do nothing."")
            return False",Returns True if a commit was made and False otherwise.,???Commit staged changes with optional verification and bot authoring.???
3134,chunk_by_lines,"def chunk_by_lines(
    path: str = typer.Argument(..., help=""Path to the local repository.""),
    file_path: str = typer.Argument(..., help=""Relative path to the file within the repository.""),
    max_lines: int = typer.Option(50, ""--max-lines"", ""-n"", help=""Maximum lines per chunk.""),
    output: Optional[str] = typer.Option(None, ""--output"", ""-o"", help=""Output to JSON file instead of stdout.""),
):
    
    from kit import Repository

    try:
        repo = Repository(path)
        chunks = repo.chunk_file_by_lines(file_path, max_lines)

        if output:
            Path(output).write_text(json.dumps(chunks, indent=2))
            typer.echo(f""File chunks written to {output}"")
        else:
            for i, chunk in enumerate(chunks, 1):
                typer.echo(f""--- Chunk {i} ---"")
                typer.echo(chunk)
                if i < len(chunks):
                    typer.echo()
    except Exception as e:
        typer.secho(f""Error: {e}"", fg=typer.colors.RED)
        raise typer.Exit(code=1)",Chunk a file's content by line count.,???Split a file into line-based chunks and optionally save as JSON.???
3135,_save_lora_weights,"def _save_lora_weights(
        self,
        directory: str,
        transformer: torch.nn.Module,
        transformer_state_dict: Optional[Dict[str, torch.Tensor]] = None,
        norm_state_dict: Optional[Dict[str, torch.Tensor]] = None,
        scheduler: Optional[SchedulerType] = None,
        metadata: Optional[Dict[str, str]] = None,
    ) -> None:
        
        raise NotImplementedError(
            f""ControlModelSpecification::save_lora_weights is not implemented for {self.__class__.__name__}""
        )",Save the lora state dicts of the model to the given directory.,???Function placeholder for saving model weights with error on unimplemented method???
3136,button,"def button(
    label: str,
    variant: str = ""default"",
    disabled: bool = False,
    loading: bool = False,
    size: float = 1.0,
    component_id: str | None = None,
    **kwargs
) -> ComponentReturn:
    
    service = PreswaldService.get_instance()

    # Get current state or use default
    current_value = service.get_component_state(component_id)
    if current_value is None:
        current_value = False

    component = {
        ""type"": ""button"",
        ""id"": component_id,
        ""label"": label,
        ""variant"": variant,
        ""disabled"": disabled,
        ""loading"": loading,
        ""size"": size,
        ""value"": current_value,
        ""onClick"": True,  # Always enable click handling
    }

    return ComponentReturn(current_value, component)",Create a button component that returns True when clicked.,???Create a customizable button component with state management.???
3137,_create_error_chunk,"def _create_error_chunk(self, error_message: str) -> Dict[str, Any]:
        
        return {
            ""type"": ""error"",
            ""message"": error_message,
            ""tool_call"": {
                ""name"": ""report_error"",
                ""args"": {""error"": error_message},
            },
        }",Create an error chunk for the agent output stream.,???Generate a structured error response with message and tool call details.???
3138,get_credentials,"def get_credentials(
    filter_dict: Optional[dict[str, Any]] = None, temperature: float = 0.0, fail_if_empty: bool = True
) -> Optional[Credentials]:
    
    try:
        config_list = autogen.config_list_from_json(
            OAI_CONFIG_LIST,
            filter_dict=filter_dict,
            file_location=KEY_LOC,
        )
    except Exception:
        config_list = []

    if len(config_list) == 0:
        if fail_if_empty:
            raise ValueError(""No config list found"")
        return None

    return Credentials(
        llm_config={
            ""config_list"": config_list,
            ""temperature"": temperature,
        }
    )",Fixture to load the LLM config.,???Retrieve and validate configuration credentials based on filters and conditions.???
3139,_parse_config,"def _parse_config(request_config: Dict[str, Any]) -> Dict[str, Any]:
    
    
    default_config = {
        ""deepthink_samples"": 3,
        ""confidence_threshold"": 0.7,
        ""max_tokens"": 16382,
        ""temperature"": 0.7,
        ""top_p"": 0.95,
        ""enable_self_discover"": True,
        ""reasoning_modules_limit"": 7
    }
    
    # Override with request config values
    for key, value in request_config.items():
        if key in default_config:
            default_config[key] = value
    
    # Validate ranges
    default_config[""deepthink_samples""] = max(1, min(10, default_config[""deepthink_samples""]))
    default_config[""confidence_threshold""] = max(0.0, min(1.0, default_config[""confidence_threshold""]))
    default_config[""temperature""] = max(0.0, min(2.0, default_config[""temperature""]))
    default_config[""top_p""] = max(0.0, min(1.0, default_config[""top_p""]))
    default_config[""reasoning_modules_limit""] = max(3, min(15, default_config[""reasoning_modules_limit""]))
    
    return default_config",Parse and validate configuration parameters.,???Merge and validate configuration settings with default parameters.???
3140,_analyze_insider_activity,"def _analyze_insider_activity(insider_trades):
    

    max_score = 2
    score = 0
    details: list[str] = []

    if not insider_trades:
        details.append(""No insider trade data"")
        return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}

    shares_bought = sum(t.transaction_shares or 0 for t in insider_trades if (t.transaction_shares or 0) > 0)
    shares_sold = abs(sum(t.transaction_shares or 0 for t in insider_trades if (t.transaction_shares or 0) < 0))
    net = shares_bought - shares_sold
    if net > 0:
        score += 2 if net / max(shares_sold, 1) > 1 else 1
        details.append(f""Net insider buying of {net:,} shares"")
    else:
        details.append(""Net insider selling"")

    return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}",Net insider buying over the last 12 months acts as a hard catalyst.,???Evaluate insider trading activity to generate a score and detailed summary.???
3141,get_llm_thread_id,"def get_llm_thread_id(self, wxid: str, namespace: str = None) -> Union[dict, str]:
        
        session = self.DBSession()
        try:
            # Check if it's a chatroom ID
            if wxid.endswith(""@chatroom""):
                chatroom = session.query(Chatroom).filter_by(chatroom_id=wxid).first()
                if namespace:
                    return chatroom.llm_thread_id.get(namespace, """") if chatroom else """"
                else:
                    return chatroom.llm_thread_id if chatroom else {}
            else:
                # Regular user
                user = session.query(User).filter_by(wxid=wxid).first()
                if namespace:
                    return user.llm_thread_id.get(namespace, """") if user else """"
                else:
                    return user.llm_thread_id if user else {}
        finally:
            session.close()",Get LLM thread id for user or chatroom,???Retrieve LLM thread ID for user or chatroom based on wxid and optional namespace.???
3142,save_dataset_statistics,"def save_dataset_statistics(dataset_statistics, run_dir):
    
    out_path = run_dir / ""dataset_statistics.json""
    with open(out_path, ""w"") as f_json:
        for _, stats in dataset_statistics.items():
            for k in stats[""action""].keys():
                if isinstance(stats[""action""][k], np.ndarray):
                    stats[""action""][k] = stats[""action""][k].tolist()
            if ""proprio"" in stats:
                for k in stats[""proprio""].keys():
                    if isinstance(stats[""proprio""][k], np.ndarray):
                        stats[""proprio""][k] = stats[""proprio""][k].tolist()
            if ""num_trajectories"" in stats:
                if isinstance(stats[""num_trajectories""], np.ndarray):
                    stats[""num_trajectories""] = stats[""num_trajectories""].item()
            if ""num_transitions"" in stats:
                if isinstance(stats[""num_transitions""], np.ndarray):
                    stats[""num_transitions""] = stats[""num_transitions""].item()
        json.dump(dataset_statistics, f_json, indent=2)
    logging.info(f""Saved dataset statistics file at path {out_path}"")",Saves a `dataset_statistics.json` file.,???Convert and save dataset statistics to JSON format in specified directory.???
3143,_make_batch_request,"def _make_batch_request(self, chunks: List[Chunk]) -> Dict:
        
        headers = {""Authorization"": f""Bearer {os.environ['VOYAGE_API_KEY']}"", ""Content-Type"": ""application/json""}
        payload = {""input"": [chunk.content for chunk in chunks], ""model"": self.embedding_model}

        response = requests.post(url, json=payload, headers=headers)
        if not response.status_code == 200:
            raise ValueError(f""Failed to make batch request. Response: {response.text}"")

        return response.json()",Makes a batch request to the Voyage API with exponential backoff when we hit rate limits.,???Send a batch API request with authentication and handle response errors???
3144,on_build_start,"def on_build_start(self, info: BuildStateInfo) -> None:
        
        try:
            # Ensure experiment is set and active
            self.experiment_id = self._get_or_create_experiment()

            # Get model info and timestamp
            model_id = (info.model_identifier or ""unknown"")[0:12] + ""...""
            timestamp = self._timestamp()

            # End any active run before starting parent
            if mlflow.active_run():
                mlflow.end_run()

            # Ensure the experiment is active before starting the run
            mlflow.set_experiment(experiment_id=self.experiment_id)

            # Start parent run
            parent_run = mlflow.start_run(
                run_name=f""{model_id}-{timestamp}"",
                experiment_id=self.experiment_id,
                description=f""Model building: {info.intent[:100]}..."",
            )
            self.parent_run_id = parent_run.info.run_id
            logger.info(f""Started parent run '{parent_run.info.run_id}' in experiment '{self.experiment_name}'"")

            # Log common parameters and tags
            mlflow.log_params(self._extract_model_context(info))
            mlflow.set_tags({""provider"": str(info.provider), ""run_type"": ""parent"", ""build_timestamp"": timestamp})

            # Log intent
            if info.intent:
                self._safe_log_artifact(content=info.intent, filename=""intent.txt"")

        except Exception as e:
            logger.error(f""Error starting build in MLFlow: {e}"")",Start MLFlow parent run and log initial parameters.,"???Initialize and manage an MLFlow experiment and run for model building, logging parameters and handling exceptions.???"
3145,create_broker,"def create_broker(
        broker_name: str,
        engine_type: str,
        engine_version: str,
        host_instance_type: str,
        deployment_mode: str,
        publicly_accessible: bool,
        auto_minor_version_upgrade: bool,
        users: List[Dict[str, str]],
        region: str = 'us-east-1',
    ):
        
        create_params = {
            'BrokerName': broker_name,
            'EngineType': engine_type,
            'EngineVersion': engine_version,
            'HostInstanceType': host_instance_type,
            'DeploymentMode': deployment_mode,
            'PubliclyAccessible': publicly_accessible,
            'AutoMinorVersionUpgrade': auto_minor_version_upgrade,
            'Users': users,
            'Tags': {
                'mcp_server_version': MCP_SERVER_VERSION,
            },
        }
        mq_client = mq_client_getter(region)
        response = mq_client.create_broker(**create_params)
        return response",Create a ActiveMQ or RabbitMQ broker on AmazonMQ.,???Create a message broker with specified configuration and return the response.???
3146,generate_bot_config,"def generate_bot_config(meeting_url, native_meeting_id, platform, bot_name, language, task, token):
    
    connection_id = str(uuid.uuid4())
    config = {
        # ""meeting_id"": None, # Removed: Set explicitly to null, but schema expects optional number (not null)
        ""platform"": platform,
        ""meetingUrl"": meeting_url,
        ""botName"": bot_name,
        ""token"": token,
        ""nativeMeetingId"": native_meeting_id,
        ""connectionId"": connection_id,
        ""language"": language,
        ""task"": task,
        ""redisUrl"": REDIS_URL,
        ""automaticLeave"": {
            ""waitingRoomTimeout"": DEFAULT_WAITING_ROOM_TIMEOUT,
            ""noOneJoinedTimeout"": DEFAULT_NO_ONE_JOINED_TIMEOUT,
            ""everyoneLeftTimeout"": DEFAULT_EVERYONE_LEFT_TIMEOUT
        }
    }
    # Remove keys with None values if bot schema expects them to be potentially undefined
    if language is None:
        del config[""language""]
    if task is None:
        del config[""task""]

    return config, connection_id",Generates the BOT_CONFIG dictionary.,???Generate a bot configuration dictionary for a meeting platform with dynamic connection ID.???
3147,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Console.enable"",
    }
    json = yield cmd_dict","Enables console domain, sends the messages collected so far to the client by means of the ``messageAdded`` notification.",???Enable console logging by sending a command and yielding a JSON response???
3148,set_up_base_branch,"def set_up_base_branch(self, base_branch: str | None) -> None:
        
        # If base branch is already checked out, do nothing
        if self.codebase.op.is_branch_checked_out(base_branch):
            return

        # fetch the base branch from highside (do not checkout yet)
        highside_remote = self.codebase.op.git_cli.remote(name=""origin"")
        self.codebase.op.fetch_remote(highside_remote.name, refspec=f""{base_branch}:{base_branch}"")

        # checkout the base branch (and possibly sync graph)
        self.codebase.checkout(branch=base_branch)",Set-up base branch by pushing latest highside branch to lowside and checking out the branch.,???Ensure the specified base branch is fetched and checked out if not already active.???
3149,convert_message_to_message_param,"def convert_message_to_message_param(
        cls, message: ChatCompletionMessage, **kwargs
    ) -> ChatCompletionMessageParam:
        
        assistant_message_params = {
            ""role"": ""assistant"",
            ""audio"": message.audio,
            ""refusal"": message.refusal,
            **kwargs,
        }
        if message.content is not None:
            assistant_message_params[""content""] = message.content
        if message.tool_calls is not None:
            assistant_message_params[""tool_calls""] = message.tool_calls

        return ChatCompletionAssistantMessageParam(**assistant_message_params)",Convert a response object to an input parameter object to allow LLM calls to be chained.,"???  
Transforms chat message into assistant message parameters with optional content and tool calls.  
???"
3150,edit_node_range,"def edit_node_range(self, start: int, end: int, node: Union[str, ASTT]):
        
        _node = self._render_node(node)

        self.source = self.source[:start] + _node + self.source[end:]
        # In order to continue accurately modifying the AST, we need to re-parse the source.
        self.atok = asttokens.ASTTokens(self.source, parse=True)

        if self.atok.tree:
            self.tree = self.atok.tree
        else:
            raise ValidationError(f""Failed to parse {self.filename} after edit"")",Splice a new node or string into the source code at the given range.,???Replace a code segment with a new node and update the abstract syntax tree.???
3151,_get_dump_dir,"def _get_dump_dir(self, dataset_name: str, sample_name: str, seed: int) -> str:
        
        dump_dir = os.path.join(
            self.base_dir, dataset_name, sample_name, f""seed_{seed}""
        )
        return dump_dir","Generate the directory path for dumping data based on the dataset name, sample name, and seed.","???  
Constructs a directory path for dataset samples using base directory and seed.  
???"
3152,load_tools,"def load_tools(self):
        
        logger = get_logger()
        self.tools, self.tools_schema = load_tools(self.config['tools_file'])
        if self.tools_schema and not (len(self.tools) == len(self.tools_schema)):
            logger.warning(
                f""{ConsoleColor.RED}If providing a schema, make sure to provide a schema for each tool. Found {len(self.tools)} tools and {len(self.tools_schema)} schemas.""
                f""Using the default tools schema for all tools.{ConsoleColor.RESET}"")
            self.tools_schema = []",Load the tools from the tools folder :return:,"???Load and validate tool configurations, logging warnings for schema mismatches.???"
3153,stop_operation_backend,"def stop_operation_backend(self, wait_operation: bool = False):
        
        self._loop_switch = False
        if self._backend_task:
            if wait_operation:
                self._backend_task.result()
                self.logger.info(f""stop operation={self.name}..."")
            else:
                self.logger.info(f""send stop signal to operation={self.name}..."")",Stops the background operation loop by setting the _loop_switch to False.,"???  
Gracefully halt backend operation with optional wait for completion.  
???"
3154,_load_ast_verified_links,"def _load_ast_verified_links() -> List[Dict[str, str]]:
    
    if _CORE_DIR_FOR_AST_LINKS is None:
        logger.error(""TrackerIO: _CORE_DIR_FOR_AST_LINKS not set. Cannot load AST verified links."")
        return []

    ast_links_path = normalize_path(os.path.join(_CORE_DIR_FOR_AST_LINKS, AST_VERIFIED_LINKS_FILENAME))

    if not os.path.exists(ast_links_path):
        logger.info(f""AST verified links file not found at {ast_links_path}. No AST overrides will be applied from file."")
        return []
    
    try:
        with open(ast_links_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if not isinstance(data, list):
            logger.error(f""AST verified links file {ast_links_path} does not contain a list. Format error."")
            return []
        # Optionally, add more validation for the structure of dicts within the list
        logger.info(f""Successfully loaded {len(data)} AST-verified links from {ast_links_path}."")
        return data
    except json.JSONDecodeError as e:
        logger.error(f""Error decoding JSON from AST verified links file {ast_links_path}: {e}"")
        return [] # Corrected from pass to return []
    except Exception as e: # Catch other potential errors like IOError
        logger.error(f""Error loading AST verified links file {ast_links_path}: {e}"", exc_info=True)
        return []",Loads the ast_verified_links.json file from the core directory.,???Load and validate AST verified links from a specified directory path.???
3155,get_browser_contexts,"def get_browser_contexts() -> typing.Generator[
    T_JSON_DICT, T_JSON_DICT, typing.List[browser.BrowserContextID]
]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Target.getBrowserContexts"",
    }
    json = yield cmd_dict
    return [browser.BrowserContextID.from_json(i) for i in json[""browserContextIds""]]",Returns all browser contexts created with ``Target.createBrowserContext`` method.,???Retrieve browser context IDs using a command dictionary and JSON response.???
3156,_format_feed_results,"def _format_feed_results(self, data: dict) -> str:
        
        result = [f""Element Count: {data.get('element_count', 0)} asteroids found""]
        
        for date, asteroids in data.get('near_earth_objects', {}).items():
            result.extend([
                f""\nDate: {date}"",
                f""Number of asteroids: {len(asteroids)}""
            ])
            
            for ast_data in asteroids:
                asteroid = Asteroid(**ast_data)
                result.extend([
                    ""\n"" + asteroid.format_info(),
                    ""-"" * 50
                ])
        
        return ""\n"".join(result)",Format feed results into readable text.,"???  
Formats and summarizes asteroid data into a structured string report.  
???"
3157,get_bin_params,"def get_bin_params(cfg: ConfigDict) -> dict:
    
    return {""min_bin"": cfg.min_bin, ""max_bin"": cfg.max_bin, ""no_bins"": cfg.no_bins}",Extract bin parameters from the configuration object.,"??? 
Extracts bin configuration parameters from a configuration dictionary. 
???"
3158,_get_peer_from_id,"def _get_peer_from_id(chat_id: str) -> Union[""PeerChat"", ""PeerChannel"", ""PeerUser""]:  # type: ignore[no-any-unimported]
        
        try:
            # Convert string to integer
            id_int = int(chat_id)

            # Channel/Supergroup: -100 prefix
            if str(chat_id).startswith(""-100""):
                channel_id = int(str(chat_id)[4:])  # Remove -100 prefix
                return PeerChannel(channel_id)

            # Group: negative number without -100 prefix
            elif id_int < 0:
                group_id = -id_int  # Remove the negative sign
                return PeerChat(group_id)

            # User/Bot: positive number
            else:
                return PeerUser(id_int)

        except ValueError as e:
            raise ValueError(f""Invalid chat_id format: {chat_id}. Error: {str(e)}"")",Convert a chat ID string to appropriate Peer type.,"???Determine peer type from chat ID, returning channel, group, or user object.???"
3159,get_profile_arn,"def get_profile_arn() -> Optional[str]:
    
    region = get_region()
    account_id = get_account_id()
    return f'arn:aws:bedrock:{region}:{account_id}:data-automation-profile/us.data-automation-v1'",Get the Bedrock Data Automation profile ARN.,"???  
Generate an AWS ARN for a data automation profile using region and account ID.  
???"
3160,get_current_code_codebase,"def get_current_code_codebase(config: CodebaseConfig | None = None, secrets: SecretsConfig | None = None, subdirectories: list[str] | None = None) -> CodebaseType:
    
    codegen_repo_path = get_graphsitter_repo_path()
    base_dir = get_codegen_codebase_base_path()
    logger.info(f""Creating codebase from repo at: {codegen_repo_path} with base_path {base_dir}"")

    repo_config = RepoConfig.from_repo_path(codegen_repo_path)
    repo_config.respect_gitignore = False
    op = RepoOperator(repo_config=repo_config, bot_commit=False)

    config = (config or CodebaseConfig()).model_copy(update={""base_path"": base_dir})
    projects = [ProjectConfig(repo_operator=op, programming_language=ProgrammingLanguage.PYTHON, subdirectories=subdirectories, base_path=base_dir)]
    codebase = Codebase(projects=projects, config=config, secrets=secrets)
    return codebase",Returns a Codebase for the code that is *currently running* (i.e. the Codegen repo),???Constructs a codebase object from a repository path with optional configurations and secrets???
3161,update_1dSE,"def update_1dSE(self, original_1dSE, new_edges):
        
    
        affected_nodes = []
        for edge in new_edges:
            affected_nodes += [edge[0], edge[1]]
        affected_nodes = set(affected_nodes)

        original_vol = self.vol
        original_degree_dict = {node:0 for node in affected_nodes}
        for node in affected_nodes.intersection(set(self.graph.nodes)):
            original_degree_dict[node] = self.graph.degree(node, weight = 'weight')

        # insert new edges into the graph
        self.graph.add_weighted_edges_from(new_edges)

        self.vol = self.get_vol()
        updated_vol = self.vol
        updated_degree_dict = {}
        for node in affected_nodes:
            updated_degree_dict[node] = self.graph.degree(node, weight = 'weight')
        
        updated_1dSE = (original_vol / updated_vol) * (original_1dSE - math.log2(original_vol / updated_vol))
        for node in affected_nodes:
            d_original = original_degree_dict[node]
            d_updated = updated_degree_dict[node]
            if d_original != d_updated:
                if d_original != 0:
                    updated_1dSE += (d_original / updated_vol) * math.log2(d_original / updated_vol)
                updated_1dSE -= (d_updated / updated_vol) * math.log2(d_updated / updated_vol)

        return updated_1dSE",get the updated 1D SE after new edges are inserted into the graph,???Update graph entropy by recalculating after adding new edges.???
3162,export_model,"def export_model(model_id="""", format=""torchscript""):
    
    assert format in export_fmts_hub(), f""Unsupported export format '{format}', valid formats are {export_fmts_hub()}""
    r = requests.post(
        f""{HUB_API_ROOT}/v1/models/{model_id}/export"", json={""format"": format}, headers={""x-api-key"": Auth().api_key}
    )
    assert r.status_code == 200, f""{PREFIX}{format} export failure {r.status_code} {r.reason}""
    LOGGER.info(f""{PREFIX}{format} export started ✅"")",Export a model to all formats.,"???Initiates model export via API, ensuring valid format and logging success.???"
3163,palette,"def palette(element: Element) -> IntegerSet:
    
    if isinstance(element, tuple):
        return frozenset({v for r in element for v in r})
    return frozenset({v for v, _ in element})",colors occurring in object or grid,???Convert elements into a set of unique values based on their structure.???
3164,load_from_yaml,"def load_from_yaml(cls, config_path: Optional[Path] = None) -> ""Settings"":
        
        config_file = BASE_DIR / ""config.yaml""

        if not config_file.exists():
            raise FileNotFoundError(
                f""Config file not found at {config_file}. ""
                ""Please ensure config.yaml exists in the project root directory.""
            )

        logger.info(f""Loading configuration from {config_file}"")
        with open(config_file, ""r"") as f:
            config_data = yaml.safe_load(f) or {}

        # Ensure base_dir is set to BASE_DIR
        if ""paths"" not in config_data:
            config_data[""paths""] = {}
        config_data[""paths""][""base_dir""] = str(BASE_DIR)

        return cls(**config_data)",Load settings from config.yaml in the base directory.,"???Load settings from a YAML configuration file, ensuring base directory path is set.???"
3165,mock_issue_response,"def mock_issue_response():
        
        return MagicMock(
            json=lambda: [
                {
                    ""title"": ""Found a bug"",
                    ""body"": ""I'm having a problem with this."",
                    ""comments"": 2,
                }
            ]
        )",A mock response for GitHub issues.,"???  
Simulate API response for issue details using a mock object.  
???"
3166,edit_message,"def edit_message(st: Any, button_idx: int, message_type: str) -> None:
        
        button_id = f""edit_box_{button_idx}""
        # Handle Event type messages
        message = st.session_state.user_chats[st.session_state[""session_id""]][""messages""][button_idx]
        # Convert to Event if it's not already
        event = message if isinstance(message, Event) else Event.model_validate(message)
        # Update the text content in the event
        if hasattr(event.content, 'parts'):
            for part in event.content.parts:
                if hasattr(part, 'text'):
                    part.text = st.session_state[button_id]
                    break
        # Update the message in the session state
        st.session_state.user_chats[st.session_state[""session_id""]][""messages""][button_idx] = event.model_dump()",Edit a message in the chat history.,???Edit session message content based on user input and update session state???
3167,create_runtime_patch,"def create_runtime_patch():
    
    print(""\n🔧 Creating runtime patch..."")
    
    patch_code = 
    
    with open('mcp_timeout_patch.py', 'w') as f:
        f.write(patch_code)
    
    print(""✅ Created mcp_timeout_patch.py"")
    print(""   Usage: python -c 'import mcp_timeout_patch' && mcp-cli chat"")",Create a runtime patch to override the timeout.,???Generate and save a Python runtime patch for MCP timeout handling.???
3168,classify_image,"def classify_image(self, image_path: str) -> str:
        
        print(f""[ImageAnalyzer] Analyzing image: {image_path}"")

        vision_prompt = [
            {""role"": ""system"", ""content"": ""You are an expert in medical imaging. Analyze the uploaded image.""},
            {""role"": ""user"", ""content"": [
                {""type"": ""text"", ""text"": (
                    
                )},
                {""type"": ""image_url"", ""image_url"": {""url"": self.local_image_to_data_url(image_path)}}  # Correct format
            ]}
        ]
        
        # Invoke LLM to classify the image
        response = self.vision_model.invoke(vision_prompt)

        try:
            # Ensure the response is parsed as JSON
            response_json = self.json_parser.parse(response.content)
            return response_json  # Returns a dictionary instead of a string
        except json.JSONDecodeError:
            print(""[ImageAnalyzer] Warning: Response was not valid JSON."")
            return {""image_type"": ""unknown"", ""reasoning"": ""Invalid JSON response"", ""confidence"": 0.0}",Analyzes the image to classify it as a medical image and determine it's type.,???Classifies medical images using a vision model and returns analysis results.???
3169,output_spec,"def output_spec(self) -> list:
        
        specs = []
        for i, out in enumerate(self.outputs):
            specs.append((out[""name""], out[""shape""], out[""dtype""]))
            if self.debug:
                print(f""trt output {i} -> {out['name']} -> {out['shape']} -> {out['dtype']}"")
        return specs","Restituisce le specifiche degli output (nome, shape, dtype) utili per preparare gli array.","???  
Generate a list of output specifications with optional debug logging.  
???"
3170,llm_translate,"def llm_translate(self, text, ignore_cache=False, rate_limit_params: dict = None):
        
        self.translate_call_count += 1
        if not (self.ignore_cache or ignore_cache):
            try:
                cache = self.cache.get(text)
                if cache is not None:
                    self.translate_cache_call_count += 1
                    return cache
            except Exception as e:
                logger.debug(f""try get cache failed, ignore it: {e}"")
        _translate_rate_limiter.wait()
        translation = self.do_llm_translate(text, rate_limit_params)
        if not (self.ignore_cache or ignore_cache):
            self.cache.set(text, translation)
        return translation","Translate the text, and the other part should call this method.",???Translate text using LLM with caching and rate limiting features???
3171,_create_wrapped_method,"def _create_wrapped_method(self, method: Callable) -> Callable:
        

        @functools.wraps(method)
        async def wrapped_method(message: dict):
            try:
                if getattr(method, ""_is_workflow"", False):
                    workflow_name = getattr(method, ""_workflow_name"", method.__name__)
                    instance_id = self.run_workflow(workflow_name, input=message)
                    asyncio.create_task(self.monitor_workflow_completion(instance_id))
                    return None

                if inspect.iscoroutinefunction(method):
                    return await method(message=message)
                else:
                    return method(message=message)

            except Exception as e:
                logger.error(
                    f""Error invoking handler '{method.__name__}': {e}"", exc_info=True
                )
                return None

        return wrapped_method","Wraps a message handler method to ensure it runs asynchronously, with special handling for workflows.",???Wraps a method to handle asynchronous workflow execution and error logging.???
3172,count_tokens,"def count_tokens(self, text: str) -> int:
        
        try:
            tokens = self.tokenizer.encode(text)
            return len(tokens)
        except Exception as e:
            logger.error(f""Error counting tokens: {str(e)}"")
            return 0",Count the number of tokens in a text string.,"???  
Counts and returns the number of tokens in a given text using a tokenizer.  
???"
3173,_tokenize,"def _tokenize(batch: dict[str, list[Any]], tokenizer: ""PreTrainedTokenizerBase"") -> dict[str, list[Any]]:
    
    new_examples = {
        ""input_ids_chosen"": [],
        ""attention_mask_chosen"": [],
        ""input_ids_rejected"": [],
        ""attention_mask_rejected"": [],
    }
    for chosen, rejected in zip(batch[""chosen""], batch[""rejected""]):
        tokenized_chosen = tokenizer(chosen)
        tokenized_rejected = tokenizer(rejected)
        new_examples[""input_ids_chosen""].append(tokenized_chosen[""input_ids""])
        new_examples[""attention_mask_chosen""].append(tokenized_chosen[""attention_mask""])
        new_examples[""input_ids_rejected""].append(tokenized_rejected[""input_ids""])
        new_examples[""attention_mask_rejected""].append(tokenized_rejected[""attention_mask""])

    return new_examples",Tokenize a batch from a reward modelling dataset.,???Tokenize and organize text data into chosen and rejected categories using a tokenizer???
3174,prompt,"def prompt(self, text: str):
        
        return [{""role"": ""user"", ""content"": text[""prompt""]}]",Convert input text into a patient message.,???Transforms input text into a user role content dictionary.???
3175,_assert_has_issue,"def _assert_has_issue(issues, issue_type: Type):
  
  if not any(isinstance(issue, issue_type) for issue in issues):
    raise AssertionError(f""Expected issue of type {issue_type.__name__} not found in issues."")",Assert that the issues list contains at least one issue of the given type.,"???Ensure presence of specific issue type in a list, raising error if absent.???"
3176,initialize,"def initialize(cls):
        
        if not cls._engine:
            try:
                config = Config.from_env()
                db_config = config.database.to_dict()

                # Ensure database directory exists
                db_dir = os.path.dirname(db_config['db_file'])
                if not os.path.exists(db_dir):
                    os.makedirs(db_dir)
                
                # Build SQLite connection URL

                cls._engine = create_engine(
                    db_url,
                    echo=False,
                    pool_pre_ping=True,
                    pool_recycle=db_config['pool_recycle'],
                    pool_size=db_config['maxsize'],
                    max_overflow=20,
                )
                cls._session_factory = sessionmaker(bind=cls._engine)
                logger.info(""SQLite database engine and session factory initialized"")
            except Exception as e:
                logger.error(f""Failed to initialize database: {str(e)}"")
                raise",Initialize database engine and session factory,???Initialize SQLite database engine and session factory with environment configuration.???
3177,save_text,"def save_text(text: str, filename: Optional[str] = None) -> Optional[str]:
    
    if not text.strip():
        return None

    if filename is None:
        # Use input_1.txt, input_2.txt, etc.
        base = ""input""
        counter = 1
        while True:
            filename = f""{base}_{counter}.txt""
            if not os.path.exists(os.path.join(INPUTS_DIR, filename)):
                break
            counter += 1
    else:
        # Handle duplicate filenames by adding _1, _2, etc.
        base = os.path.splitext(filename)[0]
        ext = os.path.splitext(filename)[1] or "".txt""
        counter = 1
        while os.path.exists(os.path.join(INPUTS_DIR, filename)):
            filename = f""{base}_{counter}{ext}""
            counter += 1

    filepath = os.path.join(INPUTS_DIR, filename)
    try:
        with open(filepath, ""w"", encoding=""utf-8"") as f:
            f.write(text)
        return filename
    except Exception as e:
        print(f""Error saving file: {e}"")
        return None",Save text to a file.,"???Automatically save text to a unique file, handling duplicates and empty input.???"
3178,text_to_video,"def text_to_video(
        self, prompt: str, save_at: str, duration: float, config: dict
    ):
        
        try:
            model_name = config.get(""model_name"", ""fal-ai/minimax-video"")
            res = fal_client.run(
                model_name,
                arguments={""prompt"": prompt, ""duration"": duration},
            )
            video_url = res[""video""][""url""]
            with open(save_at, ""wb"") as f:
                f.write(requests.get(video_url).content)

        except Exception as e:
            raise Exception(f""Error generating video: {type(e).__name__}: {str(e)}"")

        return {""status"": ""success"", ""video_path"": save_at}",Generates a video from using text-to-video models.,???Convert text prompts into videos and save them locally.???
3179,_convert_html_to_markdown,"def _convert_html_to_markdown(self, text: str) -> str:
        
        if re.search(r""<[^>]+>"", text):
            try:
                with warnings.catch_warnings():
                    warnings.filterwarnings(""ignore"", category=UserWarning)
                    soup = BeautifulSoup(f""<div>{text}</div>"", ""html.parser"")
                    html = str(soup.div.decode_contents()) if soup.div else text
                    text = md(html)
            except Exception as e:
                logger.warning(f""Error converting HTML to markdown: {str(e)}"")
        return text",Convert HTML content to markdown if needed.,"???  
Convert HTML content to Markdown format while handling exceptions.  
???"
3180,execute,"def execute(self, file_path: str, patch: str):
        
        error_context = {
            ""File"": file_path,
            ""File exists"": os.path.exists(file_path),
        }

        try:
            if os.path.exists(file_path):
                with open(file_path, encoding=""utf-8"") as f:
                    original_content = f.read()
                    error_context[""File preview""] = (
                        original_content[:200] + ""..."" if len(original_content) > 200 else original_content
                    )
            else:
                original_content = """"

            patch_obj = Patch(patch)
            new_content = patch_obj.apply_to_text(original_content, lenient=self.lenient, tolerance=self.tolerance)

            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, ""w"", encoding=""utf-8"") as f:
                f.write(new_content)

            return ""Patch applied successfully""

        except Exception as e:
            raise PatchError(f""Unexpected error: {str(e)}"", error_context)",Apply the patch to the specified file.,"???Apply a patch to a file, handling errors and ensuring directory creation.???"
3181,_start_tqdm_tracker,"def _start_tqdm_tracker(self):
        
        self.pbar = tqdm.tqdm(
            total=self.n_total_requests,
            initial=self.n_finished_requests,
            desc=f""Processing {self.model}"",
            unit=""req"",
        )
        # Log initial stats
        self._format_cost_str()
        self._log_stats()",Start the tqdm progress tracker.,???Initialize and log a progress tracker for processing model requests.???
3182,send_alert,"def send_alert(self, alert_type: AlertLevel, message: str, metrics: Dict[str, Any] = None):
        
        alert = Alert(
            level=alert_type,
            message=message,
            timestamp=datetime.now(),
            metrics=metrics or {}
        )
        
        for channel in self.channels:
            try:
                channel.send_alert(alert)
            except Exception as e:
                logging.error(f""Failed to send alert through channel {channel.__class__.__name__}: {e}"")",Send alert through all configured channels,"???Send alerts with specified level and message to multiple channels, handling exceptions.???"
3183,_initialize_bootcamp,"def _initialize_bootcamp(self):
        
        try:
            # Create bootcamp instance using the registry
            self.bootcamp_instance = create_bootcamp(
                self.config.task_name, **self.config.task_params
            )
            logger.info(
                f""Initialized {self.config.task_name} with params: {self.config.task_params}""
            )
        except ValueError as e:
            # If task not found, list available tasks
            available = get_available_bootcamps()
            logger.error(f""Task '{self.config.task_name}' not found!"")
            logger.error(f""Available tasks (showing first 20): {available[:20]}"")
            raise e
        except Exception as e:
            logger.error(f""Failed to initialize bootcamp: {e}"")
            raise",Initialize the bootcamp instance based on task name.,???Initialize a bootcamp task with error handling and logging???
3184,create_or_update_configmap,"def create_or_update_configmap(self, name: str, namespace: str, data: dict):
        
        try:
            existing_configmap = self.core_v1_api.read_namespaced_config_map(
                name, namespace
            )
            # ConfigMap exists, update it
            existing_configmap.data = data
            self.core_v1_api.replace_namespaced_config_map(
                name, namespace, existing_configmap
            )
            print(f""ConfigMap '{name}' updated in namespace '{namespace}'"")
        except ApiException as e:
            if e.status == 404:
                # ConfigMap doesn't exist, create it
                body = client.V1ConfigMap(
                    metadata=client.V1ObjectMeta(name=name), data=data
                )
                self.core_v1_api.create_namespaced_config_map(namespace, body)
                print(f""ConfigMap '{name}' created in namespace '{namespace}'"")
            else:
                print(f""Error creating/updating ConfigMap '{name}': {e}"")","Create a configmap if it doesn't exist, or update it if it does.",???Manage Kubernetes ConfigMap by creating or updating based on existence check.???
3185,_load_and_sync,"def _load_and_sync(self) -> Dict[str, Dict[str, Any]]:
        
        on_disk: Dict[str, Dict[str, Any]] = {}
        if self._path.is_file():
            try:
                on_disk = json.loads(self._path.read_text())
            except json.JSONDecodeError:
                pass

        # Deep-copy defaults then overlay user config
        merged: Dict[str, Dict[str, Any]] = json.loads(json.dumps(DEFAULTS))
        for prov, cfg in on_disk.items():
            merged.setdefault(prov, {}).update(cfg)

        # Prune legacy keys
        for prov_cfg in merged.values():
            prov_cfg.pop(""client"", None)

        # Write back if changed
        if merged != on_disk:
            self._path.parent.mkdir(parents=True, exist_ok=True)
            self._path.write_text(json.dumps(merged, indent=2))

        return merged",Load and sync configuration with defaults.,"???Synchronize configuration by merging defaults with disk data, pruning, and updating if necessary.???"
3186,generate_vae_cache_filename,"def generate_vae_cache_filename(self, filepath: str) -> tuple:
        
        if filepath.endswith("".pt""):
            return filepath, os.path.basename(filepath)
        # Extract the base name from the filepath and replace the image extension with .pt
        base_filename = os.path.splitext(os.path.basename(filepath))[0]
        if self.hash_filenames:
            base_filename = str(sha256(str(base_filename).encode()).hexdigest())
        base_filename = str(base_filename) + "".pt""
        # Find the subfolders the sample was in, and replace the instance_data_dir with the cache_dir
        subfolders = """"
        if self.instance_data_dir is not None:
            subfolders = os.path.dirname(filepath).replace(self.instance_data_dir, """")
            subfolders = subfolders.lstrip(os.sep)

        if len(subfolders) > 0:
            full_filename = os.path.join(self.cache_dir, subfolders, base_filename)
            # logger.debug(
            #     f""full_filename: {full_filename} = os.path.join({self.cache_dir}, {subfolders}, {base_filename})""
            # )
        else:
            full_filename = os.path.join(self.cache_dir, base_filename)
            # logger.debug(
            #     f""full_filename: {full_filename} = os.path.join({self.cache_dir}, {base_filename})""
            # )
        return full_filename, base_filename",Get the cache filename for a given image filepath and its base name.,???Generate a cache filename for a VAE model based on the given file path and configuration.???
3187,_log_chat_progress,"def _log_chat_progress(
        self, chat_turn: Optional[int] = None, model: Optional[str] = None
    ):
        
        data = {
            ""progress_action"": ""Chatting"",
            ""model"": model,
            ""agent_name"": self.name,
            ""chat_turn"": chat_turn if chat_turn is not None else None,
        }
        self.logger.debug(""Chat in progress"", data=data)",Log a chat progress event,???Log chat progress with model and turn details for debugging???
3188,load_js,"def load_js(self, file_name: str) -> str:
        
        path = os.path.join(self.js_scripts_folder, file_name)
        self.logger.info(f""Loading js at {path}"")
        try:
            with open(path, 'r') as f:
                return f.read()
        except FileNotFoundError as e:
            raise Exception(f""Could not find: {path}"") from e
        except Exception as e:
            raise e",Load javascript from script folder to inject to page.,"???Load JavaScript file content from a specified directory, handling errors.???"
3189,ensure_divisibility,"def ensure_divisibility(numerator, denominator):
    
    assert numerator % denominator == 0, ""{} is not divisible by {}"".format(
        numerator, denominator
    )",Ensure that numerator is divisible by the denominator.,"???Ensure numerator is divisible by denominator, raising an error if not.???"
3190,get_metrics,"def get_metrics(self) -> dict:
        
        return None if self.metric is None else {self.metric.name: self.metric.value}",Return metrics about the model.,"???Return a dictionary of metric name and value if metric exists, else None.???"
3191,to_dict_exclude_params,"def to_dict_exclude_params(self):
        
        return super().to_dict_exclude_params | {
            ""CREATE_TABLE_QUERY"": True,
            ""VALIDATE_TABLE_QUERY"": True,
            ""INSERT_MESSAGE_QUERY"": True,
            ""SELECT_ALL_MESSAGES_QUERY"": True,
            ""CHECK_IF_EMPTY_QUERY"": True,
            ""CLEAR_TABLE_QUERY"": True,
            ""SEARCH_MESSAGES_QUERY"": True,
        }",Define parameters to exclude during serialization.,"???  
Extend dictionary with specific query flags for database operations.  
???"
3192,_add,"def _add(
            inner_self: T,
            inner_name: str = name,
            inner_desc: Optional[str] = desc,
            inner_cls: Type[BaseRegistry] = cls,
        ) -> T:
            

            existing = inner_cls.get(inner_name, raise_on_missing=False)

            if existing and existing != inner_self:
                if inner_self.__module__ == ""__main__"":
                    return inner_self

                raise ValueError(f""Tagger {inner_name} already exists"")
            inner_cls._get_storage()[inner_name] = (inner_self, inner_desc)
            return inner_self",Add a tagger to the registry using tagger_name as the name.,???Register or update an object in a registry with conflict handling.???
3193,add_event,"def add_event(event_type: EventType, event_data: EventData) -> None:
    
    if not ENABLE_TELEMETRY:
        return  # Skip event logging if telemetry is disabled

    global event_collector
    event = {
        ""timestamp"": datetime.now().isoformat(),
        ""data"": event_data.model_dump(),
    }

    # Add event to the relevant bucket
    if event_type.value not in event_collector[""buckets""]:
        event_collector[""buckets""][event_type.value] = {""events"": [], ""event_count"": 0}
    event_collector[""buckets""][event_type.value][""events""].append(event)
    event_collector[""buckets""][event_type.value][""event_count""] += 1","Adds an event to the event collector in the appropriate event_type bucket, only if telemetry is enabled.",???Log events into categorized buckets if telemetry is enabled???
3194,compute_Q,"def compute_Q(self, state: State, rollout: str) -> float:
        
        # Count words in the rollout
        word_count = len(rollout.split())
        length_penalty = word_count / self.L
        Q_value = (self.alpha ** (1 - state.MC)) * (self.beta ** length_penalty)
        return Q_value","Compute Q(s, r) = alpha^{1 - MC(s)} * beta^{len(r)/L}, where len(r) is based on word count.",???Calculate a quality score based on state and text length.???
3195,_create_property_entity,"def _create_property_entity(
        self, prop_name: str, prop_value: dict, schema_prop: dict, page_id: str, database_id: str
    ) -> NotionPropertyEntity:
        
        prop_type = prop_value.get(""type"", """")
        formatted_value = self._format_property_value(prop_value, prop_type)

        return NotionPropertyEntity(
            entity_id=f""{page_id}_{schema_prop.get('id', prop_name)}"",
            breadcrumbs=[],
            property_id=schema_prop.get(""id"", """"),
            property_name=prop_name,
            property_type=prop_type,
            page_id=page_id,
            database_id=database_id,
            value=prop_value.get(prop_type),
            formatted_value=formatted_value,
        )",Create a property entity from page property data.,???Create a Notion property entity with formatted values and identifiers.???
3196,display_sync_summary,"def display_sync_summary(knowledge: SyncReport):
    
    total_changes = knowledge.total
    project_name = config.project

    if total_changes == 0:
        console.print(f""[green]Project '{project_name}': Everything up to date[/green]"")
        return

    # Format as: ""Synced X files (A new, B modified, C moved, D deleted)""
    changes = []
    new_count = len(knowledge.new)
    mod_count = len(knowledge.modified)
    move_count = len(knowledge.moves)
    del_count = len(knowledge.deleted)

    if new_count:
        changes.append(f""[green]{new_count} new[/green]"")
    if mod_count:
        changes.append(f""[yellow]{mod_count} modified[/yellow]"")
    if move_count:
        changes.append(f""[blue]{move_count} moved[/blue]"")
    if del_count:
        changes.append(f""[red]{del_count} deleted[/red]"")

    console.print(f""Project '{project_name}': Synced {total_changes} files ({', '.join(changes)})"")",Display a one-line summary of sync changes.,???Display synchronization summary of file changes in a project.???
3197,find_available_port,"def find_available_port(start_port: int = 8000, max_tries: int = 100) -> int:
    
    for port in range(start_port, start_port + max_tries):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(("""", port))
                return port
        except OSError:
            continue
    msg = f""No available ports found between {start_port} and {start_port + max_tries}""
    raise click.ClickException(msg)",Find an available port starting from start_port.,???Finds an available network port starting from a specified port number.???
3198,create_s3_bucket,"def create_s3_bucket(self, bucket_name: str):
        
        self.data_bucket_name = bucket_name
        try:
            self.s3_client.head_bucket(Bucket=bucket_name)
            print(f""Bucket {bucket_name} already exists - retrieving it!"")
        except ClientError as e:
            print(f""Creating bucket {bucket_name}"")
            if self.region_name == ""us-east-1"":
                self.s3_client.create_bucket(Bucket=bucket_name)
            else:
                self.s3_client.create_bucket(
                    Bucket=bucket_name,
                    CreateBucketConfiguration={""LocationConstraint"": self.region_name},
                )","Check if bucket exists, and if not create S3 bucket for knowledge base data source",???Creates or retrieves an S3 bucket based on its existence status.???
3199,get_bottom_toolbar,"def get_bottom_toolbar():
    
    # If the toolbar is empty, initialize it
    if not toolbar_cache['html']:
        # Create a simple initial toolbar while the full one loads
        current_time = datetime.datetime.now().strftime(""%H:%M"")
        timezone_name = datetime.datetime.now().astimezone().tzname()
        toolbar_cache['html'] = HTML(
            f""<ansigray>Loading system information... {current_time} {timezone_name}</ansigray>""
        )
        # Start background update
        threading.Thread(
            target=update_toolbar_in_background,
            daemon=True
        ).start()
    
    # Return the cached toolbar HTML
    return toolbar_cache['html']",Get the bottom toolbar with system information (cached).,???Initialize and update a dynamic toolbar with system time and timezone information.???
3200,_format_metrics,"def _format_metrics(self, metrics: List[UncertaintyMetrics]) -> str:
        
        formatted = []
        for idx, metric in enumerate(metrics):
            top_predictions = [f""{t.token} ({t.probability:.3f})"" for t in metric.token_predictions[:3]]

            metric_text = (
                f""Token {idx}:\n""
                f""- Raw Entropy: {metric.raw_entropy:.4f}\n""
                f""- Semantic Entropy: {metric.semantic_entropy:.4f}\n""
                f""- Top Predictions: {' | '.join(top_predictions)}""
            )
            formatted.append(metric_text)

        return ""\n\n"".join(formatted)",Format metrics for the prompt,???Format and summarize uncertainty metrics into a structured string report.???
3201,export,"def export(self, spans):
        
        retries = 0

        while retries <= self.max_retries:
            try:
                return self.exporter.export(spans)
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                ProtocolError,
                ReadTimeoutError,
            ) as e:
                retries += 1

                if retries <= self.max_retries:
                    # Use exponential backoff
                    delay = self.retry_delay * (2 ** (retries - 1))
                    self.logger.warning(
                        f""Honeycomb trace export attempt {retries} failed: {str(e)}. "" f""Retrying in {delay}s...""
                    )
                    time.sleep(delay)
                else:
                    self.logger.error(f""Failed to export traces to Honeycomb after {retries} attempts: {str(e)}"")
            except Exception as e:
                # For non-connection errors, don't retry
                self.logger.error(f""Unexpected error exporting traces to Honeycomb: {str(e)}"")
                return False

        # If we get here, all retries failed
        return False",Export spans with retry logic for handling connection issues.,???Implements a retry mechanism with exponential backoff for exporting trace data to Honeycomb.???
3202,_craft_question_prompt,"def _craft_question_prompt(self, topic: str, difficulty: str) -> str:
        
        grade_level = self.profile.get(""target_grade"", ""high school"")

        prompt = f
        return prompt",Craft a prompt for the LLM to generate a multiple-choice question.,"???  
Generates a question prompt based on topic, difficulty, and grade level.  
???"
3203,print_results,"def print_results(self):
        
        self.avg_scores, self.stds = self.calculate_avg_and_std()
        for i, (avg_score, std) in enumerate(zip(self.avg_scores, self.stds), 1):
            logger.info(f""Round {i}: Average Score = {avg_score:.4f}, Standard Deviation = {std:.4f}"")",Print average score and standard deviation for all rounds.,???Log average scores and standard deviations for each round.???
3204,render,"def render(game_state: dict[str, Any]) -> str:
        
        grid = game_state[""grid""]
        size = game_state[""size""]

        output = [""\n""]

        # Create a visual representation of the grid
        max_digits = len(str(size * size - 1))

        # Top border
        output.append(""  "" + ""+"" + ""-"" * (max_digits + 2) * size + ""+"")

        # Rows
        for i, row in enumerate(grid):
            row_str = f""{i + 1} |""
            for val in row:
                if val == 0:
                    # Empty space
                    row_str += "" "" * (max_digits + 2)
                else:
                    # Tile with number
                    row_str += f"" {val:>{max_digits}} ""
            row_str += ""|""
            output.append(row_str)

        # Bottom border
        output.append(""  "" + ""+"" + ""-"" * (max_digits + 2) * size + ""+"")

        # Column labels
        col_labels = ""    ""
        for i in range(size):
            col_labels += f""{i + 1:^{max_digits + 2}}""
        output.append(col_labels)

        return ""\n"".join(output)",Render the current Sliding Puzzle game state.,???Generate a formatted string to visually represent a game grid from the game state.???
3205,_check_for_end_statement_match,"def _check_for_end_statement_match(self) -> bool:
        
        if not self.dialogue_history_for_agent: # Check if history is empty
            return False
        if not self.current_level_end_statements: # No end statements defined for this level
            return False

        current_dialogue_line = self.dialogue_history_for_agent[-1] # Use the last line from history
        for end_statement in self.current_level_end_statements:
            if current_dialogue_line == end_statement: # Exact match
                print(f""[AceAttorneyEnv DEBUG _check_for_end_statement_match] Matched end statement: '{end_statement}'"")
                return True
        return False",Checks if the last dialogue line in history matches any defined end statement for the current level.,???Check if the last dialogue matches predefined end statements???
3206,info,"def info(self):
        
        for key, value in self.dataset['info'].items():
            print('{}: {}'.format(key, value))",Print information about the annotation file.,"??? 
Prints each key-value pair from the 'info' section of a dataset. 
???"
3207,update_api_key,"def update_api_key(api_key_value, state):
        
        state[""planner_api_key""] = api_key_value
        if state[""planner_provider""] == ""ssh"":
            state[""api_key""] = api_key_value
        logger.info(f""API key updated: provider={state['planner_provider']}, api_key={state['api_key']}"")",Handle API key updates,???Update API key in state and log the change based on provider type.???
3208,display_execution_time,"def display_execution_time(metrics=None):
    
    if metrics is None:
        return

    # Create a panel for the execution time
    content = []
    content.append(f""Session Time: {metrics['session_time']}"")
    content.append(f""Active Time: {metrics['active_time']}"")
    content.append(f""Idle Time: {metrics['idle_time']}"")

    if metrics.get('llm_time') and metrics['llm_time'] != ""0.0s"":
        content.append(
            f""LLM Processing Time: [bold yellow]{metrics['llm_time']}[/bold yellow] ""
            f""[dim]({metrics['llm_percentage']:.1f}% of session)[/dim]""
        )

    time_panel = Panel(
        Group(*[Text(line) for line in content]),
        border_style=""blue"",
        box=ROUNDED,
        padding=(0, 1),
        title=""[bold]Session Statistics[/bold]"",
        title_align=""left""
    )
    console.print(time_panel)",Display the total execution time with our local console.,???Display a styled panel summarizing session and processing times if metrics are provided.???
3209,read_key_definitions_from_lines,"def read_key_definitions_from_lines(lines: List[str]) -> List[Tuple[str, str]]:
    
    key_path_pairs: List[Tuple[str, str]] = []
    in_section = False
    key_def_start_pattern = re.compile(r'^---KEY_DEFINITIONS_START---$', re.IGNORECASE)
    key_def_end_pattern = re.compile(r'^---KEY_DEFINITIONS_END---$', re.IGNORECASE)
    # Regex now includes optional #instance part
    definition_pattern = re.compile(fr""^({KEY_GI_PATTERN_PART})\s*:\s*(.*)$"")

    for line in lines:
        if key_def_end_pattern.match(line.strip()): break
        if in_section:
            line_content = line.strip()
            if not line_content or line_content.lower().startswith(""key definitions:""): continue
            match = definition_pattern.match(line_content) # Use updated pattern
            if match:
                k_gi, v_path = match.groups() # k_gi is now the full KEY#GI or KEY
                # validate_key already handles KEY#GI format
                if validate_key(k_gi): 
                    key_path_pairs.append((k_gi, normalize_path(v_path.strip())))
                else: # Should be caught by regex, but as fallback
                    logger.warning(f""TrackerUtils.ReadDefinitions: Skipping invalid key format '{k_gi}'."") 
            # else: logger.debug(f""ReadDefs: Line did not match key def pattern: '{line_content}'"")
        elif key_def_start_pattern.match(line.strip()): in_section = True
    return key_path_pairs",Reads key definitions from lines.,???Extract key-path pairs from lines between specific start and end markers.???
3210,save_generations,"def save_generations(responses, model_name, dataset_name):
    
    output_dir = ""results/generations""
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, f""{dataset_name}_{model_name}_generations.json"")
    with open(output_file, 'w') as f:
        json.dump(responses, f, indent=2)
    print(f""Saved generations to {output_file}"")",Save generation results to a JSON file.,???Save AI model outputs to a JSON file in a specified directory.???
3211,log_snapshot,"def log_snapshot(self, graph: WorkFlowGraph, metrics: dict) -> None:
        
        self._snapshot.append(
            {
                ""index"": len(self._snapshot),
                ""graph"": deepcopy(graph.get_config()),
                ""metrics"": metrics,
            }
        )",Log the snapshot of the workflow.,???Capture and store workflow graph configuration and metrics snapshot.???
3212,_parse_hunk_header,"def _parse_hunk_header(self, header_line: str) -> HunkHeader:
        
        if not header_line.startswith(""@@ ""):
            raise PatchError(""Malformed hunk header"", {""Header line"": header_line})

        parts = header_line.split(""@@"")
        if len(parts) < 3:
            raise PatchError(""Malformed hunk header"", {""Header line"": header_line})

        ranges = parts[1].strip().split("" "")
        if len(ranges) != 2:
            raise PatchError(""Malformed hunk ranges"", {""Header line"": header_line, ""Ranges"": ranges})

        orig_range = ranges[0][1:]
        new_range = ranges[1][1:]

        try:
            orig_start, orig_count = self._parse_range(orig_range)
            new_start, new_count = self._parse_range(new_range)
        except ValueError as e:
            raise PatchError(
                ""Invalid range format"",
                {""Header line"": header_line, ""Original range"": orig_range, ""New range"": new_range, ""Error"": str(e)},
            )

        section_header = "" "".join(parts[2:]).strip() if len(parts) > 2 else None
        return HunkHeader(orig_start, orig_count, new_start, new_count, section_header)",Parse a hunk header line.,"???Parse and validate patch hunk headers, extracting range and section details.???"
3213,get_playlist_watch_status,"def get_playlist_watch_status(playlist_spotify_id):
    
    logger.info(f""Checking watch status for playlist {playlist_spotify_id}."")
    try:
        playlist = get_watched_playlist(playlist_spotify_id)
        if playlist:
            return jsonify({""is_watched"": True, ""playlist_data"": playlist}), 200
        else:
            # Return 200 with is_watched: false, so frontend can clearly distinguish
            # between ""not watched"" and an actual error fetching status.
            return jsonify({""is_watched"": False}), 200
    except Exception as e:
        logger.error(
            f""Error checking watch status for playlist {playlist_spotify_id}: {e}"",
            exc_info=True,
        )
        return jsonify({""error"": f""Could not check watch status: {str(e)}""}), 500",Checks if a specific playlist is being watched.,???Check if a Spotify playlist has been watched and return status or error.???
3214,chat_component,"def chat_component():
    
    st.subheader(f""{t('Welcome to ')}UltraRAG-Adaptive-Note"")

    # Setup chat container
    his_container = st.container(height=500)
    
    with his_container:
        # Display chat history
        for (query, response) in st.session_state.renote_history:
            with st.chat_message(name=""user"", avatar=""user""):
                st.markdown(query, unsafe_allow_html=True)
            with st.chat_message(name=""assistant"", avatar=""assistant""):
                st.markdown(response, unsafe_allow_html=True)
        container = st.empty()
        container_a = st.empty()
        
    # Handle user input
    chat_input_container = st.container()
    with chat_input_container:
        if query := st.chat_input(t(""Please input your question"")):
            container.chat_message(""user"").markdown(query, unsafe_allow_html=True)
            response_with_extra_info, pure_response = asyncio.run(listen(query,container_a))
            
            # Update chat history
            st.session_state.renote_messages.append({""role"": ""user"", ""content"": query})
            st.session_state.renote_messages.append({""role"": ""assistant"", ""content"": pure_response})
            st.session_state.renote_history.append((query, response_with_extra_info))
            st.rerun()",Display the chat interface component.,???Interactive chat interface for adaptive note-taking with user input handling and history display.???
3215,update_finished,"def update_finished(self, zip_path):
        
        self.progressBar.setVisible(False)
        if os.path.exists(zip_path):
            title = '更新完成'
            content = f'压缩包已下载至{zip_path}，即将重启更新'
            message_box = MessageBox(title, content, self.parent.window())
            message_box.cancelButton.setVisible(False)
            if message_box.exec():
                subprocess.Popen([sys.executable, 'update.py', zip_path])
                self.parent.close()
        else:
            InfoBar.error(
                '更新下载失败',
                f'请前往github/gitee自行下载release，或者去群{QQ}找最新文件下载',
                isClosable=True,
                duration=-1,
                parent=self
            )",Hide progress bar and show completion message,???Handle update completion by notifying user and initiating restart if successful???
3216,silero_vad,"def silero_vad(onnx=False, force_onnx_cpu=False, opset_version=16):
    
    available_ops = [15, 16]
    if onnx and opset_version not in available_ops:
        raise Exception(f'Available ONNX opset_version: {available_ops}')

    if not onnx:
        installed_version = torch.__version__
        supported_version = '1.12.0'
        if versiontuple(installed_version) < versiontuple(supported_version):
            raise Exception(f'Please install torch {supported_version} or greater ({installed_version} installed)')

    model_dir = os.path.join(os.path.dirname(__file__), 'src', 'silero_vad', 'data')
    if onnx:
        if opset_version == 16:
            model_name = 'silero_vad.onnx'
        else:
            model_name = f'silero_vad_16k_op{opset_version}.onnx'
        model = OnnxWrapper(os.path.join(model_dir, model_name), force_onnx_cpu)
    else:
        model = init_jit_model(os.path.join(model_dir, 'silero_vad.jit'))
    utils = (get_speech_timestamps,
             save_audio,
             read_audio,
             VADIterator,
             collect_chunks)

    return model, utils",Silero Voice Activity Detector,???Initialize and validate Silero VAD model with ONNX or JIT based on configuration.???
3217,get_protocols,"def get_protocols(
        self,
        protocol_id: Optional[str] = None,
        tags: Optional[List[str]] = None,
        opportunity_tag: Optional[str] = None,
        page: int = 0,
        items: int = 20,
    ) -> Dict:
        
        params = {k: v for k, v in locals().items() if v is not None and k != ""self""}

        return self._sync_request(""get"", ""/protocols/"", params=params)",Get list of protocols with optional filters,???Fetch protocols with optional filters and pagination parameters.???
3218,_extract_chunk_content,"def _extract_chunk_content(self, chunk: Dict[str, Any]) -> str:
        
        try:
            # chuk-llm streaming format - chunk has ""response"" field with content
            if isinstance(chunk, dict):
                # Primary format for chuk-llm
                if ""response"" in chunk:
                    return str(chunk[""response""]) if chunk[""response""] is not None else """"
                
                # Alternative formats (for compatibility)
                elif ""content"" in chunk:
                    return str(chunk[""content""])
                elif ""text"" in chunk:
                    return str(chunk[""text""])
                elif ""delta"" in chunk and isinstance(chunk[""delta""], dict):
                    delta_content = chunk[""delta""].get(""content"")
                    return str(delta_content) if delta_content is not None else """"
                elif ""choices"" in chunk and chunk[""choices""]:
                    choice = chunk[""choices""][0]
                    if ""delta"" in choice and ""content"" in choice[""delta""]:
                        delta_content = choice[""delta""][""content""]
                        return str(delta_content) if delta_content is not None else """"
            elif isinstance(chunk, str):
                return chunk
                
        except Exception as e:
            logger.debug(f""Error extracting content from chunk: {e}"")
            
        return """"",Extract text content from a chuk-llm streaming chunk.,"???Extracts content from various chunk formats, handling exceptions gracefully.???"
3219,_identifying_params,"def _identifying_params(self) -> Mapping[str, Any]:
        
        return {""model_path"": self.model_path}",Get the identifying parameters.,???Returns a dictionary with the model's file path as a key-value pair.???
3220,perform_action,"def perform_action(self, action_name: str, kwargs: Dict[str, Any]) -> Any:
        
        if action_name not in self.actions:
            raise KeyError(f""Unknown action: {action_name}"")

        load_dotenv()
        
        if not self.is_configured(verbose=True):
            raise EthereumConnectionError(""Ethereum connection is not properly configured"")

        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f""Invalid parameters: {', '.join(errors)}"")

        method_name = action_name.replace('-', '_')
        method = getattr(self, method_name)
        return method(**kwargs)",Execute an Ethereum action with validation,???Execute validated action with configuration check and error handling.???
3221,ensure_stats_dir,"def ensure_stats_dir():
    
    if not STATS_DIR:
        logger.error(""No writable stats directory found"")
        return False
    
    try:
        os.makedirs(STATS_DIR, exist_ok=True)
        logger.debug(f""Stats directory ensured: {STATS_DIR}"")
        return True
    except Exception as e:
        logger.error(f""Failed to create stats directory: {e}"")
        return False",Ensure the statistics directory exists,"???Ensure the existence of a writable statistics directory, logging errors if creation fails.???"
3222,_create_union_schema,"def _create_union_schema(self, types: tuple) -> dict:
        
        return {
            ""type"": ""object"",
            ""properties"": {
                ""response"": {""anyOf"": [self._get_type_info(t) for t in types]}
            },
            ""required"": self.required,
        }",Create schema for Union types.,???Generate a JSON schema with a union of specified type constraints.???
3223,format_task_result_xml,"def format_task_result_xml(task_result: TaskWithResult) -> str:
    
    from mcp_agent.llm.prompt_utils import format_fastagent_tag

    return format_fastagent_tag(
        ""task-result"",
        f""\n<fastagent:description>{task_result.description}</fastagent:description>\n""
        f""<fastagent:result>{task_result.result}</fastagent:result>\n"",
        {
            ""description"": task_result.description[:50] + ""...""
            if len(task_result.description) > 50
            else task_result.description
        },
    )",Format a task result with XML tags for better semantic understanding,???Format task result into XML with description and result tags.???
3224,create_namespace_if_not_exist,"def create_namespace_if_not_exist(self, namespace: str):
        
        try:
            self.core_v1_api.read_namespace(name=namespace)
            print(f""Namespace '{namespace}' already exists."")
        except ApiException as e:
            if e.status == 404:
                print(f""Namespace '{namespace}' not found. Creating namespace."")
                body = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace))
                self.core_v1_api.create_namespace(body=body)
                print(f""Namespace '{namespace}' created successfully."")
            else:
                print(f""Error checking/creating namespace '{namespace}': {e}"")",Create a namespace if it doesn't exist.,???Check and create a Kubernetes namespace if it does not already exist.???
3225,stats,"def stats(self) -> dict[str, float]:
        
        return {
            ""throughput_avg_items"": self.processed
            / (self.last_updated - self.created_at).total_seconds(),
            ""throughput_avg_mb"": self.network_used_mb
            / (self.last_updated - self.created_at).total_seconds(),
            ""time_elapsed"": (self.last_updated - self.created_at).total_seconds(),
        }",Computed statistics for the job.,???Calculate average throughput and elapsed time for a process???
3226,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Log.enable"",
    }
    json = yield cmd_dict","Enables log domain, sends the entries collected so far to the client by means of the ``entryAdded`` notification.",???Enable logging by sending a command and awaiting a JSON response.???
3227,validate_provider,"def validate_provider(cls, value: str) -> str:
        
        valid_providers = {""jwks"", ""google"", ""github"", ""custom""}
        if value not in valid_providers and not value.startswith(""custom:""):
            raise ValueError(
                f""Invalid provider '{value}'. Must be one of {valid_providers} ""
                ""or start with 'custom:'""
            )
        return value",Validate the provider value.,???Ensure input string is a valid or custom provider name???
3228,_log_processing_stats,"def _log_processing_stats(
        self, filtered_texts: List[str], empty_indices: set, context_prefix: str
    ):
        
        total_chars = sum(len(text) for text in filtered_texts)
        avg_chars = total_chars / len(filtered_texts) if filtered_texts else 0

        logger.info(
            f""📊 OPENAI_STATS [{context_prefix}] Processing {len(filtered_texts)} non-empty texts ""
            f""(skipped {len(empty_indices)} empty, avg chars: {avg_chars:.0f})""
        )",Log statistics about texts being processed.,"???Log processing statistics for text data, including count and average character length.???"
3229,resize_image_if_needed,"def resize_image_if_needed(image, max_size=1024):
    
    if isinstance(image, str) and image.startswith('data:image'):
        return image  # Already a data URI, skip processing
    
    if isinstance(image, np.ndarray):
        image = Image.fromarray(image)
    elif not isinstance(image, Image.Image):
        image = Image.open(image)
    
    # Get original dimensions
    width, height = image.size
    
    # Calculate new dimensions if needed
    if width > max_size or height > max_size:
        if width > height:
            new_width = max_size
            new_height = int(height * (max_size / width))
        else:
            new_height = max_size
            new_width = int(width * (max_size / height))
        
        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    
    # Convert to RGB if necessary
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # Convert to base64
    buffered = io.BytesIO()
    image.save(buffered, format=""JPEG"", quality=85)
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return f""data:image/jpeg;base64,{img_str}""",Resize image if either dimension exceeds max_size while maintaining aspect ratio,???Resize and convert images to base64 JPEG if dimensions exceed a specified maximum size.???
3230,delete_model,"def delete_model(model_name: str) -> bool:
    
    # Check if we're running in Docker
    
    # In Docker environment, delegate to docker module
    if in_docker:
        return docker.delete_model(model_name, ollama_url)
        
    # Non-Docker environment
    if not is_ollama_server_running():
        if not start_ollama_server():
            return False
    
    print(f""{Fore.YELLOW}Deleting model {model_name}...{Style.RESET_ALL}"")
    
    try:
        # Use the Ollama CLI to delete the model
        process = subprocess.run([""ollama"", ""rm"", model_name], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        
        if process.returncode == 0:
            print(f""{Fore.GREEN}Model {model_name} deleted successfully.{Style.RESET_ALL}"")
            return True
        else:
            print(f""{Fore.RED}Failed to delete model {model_name}. Error: {process.stderr}{Style.RESET_ALL}"")
            return False
    except Exception as e:
        print(f""{Fore.RED}Error deleting model {model_name}: {e}{Style.RESET_ALL}"")
        return False",Delete a locally downloaded Ollama model.,???Determine and execute the appropriate method to delete a model based on the environment.???
3231,track_usage,"def track_usage(method_name: str | None = None) -> Callable[[F], F]:
    

    def decorator(func: F) -> F:
        @wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            event_name = method_name if method_name is not None else f""{func.__module__}.{func.__name__}""
            capture_event(f""method.called.{event_name}"", {""args"": args, ""kwargs"": kwargs})
            result = func(*args, **kwargs)
            return result

        return wrapper  # type: ignore

    return decorator",Decorator to track usage of a method.,"???  
Decorator logs function calls with optional custom event names.  
???"
3232,generate_erd_section,"def generate_erd_section(
    md: List[str], tables: List[str], table_metadata: Dict[str, dict], fk_relationships: List[dict]
) -> None:
    
    # Define tables with their columns
    for table in tables:
        table_upper = table.upper()
        md.append(f""    {table_upper} {{\n"")
        for col in table_metadata[table][""columns""]:
            col_type = str(col[""type""]).split(""("")[0].upper()  # Simplify type names
            annotations = []
            if col[""name""] in table_metadata[table][""primary_keys""]:
                annotations.append(""PK"")
            # Check if column is a foreign key
            for fk in fk_relationships:
                if fk[""source_table""] == table and fk[""source_column""] == col[""name""]:
                    annotations.append(""FK"")
                    break
            annotation_str = "" "".join(annotations)
            md.append(f""        {col_type} {col['name']} {annotation_str}\n"")
        md.append(""    }\n"")

    # Define relationships with cardinality
    for fk in fk_relationships:
        target_table = fk[""target_table""].upper()
        source_table = fk[""source_table""].upper()
        source_cardinality = get_source_cardinality(fk[""is_unique""], fk[""is_nullable""])
        md.append(f""    {target_table} ||--{source_cardinality} {source_table} : \""{fk['constraint_name']}\""\n"")",Generate Mermaid ER diagram section.,"???Generate an Entity-Relationship Diagram section with tables, columns, and relationships.???"
3233,verify_model_description,"def verify_model_description(description):
    
    assert isinstance(description, ModelDescription), ""Model description should be a 'ModelDescription' object""
    required_fields = [""intent"", ""schemas"", ""code""]
    for field in required_fields:
        assert hasattr(description, field), f""Model description missing required field: {field}""",Verify that a model description contains expected fields.,???Ensure model description object contains essential fields for validation???
3234,set_timing_if_not_set,"def set_timing_if_not_set(
        self, span_key, span, start_time_iso, end_time_iso, latency_ms
    ):
        
        if span_key not in self.spans_with_set_times:
            span.set_attribute(SpanAttributes.SPAN_START_TIME, start_time_iso)
            span.set_attribute(SpanAttributes.SPAN_END_TIME, end_time_iso)
            span.set_attribute(SpanAttributes.SPAN_DURATION, latency_ms)
            self.spans_with_set_times.add(span_key)
            logger.debug(f""Set timing on span {span_key}"")
            return True
        return False",Set timing data on a span if it hasn't been set already.,???Ensure span timing attributes are set if not already defined???
3235,mock_google_cloud_credentials,"def mock_google_cloud_credentials() -> Generator[None, None, None]:
    
    with patch.dict(
        os.environ,
        {
            ""GOOGLE_APPLICATION_CREDENTIALS"": ""/path/to/mock/credentials.json"",
            ""GOOGLE_CLOUD_PROJECT_ID"": ""mock-project-id"",
        },
    ):
        yield",Mock Google Cloud credentials for testing.,???Simulate Google Cloud credentials for testing by patching environment variables.???
3236,ref_to_python,"def ref_to_python(ref: str) -> str:
    
    if ""."" in ref:
        domain, subtype = ref.split(""."")
        ref = ""{}.{}"".format(snake_case(domain), subtype)
    return f""{ref}""",Convert a CDP ``$ref`` to the name of a Python type.,"???  
Converts a reference string to a snake_case domain format if it contains a dot.  
???"
3237,validate_pdf_path,"def validate_pdf_path(pdf_path: str) -> bool:
        
        if not pdf_path:
            logger.error(""PDF path is required"")
            return False
        if not os.path.exists(pdf_path):
            logger.error(f""PDF file not found: {pdf_path}"")
            return False
        if not pdf_path.lower().endswith("".pdf""):
            logger.error(f""File must be a PDF: {pdf_path}"")
            return False
        return True",Validate the PDF file path.,"???  
Check if a given path is a valid existing PDF file.  
???"
3238,_dict_to_yaml,"def _dict_to_yaml(d, level=0):
        
        yaml_str = """"
        indent = ""  "" * level
        for k, v in d.items():
            if isinstance(v, dict):
                yaml_str += f""{indent}- {k}:\n{_dict_to_yaml(v, level + 1)}""
            else:
                yaml_str += f""{indent}- {k}: {str(v).replace('docs/en/', '')}\n""
        return yaml_str",Converts a nested dictionary to a YAML-formatted string with indentation.,??? Convert nested dictionary to YAML string format recursively. ???
3239,_parse_metadata,"def _parse_metadata(self, metadata: Optional[str]) -> Dict[str, Any]:
        
        try:
            if not metadata:
                return self.DEFAULT_METADATA.copy()
            
            custom_metadata = json.loads(metadata)
            metadata = self.DEFAULT_METADATA.copy()
            metadata.update(custom_metadata)
            return metadata
        except json.JSONDecodeError as e:
            logger.error(f""Invalid metadata JSON: {e}"")
            return self.DEFAULT_METADATA.copy()",Parse and validate notebook metadata.,"???Parse and merge JSON metadata with default settings, handling errors.???"
3240,get_playback_rate,"def get_playback_rate() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, float]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Animation.getPlaybackRate"",
    }
    json = yield cmd_dict
    return float(json[""playbackRate""])",Gets the playback rate of the document timeline.,??? Retrieve animation playback rate using a generator for asynchronous communication. ???
3241,generate_one_between,"def generate_one_between(puzzle: Puzzle, solution: dict[Literal, int]) -> set[Clue]:
    

    clues: set[Clue] = set()
    for left, right in zip(puzzle.houses, puzzle.houses[2:]):
        items_left = {item: loc for item, loc in solution.items() if loc == left}
        items_right = {item: loc for item, loc in solution.items() if loc == right}
        pairs: set[tuple[Literal, Literal]] = {(item1, item2) for item1, item2 in product(items_left, items_right)}
        for pair in pairs:
            clues.add(one_between(pair[0], pair[1], puzzle.houses))

    return clues",Generate the `one_between` Clue instances,???Generate clues for items with one house between them in a puzzle solution.???
3242,gen_version,"def gen_version() -> Version:
    
    now = datetime.datetime.now(datetime.UTC)
    return Version(f'{now.year}.{now.month}.{gen_buildrelease()}')",Generate release version based on current time.,???Generate a version string using the current date and a build release number.???
3243,matplotlib,"def matplotlib(fig: plt.Figure | None = None, label: str = ""plot"", component_id: str | None = None, **kwargs) -> ComponentReturn:
    

    if fig is None:
        fig, ax = plt.subplots()
        ax.plot([0, 1, 2], [0, 1, 4])

    # Save the figure as a base64-encoded PNG
    buf = io.BytesIO()
    fig.savefig(buf, format=""png"")
    buf.seek(0)
    img_b64 = base64.b64encode(buf.read()).decode()

    component = {
        ""type"": ""matplotlib"",
        ""id"": component_id,
        ""label"": label,
        ""image"": img_b64,  # Store the image data
    }

    return ComponentReturn(
        component_id, component
    )",Render a Matplotlib figure as a component.,???Convert a matplotlib figure to a base64-encoded image for component integration.???
3244,severity_levels_response_data,"def severity_levels_response_data() -> Dict[str, Any]:
    
    return {
        ""severityLevels"": [
            {""code"": ""low"", ""name"": ""General guidance""},
            {""code"": ""normal"", ""name"": ""System impaired""},
            {""code"": ""high"", ""name"": ""Production system impaired""},
            {""code"": ""urgent"", ""name"": ""Production system down""},
            {""code"": ""critical"", ""name"": ""Business-critical system down""},
        ]
    }",Return a dictionary with sample severity levels response data.,"???  
Define a dictionary mapping severity levels to their corresponding descriptions.  
???"
3245,pixels_to_algebraic,"def pixels_to_algebraic(x_px: float, y_px: float, square_size: float) -> str:
	
	if not square_size:
		raise ValueError('Square size cannot be zero or None.')

	x_index = int(round(x_px / square_size))
	y_index = int(round(y_px / square_size))

	if 0 <= x_index < 8 and 0 <= y_index < 8:
		return f'{FILES[x_index]}{RANKS[y_index]}'

	raise ValueError(f'Pixel coordinates out of bounds: ({x_px}, {y_px})')",Converts Lichess pixel coordinates to algebraic notation using dynamic size.,???Convert pixel coordinates to chessboard algebraic notation.???
3246,get_remote_version,"def get_remote_version():
    
    if os.path.exists(REMOTE_VERSION_FILE):
        try:
            with open(REMOTE_VERSION_FILE, ""r"") as f:
                data_content = json.load(f)
                return (
                    data_content.get(""version"", ""0.0.0""),
                    data_content.get(""changelog"", []),
                    data_content.get(""download_url"", ""#""),
                )
        except json.JSONDecodeError:
            print(f""Error: Invalid JSON in remote file: {REMOTE_VERSION_FILE}"")
            return ""0.0.0"", [], ""#""
        except Exception as e:
            print(f""Error reading remote version file {REMOTE_VERSION_FILE}: {e}"")
            return ""0.0.0"", [], ""#""
    return ""0.0.0"", [], ""#""","Reads the downloaded remote file and returns (version, changelog, download_url).","???Retrieve version, changelog, and URL from a remote JSON file if it exists.???"
3247,discover_all_files,"def discover_all_files(self):
        
        all_image_files = StateTracker.get_image_files(
            data_backend_id=self.id
        ) or StateTracker.set_image_files(
            self.image_data_backend.list_files(
                instance_data_dir=self.instance_data_dir,
                file_extensions=image_file_extensions,
            ),
            data_backend_id=self.id,
        )
        # This isn't returned, because we merely check if it's stored, or, store it.
        (
            StateTracker.get_vae_cache_files(data_backend_id=self.id)
            or StateTracker.set_vae_cache_files(
                self.cache_data_backend.list_files(
                    instance_data_dir=self.cache_dir,
                    file_extensions=[""pt""],
                ),
                data_backend_id=self.id,
            )
        )
        self.debug_log(
            f""VAECache discover_all_files found {len(all_image_files)} images""
        )
        return all_image_files",Identify all files in the data backend.,"???Retrieve and store image and cache files, logging the total images found.???"
3248,get_image,"def get_image(self, image_id: str = None):
        
        try:
            image = self.collection.get_image(image_id)
            return {
                ""id"": image.id,
                ""url"": image.url,
                ""name"": image.name,
                ""description"": getattr(image, ""description"", None),
                ""collection_id"": image.collection_id,
            }
        except Exception as e:
            raise Exception(f""Failed to fetch image with ID {image_id}: {e}"")",Fetch image details by ID or validate an image URL.,"???Retrieve image details by ID, handling exceptions for retrieval errors.???"
3249,sample_memory_item_fixture,"def sample_memory_item_fixture():
    

    conversation = [
        {
            ""role"": ""user"",
            ""content"": ""Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts."",
        },
        {
            ""role"": ""assistant"",
            ""content"": ""Hello Alex! I've noted that you're a vegetarian and have a nut allergy."",
        },
    ]

    return MemoryItem(conversation=conversation,
                      user_id=""user123"",
                      memory=""Sample memory"",
                      metadata={""key1"": ""value1""},
                      tags=[""tag1"", ""tag2""])",Fixture to provide a sample MemoryItem.,???Create a sample memory item with user conversation and metadata for testing.???
3250,get_tweet_replies,"def get_tweet_replies(self, tweet_id: str, count: int = 10, **kwargs) -> List[dict]:
        
        logger.debug(f""Fetching replies for tweet {tweet_id}, count: {count}"")
        
        params = {
            ""query"": f""conversation_id:{tweet_id} is:reply"",
            ""tweet.fields"": ""author_id,created_at,text"",
            ""max_results"": min(count, 100)
        }
        
        response = self._make_request('get', 'tweets/search/recent', params=params)
        replies = response.get(""data"", [])
        
        logger.info(f""Retrieved {len(replies)} replies"")
        return replies",Fetch replies to a specific tweet,???Fetch and return a specified number of recent replies to a given tweet.???
3251,_run_async_loop,"def _run_async_loop(self) -> None:
        
        self._loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self._loop)
        try:
            # Create a task that we can cancel
            self._ws_task = self._loop.create_task(self.watch())
            self._loop.run_until_complete(self._ws_task)  # pyright: ignore [reportUnknownMemberType, reportUnknownArgumentType]
        except asyncio.CancelledError:
            pass  # Task was cancelled, which is expected during shutdown
        except Exception as e:
            logger.debug(f""Unexpected exception in recording loop: {e}"")
        finally:
            # Run all remaining tasks to completion
            pending = asyncio.all_tasks(self._loop)
            for task in pending:
                _ = task.cancel()
            if pending:
                # Allow tasks to perform cleanup
                _ = self._loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
            self._loop.close()
            self._loop = None
            self._ws_task = None",Run the async event loop in a separate thread.,???Initialize and manage an asynchronous event loop for task execution and cleanup???
3252,yaml_model_load,"def yaml_model_load(path):
    
    path = Path(path)
    if path.stem in (f""yolov{d}{x}6"" for x in ""nsmlx"" for d in (5, 8)):
        new_stem = re.sub(r""(\d+)([nslmx])6(.+)?$"", r""\1\2-p6\3"", path.stem)
        LOGGER.warning(f""WARNING ⚠️ Ultralytics YOLO P6 models now use -p6 suffix. Renaming {path.stem} to {new_stem}."")
        path = path.with_name(new_stem + path.suffix)

    unified_path = re.sub(r""(\d+)([nslmx])(.+)?$"", r""\1\3"", str(path))  # i.e. yolov8x.yaml -> yolov8.yaml
    yaml_file = check_yaml(unified_path, hard=False) or check_yaml(path)
    d = yaml_load(yaml_file)  # model dict
    d[""scale""] = guess_model_scale(path)
    d[""yaml_file""] = str(path)
    return d",Load a YOLOv8 model from a YAML file.,???Load and adjust YOLO model configuration from a YAML file path.???
3253,save_uploaded_file,"def save_uploaded_file(uploaded_file):
    
    # Create a temporary directory if it doesn't exist
    temp_dir = Path(tempfile.gettempdir()) / ""secure_chat_uploads""
    temp_dir.mkdir(exist_ok=True)

    # Generate a unique filename
    file_extension = os.path.splitext(uploaded_file.name)[1]
    unique_filename = f""{uuid.uuid4()}{file_extension}""
    file_path = temp_dir / unique_filename

    # Save the file
    with open(file_path, ""wb"") as f:
        f.write(uploaded_file.getbuffer())

    return str(file_path)",Save uploaded file to a temporary directory and return the path.,??? Save uploaded file to a unique path in a temporary directory ???
3254,content,"def content(self) -> str:
        
        if self.start_comment == 0:
            # This is the first subsequence of comments. We'll include the entire body of the issue.
            issue_str = self.issue.pretty
        else:
            # This is a middle subsequence of comments. We'll only include the title of the issue.
            issue_str = f""# Issue: {self.issue.title}""
        # Now add the comments themselves.
        comments = self.issue.comments[self.start_comment : self.end_comment]
        comments_str = ""\n\n"".join([comment.pretty for comment in comments])
        return issue_str + ""\n\n"" + comments_str","The title of the issue, followed by the comments in the chunk.",???Generate a formatted string of issue details and selected comments.???
3255,create_exact_match_pattern,"def create_exact_match_pattern(string_list: List[Union[uuid.UUID, str]]) -> re.Pattern:
    
    escaped_strings = [re.escape(str(s)) for s in string_list]
    pattern = f""({'|'.join(escaped_strings)})$""
    return re.compile(pattern)",Create exact match patterns for filtering out the desired respones.,???Generate regex pattern for exact string or UUID match from list???
3256,raise_error_if_cannot_convert,"def raise_error_if_cannot_convert(
        json_schema_dict: dict[str, Any],
        api_option: Literal['VERTEX_AI', 'GEMINI_API'],
        raise_error_on_unsupported_field: bool,
    ) -> None:
      
      if not raise_error_on_unsupported_field:
        return
      for field_name, field_value in json_schema_dict.items():
        if field_value is None:
          continue
        if field_name not in google_schema_field_names:
          raise ValueError((
              f'JSONSchema field ""{field_name}"" is not supported by the '
              'Schema object. And the ""raise_error_on_unsupported_field"" '
              'argument is set to True. If you still want to convert '
              'it into the Schema object, please either remove the field '
              f'""{field_name}"" from the JSONSchema object, or leave the '
              '""raise_error_on_unsupported_field"" unset.'
          ))
        if (
            field_name in gemini_api_unsupported_field_names
            and api_option == 'GEMINI_API'
        ):
          raise ValueError((
              f'The ""{field_name}"" field is not supported by the Schema '
              'object for GEMINI_API.'
          ))",Raises an error if the JSONSchema cannot be converted to the specified Schema object.,"???Validate JSON schema fields against API compatibility, raising errors for unsupported fields.???"
3257,check_and_restart_threads,"def check_and_restart_threads():
    
    configured_apps_list = settings_manager.get_configured_apps() # Corrected function name
    configured_apps = {app: True for app in configured_apps_list} # Convert list to dict format expected below

    for app_type, thread in list(app_threads.items()):
        if not thread.is_alive():
            logger.warning(f""{app_type} thread died unexpectedly."")
            del app_threads[app_type] # Remove dead thread
            # Only restart if it's still configured
            if configured_apps.get(app_type, False):
                logger.info(f""Restarting thread for {app_type}..."")
                new_thread = threading.Thread(target=app_specific_loop, args=(app_type,), name=f""{app_type}-Loop"", daemon=True)
                app_threads[app_type] = new_thread
                new_thread.start()
            else:
                logger.info(f""Not restarting {app_type} thread as it is no longer configured."")",Check if any threads have died and restart them if the app is still configured.,???Monitor and restart application threads if they unexpectedly terminate and are still configured.???
3258,_get_timing_info,"def _get_timing_info(execution_info=None):
    
    import time
    
    # Get session timing information
    try:
        from cai.cli import START_TIME
        total_time = time.time() - START_TIME if START_TIME else None
    except ImportError:
        total_time = None
    
    # Extract execution timing info
    tool_time = None
    if execution_info:
        tool_time = execution_info.get('tool_time')
    
    # Format timing info for display
    timing_info = []
    if total_time:
        timing_info.append(f""Total: {format_time(total_time)}"")
    if tool_time:
        timing_info.append(f""Tool: {format_time(tool_time)}"")
    
    return timing_info, tool_time",Get timing information for display.,???Retrieve and format session and execution timing details for display.???
3259,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DeviceAccess.disable"",
    }
    json = yield cmd_dict",Disable events in this domain.,???Disables device access by sending a command via a generator.???
3260,convert_to_anthropic_tools,"def convert_to_anthropic_tools(
        tools_schema: list[dict],
) -> list[dict]:
    
    # already in Anthropic tool format
    result_schema = []
    for tool in tools_schema:
        if isinstance(tool, dict) and all(
                k in tool for k in (""name"", ""description"", ""input_schema"")
        ):
            result_schema.append(tool)
        else:
            if 'function' not in tool.keys():
                raise ValueError(""not valid openAI schema"")
            oai_formatted = tool[""function""]
            if not all(k in oai_formatted for k in (""name"", ""description"", ""parameters"")):
                raise ValueError(""not valid openAI schema"")
            result_schema.append({'name': oai_formatted[""name""],
                                  'description': oai_formatted['description'],
                                  'input_schema': oai_formatted[""parameters""]})
    return result_schema",Convert an openAI tools schema to anthropic,???Transform OpenAI tool schemas into Anthropic-compatible format.???
3261,show,"def show(self, title=None):
        
        im = Image.fromarray(np.asarray(self.im)[..., ::-1])  # Convert numpy array to PIL Image with RGB to BGR
        if IS_COLAB or IS_KAGGLE:  # can not use IS_JUPYTER as will run for all ipython environments
            try:
                display(im)  # noqa - display() function only available in ipython environments
            except ImportError as e:
                LOGGER.warning(f""Unable to display image in Jupyter notebooks: {e}"")
        else:
            im.show(title=title)",Show the annotated image.,"???Display an image using PIL, adapting for Jupyter or standalone environments.???"
3262,save_current_chat,"def save_current_chat(self, title: str = None, overwrite: bool = False):
        
        if not self.chat_history:
            return
            
        if title:
            # If overwriting, use existing chat_id if it matches the title
            if not overwrite or self.current_chat_id != title:
                self.current_chat_id = title
        elif not self.current_chat_id:
            self.current_chat_id = datetime.now().strftime(""%Y%m%d_%H%M%S"")
            
        # Convert chat history to serializable format
        chat_data = {
            'id': self.current_chat_id,
            'messages': [msg.to_dict() for msg in self.chat_history]
        }
        
        # Save to file
        filename = f""{self.chats_dir}/chat_{self.current_chat_id}.json""
        with open(filename, 'w') as f:
            json.dump(chat_data, f)
            
        # Update original state to reflect saved state
        self._original_chat_state = [msg.to_dict() for msg in self.chat_history]",Save the current chat to disk if it has any messages,???Save chat history to a JSON file with optional title and overwrite settings.???
3263,valid_config,"def valid_config(tmp_path):
    
    config_content = {
        ""mcpServers"": {
            ""ServerA"": {""param"": ""valueA""},
            ""ServerB"": {""param"": ""valueB""},
            ""ServerC"": {""param"": ""valueC""}
        }
    }
    config_file = tmp_path / ""config_valid.json""
    config_file.write_text(json.dumps(config_content))
    return config_file",Create a temporary config file with valid JSON.,"???  
Creates a JSON configuration file with server parameters at a temporary path.  
???"
3264,get_author_details,"def get_author_details(api_url: str, api_key: str, author_id: int, api_timeout: int = 120) -> Optional[Dict]:
    
    endpoint = f""{api_url}/api/v1/author/{author_id}""
    headers = {'X-Api-Key': api_key}
    try:
        response = requests.get(endpoint, headers=headers, timeout=api_timeout)
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
        author_data = response.json()
        logger.debug(f""Successfully fetched details for author ID {author_id}."")
        return author_data
    except requests.exceptions.RequestException as e:
        logger.error(f""Error fetching author details for ID {author_id} from {endpoint}: {e}"")
        return None
    except Exception as e:
        logger.error(f""An unexpected error occurred fetching author details for ID {author_id}: {e}"")
        return None",Fetches details for a specific author from the Readarr API.,???Fetch author details from API with error handling and logging???
3265,patch_owl_logging,"def patch_owl_logging():
    
    try:
        from owl import utils
        
        # If run_society exists in utils, patch it to log
        if hasattr(utils, 'run_society'):
            original_run = utils.run_society
            
            def logged_run_society(*args, **kwargs):
                logging.info(""🦉 OWL run_society called"")
                try:
                    result = original_run(*args, **kwargs)
                    logging.info(""🦉 OWL run_society completed"")
                    return result
                except Exception as e:
                    logging.error(f""🦉 OWL run_society error: {str(e)}"")
                    raise
            
            # Replace the original function
            utils.run_society = logged_run_society
            logging.info(""🦉 OWL run_society patched with logging"")
            
        return True
    except ImportError:
        logging.warning(""⚠️ Could not patch OWL logging - module not found"")
        return False
    except Exception as e:
        logging.warning(f""⚠️ Error patching OWL logging: {str(e)}"")
        return False",Try to patch specific OWL functions to add logging.,???Enhances OWL's run_society function with logging for execution tracking and error handling.???
3266,_calculate_n_bars,"def _calculate_n_bars(self, interval: Interval, lookback_periods: int) -> int:
        
        interval_minutes = {
            Interval.in_1_minute: 1,
            Interval.in_3_minute: 3,
            Interval.in_5_minute: 5,
            Interval.in_15_minute: 15,
            Interval.in_30_minute: 30,
            Interval.in_45_minute: 45,
            Interval.in_1_hour: 60,
            Interval.in_2_hour: 120,
            Interval.in_3_hour: 180,
            Interval.in_4_hour: 240,
            Interval.in_daily: 1440,
            Interval.in_weekly: 10080,
            Interval.in_monthly: 43200
        }
        
        minutes_in_lookback = lookback_periods * interval_minutes[interval]",Calculate number of bars needed based on interval and lookback period.,???Calculate total minutes for a given interval and lookback period.???
3267,store_essence_marker,"def store_essence_marker(self, marker_type: str, marker_text: str) -> None:
        
        try:
            with self.get_connection() as conn:
                now = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
                conn.execute(
                    ,
                    (marker_type, marker_text, now),
                )
                conn.commit()
        except sqlite3.Error as e:
            self.logger.error(f""Database error storing essence marker: {e}"")",Store essence marker in database,???Store essence marker in database with timestamp and error logging.???
3268,_process_paragraph,"def _process_paragraph(self, doc: Document, element: Tag, style_config: Dict) -> None:
        
        try:
            p = doc.add_paragraph()
            for child in element.children:
                if child.name == 'strong':
                    run = p.add_run(child.get_text())
                    run.bold = True
                elif child.name == 'em':
                    run = p.add_run(child.get_text())
                    run.italic = True
                elif child.name == 'code':
                    run = p.add_run(child.get_text())
                    run.font.name = style_config[""code_font""]
                elif child.name == 'a':
                    run = p.add_run(child.get_text())
                    run.font.color.rgb = RGBColor(*style_config[""link_color""])
                    run.underline = True
                else:
                    p.add_run(str(child))
        except Exception as e:
            logger.error(f""Failed to process paragraph: {e}"")",Process a paragraph element and add it to document.,???Convert HTML elements to styled document text with error handling.???
3269,convert_raw_input,"def convert_raw_input(file):
    
    print(f""\n📁 Processing file: {file.filepath}"")
    for call in file.function_calls:
        if call.name == ""raw_input"":
            print(f""  🔄 Found raw_input: {call.source}"")
            print(f""  ✨ Converting to: input{call.source[len('raw_input') :]}"")
            call.edit(f""input{call.source[len('raw_input') :]}"")",Convert raw_input() calls to input(),???Convert 'raw_input' calls to 'input' in a file's function calls.???
3270,get_bert_embeddings,"def get_bert_embeddings(self, text):
        
        inputs = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, truncation=True,
                                padding='max_length')
        inputs = {key: val.to(self.device) for key, val in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
        last_hidden_states = outputs.last_hidden_state
        mean_embedding = torch.mean(last_hidden_states, dim=1).squeeze().cpu().numpy()
        return mean_embedding",Get BERT embeddings for a given text.,???Extracts mean BERT embeddings from input text using a tokenizer and model.???
3271,validate_expert_provider,"def validate_expert_provider(provider: str) -> ValidationResult:
    
    if not provider:
        return ValidationResult(valid=True, missing_vars=[])

    strategy = ProviderFactory.create(provider)
    if not strategy:
        return ValidationResult(
            valid=False, missing_vars=[f""Unknown expert provider: {provider}""]
        )

    # Copy base vars to expert vars for fallback
    copy_base_to_expert_vars(provider, provider)

    # Validate expert configuration
    result = strategy.validate()
    missing = []

    for var in result.missing_vars:
        key = var.split()[0]  # Get the key name without the error message
        expert_key = f""EXPERT_{key}""
        if not os.environ.get(expert_key):
            missing.append(f""{expert_key} environment variable is not set"")

    return ValidationResult(valid=len(missing) == 0, missing_vars=missing)",Validate expert provider configuration with fallback.,"???Validate expert provider configuration and environment variables, returning validation results.???"
3272,reset_state,"def reset_state(self):
        

        st.session_state.clear()

        # Reset to default values
        for key, value in self.default_values.items():
            st.session_state[key] = value

        # Initialize empty collections
        st.session_state.messages = []
        st.session_state.responses = {}
        st.session_state.tools = {}
        st.session_state.session_id = str(uuid.uuid4())",Reset application state while preserving authentication,???Reset session state to default values and initialize collections???
3273,avg_pool_nd,"def avg_pool_nd(dims, *args, **kwargs):
    
    if dims == 1:
        return nn.AvgPool1d(*args, **kwargs)
    elif dims == 2:
        return nn.AvgPool2d(*args, **kwargs)
    elif dims == 3:
        return nn.AvgPool3d(*args, **kwargs)
    raise ValueError(f""unsupported dimensions: {dims}"")","Create a 1D, 2D, or 3D average pooling module.",???Selects appropriate average pooling layer based on input dimensions.???
3274,show_diff,"def show_diff(expected: str, got: str, context: int = 3) -> None:
    
    diff = list(
        unified_diff(
            expected.splitlines(keepends=True),
            got.splitlines(keepends=True),
            fromfile=""expected"",
            tofile=""got"",
            n=context,
        )
    )

    if diff:
        print(""\n[yellow]Differences found:[/yellow]"")
        print("""".join(diff))

    # Also show full outputs with line numbers for reference
    print(""\n[blue]Expected output[/blue] ({} lines):"".format(len(expected.splitlines())))
    syntax = Syntax(expected, ""text"", line_numbers=True, word_wrap=True)
    console.print(syntax)

    print(""\n[blue]Actual output[/blue] ({} lines):"".format(len(got.splitlines())))
    syntax = Syntax(got, ""text"", line_numbers=True, word_wrap=True)
    console.print(syntax)",Show a readable diff between expected and got.,???Display and compare text differences with context and line numbers.???
3275,pred_to_json,"def pred_to_json(self, predn, filename):
        
        stem = Path(filename).stem
        image_id = int(stem) if stem.isnumeric() else stem
        box = ops.xyxy2xywh(predn[:, :4])  # xywh
        box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner
        for p, b in zip(predn.tolist(), box.tolist()):
            self.jdict.append(
                {
                    ""image_id"": image_id,
                    ""category_id"": self.class_map[int(p[5])],
                    ""bbox"": [round(x, 3) for x in b],
                    ""score"": round(p[4], 5),
                }
            )",Serialize YOLO predictions to COCO json format.,???Convert prediction data to JSON format with image and category details.???
3276,validate_role_content,"def validate_role_content(self):
        
        if self.role == MessageRole.TOOL:
            for item in self.content:
                if not isinstance(item, ToolResultContent):
                    raise ValueError(
                        ""When role is 'tool', content can only contain ToolResultContent objects""
                    )
        elif self.role == MessageRole.USER:
            for item in self.content:
                if not isinstance(item, (TextContent, FileContent)):
                    raise ValueError(
                        f""When role is '{self.role}', content can only contain TextContent or FileContent objects""
                    )
        elif self.role in MessageRole.SYSTEM:
            for item in self.content:
                if not isinstance(item, TextContent):
                    raise ValueError(
                        f""When role is '{self.role}', content can only contain TextContent objects""
                    )
        elif self.role == MessageRole.ASSISTANT:
            for item in self.content:
                if not isinstance(item, (TextContent, ToolCallContent)):
                    raise ValueError(
                        ""When role is 'assistant', content can only contain TextContent or ToolCallContent objects""
                    )
        return self",Validate the role and content of the message.,??? Validate message content types based on user role constraints ???
3277,_format_agent_category,"def _format_agent_category(self, category: AgentRouterCategory) -> str:
        
        description = category.description or ""No description provided""
        servers = ""\n"".join(
            [f""- {server.name} ({server.description})"" for server in category.servers]
        )

        return f""Agent Category: {category.name}\nDescription: {description}\nServers in agent:\n{servers}""",Format an agent category into a readable string.,???Formats agent category details into a structured string output.???
3278,_format_error,"def _format_error(self, error_type: str, message: str, additional_info: Dict[str, str] = None) -> str:
        
        output = [f""## {error_type}"", f""**Message:** {message}""]

        if additional_info:
            output.append(""**Additional Information:**"")
            for key, value in additional_info.items():
                output.append(f""- **{key}:** {value}"")

        output.append(f""## End {error_type}"")
        return ""\n\n"".join(output)",Format error messages consistently using Markdown,"???  
Formats error details into a structured string with optional additional information.  
???"
3279,get_value,"def get_value(self, name: str, frame_index: int) -> Any:
        
        if name in self.parameters:
            return self.parameters[name].get_value(frame_index)
        raise KeyError(f""Parameter {name} not registered"")",Get the value of a parameter for a specific frame,"???Retrieve parameter value by name and frame index, or raise error if not found.???"
3280,plot_mc_curve,"def plot_mc_curve(px, py, save_dir=Path(""mc_curve.png""), names={}, xlabel=""Confidence"", ylabel=""Metric"", on_plot=None):
    
    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)

    if 0 < len(names) < 21:  # display per-class legend if < 21 classes
        for i, y in enumerate(py):
            ax.plot(px, y, linewidth=1, label=f""{names[i]}"")  # plot(confidence, metric)
    else:
        ax.plot(px, py.T, linewidth=1, color=""grey"")  # plot(confidence, metric)

    y = smooth(py.mean(0), 0.05)
    ax.plot(px, y, linewidth=3, color=""blue"", label=f""all classes {y.max():.2f} at {px[y.argmax()]:.3f}"")
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.legend(bbox_to_anchor=(1.04, 1), loc=""upper left"")
    ax.set_title(f""{ylabel}-Confidence Curve"")
    fig.savefig(save_dir, dpi=250)
    plt.close(fig)
    if on_plot:
        on_plot(save_dir)",Plots a metric-confidence curve.,???Generate and save a confidence-metric curve plot with optional class legends.???
3281,to_input_item,"def to_input_item(self) -> TResponseInputItem:
        
        if isinstance(self.raw_item, dict):
            # We know that input items are dicts, so we can ignore the type error
            return self.raw_item  # type: ignore
        elif isinstance(self.raw_item, BaseModel):
            # All output items are Pydantic models that can be converted to input items.
            return self.raw_item.model_dump(exclude_unset=True)  # type: ignore
        else:
            raise AgentsException(f""Unexpected raw item type: {type(self.raw_item)}"")",Converts this item into an input item suitable for passing to the model.,"???Convert raw data into a standardized input item format, handling dictionaries and Pydantic models.???"
3282,add_user_message,"def add_user_message(self, content: str) -> None:
        
        self.conversation_history.append({""role"": ""user"", ""content"": content})",Add user message to conversation.,???Appends a user message to the conversation history list.???
3283,chat_loop,"def chat_loop(agent: SimpleAgent):
    
    print(""\nWelcome to the AI Chat (DynamoDB Backend)! (Type 'exit' to end)"")

    user_id = f""user_{uuid.uuid4().hex[:6]}""
    session_id = f""session_{uuid.uuid4().hex[:8]}""

    print(""--- Starting New Chat ---"")
    print(f""   User ID: {user_id}"")
    print(f""   Session ID: {session_id}"")
    print(""-------------------------"")

    while True:
        try:
            user_input = input(f""{user_id} You: "")
            if user_input.lower() == ""exit"":
                break
            if not user_input.strip():
                continue

            agent_input = {""input"": user_input, ""user_id"": user_id, ""session_id"": session_id}

            response = agent.run(agent_input)
            response_content = response.output.get(""content"", ""..."")
            print(f""AI ({agent.name}): {response_content}"")

        except KeyboardInterrupt:
            print(""\nExiting chat loop."")
            break
        except Exception as e:
            print(f""\nAn error occurred: {e}"", file=sys.stderr)
            break

    print(""\n--- Chat Session Ended ---"")",Runs the main chat loop.,???Interactive AI chat session with unique user and session identifiers.???
3284,expose_to_js,"def expose_to_js():
    
    import asyncio

    # from js import window  # type: ignore
    from pyodide.ffi import create_proxy, to_js  # type: ignore

    def wrap_async_function(func):
        

        async def wrapper(*args, **kwargs):
            future = asyncio.ensure_future(func(*args, **kwargs))
            return to_js(future)

        return create_proxy(wrapper)

    # Export functions to JavaScript

    window.preswaldInit = wrap_async_function(initialize_preswald)
    window.preswaldRunScript = wrap_async_function(run_script)
    window.preswaldUpdateComponent = wrap_async_function(update_component)
    window.preswaldShutdown = wrap_async_function(shutdown)
    window.preswaldExportHtml = wrap_async_function(export_html)  # Expose new function

    # Message handling from JS to Python
    def handle_js_message(client_id, message_type, data):
        
        if _service:
            asyncio.create_task(  # noqa: RUF006
                _service.handle_client_message(
                    client_id, {""type"": message_type, ""data"": data}
                )
            )
        return True

    window.handleMessageFromJS = create_proxy(handle_js_message)

    console.log(""Preswald Python API exposed to JavaScript"")",Expose Python functions to JavaScript,???Expose Python functions to JavaScript using async wrappers and message handling.???
3285,_count_tokens_for_messages,"def _count_tokens_for_messages(messages: List[Dict[str, str]], model_name: str) -> int:
    
    enc = tiktoken.encoding_for_model(model_name)
    text = """"
    for msg in messages:
        role = msg.get(""role"", """")
        content = msg.get(""content"", """")
        text += f""{role}: {content}\n""
    return len(enc.encode(text))",Count tokens for the conversation messages using tiktoken.,??? Calculate token count for message list using model-specific encoding. ???
3286,_extract_map_valid_actions,"def _extract_map_valid_actions(self, entry: Dict, actions: List[str]):
        
        mapped_actions = []
        action_lookup = getattr(entry['env'].config, 'action_lookup', None)
        if action_lookup is None:
            mapped_actions = actions
        else: # the envs have pre-defined action lookup
            rev_action_lookup = {v.lower(): k for k, v in action_lookup.items()}
            actions = [action.lower() for action in actions]
            mapped_actions = [rev_action_lookup[action] for action in actions if action in rev_action_lookup]
        return mapped_actions",extract valid actions from the action lookup table (if exists),???Map and validate actions using environment configuration lookup???
3287,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Inspector.enable"",
    }
    json = yield cmd_dict",Enables inspector domain notifications.,???Enable the Inspector tool by sending a command dictionary.???
3288,get_watched_playlists,"def get_watched_playlists():
    
    try:
        with _get_playlists_db_connection() as conn:  # Use playlists connection
            cursor = conn.cursor()
            cursor.execute(""SELECT * FROM watched_playlists WHERE is_active = 1"")
            playlists = [dict(row) for row in cursor.fetchall()]
            return playlists
    except sqlite3.Error as e:
        logger.error(
            f""Error retrieving watched playlists from {PLAYLISTS_DB_PATH}: {e}"",
            exc_info=True,
        )
        return []",Retrieves all active playlists from the watched_playlists table in playlists.db.,"???Retrieve active watched playlists from the database, handling errors.???"
3289,selectbox,"def selectbox(
    label: str,
    options: list[str],
    default: str | None = None,
    size: float = 1.0,
    component_id: str | None = None,
    **kwargs
) -> ComponentReturn:
    
    service = PreswaldService.get_instance()
    current_value = service.get_component_state(component_id)
    if current_value is None:
        current_value = (
            default if default is not None else (options[0] if options else None)
        )

    component = {
        ""type"": ""selectbox"",
        ""id"": component_id,
        ""label"": label,
        ""options"": options,
        ""value"": current_value,
        ""size"": size,
    }

    logger.debug(f""[selectbox] ID={component_id}, selected={current_value}"")

    return ComponentReturn(current_value, component)",Create a select component with consistent ID based on label.,???Create a dropdown component with default and current selection handling.???
3290,add_path,"def add_path(self, path: str, is_file: bool = True):
        
        parts = path.split(os.sep)
        if is_file:
            self.files_found += 1

        # Build indentation for each level
        for i in range(len(parts)):
            prefix = ""    "" * i
            if i == len(parts) - 1:
                marker = ""📄 "" if is_file else ""📁 ""
            else:
                marker = ""📁 ""
            self.tree.append(f""{prefix}{marker}{parts[i]}"")",Add a path to the tree,???Add file or directory path to hierarchical tree structure with visual markers.???
3291,extract_cvss_score,"def extract_cvss_score(vector):
    
    try:
        from cvss import CVSS3
        c = CVSS3(vector)
        return c.scores()[0]
    except Exception as e:
        print(f""Error calculating CVSS score: {e}"")
        return None",Extracts a score from a CVSS vector string using the CVSS library.,"???  
Extracts CVSS score from a vector using CVSS3 library, handling errors.  
???"
3292,deploy_docs,"def deploy_docs():
    
    config_path = get_config_path()
    try:
        subprocess.run([""mkdocs"", ""gh-deploy"", ""--config-file"", config_path], check=True)
        print(""Documentation deployed successfully."")
    except subprocess.CalledProcessError as e:
        print(f""Error deploying documentation: {e}"")
        sys.exit(1)",Deploy MkDocs documentation to GitHub Pages.,???Deploys documentation using MkDocs with error handling for deployment failures.???
3293,_save_model,"def _save_model(
        self,
        directory: str,
        transformer: torch.nn.Module,
        transformer_state_dict: Optional[Dict[str, torch.Tensor]] = None,
        scheduler: Optional[SchedulerType] = None,
    ) -> None:
        
        raise NotImplementedError(f""ModelSpecification::save_model is not implemented for {self.__class__.__name__}"")",Save the state dicts to the given directory.,"???Defines a method to save a model, but raises a not-implemented error.???"
3294,_validate_provider,"def _validate_provider(self, expected_provider: str) -> None:
        
        if self.provider != expected_provider:
            raise ValueError(f""This method requires the '{expected_provider}' provider."")",Validate that the current provider matches the expected provider.,"???Ensure the provider matches the expected value, raising an error if not.???"
3295,mmlu_cloze_prompt,"def mmlu_cloze_prompt(line, task_name: str = None):
    
    topic = line[""subject""]
    prompt = f""The following are questions about {topic.replace('_', ' ')}.\nQuestion: ""
    prompt += line[""question""] + ""\nAnswer:""

    return Doc(
        task_name=task_name,
        query=prompt,
        choices=[f"" {c}"" for c in line[""choices""]],
        gold_index=int(line[""answer""]),
        instruction=f""The following are questions about {topic.replace('_', ' ')}.\n"",
    )",MMLU prompt without choices,???Generate a formatted question prompt with choices and correct answer for a given topic.???
3296,_create_default_profile,"def _create_default_profile(self) -> Dict[str, Any]:
        
        return {
            ""student_id"": ""adaptive_learner_001"",
            ""target_grade"": ""11th grade"",
            ""learning_goal"": ""Master linear algebra concepts"",
            ""topics"": [
                {""name"": ""vectors"", ""proficiency"": 0.45},
                {""name"": ""matrices"", ""proficiency"": 0.30},
                {""name"": ""linear_systems"", ""proficiency"": 0.25},
                {""name"": ""eigenvalues_eigenvectors"", ""proficiency"": 0.20},
            ],
        }",Create default student profile.,???Create a default student profile with learning goals and topic proficiencies.???
3297,split_num,"def split_num(match: re.Match) -> str:
    
    num = match.group(0)
    if "":"" in num:
        h, m = num.split("":"")
        return f""{h} {m}""
    if num.endswith(""s""):
        return f""{num[:-1]} s""
    return num",Split numbers for TTS processing,"???  
Convert matched time formats into standardized string representations.  
???"
3298,sample_pricing_data_api,"def sample_pricing_data_api() -> Dict[str, Any]:
    
    return {
        'status': 'success',
        'service_name': 'AWSLambda',
        'data': [
            {
                'product': {
                    'attributes': {
                        'productFamily': 'Serverless',
                        'description': 'Run code without thinking about servers',
                    },
                },
                'terms': {
                    'OnDemand': {
                        'rate1': {
                            'priceDimensions': {
                                'dim1': {
                                    'unit': 'requests',
                                    'pricePerUnit': {'USD': '0.20'},
                                    'description': 'per 1M requests',
                                },
                            },
                        },
                    },
                },
            },
        ],
        'message': 'Retrieved pricing for AWSLambda in us-west-2 from AWS Pricing API',
    }",Sample pricing data from AWS Price List API.,"???  
Returns mock AWS Lambda pricing data with success status and service details.  
???"
3299,evaluate_jmmmu,"def evaluate_jmmmu(samples):
    
    pred_correct = 0
    judge_dict = dict()
    for sample in samples:
        gold_i = sample[""answer""]
        pred_i = sample[""parsed_pred""]
        if sample[""question_type""] == ""multiple-choice"":
            correct = eval_multi_choice(gold_i, pred_i)
        else:  # open question
            correct = eval_open(gold_i, pred_i)

        if correct:
            judge_dict[sample[""id""]] = ""Correct""
            pred_correct += 1
        else:
            judge_dict[sample[""id""]] = ""Wrong""

    if len(samples) == 0:
        return {""acc"": 0}
    return judge_dict, {""acc"": pred_correct / len(samples)}",Batch evaluation for multiple choice and open questions.,???Evaluate prediction accuracy for multiple-choice and open questions???
3300,_jwks_fallback,"def _jwks_fallback(token: str, alg: str) -> dict:
    
    jwks = _jwks.fetch_data()        # public method → {""keys"":[…]}
    for jwk_dict in jwks[""keys""]:
        pubkey = ECAlgorithm.from_jwk(json.dumps(jwk_dict))
        try:
            print(""no kid or x5c; using JWKS, brute force"")
            return jwt.decode(token, key=pubkey, algorithms=[alg])
        except jwt.InvalidSignatureError:
            continue
    raise jwt.InvalidSignatureError(""None of the JWKS keys matched this signature."")",Last-chance attempt: download the JWKS and try every key until one verifies the signature.,"???Attempt to decode a JWT using a set of public keys from JWKS, handling signature errors.???"
3301,_start_tqdm_tracker,"def _start_tqdm_tracker(self):
        
        self.pbar = tqdm.tqdm(
            total=self.max_turns,
            initial=0,
            desc=f""Conversation between {self.seeder_name} and {self.partner_name}"",
            unit=""turn"",
        )",Start the tqdm progress tracker.,???Initialize a progress tracker for conversation turns using tqdm.???
3302,_separate_openai_config,"def _separate_openai_config(self, config: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        
        openai_config = {k: v for k, v in config.items() if k in self.openai_kwargs}
        extra_kwargs = {k: v for k, v in config.items() if k not in self.openai_kwargs}
        return openai_config, extra_kwargs",Separate the config into openai_config and extra_kwargs.,"???  
Splits configuration into OpenAI-specific and additional settings dictionaries.  
???"
3303,_get_pipeline,"def _get_pipeline(self, lang_code: str) -> KokoroPipeline:
        
        if lang_code not in self._pipelines:
            logger.info(f""Creating new KokoroPipeline for language: {lang_code}"")
            self._pipelines[lang_code] = KokoroPipeline(
                model=self,
                repo_id=self.REPO_ID if self.repo_id is None else self.repo_id,
                lang_code=lang_code,
            )
        return self._pipelines[lang_code]",Retrieves or creates a cached KokoroPipeline for the given language code.,???Initialize or retrieve a language-specific processing pipeline instance.???
3304,_handle_custom,"def _handle_custom(self, payload: IntermediateStepPayload,
                       ancestry: InvocationNode) -> AIQResponseSerializable | None:
        
        escaped_payload = html.escape(str(payload), quote=False)
        escaped_payload = escaped_payload.replace(""\n"", """")

        # Attempt to determine type
        format_type = ""json"" if is_valid_json(escaped_payload) else ""python""

        # Don't use f-strings here because the payload is markdown and screws up the dedent
        payload_str = dedent().strip(""\n"").format(payload=escaped_payload, format_type=format_type)

        # Return the event
        event = AIQResponseIntermediateStep(id=payload.UUID,
                                            name=f""{payload.event_type}"",
                                            payload=payload_str,
                                            parent_id=ancestry.function_id)

        return event",Handles the CUSTOM event,???Process and format payload into a structured event response object???
3305,update_images,"def update_images(self) -> None:
        
        # Destroy existing frames.
        for frame in self.frames:
            frame.destroy()
        self.frames = []

        start: int = self.page * self.images_per_page
        end: int = min(start + self.images_per_page, len(self.images))
        images_to_show: List[Image.Image] = self.images[start:end]

        total_images: int = len(self.images)
        total_pages: int = (
        )
        current_page_num: int = self.page + 1 if total_pages > 0 else 1
        self.page_info_label.config(text=f""Total Images: {total_images} | Page {current_page_num} of {total_pages}"")

        for i, image in enumerate(images_to_show):
            row, col = divmod(i, self.images_per_row)
            frame = ttk.Frame(self.scrollable_frame, padding=5)
            frame.grid(row=row, column=col, padx=5, pady=5)

            img = ImageTk.PhotoImage(image)
            img_label = ttk.Label(frame, image=img)
            img_label.image = img  # Retain a reference to avoid garbage collection.

            full_index: int = start + i
            img_label.bind(""<Button-1>"", lambda e, idx=full_index: self.show_full_size(idx))
            img_label.pack()

            self.frames.append(frame)

        self.scrollable_frame.update_idletasks()
        self.canvas.config(scrollregion=self.canvas.bbox(""all""))",Update the scrollable frame with the current page of thumbnail images.,???Refreshes image gallery display with pagination and interactive thumbnails.???
3306,render,"def render(self):
        
        try:
            g = Digraph()
            
            def visit(n):
                # Truncate text for display
                display_text = n.text[:40] + ""..."" if len(n.text) > 40 else n.text
                g.node(n.hash[:6], f""{n.name}\n{display_text}"")
                for k in n.kids:
                    g.edge(n.hash[:6], k.hash[:6])
                    visit(k)
            
            for r in self.root_nodes:
                visit(r)
            
            output_path = os.path.join(self.cache_dir, f""prompt_graph_{self.timestamp}"")
            g.render(output_path, format=""png"", view=False)
            return output_path + "".png""
        except ImportError:
            print(""Graphviz not installed. Install with: pip install graphviz"")
            return None",Generate a visualization of the prompt graph using Graphviz,???Generate a visual graph representation of hierarchical data using Graphviz.???
3307,handle_merge_trackers,"def handle_merge_trackers(args: argparse.Namespace) -> int:
    
    try:
        primary_path = normalize_path(args.primary_tracker_path)
        secondary_path = normalize_path(args.secondary_tracker_path)
        output_p = normalize_path(args.output) if args.output else primary_path
        
        merged_result_data = merge_trackers(primary_path, secondary_path, output_p)
        
        if merged_result_data and isinstance(merged_result_data, dict):
            print(f""Merged trackers into {output_p}. Total items in merged definitions: {len(merged_result_data.get('key_info_list', []))}"")
            return 0
        else: 
            print(f""Error merging trackers. `merge_trackers` returned: {merged_result_data}""); return 1
    except Exception as e_merge: logger.exception(f""Failed merge: {e_merge}""); print(f""Error: {e_merge}""); return 1",Handle the merge-trackers command.,"???Merge two tracker files and output the result, handling errors gracefully.???"
3308,open_repo,"def open_repo(body: RepoIn):
    
    repo_id = registry.add(body.path_or_url, body.ref)
    _ = registry.get_repo(repo_id)
    return {""id"": repo_id}",Register a repository path/URL and return its deterministic ID.,???Function adds a repository to a registry and returns its ID.???
3309,mock_on_context_condition_require_wrapping,"def mock_on_context_condition_require_wrapping(self, mock_agent_target: AgentTarget) -> OnContextCondition:
        
        condition = StringContextCondition(variable_name=""test_condition"")
        nested_chat_config = {""chat_queue"": [""agent1"", ""agent2""], ""use_async"": True}
        return OnContextCondition(target=NestedChatTarget(nested_chat_config=nested_chat_config), condition=condition)",Create a mock OnContextCondition for testing.,??? Create a context condition with nested chat configuration for an agent target ???
3310,is_quality_content,"def is_quality_content(self, text: str) -> bool:
        
        
        # Must be substantial
        if len(text) < 100:
            return False
        
        # Should not be all caps (likely headers)
        if text.isupper():
            return False
        
        # Should contain common English words
        common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        words = set(text.lower().split())
        if len(words.intersection(common_words)) == 0:
            return False
        
        # Should not be mostly punctuation
        alpha_chars = sum(1 for c in text if c.isalpha())
        if alpha_chars < len(text) * 0.6:
            return False
        
        return True",Final quality check for content.,"???Determine if text is substantial, not all caps, contains common words, and is mostly alphabetic.???"
3311,transcribe_audio,"def transcribe_audio(file_path: str) -> str:
    
    try:
        client = _get_client()
        with open(file_path, ""rb"") as audio_file:
            transcription = client.audio.transcriptions.create(model=""whisper-1"", file=audio_file)
        return transcription.text
    except Exception as e:
        logger.error(f""Error transcribing audio: {str(e)}"")
        raise",Transcribe audio to text using OpenAI's Whisper model,???Transcribe audio file to text using a client API???
3312,SaveImage,"def SaveImage(input_imgs, save_path):
    
    batch,_,_,_ = input_imgs.shape

    for idx in range(batch):
        save_image(input_imgs[idx], save_path+f'{idx}.png')","input_imgs :except tensor [B,C,H,W] save_path: path to saving imgs",???Save multiple images to a specified directory with indexed filenames.???
3313,mock_tool_spec,"def mock_tool_spec(self):
        
        spec = MagicMock()
        spec.name = ""calculator""
        spec.description = ""A tool for performing arithmetic calculations""
        spec.input_params = [
            {""name"": ""expression"", ""type"": ""string"", ""description"": ""Math expression to evaluate"", ""required"": True},
            {""name"": ""precision"", ""type"": ""integer"", ""description"": ""Decimal precision"", ""required"": False, ""default"": 2}
        ]
        spec.output_format = {""description"": ""Result of the calculation"", ""type"": ""number""}
        spec.constraints = [""No unsafe eval"", ""Handle division by zero""]
        return spec",Fixture for a mock tool specification.,???Create a mock specification for a calculator tool with input and output details.???
3314,_semantic_chunk_optimized,"def _semantic_chunk_optimized(chunker, text, context):
                
                logger.info(
                    f""🔧 CHUNKER_SEMANTIC_PROCESSING [{context}] Processing {len(text)} characters""
                )

                try:
                    result = chunker.chunk(text)
                    logger.info(
                        f""🔧 CHUNKER_SEMANTIC_INTERNAL_DONE [{context}] Internal chunking complete""
                    )
                    return result
                except Exception as e:
                    logger.error(
                        f""💥 CHUNKER_SEMANTIC_ERROR [{context}] Semantic chunking failed: {str(e)}""
                    )
                    # Fallback: split text manually into smaller parts
                    text_length = len(text)
                    parts = [text[i : i + part_size] for i in range(0, text_length, part_size)]

                    # Create mock chunk objects
                    class MockChunk:
                        def __init__(self, text):
                            self.text = text

                    return [MockChunk(part) for part in parts if part.strip()]",Optimized semantic chunking with progress reporting.,??? Log and handle semantic text chunking with error fallback ???
3315,_analyze_css_file,"def _analyze_css_file(file_path: str, content: str, result: Dict[str, Any]) -> None:
    
    result[""imports""] = [] # For @import rules

    try:
        for match in CSS_IMPORT_PATTERN.finditer(content):
             url = match.group(1)
             if url and not url.startswith(('#', 'http:', 'https:', 'data:')): result[""imports""].append({""url"": url.strip(), ""line"": content[:match.start()].count('\n') + 1})
    except Exception as e: logger.warning(f""Regex error during CSS import analysis in {file_path}: {e}"")",Analyzes CSS file content using regex.,???Extracts and logs non-URL CSS imports from a file's content into a result dictionary.???
3316,find_last_session_path,"def find_last_session_path(self, path) -> str:
        
        saved_sessions = []
        for filename in os.listdir(path):
            if filename.startswith('memory_'):
                date = filename.split('_')[1]
                saved_sessions.append((filename, date))
        saved_sessions.sort(key=lambda x: x[1], reverse=True)
        if len(saved_sessions) > 0:
            self.logger.info(f""Last session found at {saved_sessions[0][0]}"")
            return saved_sessions[0][0]
        return None",Find the last session path.,???Identify the most recent session file in a directory by date.???
3317,search_data,"def search_data(self, collection, vector, top_k=10, **kwargs):
        
        self.search_called = True
        self.last_search_collection = collection
        self.last_search_vector = vector
        self.last_search_top_k = top_k
        
        return [
            RetrievalResult(
                embedding=vector,
                text=f""Test result {i} for collection {collection}"",
                reference=f""test_reference_{collection}_{i}"",
                metadata={""a"": i, ""wider_text"": f""Wider context for test result {i} in collection {collection}""}
            )
            for i in range(min(3, top_k))
        ]",Mock implementation that returns test results.,"???Simulates a vector search in a collection, returning mock retrieval results.???"
3318,observation,"def observation(self):
        
        html = self.state['html']
        if self.observation_mode == 'html':
            return html
        elif self.observation_mode == 'text':
            return self.convert_html_to_text(html)
        else:
            raise ValueError(
                f'Observation mode {self.observation_mode} not supported.'
            )",Compiles state into either the `html` or `text` observation mode,??? Determine output format based on observation mode: HTML or text. ???
3319,get_instruction_args,"def get_instruction_args(self):
        
        return {
            ""letter"": self._letter,
            ""let_frequency"": self._frequency,
            ""let_relation"": self._comparison_relation,
        }",Returns the keyword args of build description.,???Retrieve letter attributes as a dictionary for instruction processing.???
3320,get_migrations_query,"def get_migrations_query(
        cls, limit: int = 50, offset: int = 0, name_pattern: str = """", include_full_queries: bool = False
    ) -> str:
        
        query = cls.load_sql(""get_migrations"")
        return (
            query.replace(""{limit}"", str(limit))
            .replace(""{offset}"", str(offset))
            .replace(""{name_pattern}"", name_pattern)
            .replace(""{include_full_queries}"", str(include_full_queries).lower())
        )",Get a query to list migrations.,???Generate SQL query for migrations with customizable parameters.???
3321,delimited_prompt_file,"def delimited_prompt_file(self):
        
        with tempfile.NamedTemporaryFile(mode=""w+"", suffix="".txt"", delete=False) as tf:
            tf.write()
            tf_path = Path(tf.name)

        yield tf_path

        # Cleanup
        os.unlink(tf_path)",Create a delimited prompt file for testing,"???Create temporary text file, yield its path, then delete it.???"
3322,remove_macros,"def remove_macros():
    
    shutil.rmtree(SITE / ""macros"", ignore_errors=True)
    (SITE / ""sitemap.xml.gz"").unlink(missing_ok=True)

    # Process sitemap.xml
    sitemap = SITE / ""sitemap.xml""
    lines = sitemap.read_text(encoding=""utf-8"").splitlines(keepends=True)

    # Find indices of '/macros/' lines
    macros_indices = [i for i, line in enumerate(lines) if ""/macros/"" in line]

    # Create a set of indices to remove (including lines before and after)
    indices_to_remove = set()
    for i in macros_indices:
        indices_to_remove.update(range(i - 1, i + 3))  # i-1, i, i+1, i+2, i+3

    # Create new list of lines, excluding the ones to remove
    new_lines = [line for i, line in enumerate(lines) if i not in indices_to_remove]

    # Write the cleaned content back to the file
    sitemap.write_text("""".join(new_lines), encoding=""utf-8"")

    print(f""Removed {len(macros_indices)} URLs containing '/macros/' from {sitemap}"")",Removes the /macros directory and related entries in sitemap.xml from the built site.,???Remove macros directory and clean sitemap by excluding specific URL entries.???
3323,decode,"def decode(self, tokens: Sequence[int]) -> str:
        
        try:
            return "" "".join([self.vocab[token] for token in tokens])
        except Exception as e:
            raise ValueError(
                f""Decoding failed. Tokens: {tokens} not found in vocab.""
            ) from e",Decode token ids back to text.,"???  
Converts a sequence of token IDs into a string using a vocabulary mapping.  
???"
3324,export_command,"def export_command(
    db_path: Path,
    fmt: str,
    output_path: Path | None,
    start: str | None,
    end: str | None,
    metrics: tuple[str, ...],
) -> None:
    
    db = TelemetryDB(db_path)
    if output_path is None:
        suffix = ""json"" if fmt == ""json"" else ""csv"" if fmt == ""csv"" else fmt
        output_path = Path(f""telemetry_export.{suffix}"")
    if fmt == ""json"":
        db.export_json(output_path, start=start, end=end, metrics=metrics or None)
    elif fmt == ""csv"":
        db.export_csv(output_path, start=start, end=end, metrics=metrics or None)
    else:
        db.export(output_path, fmt=fmt, start=start, end=end, metrics=metrics or None)
    click.echo(f""Exported telemetry to {output_path}"")
    db.close()",Export telemetry data from the database.,???Export telemetry data from a database to a specified file format.???
3325,mock_agent,"def mock_agent():
    
    mock = MagicMock()
    # Make sure we get one iteration through the stream loop
    mock.stream.return_value = [{""agent"": {""messages"": []}}]
    mock_state = MagicMock()
    mock_state.next = None
    mock.get_state.return_value = mock_state
    return mock",Create a mock agent that returns a simple stream and state.,"??? 
Create a mock agent with predefined stream and state behavior. 
???"
3326,context_with_results,"def context_with_results(search_result):
    
    return {
        ""query"": ""test query"",
        ""timeframe"": ""30d"",
        ""has_results"": True,
        ""result_count"": 1,
        ""results"": [search_result],
    }",Create a sample context with search results.,"???  
Create a dictionary encapsulating search query metadata and results.  
???"
3327,query_objects,"def query_objects(
    prototype_object_id: RemoteObjectId, object_group: typing.Optional[str] = None
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, RemoteObject]:
    
    params: T_JSON_DICT = dict()
    params[""prototypeObjectId""] = prototype_object_id.to_json()
    if object_group is not None:
        params[""objectGroup""] = object_group
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Runtime.queryObjects"",
        ""params"": params,
    }
    json = yield cmd_dict
    return RemoteObject.from_json(json[""objects""])",:param prototype_object_id: Identifier of the prototype to return objects for.,"???Generate objects from a prototype ID, optionally grouped, using a command dictionary.???"
3328,load_bash_state_by_id,"def load_bash_state_by_id(thread_id: str) -> Optional[dict[str, Any]]:
    
    if not thread_id:
        return None

    bash_state_dir = get_bash_state_dir_xdg()
    state_file = os.path.join(bash_state_dir, f""{thread_id}_bash_state.json"")

    if not os.path.exists(state_file):
        return None

    with open(state_file) as f:
        return json.load(f)",Load bash state from XDG directory with the given thread_id.,???Load and return bash state from JSON file by thread ID.???
3329,annotate_var_with_meta,"def annotate_var_with_meta(var, meta_var):
    
    assert isinstance(var, (list, dict, str))

    if isinstance(var, list):
        Incomplete = IncompleteList
        at = lambda k: int(k)  # noqa; (for some reason the meta_k for list lookups is stored as a string)

    elif isinstance(var, dict):
        Incomplete = IncompleteDict
        at = lambda k: k  # noqa

    else:  # str
        # The case I've seen: var == '[Filtered]' and meta_var == {"""": {""rem"": [[""!config"", ""s""]]}}
        # but I'm not going to assert on that.
        return var

    for meta_k, meta_v in meta_var.items():
        if meta_k == """":
            var = Incomplete(var, meta_v[""len""] - len(var))
        else:
            var[at(meta_k)] = annotate_var_with_meta(var[at(meta_k)], meta_v)

    return var","'var' is a (potentially trimmed) list or dict, 'meta_var' is a dict describing the trimming.",??? Recursively enrich data structures with metadata annotations based on type ???
3330,_assess_change_coverage,"def _assess_change_coverage(self, review: str, major_changes: List[str]) -> float:
        
        if not major_changes:
            return 1.0

        coverage_keywords = {
            ""function_definition"": [""function"", ""method"", ""def""],
            ""class_definition"": [""class""],
            ""import_change"": [""import"", ""dependency"", ""module""],
            ""error_handling"": [""error"", ""exception"", ""try"", ""catch"", ""handling""],
        }

        covered = 0
        for change_type in major_changes:
            keywords = coverage_keywords.get(change_type, [])
            if any(keyword in review.lower() for keyword in keywords):
                covered += 1

        return covered / len(major_changes)",Assess how well the review covers major changes.,???Evaluate review text for coverage of specified major code changes.???
3331,create_empty_result,"def create_empty_result(error_message=None):
    
    result = {
        ""total_cost"": 0.0,
        ""total_input_tokens"": 0,
        ""total_output_tokens"": 0,
        ""total_tokens"": 0
    }
    
    if error_message:
        result[""error""] = error_message
        
    return result",Create a default result dictionary with zeros for all metrics.,???Initialize a result dictionary with default values and optional error message.???
3332,parse,"def parse(self, input: dict, response: Cuisines) -> dict:
        
        return [{""cuisine"": t} for t in response.cuisines_list]",Parse the model response along with the input to the model into the desired output format..,???Transforms a response object into a list of cuisine dictionaries.???
3333,build_prompt,"def build_prompt(self, request: ChatRequest, context: Optional[Any] = None) -> str:
        
        # Get role_id from metadata if available
        role_id = None
        if hasattr(request, 'metadata') and request.metadata:
            role_id = request.metadata.get('role_id')
        
        if role_id:
            role = role_service.get_role_by_uuid(role_id)
            if role:
                prompt = role.system_prompt
                logger.info(f""RoleBasedStrategy (from role): {prompt}"")
                return prompt
                
        prompt = self.base_strategy.build_prompt(request, context)
        # logger.info(f""RoleBasedStrategy (from base): {prompt}"")
        return prompt",Build system prompt based on role,???Generate a chat prompt using role-based or default strategy based on request metadata.???
3334,upgrade,"def upgrade(package: str, use_venv: bool = True):
    

    # TODO should we try to update the project's pyproject.toml as well?
    def on_progress(line: str):
        if RE_UV_PROGRESS.match(line):
            log.info(line.strip())

    def on_error(line: str):
        log.error(f""uv: [error]\n {line.strip()}"")

    extra_args = []
    if not use_venv:
        # uv won't let us install without a venv if we don't specify a target
        extra_args = ['--target', site.getusersitepackages()]

    log.info(f""Upgrading {package}"")
    _wrap_command_with_callbacks(
        [get_uv_bin(), 'pip', 'install', '-U', '--python', _python_executable, *extra_args, package],
        on_progress=on_progress,
        on_error=on_error,
        use_venv=use_venv,
    )",Upgrade a package with `uv`.,???Upgrade a package with optional virtual environment support and progress/error logging.???
3335,get_artists,"def get_artists(api_url: str, api_key: str, api_timeout: int, artist_id: Optional[int] = None) -> Union[List, Dict, None]:
    
    endpoint = f""artist/{artist_id}"" if artist_id else ""artist""
    return arr_request(api_url, api_key, api_timeout, endpoint)",Get artist information from Lidarr.,???Fetch artist data from API using optional artist ID???
3336,_write_grid_section,"def _write_grid_section(
    file_obj: io.TextIOBase,
    key_info_list: List[KeyInfo], # Defines order and provides KIs
    grid_compressed_rows: List[str],
    current_global_map: Dict[str, KeyInfo], # Needed for display key
    global_key_counts: Dict[str, int]      # Precomputed counts
):
    
    file_obj.write(""---GRID_START---\n"")
    
    # Get display keys for header and row labels
    display_keys = [
        _get_display_key_for_tracker(ki, current_global_map, global_key_counts)
        for ki in key_info_list
    ]

    if not display_keys: 
        file_obj.write(""X \n"")
    else:
        file_obj.write(f""X {' '.join(display_keys)}\n"")
        if len(display_keys) != len(grid_compressed_rows):
            logger.critical(f""CRITICAL WRITE GRID: Display key count {len(display_keys)} != row count {len(grid_compressed_rows)}."")
        
        for i in range(min(len(display_keys), len(grid_compressed_rows))):
            file_obj.write(f""{display_keys[i]} = {grid_compressed_rows[i]}\n"")
    file_obj.write(""---GRID_END---\n"")",Writes the grid section.,???Writes a grid section to a file using display keys and compressed rows.???
3337,_preload_datasets,"def _preload_datasets(self):
        
        try:
            # Check if datasets can be loaded
            self.validation_data = load_gaia_data(data_dir=""data/"", dataset=""validation"")
            self.test_data = load_gaia_data(data_dir=""data/"", dataset=""test"")
            print(f""Preloaded validation dataset with {len(self.validation_data)} items"")
            print(f""Preloaded test dataset with {len(self.test_data)} items"")
        except Exception as e:
            print(f""Warning: Could not preload datasets: {e}"")
            self.validation_data = None
            self.test_data = None",Pre-load available GAIA datasets,"???  
Preload and validate datasets, handling exceptions if loading fails.  
???"
3338,_calculate_sma,"def _calculate_sma(self, df: pd.DataFrame, period: int) -> pd.DataFrame:
        
        with st.spinner(f""Calculating {period}-day SMA...""):
            df[""SMA""] = df[""Close""].rolling(window=period).mean()
            return df[[""Date"", ""Close"", ""SMA""]].dropna()",Calculate Simple Moving Average,???Compute and return a DataFrame with a specified period's Simple Moving Average.???
3339,get_cost_subtitle,"def get_cost_subtitle() -> Optional[str]:
    

    try:
        config = get_config_repository()
        show_cost = config.get(""show_cost"", DEFAULT_SHOW_COST)
    except Exception:
        # Failsafe for early initialization before config repository is available
        show_cost = DEFAULT_SHOW_COST

    if not show_cost:
        return None

    # Local import to break circular dependency
    from ra_aid.callbacks.default_callback_handler import DefaultCallbackHandler

    # Get the current singleton instance
    callback = DefaultCallbackHandler._instances.get(DefaultCallbackHandler, None)

    if not callback:
        return None

    if hasattr(callback, ""session_totals"") and callback.session_totals:
        cost = callback.session_totals.get(""cost"", 0.0)
        tokens = callback.session_totals.get(""tokens"", 0)
        return f""Cost: ${cost:.2f} | Tokens: {tokens}""

    # Fallback to direct values if session_totals not available
    return f""Cost: ${callback.total_cost:.2f} | Tokens: {callback.total_tokens}""",Generate a subtitle with cost information if cost tracking is enabled and show_cost is enabled.,???Retrieve and format cost and token usage details from a callback handler if enabled in configuration.???
3340,serena_version,"def serena_version() -> str:
    
    version = __version__
    try:
        from sensai.util.git import git_status
        from sensai.util.logging import LoggingDisabledContext

        with LoggingDisabledContext():
            git_status = git_status()
        version += f""-{git_status.commit[:8]}""
        if not git_status.is_clean:
            version += ""-dirty""
    except:
        pass
    return version",":return: the version of the package, including git status if available.",???Retrieve and format software version with Git commit details???
3341,parse_tool_calls,"def parse_tool_calls(text: str) -> Optional[list[ToolCall]]:
    
    try:
        tool_calls = _extract_tools(text)
        if tool_calls:
            results = []
            for call in tool_calls:
                # Process arguments
                args = call[""arguments""]
                arguments = args if isinstance(args, str) else json.dumps(args)

                tool_call = ToolCall(
                    id=f""call_{uuid.uuid4().hex[:8]}"",
                    function=FunctionCall(
                        name=call[""name""],
                        arguments=arguments,
                    ),
                )
                results.append(tool_call)
            return results

        return None
    except Exception as e:
        logger.error(f""Error during regex matching: {str(e)}"")
        return None",Parse tool calls from text using regex to find JSON patterns containing name and arguments.,???Extract and process tool call data from text into structured objects.???
3342,get_parallel_groups,"def get_parallel_groups(self) -> list[set[str]]:
        
        if not self._is_graph_current():
            self.build_graph()

        try:
            return list(nx.topological_generations(self.graph))
        except nx.NetworkXException as e:
            print(f""Error finding parallel groups: {e}"")
            return []",Identifies groups of atoms that could potentially be executed in parallel.,"???Retrieve parallel node groups from a graph, rebuilding if outdated.???"
3343,__get_client,"def __get_client(self, region: str = 'us-east-1') -> Any:
        
        client_key = f'{self.service_name}_{region}'
        if client_key not in self.clients:
            aws_profile = os.environ.get('AWS_PROFILE', 'default')
            self.clients[client_key] = boto3.Session(
                profile_name=aws_profile, region_name=region
            ).client(self.service_name)
        return self.clients[client_key]",Get or create a service client for the specified region.,???Retrieve or create AWS service client for specified region.???
3344,_find_regex_matches,"def _find_regex_matches(cls, line: str, line_num: int, found_values: set) -> List[Match]:
        
        matches = []
        for group in cls._signature_groups:
            for pattern_name, regex in group.patterns.items():
                regex_key = f""{group.name}:{pattern_name}""
                regex = cls._compiled_regexes.get(regex_key)
                if not regex:
                    continue
                for match in regex.finditer(line):
                    value = match.group()
                    key = cls._extract_key_from_line(line, value)
                    pattern = f""{key}:{value}""
                    if value.lower() == ""token"" or pattern in found_values:
                        continue
                    found_values.add(pattern)
                    matches.append(
                        Match(
                            group.name,
                            pattern_name,
                            key,
                            value,
                            line_num,
                            match.start(),
                            match.end(),
                        )
                    )
        return matches",Find matches using regex patterns.,"???Extract regex matches from a line, avoiding duplicates, and return match details.???"
3345,create_embedder_node,"def create_embedder_node(source_node) -> OpenAIDocumentEmbedder:
    
    return OpenAIDocumentEmbedder(
        name=""OpenAIEmbedder"",
        depends=[NodeDependency(source_node)],
        input_transformer=InputTransformer(selector={""documents"": f""${[source_node.id]}.output.documents""}),
    )",Create an OpenAI embedder node with dependencies on the source node.,???Create an OpenAI document embedder node with dependencies and input transformation.???
3346,simulate_directory_tree,"def simulate_directory_tree(operations, base_path):
    
    tree = {}
    for op in operations:
        rel_path = os.path.relpath(op['destination'], base_path)
        parts = rel_path.split(os.sep)
        current_level = tree
        for part in parts:
            if part not in current_level:
                current_level[part] = {}
            current_level = current_level[part]
    return tree",Simulate the directory tree based on the proposed operations.,???Constructs a nested directory structure from a list of file operations.???
3347,generate_user_prompt,"def generate_user_prompt(self, question: str, contexts: List[str]) -> str:
        
        context_str = ""\n"".join(contexts)
        return f""Question: {question}\nContexts:\n{context_str}""",Generate a user-level prompt.,"???  
Formats a question and its contexts into a structured prompt string.  
???"
3348,run_migrations,"def run_migrations():
    
    db_path = get_db_path()
    
    # logger.info(f""Using database at: {db_path}"")
    
    # Check if database file exists
    if not os.path.exists(db_path):
        # logger.error(f""Database file not found at {db_path}"")
        return False
    
    try:
        # Initialize migration manager
        migrations_dir = os.path.join(project_root, ""lpm_kernel"", ""database"", ""migrations"")
        manager = MigrationManager(db_path)
        
        # Apply migrations
        applied = manager.apply_migrations(migrations_dir)
        
        # if applied:
        #     logger.info(f""Successfully applied {len(applied)} migrations"")
        # else:
        #     logger.info(""No new migrations to apply"")
        
        return True
        
    except Exception as e:
        logger.error(f""Error during migrations: {e}"")
        return False",Run all pending database migrations,"???Execute database migrations if the database file exists, logging errors if encountered.???"
3349,get_scheduler_history,"def get_scheduler_history():
    
    try:
        history = get_execution_history()
        response = Response(json.dumps({
            ""success"": True,
            ""history"": history,
            ""timestamp"": datetime.now().isoformat()
        }))
        response.headers['Content-Type'] = 'application/json'
        response.headers['Access-Control-Allow-Origin'] = '*'
        return response
    
    except Exception as e:
        error_msg = f""Error getting scheduler history: {str(e)}""
        scheduler_logger.error(error_msg)
        return jsonify({""error"": error_msg}), 500",Return the execution history for the scheduler,"???Retrieve and return execution history as JSON, handling errors gracefully.???"
3350,generate_basic_tests,"def generate_basic_tests(spec: ToolSpecification) -> str:
    
    param_assignments = []
    for param in spec.input_parameters:
        value = _example_value(param.type_)
        param_assignments.append(f""{param.name}={value}"")
    args = "", "".join(param_assignments)
    test_code = f
    return test_code",Generate minimal pytest code exercising the generated tool.,??? Generate test code by assigning example values to tool parameters ???
3351,get_configs_path,"def get_configs_path() -> str:
    
    this_dir = os.path.dirname(__file__)
    candidates = (
        os.path.join(this_dir, ""configs""),
        os.path.join(this_dir, "".."", ""configs""),
    )
    for candidate in candidates:
        candidate = os.path.abspath(candidate)
        if os.path.isdir(candidate):
            return candidate
    raise FileNotFoundError(f""Could not find SGM configs in {candidates}"")",Get the `configs` directory.,???Determine and return the valid configuration directory path or raise an error.???
3352,multi_turn_example,"def multi_turn_example(client):
    
    print(""\n=== Multi-turn Example ==="")

    messages = [
        {""role"": ""system"", ""content"": ""You are a helpful AI assistant.""},
        {""role"": ""user"", ""content"": ""Tell me a joke about programming.""},
    ]

    try:
        responses, usage, done_reasons = client.chat(messages)
        print(f""Assistant: {responses[0]}"")

        # Add the assistant's response and continue the conversation
        messages.append({""role"": ""assistant"", ""content"": responses[0]})
        messages.append({""role"": ""user"", ""content"": ""That's funny! Tell me another one.""})

        responses, usage, done_reasons = client.chat(messages)
        print(f""Assistant: {responses[0]}"")
        print(f""Total usage: {usage}"")
    except Exception as e:
        print(f""Error during multi-turn chat: {e}"")",Example of multi-turn conversation.,???Simulates a multi-turn conversation with an AI assistant using a client chat interface.???
3353,get_extra_context,"def get_extra_context(_) -> dict:  # pragma: no cover
        

        return {
            'page': 'pdf_comment_overview',
            'get_next_overview_page': 'get_next_pdf_comment_overview_page',
            'kind': 'comments',
        }",get further information that needs to be passed to the template.,"???  
Returns a dictionary with page and navigation details for PDF comment overview.  
???"
3354,get_graph_edge_nodes,"def get_graph_edge_nodes(self) -> list[ast.Call]:
        
        # TODO do we need to exclude the tools?
        nodes = asttools.find_method_calls(self.get_run_method(), 'add_edge')
        for node in nodes:
            if not len(node.args) == 2:
                raise ValidationError(f""Invalid `add_edge` call in {ENTRYPOINT}"")

            source, target = node.args
            if isinstance(source, ast.Str) and source.s == GRAPH_NODE_TOOLS:
                nodes.remove(node)
            if isinstance(target, ast.Str) and target.s == GRAPH_NODE_TOOLS:
                nodes.remove(node)
        return nodes",Get all of the AST Call nodes that create the graph edges.,"???Extracts valid graph edge nodes from method calls, excluding specific tool nodes.???"
3355,get_metrics,"def get_metrics() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, typing.List[Metric]]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Performance.getMetrics"",
    }
    json = yield cmd_dict
    return [Metric.from_json(i) for i in json[""metrics""]]",Retrieve current values of run-time metrics.,???Generate performance metrics from JSON response using a command dictionary???
3356,mcp_prompt,"def mcp_prompt(
    name: str | None = None,
    description: str | None = None,
    tags: set[str] | None = None,
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        call_args = {
            ""name"": name or func.__name__,
            ""description"": description,
            ""tags"": tags,
        }

        call_args = {k: v for k, v in call_args.items() if v is not None}

        setattr(func, _MCP_REGISTRATION_PROMPT_ATTR, call_args)
        return func

    return decorator",Decorator to mark a method as an MCP prompt for later registration.,"???Decorate functions with metadata including name, description, and tags.???"
3357,scheduler_loop,"def scheduler_loop():
    
    scheduler_logger.info(""Scheduler loop started."")
    while not stop_event.is_set():
        try:
            # Check for stateful management expiration first
            scheduler_logger.debug(""Checking for stateful management expiration..."")
            check_stateful_expiration() # Call the imported function
            
            scheduler_logger.debug(""Checking and executing schedules..."")
            check_and_execute_schedules()
            
            # Sleep until the next check
            stop_event.wait(SCHEDULE_CHECK_INTERVAL)
            
        except Exception as e:
            scheduler_logger.error(f""Error in scheduler loop: {e}"")
            scheduler_logger.error(traceback.format_exc())
            # Sleep briefly to avoid rapidly repeating errors
            time.sleep(5)
    
    scheduler_logger.info(""Scheduler loop stopped"")",Main scheduler loop - runs in a background thread,"???A loop continuously manages and executes scheduled tasks, handling errors and logging events.???"
3358,_create_message,"def _create_message(self, role: str, content: str) -> dict:
        
        return {""role"": role, ""content"": content}",Create a message dictionary with role and content.,"???  
Defines a function to construct a message dictionary with role and content.  
???"
3359,_annotate_span_for_router_result,"def _annotate_span_for_router_result(
        self,
        span: trace.Span,
        result: List[LLMRouterResult],
    ):
        
        if not self.context.tracing_enabled:
            return
        for i, res in enumerate(result):
            span.set_attribute(f""result.{i}.confidence"", res.confidence)
            if res.reasoning:
                span.set_attribute(f""result.{i}.reasoning"", res.reasoning)
            if res.p_score:
                span.set_attribute(f""result.{i}.p_score"", res.p_score)

            result_key = f""result.{i}.result""
            if isinstance(res.result, str):
                span.set_attribute(result_key, res.result)
            elif isinstance(res.result, Agent):
                span.set_attribute(result_key, res.result.name)
            elif callable(res.result):
                span.set_attribute(result_key, res.result.__name__)",Annotate the span with the router result.,???Annotate tracing span with router result attributes if tracing is enabled.???
3360,get_default_features,"def get_default_features(self) -> dict:
        
        return {
            'energy': 0,
            'spectral_centroid': 0,
            'pitch': 0
        }",Return default feature values when audio is invalid,"???  
Defines a method to return a dictionary of default audio feature values.  
???"
3361,_create_free_tier_info,"def _create_free_tier_info(custom_cost_data: Dict, services_info: Dict[str, ServiceInfo]) -> str:
    
    free_tier_entries = []

    # Collect free tier info from services
    for service in services_info.values():
        if service.free_tier_info:
            free_tier_entries.append(f'- **{service.name}**: {service.free_tier_info}')

    # If no service-specific info found, check custom data
    if not free_tier_entries:
        if 'free_tier_info' in custom_cost_data:
            return custom_cost_data['free_tier_info']

        # Search for free tier mentions in custom data
        for key, value in custom_cost_data.items():
            if isinstance(value, dict):
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, str) and 'free' in sub_value.lower():
                        free_tier_entries.append(
                            f'- **{key.replace(""_"", "" "").title()}**: {sub_value}'
                        )

    # Return appropriate message based on findings
    if free_tier_entries:
        return 'Free tier information by service:\n' + '\n'.join(free_tier_entries)

    return 'AWS offers a Free Tier for many services. Check the AWS Free Tier page for current offers and limitations.'",Create the free tier information section.,???Generate a summary of free tier information from service data or custom cost data.???
3362,write_final_report,"def write_final_report(agent_state: ""AgentState"", title, sections, conclusion, citations):
    

    # Turn the report into markdown format
    report = """"
    report += f""\n# {title}""
    for section in sections:
        report += f""\n\n## {section.title}\n\n""
        report += section.content
    report += f""\n\n# Conclusion\n\n""
    report += conclusion
    report += f""\n\n# Citations\n\n""
    for citation in citations:
        report += f""- {citation}\n""

    # Write the markdown report for safekeeping into a memory block
    # (Optional, could also store elsewhere, like write to a file)
    agent_state.memory.update_block_value(label=""final_report"", value=report)

    return ""Your report has been successfully stored inside of memory section final_report. Next step: return the completed report to the user using send_message so they can review it (make sure to preserve the markdown formatting, assume the user is using a markdown-compatible viewer).""",Generate the final report based on the research process.,???Generate and store a markdown report in memory for user review.???
3363,get_context_get,"def get_context_get(self, request: HttpRequest, pdf_id: str):
        

        pdf = PdfMixin.get_object(request, pdf_id)
        form = self.form

        context = {'form': form, 'pdf_name': pdf.name}

        return context",Get the context needed to be passed to the template containing the form for adding a shared PDf.,???Retrieve PDF details and form for context in a GET request handler.???
3364,do_cs,"def do_cs(self, name: PDFStackT) -> None:
        
        try:
            self.il_creater.on_non_stroking_color_space(literal_name(name))
            self.ncs = self.csmap[literal_name(name)]
        except KeyError:
            if settings.STRICT:
                raise PDFInterpreterError(f""Undefined ColorSpace: {name!r}"") from None
        return",Set color space for nonstroking operations,???Handle non-stroking color space changes in PDF rendering process.???
3365,_benchmark_images,"def _benchmark_images(self):
        
        if not self.benchmark_exists():
            return None
        benchmark_images = []
        base_model_benchmark = self._benchmark_path(""base_model"")
        for _benchmark_image in os.listdir(base_model_benchmark):
            if _benchmark_image.endswith("".png""):
                benchmark_images.append(
                    (
                        _benchmark_image.replace("".png"", """"),
                        f""Base model benchmark image {_benchmark_image}"",
                        Image.open(
                            os.path.join(base_model_benchmark, _benchmark_image)
                        ),
                    )
                )

        return benchmark_images",We will retrieve the benchmark images so they can be stitched to the validation outputs.,???Generate a list of benchmark image data from a specified directory if benchmarks exist.???
3366,mock_scale_response,"def mock_scale_response(mocker):
    
    return {
        ""organic_results"": [
            {
                ""title"": ""Test Result 1"",
                ""snippet"": ""This is a snippet from result 1"",
            },
            {
                ""title"": ""Test Result 2"",
                ""snippet"": ""This is a snippet from result 2"",
            },
        ]
    }",Mock response from Scale SERP API.,"???  
Simulate search engine response with predefined organic results.  
???"
3367,handle_edit_command,"def handle_edit_command(
    lines: List[str], args: List[str], console: Console, session: PromptSession, history_manager: InputHistoryManager
) -> None:
    
    try:
        edit_line_num = int(args[0]) - 1
        if 0 <= edit_line_num < len(lines):
            console.print(
                f""[bold #1d3557]Editing Line {edit_line_num + 1}:[/bold #1d3557] {lines[edit_line_num]}""
            )  # Dark blue
            new_line = session.prompt(""New content: "")
            history_manager.push_state(lines)
            lines[edit_line_num] = new_line
        else:
            console.print(""[bold #ff4444]Invalid line number.[/bold #ff4444]"")
    except (ValueError, IndexError):
        console.print(""[bold #ff4444]Invalid edit command. Usage: /edit <line_number>[/bold #ff4444]"")",Edit a specific line in the input buffer.,???Edit a specified line in a text list with user input validation and history tracking.???
3368,_validate_parameters,"def _validate_parameters(action: str, app_name: Optional[str], parameters: Dict[str, Any]) -> None:
    
    required = ACTIONS[action][""required_params""]

    # Check app_name if required
    if ""app_name"" in required and (not app_name or not app_name.strip()):
        raise ValueError(f""app_name is required for action '{action}'"")

    # Check other required parameters
    for param in required:
        if param != ""app_name"" and param not in parameters:
            raise ValueError(f""Missing required parameter '{param}' for action '{action}'"")",Validate required parameters for the given action.,???Validate required parameters for a specified action and application name.???
3369,analyze_voice_content,"def analyze_voice_content(tensor):
    
    # Look at the variance along the first dimension to see where the information is concentrated
    variance = torch.var(tensor, dim=(1,2))  # Variance across features
    logger.info(f""Variance distribution:"")
    logger.info(f""First 5 rows variance: {variance[:5].mean().item():.6f}"")
    logger.info(f""Last 5 rows variance: {variance[-5:].mean().item():.6f}"")
    return variance",Analyze the content distribution in the voice tensor.,???Analyze variance in voice data tensor to identify information concentration???
3370,after_train_iter,"def after_train_iter(self, task, step, epoch):
        

        if self.rank != 0:
            return
        if step == 0 or (step + 1) % self.freq == 0:
            time_cost = task.log_buffer['time_cost']
            logging.info(""Epoch {} / {}, batch {} / {}, {:.4f} sec/batch"".format(
                epoch + 1, task.epoch_num, step + 1, task.step_per_epoch, time_cost))

            log_str = "" "" * 25
            for k, v in task.log_buffer.items():
                if k == 'time_cost':
                    continue
                if isinstance(v, list):
                    s = ', '.join(['%.6f' % x.val for x in v])
                elif isinstance(v, AverageMeter):
                    s = '%.6f' % v.val
                else:
                    s = str(v)
                log_str += '%s = [%s] ' % (k, s)
            print(log_str)",Print log info after every training step,??? Log training progress and metrics at specified intervals during model training. ???
3371,_format_agent_info,"def _format_agent_info(self, agent_name: str) -> str:
        
        agent = self.agents.get(agent_name)
        if not agent:
            return """"

        if isinstance(agent, AugmentedLLM):
            server_names = agent.agent.server_names
        elif isinstance(agent, Agent):
            server_names = agent.server_names
        else:
            logger.warning(
                f""_format_agent_info: Agent {agent_name} is not an instance of Agent or AugmentedLLM. Skipping.""
            )
            return """"

        servers = ""\n"".join(
            [
                f""- {self._format_server_info(server_name)}""
                for server_name in server_names
            ]
        )

        return f""Agent Name: {agent.name}\nDescription: {agent.instruction}\nServers in Agent: {servers}""",Format Agent information for display to planners,???Formats agent details and server information into a structured string output.???
3372,run_article_workflow,"def run_article_workflow(job_id: str, topic: str):
    
    try:
        # Get the pre-created queue from active_jobs
        sse_queue = active_jobs[job_id]
        shared = {
            ""topic"": topic,
            ""sse_queue"": sse_queue,
            ""sections"": [],
            ""draft"": """",
            ""final_article"": """"
        }
        
        # Run the workflow
        flow = create_article_flow()
        flow.run(shared)
        
    except Exception as e:
        # Send error message
        error_msg = {""step"": ""error"", ""progress"": 0, ""data"": {""error"": str(e)}}
        if job_id in active_jobs:
            active_jobs[job_id].put_nowait(error_msg)",Run the article workflow in background,???Execute article creation workflow with error handling and progress updates.???
3373,op_schema,"def op_schema(op: str, pointer: str, value_schema: dict | None) -> dict:
    
    # Ensure value_schema always has a type
    if value_schema and ""type"" not in value_schema:
        value_schema = {""type"": ""object"", **value_schema}

    # Base schema for the operation
    base = {
        ""type"": ""object"",
        ""required"": [""op"", ""path""],
        ""properties"": {""op"": {""type"": ""string"", ""enum"": [op]}, ""path"": {""type"": ""string"", ""const"": pointer}},
        ""additionalProperties"": False,
    }

    # Add value property for operations that require it
    if op in {""add"", ""replace"", ""test""}:
        base[""required""].append(""value"")
        if value_schema:
            base[""properties""][""value""] = value_schema
        else:
            base[""properties""][""value""] = {""type"": ""null""}

    # Add from property for operations that require it
    if op in {""move"", ""copy""}:
        base[""required""].append(""from"")
        base[""properties""][""from""] = {""type"": ""string""}

    return base",Build a schema fragment for one operation / one pointer.,???Generate JSON schema for operation with dynamic properties based on operation type???
3374,_upload_yt_playlist,"def _upload_yt_playlist(self, playlist_info: dict, media_type):
        
        for media in playlist_info:
            try:
                self.output_message.actions.append(
                    f""Uploading video: {media['title']} as {media_type}""
                )
                self._upload(media[""url""], ""url"", media_type)
            except Exception as e:
                self.output_message.actions.append(
                    f""Upload failed for {media['title']}""
                )
                logger.exception(f""Error in uploading {media['title']}: {e}"")
        return AgentResponse(
            status=AgentStatus.SUCCESS,
            message=f""All the videos in the playlist uploaded successfully as {media_type}"",
        )",Upload the videos in a youtube playlist.,"???Uploads YouTube playlist videos, handling exceptions and logging outcomes.???"
3375,reset_stateful,"def reset_stateful():
    
    try:
        success = reset_stateful_management()
        if success:
            # Add CORS headers to allow access from frontend
            response = Response(json.dumps({""success"": True, ""message"": ""Stateful management reset successfully""}))
            response.headers['Content-Type'] = 'application/json'
            response.headers['Access-Control-Allow-Origin'] = '*'
            return response
        else:
            # Add CORS headers to allow access from frontend
            response = Response(json.dumps({""success"": False, ""message"": ""Failed to reset stateful management""}), status=500)
            response.headers['Content-Type'] = 'application/json'
            response.headers['Access-Control-Allow-Origin'] = '*'
            return response
    except Exception as e:
        stateful_logger.error(f""Error resetting stateful management: {e}"")
        # Return error response with proper headers
        error_data = {""error"": str(e)}
        response = Response(json.dumps(error_data), status=500)
        response.headers['Content-Type'] = 'application/json'
        response.headers['Access-Control-Allow-Origin'] = '*'
        return response",Reset the stateful management system.,???Handle stateful management reset with CORS-enabled JSON responses and error logging.???
3376,run_flask,"def run_flask(flask_agent):
    
    try:
        logger.info(""Starting Flask API agent..."")
        flask_agent.run(host=""0.0.0.0"", port=5005)
    except Exception as e:
        logger.error(f""Flask API agent error: {str(e)}"")",Runs the (blocking) Flask API agent in a separate thread.,???Launches a Flask API server and logs any startup errors.???
3377,build_query_analysis_context,"def build_query_analysis_context(analysis_result: QueryAnalysisResult) -> str:
    
    context: Dict[str, Any] = {
        'cassandra knowledge': build_cassandra_knowledge(),
        'amazon keyspaces knowledge': build_amazon_keyspaces_knowledge(),
    }

    # Add query performance guidance
    performance_guidance = {
        'Partition key importance': ""In Cassandra/Keyspaces, queries that don't filter on partition key require scanning all partitions, ""
        + 'which is extremely expensive and should be avoided.',
        'clustering_column_usage': 'After partition keys, clustering columns should be used in WHERE clauses to further narrow down the data '
        + 'that needs to be read within a partition.',
        'allow_filtering_warning': 'The ALLOW FILTERING clause forces Cassandra to scan potentially all partitions, '
        + 'which is very inefficient and should be avoided in production.',
        'secondary_indexes': 'Secondary indexes in Cassandra are not as efficient as in relational databases. '
        + 'They still require reading from multiple partitions and should be used sparingly.',
        'full_table_scan': 'Full table scans in Cassandra are extremely expensive operations that should be avoided. '
        + 'Always design your data model and queries to avoid scanning entire tables.',
    }

    context['performance_guidance'] = performance_guidance

    # Add query-specific context
    query_context = {
        'uses_partition_key': analysis_result.uses_partition_key,
        'uses_clustering_columns': analysis_result.uses_clustering_columns,
        'uses_allow_filtering': analysis_result.uses_allow_filtering,
        'uses_secondary_index': analysis_result.uses_secondary_index,
        'is_full_table_scan': analysis_result.is_full_table_scan,
    }

    context['query_context'] = query_context

    return dict_to_markdown(context)",Provide LLM context for query analysis results.,???Generate a markdown report with Cassandra query analysis and performance guidance.???
3378,llm_config_dict,"def llm_config_dict(self) -> dict[str, Any]:
		
		return {'provider': self.llm_provider, 'config': {'model': self.llm_instance}}",Returns the LLM configuration dictionary.,???Returns a dictionary with language model provider and instance configuration.???
3379,create_working_perplexity_config,"def create_working_perplexity_config():
    
    
    api_key = os.getenv('PERPLEXITY_API_KEY')
    if not api_key:
        print(""❌ Cannot create config without PERPLEXITY_API_KEY"")
        return
    
    config = {
        ""mcpServers"": {
            ""perplexity"": {
                ""command"": ""mcp-server-perplexity"",
                ""args"": [],
                ""env"": {
                    ""PERPLEXITY_API_KEY"": api_key
                }
            }
        },
        ""toolSettings"": {
            ""defaultTimeout"": 300.0,
            ""maxConcurrency"": 4
        }
    }
    
    with open('server_config_perplexity.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    print(""✅ Created server_config_perplexity.json"")
    print(""\n🚀 Test with:"")
    print(""   export MCP_TOOL_TIMEOUT=300"")
    print(""   mcp-cli chat --config server_config_perplexity.json"")",Create a working perplexity configuration.,???Generate and save server configuration for Perplexity API integration.???
3380,_inference,"def _inference(self, x):
        
        # Inference path
        shape = x[0].shape  # BCHW
        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)
        if self.format != ""imx"" and (self.dynamic or self.shape != shape):
            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))
            self.shape = shape

        if self.export and self.format in {""saved_model"", ""pb"", ""tflite"", ""edgetpu"", ""tfjs""}:  # avoid TF FlexSplitV ops
            box = x_cat[:, : self.reg_max * 4]
            cls = x_cat[:, self.reg_max * 4 :]
        else:
            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)

        if self.export and self.format in {""tflite"", ""edgetpu""}:
            # Precompute normalization factor to increase numerical stability
            grid_h = shape[2]
            grid_w = shape[3]
            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)
            norm = self.strides / (self.stride[0] * grid_size)
            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])
        elif self.export and self.format == ""imx"":
            dbox = self.decode_bboxes(
                self.dfl(box) * self.strides, self.anchors.unsqueeze(0) * self.strides, xywh=False
            )
            return dbox.transpose(1, 2), cls.sigmoid().permute(0, 2, 1)
        else:
            dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides

        return torch.cat((dbox, cls.sigmoid()), 1)",Decode predicted bounding boxes and class probabilities based on multiple-level feature maps.,"???Performs model inference, handling different export formats and dynamic shapes.???"
3381,encode_image,"def encode_image(data: np.ndarray) -> dict:
    
    with BytesIO() as output_bytes:
        pil_image = Image.fromarray(data)
        pil_image.save(output_bytes, ""JPEG"")
        bytes_data = output_bytes.getvalue()
    base64_str = str(base64.b64encode(bytes_data), ""utf-8"")
    return {""mime_type"": ""image/jpeg"", ""data"": base64_str}",Encode image data to send to the server,???Convert image array to base64-encoded JPEG string with MIME type.???
3382,with_resolution_frame,"def with_resolution_frame(self, child: Editable, *args, generic_parameters: list | None = None, generics: dict | None = None, **kwargs) -> Generator[ResolutionStack[""Self""], None, None]:
        
        if isinstance(child, Chainable):
            assert child is not self
            if not child._resolving:
                resolved = child.resolved_type_frames
                if len(resolved) > 0:
                    for resolution in resolved:
                        yield from self.with_resolution(resolution, *args, generic_parameters=generic_parameters, generics=generics, **kwargs)
                    return
        if generics is None:
            generics = {i: v for i, v in enumerate(generic_parameters)} if generic_parameters else None
        yield ResolutionStack(child).with_frame(self, *args, **kwargs, generics=generics)",Resolve the definition(s) of this object.,"???Generate resolution frames for a child object, handling generics and recursion.???"
3383,register_actions,"def register_actions(self) -> None:
        
        self.actions = {
            ""generate-text"": Action(
                name=""generate-text"",
                parameters=[
                    ActionParameter(""prompt"", True, str, ""The input prompt for text generation""),
                    ActionParameter(""system_prompt"", True, str, ""System prompt to guide the model""),
                    ActionParameter(""model"", False, str, ""Model to use for generation"")
                ],
                description=""Generate text using OpenAI models""
            ),
            ""check-model"": Action(
                name=""check-model"",
                parameters=[
                    ActionParameter(""model"", True, str, ""Model name to check availability"")
                ],
                description=""Check if a specific model is available""
            ),
            ""list-models"": Action(
                name=""list-models"",
                parameters=[],
                description=""List all available OpenAI models""
            )
        }",Register available OpenAI actions,???Define and register actions for text generation and model management.???
3384,get_tweet_text,"def get_tweet_text(tweet_id: str) -> Optional[str]:
    
    try:
        tweet = client.get_tweet(tweet_id, tweet_fields=[""text""])
        if tweet.data:
            return tweet.data.text
        return None
    except Exception as e:
        print(f""Error fetching tweet text {tweet_id}: {str(e)}"")
        return None",Get just the text content of a tweet,???Fetches and returns the text of a tweet by its unique identifier.???
3385,_create_ssl_context_for_keyspaces,"def _create_ssl_context_for_keyspaces(self) -> ssl.SSLContext:
        
        # Create an SSL context
        ssl_context = ssl.create_default_context()

        # Use the local certificate file
        cert_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), CERT_DIRECTORY, CERT_FILENAME
        )

        try:
            ssl_context.load_verify_locations(cafile=cert_path)
            logger.info(f'Loaded certificate from {cert_path}')
        except Exception as e:
            logger.error(f'Failed to load certificate from {cert_path}: {str(e)}')
            # Fall back to default CA certs, and best of luck
            ssl_context.load_default_certs()

        # Disable hostname verification: Keyspaces doesn't support peer hostname validation
        ssl_context.check_hostname = False

        return ssl_context",Create an SSL context for Amazon Keyspaces.,???Establish SSL context for secure Keyspaces connection with custom certificate handling???
3386,plot_predictions,"def plot_predictions(self, batch, preds, ni):
        
        plot_images(
            batch[""img""],
            *output_to_rotated_target(preds, max_det=self.args.max_det),
            paths=batch[""im_file""],
            fname=self.save_dir / f""val_batch{ni}_pred.jpg"",
            names=self.names,
            on_plot=self.on_plot,
        )",Plots predicted bounding boxes on input images and saves the result.,???Visualize model predictions on image batch and save output???
3387,display_changes,"def display_changes(project_name: str, title: str, changes: SyncReport, verbose: bool = False):
    
    tree = Tree(f""{project_name}: {title}"")

    if changes.total == 0:
        tree.add(""No changes"")
        console.print(Panel(tree, expand=False))
        return

    if verbose:
        # Full file listing with checksums
        if changes.new:
            new_branch = tree.add(""[green]New Files[/green]"")
            add_files_to_tree(new_branch, changes.new, ""green"", changes.checksums)
        if changes.modified:
            mod_branch = tree.add(""[yellow]Modified[/yellow]"")
            add_files_to_tree(mod_branch, changes.modified, ""yellow"", changes.checksums)
        if changes.moves:
            move_branch = tree.add(""[blue]Moved[/blue]"")
            for old_path, new_path in sorted(changes.moves.items()):
                move_branch.add(f""[blue]{old_path}[/blue] → [blue]{new_path}[/blue]"")
        if changes.deleted:
            del_branch = tree.add(""[red]Deleted[/red]"")
            add_files_to_tree(del_branch, changes.deleted, ""red"")
    else:
        # Show directory summaries
        by_dir = group_changes_by_directory(changes)
        for dir_name, counts in sorted(by_dir.items()):
            summary = build_directory_summary(counts)
            if summary:  # Only show directories with changes
                tree.add(f""[bold]{dir_name}/[/bold] {summary}"")

    console.print(Panel(tree, expand=False))",Display changes using Rich for better visualization.,???Display project changes in a structured tree format with optional verbosity.???
3388,eval_json,"def eval_json(self, stats):
        
        if self.args.save_json and self.is_coco and len(self.jdict):
            anno_json = self.data[""path""] / ""annotations/person_keypoints_val2017.json""  # annotations
            pred_json = self.save_dir / ""predictions.json""  # predictions
            LOGGER.info(f""\nEvaluating pycocotools mAP using {pred_json} and {anno_json}..."")
                check_requirements(""pycocotools>=2.0.6"")
                from pycocotools.coco import COCO  # noqa
                from pycocotools.cocoeval import COCOeval  # noqa

                for x in anno_json, pred_json:
                    assert x.is_file(), f""{x} file not found""
                anno = COCO(str(anno_json))  # init annotations api
                pred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)
                for i, eval in enumerate([COCOeval(anno, pred, ""bbox""), COCOeval(anno, pred, ""keypoints"")]):
                    if self.is_coco:
                        eval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # im to eval
                    eval.evaluate()
                    eval.accumulate()
                    eval.summarize()
                    idx = i * 4 + 2
                    stats[self.metrics.keys[idx + 1]], stats[self.metrics.keys[idx]] = eval.stats[
                        :2
                    ]  # update mAP50-95 and mAP50
            except Exception as e:
                LOGGER.warning(f""pycocotools unable to run: {e}"")
        return stats",Evaluates object detection model using COCO JSON format.,???Evaluate model performance using COCO metrics and update statistics if conditions are met.???
3389,rollback,"def rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):
    
    logger.info(""Removing status field from session table"")
    try:
         # Check if the column exists before trying to remove it
         introspector = migrator.router.model_introspector
         columns = introspector.get_columns('session')
         if 'status' in [c.name for c in columns]:
             migrator.remove_fields('session', 'status')
             logger.info(""Successfully removed status field from session table"")
         else:
             logger.warning(""Column 'status' does not exist in 'session' table. Skipping remove."")
    except Exception as e:
        logger.error(f""Error checking or removing 'status' column: {e}"")",Write your rollback migrations here.,"???Remove 'status' column from 'session' table if it exists, with logging and error handling.???"
3390,check_suffix,"def check_suffix(file=""yolo11n.pt"", suffix="".pt"", msg=""""):
    
    if file and suffix:
        if isinstance(suffix, str):
            suffix = (suffix,)
        for f in file if isinstance(file, (list, tuple)) else [file]:
            s = Path(f).suffix.lower().strip()  # file suffix
            if len(s):
                assert s in suffix, f""{msg}{f} acceptable suffix is {suffix}, not {s}""",Check file(s) for acceptable suffix.,"???Validate file extensions against allowed suffixes, raising errors if mismatched.???"
3391,get_prompts,"def get_prompts(self, row: _DictOrBaseModel) -> str | list[str]:
        
        sig = inspect.signature(self.prompt_func)
        if len(sig.parameters) == 0:
            return self.prompt_func()
        if len(sig.parameters) == 1:
            if isinstance(row, dict):
                if _INTERNAL_PROMPT_KEY in row:
                    return self.prompt_func(row[_INTERNAL_PROMPT_KEY])
            return self.prompt_func(row)
        raise ValueError(f""Prompting function {self.prompt_func} must have 0 or 1 arguments."")",Get the prompts for the given row.,???Determine prompt output based on function signature and input type validation.???
3392,task_move_block_by_own_size,"def task_move_block_by_own_size(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    pos = rng.randint(0, size - block_size * 2)  # Space for block and movement
    color = rng.randint(1, 9)

    question = gen_field(size)
    block = [color] * block_size
    question = write_block(pos, block, question)

    answer = write_block(pos + block_size, block, gen_field(size))

    return {""input"": question, ""output"": answer}",Generate a task where a block moves right by its own size.,??? Generate a block-moving task with randomized position and color. ???
3393,_fetch_tp_shard_tensor,"def _fetch_tp_shard_tensor(tensor, name, chunk_dim=0, mutate_func=None) -> torch.Tensor:
        
        nonlocal state_dict
        tp_rank = mpu.get_tensor_model_parallel_rank()
        tp_size = mpu.get_tensor_model_parallel_world_size()
        if name in state_dict:
            full_weight = state_dict[name]

            if mutate_func is not None:
                full_weight = mutate_func(full_weight)
            tensor_chunk = torch.chunk(full_weight, tp_size, dim=chunk_dim)
            if tensor is not None:
                tensor = tensor.data.copy_(tensor_chunk[tp_rank], non_blocking=True)
        else:
            print(f""tp_shard tensor:[{name}] not in state_dict, skip loading"")",fetch tensor in tp shards,??? Retrieve and optionally mutate a tensor shard based on parallel rank and size ???
3394,get_log_summary,"def get_log_summary(self, markdown=False):
        
        if self.response:
            if self.response.get('data_cleaner_function_path'):
                log_details = f
                if markdown:
                    return Markdown(log_details) 
                else:
                    return log_details","Logs a summary of the agent's operations, if logging is enabled.","???  
Return formatted log details based on response and markdown preference.  
???"
3395,_validate_voyage_embedding_args,"def _validate_voyage_embedding_args(args):
    
    if args.embedding_provider == ""voyage"" and not os.getenv(""VOYAGE_API_KEY""):
        raise ValueError(""Please set the VOYAGE_API_KEY environment variable."")

    if not args.embedding_model:
        args.embedding_model = ""voyage-code-2""

    if not args.tokens_per_chunk:
        args.tokens_per_chunk = 800

    if not args.chunks_per_batch:
        args.chunks_per_batch = VOYAGE_MAX_CHUNKS_PER_BATCH
    elif args.chunks_per_batch > VOYAGE_MAX_CHUNKS_PER_BATCH:
        args.chunks_per_batch = VOYAGE_MAX_CHUNKS_PER_BATCH
        logging.warning(f""Voyage enforces a limit of {VOYAGE_MAX_CHUNKS_PER_BATCH} chunks per batch. Overwriting."")

    max_tokens = get_voyage_max_tokens_per_batch(args.embedding_model)
    if args.tokens_per_chunk * args.chunks_per_batch > max_tokens:
        raise ValueError(
            f""Voyage enforces a limit of {max_tokens} tokens per batch. ""
            ""Reduce either --tokens-per-chunk or --chunks-per-batch.""
        )

    if not args.embedding_size:
        args.embedding_size = get_voyage_embedding_size(args.embedding_model)",Validates the configuration of the Voyage batch embedder and sets defaults.,???Validate and configure voyage embedding parameters with defaults and constraints.???
3396,get_container_name,"def get_container_name(script_path: str) -> str:
    
    script_dir = Path(script_path).parent
    with open(script_dir / ""preswald.toml"") as f:
        preswald_toml = f.read()
    config = toml.loads(preswald_toml)
    container_name = f""preswald-app-{config['project']['slug']}""
    container_name = container_name.lower()
    container_name = re.sub(r""[^a-z0-9-]"", """", container_name)
    container_name = container_name.strip(""-"")
    return container_name",Generate a consistent container name for a given script,???Extracts and formats a container name from a TOML configuration file.???
3397,chunk,"def chunk(self, text: Union[str, List[str]]) -> List[Dict]:
        
        # Define the payload for the request
        payload = {
            ""text"": text,
            ""tokenizer"": self.tokenizer_or_token_counter,
            ""chunk_size"": self.chunk_size,
            ""chunk_overlap"": self.chunk_overlap,
            ""min_sentences_per_chunk"": self.min_sentences_per_chunk,
            ""min_characters_per_sentence"": self.min_characters_per_sentence,
            ""approximate"": self.approximate,
            ""delim"": self.delim,
            ""include_delim"": self.include_delim,
            ""return_type"": self.return_type,
        }

        # Make the request to the Chonkie API
        response = requests.post(
            f""{self.BASE_URL}/{self.VERSION}/chunk/sentence"",
            json=payload,
            headers={""Authorization"": f""Bearer {self.api_key}""},
        )

        # Parse the response
        result: List[Dict] = cast(List[Dict], response.json())
        return result",Chunk the text via sentence boundaries.,???Send text to API for sentence chunking based on specified parameters.???
3398,model_info,"def model_info(model, detailed=False, verbose=True, imgsz=640):
    
    if not verbose:
        return
    n_p = get_num_params(model)  # number of parameters
    n_g = get_num_gradients(model)  # number of gradients
    n_l = len(list(model.modules()))  # number of layers
    if detailed:
        LOGGER.info(f""{'layer':>5}{'name':>40}{'gradient':>10}{'parameters':>12}{'shape':>20}{'mu':>10}{'sigma':>10}"")
        for i, (name, p) in enumerate(model.named_parameters()):
            name = name.replace(""module_list."", """")
            LOGGER.info(
                f""{i:>5g}{name:>40s}{p.requires_grad!r:>10}{p.numel():>12g}{str(list(p.shape)):>20s}""
                f""{p.mean():>10.3g}{p.std():>10.3g}{str(p.dtype):>15s}""
            )

    flops = get_flops(model, imgsz)  # imgsz may be int or list, i.e. imgsz=640 or imgsz=[640, 320]
    fused = "" (fused)"" if getattr(model, ""is_fused"", lambda: False)() else """"
    fs = f"", {flops:.1f} GFLOPs"" if flops else """"
    yaml_file = getattr(model, ""yaml_file"", """") or getattr(model, ""yaml"", {}).get(""yaml_file"", """")
    model_name = Path(yaml_file).stem.replace(""yolo"", ""YOLO"") or ""Model""
    LOGGER.info(f""{model_name} summary{fused}: {n_l:,} layers, {n_p:,} parameters, {n_g:,} gradients{fs}"")
    return n_l, n_p, n_g, flops",Print and return detailed model information layer by layer.,"???Summarizes model architecture, parameters, and computational complexity if verbosity is enabled.???"
3399,delete,"def delete(resource_id: str) -> None:
    
    logger.info(""Attempting to delete agent: %s"", resource_id)
    try:
        remote_agent = agent_engines.get(resource_id)
        remote_agent.delete(force=True)
        logger.info(""Successfully deleted remote agent: %s"", resource_id)
        print(f""\nSuccessfully deleted agent: {resource_id}"")
    except google_exceptions.NotFound:
        logger.error(""Agent with resource ID %s not found."", resource_id)
        print(f""\nAgent{resource_id} not found."")
        print(f""\nAgent not found: {resource_id}"")
    except Exception as e:
        logger.error(
            ""An error occurred while deleting agent %s: %s"", resource_id, e
        )
        print(f""\nError deleting agent {resource_id}: {e}"")",Deletes the specified agent.,"???Attempt to delete a specified agent, handling errors and logging outcomes.???"
3400,get_inv_template_dict,"def get_inv_template_dict() -> Dict[int, Tuple[int, int, Optional[chess.PieceType]]]:
    
    global _INV_TEMPLATE_DICT_CACHE
    if _INV_TEMPLATE_DICT_CACHE is not None:
        return _INV_TEMPLATE_DICT_CACHE
    
    template_dict = get_template_dict()
    _INV_TEMPLATE_DICT_CACHE = {v: k for k, v in template_dict.items()}
    return _INV_TEMPLATE_DICT_CACHE","Get the inverse template dictionary, cached.",???Cache and return inverted chess template dictionary mapping.???
3401,get_battery_percentage,"def get_battery_percentage() -> int | None:
    
    if not MACHINE_IS_BATTERY_POWERED:
        return None
    percentage = None
    try:
        # running the subprocess would be expensive in a tight loop
        # but in mflux use case, we would call this only once every
        # few minutes due to N-minutes-long generation times
        result = subprocess.run([""pmset"", ""-g"", ""batt""], capture_output=True, text=True, check=True)
        if PMSET_AC_POWER_STATUS not in result.stdout:
            if match := re.search(PMSET_BATT_STATUS_PATTERN, result.stdout):
                percentage = int(match.group(1))
    except (subprocess.CalledProcessError, TypeError) as e:
        logger.warning(
            f""Cannot read battery percentage via 'pmset -g batt': {e}. Battery saver functionality is disabled and the program will continue running.""
        )

    return percentage",Get the current battery percentage of a battery-powered Mac.,"???Determine battery percentage if device is battery-powered, else return None.???"
3402,_execute_select,"def _execute_select(
        self, 
        query: str,
        params: Dict[str, Any],
        start_row: Any,
        end_row: Any
    ) -> str:
        
        start = self._convert_row_number(start_row, ""start_row"")
        end = self._convert_row_number(end_row, ""end_row"")
        
        if start > end:
            raise ValueError(f""start_row ({start}) must be <= end_row ({end})"")

        with self._engine.connect() as conn:
            result = conn.execute(text(query), params)
            columns: List[str] = result.keys()
            all_rows: List[Dict] = [dict(row._mapping) for row in result]

        total_rows = len(all_rows)
        actual_start = max(1, start)
        actual_end = min(end, total_rows)
        
        if actual_start > total_rows:
            return f""No results found (total rows: {total_rows})""

        displayed_rows = all_rows[actual_start-1:actual_end]
        
        markdown = [
            f""**SELECT Results:** `{actual_start}-{actual_end}` of `{total_rows}` rows"",
            self._format_table(columns, displayed_rows)
        ]

        if actual_end < total_rows:
            remaining = total_rows - actual_end
            markdown.append(f""\n*Showing first {actual_end} rows - {remaining} more row{'s' if remaining > 1 else ''} available*"")

        return ""\n"".join(markdown)",Execute a SELECT query with pagination.,"???Execute a database query, validate row range, and format results as a markdown table.???"
3403,get_transactions_str,"def get_transactions_str(self) -> str:
        
        return ""\n\n\n"".join([f""{file_path}:\n{self._format_transactions(transactions)}"" for file_path, transactions in self.queued_transactions.items()])",Returns a human-readable string representation of the transactions,???Generate a formatted string of queued transactions grouped by file path.???
3404,add_files_to_tree,"def add_files_to_tree(
    tree: Tree, paths: Set[str], style: str, checksums: Dict[str, str] | None = None
):
    
    # Group by directory
    by_dir = {}
    for path in sorted(paths):
        parts = path.split(""/"", 1)
        dir_name = parts[0] if len(parts) > 1 else """"
        file_name = parts[1] if len(parts) > 1 else parts[0]
        by_dir.setdefault(dir_name, []).append((file_name, path))

    # Add to tree
    for dir_name, files in sorted(by_dir.items()):
        if dir_name:
            branch = tree.add(f""[bold]{dir_name}/[/bold]"")
        else:
            branch = tree

        for file_name, full_path in sorted(files):
            if checksums and full_path in checksums:
                checksum_short = checksums[full_path][:8]
                branch.add(f""[{style}]{file_name}[/{style}] ({checksum_short})"")
            else:
                branch.add(f""[{style}]{file_name}[/{style}]"")","Add files to tree, grouped by directory.",???Organize file paths into a tree structure with optional checksum display.???
3405,scale_coordinates,"def scale_coordinates(self, source: ScalingSource, x: int, y: int):
        
        if not self._scaling_enabled:
            return x, y
        ratio = self.width / self.height
        target_dimension = None
        for dimension in MAX_SCALING_TARGETS.values():
            # allow some error in the aspect ratio - not ratios are exactly 16:9
            if abs(dimension[""width""] / dimension[""height""] - ratio) < 0.02:
                if dimension[""width""] < self.width:
                    target_dimension = dimension
                break
        if target_dimension is None:
            return x, y
        # should be less than 1
        x_scaling_factor = target_dimension[""width""] / self.width
        y_scaling_factor = target_dimension[""height""] / self.height
        if source == ScalingSource.API:
            if x > self.width or y > self.height:
                raise ToolError(f""Coordinates {x}, {y} are out of bounds"")
            # scale up
            return round(x / x_scaling_factor), round(y / y_scaling_factor)
        # scale down
        return round(x * x_scaling_factor), round(y * y_scaling_factor)",Scale coordinates to a target maximum resolution.,???Adjust coordinates based on scaling settings and aspect ratio constraints.???
3406,save_facts_to_db,"def save_facts_to_db(
    conn, facts: List[str], path: str, batch_size: int
):
    
    for i in range(0, len(facts), batch_size):
        batch = facts[i : i + batch_size]

        # Process each fact in the batch
        for fact in batch:
            try:
                print(f""Inserting fact: {fact}"")
                print(f""With path: {path}"")
                timestamp = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
                print(f""With recorded_at: {timestamp}"")

                insert_fact(conn, fact, path)
                print(""Success!"")
            except Exception as e:
                print(f""Failed to insert fact: {fact}"")
                print(f""Error: {e}"")
                continue
",Save a list of facts to the database in batches,???Batch-insert facts into a database with error handling and logging.???
3407,get_ollama_models,"def get_ollama_models() -> List[str]:
    
    try:
        import subprocess

        result = subprocess.run([""ollama"", ""list""], capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.strip().split(""\n"")
            if len(lines) < 2:  # No models or just header
                return []

            models = []
            # Skip header line
            for line in lines[1:]:
                parts = line.split()
                if parts:
                    model_name = parts[0]
                    models.append(f""OMNI: Ollama {model_name}"")
            return models
        return []
    except Exception as e:
        logging.error(f""Error getting Ollama models: {e}"")
        return []",Get available models from Ollama if installed.,???Retrieve and format a list of Ollama models using a subprocess call.???
3408,start_command,"def start_command(port: int | None, detached: bool = False, skip_build: bool = False, force: bool = False) -> None:
    
    repo_path = Path.cwd().resolve()
    repo_config = RepoConfig.from_repo_path(str(repo_path))
    if (container := DockerContainer.get(repo_config.name)) is not None:
        if force:
            rich.print(f""[yellow]Removing existing runner {repo_config.name} to force restart[/yellow]"")
            container.remove()
        else:
            return _handle_existing_container(repo_config, container)

    if port is None:
        port = get_free_port()

    try:
        if not skip_build:
            codegen_root = Path(__file__).parent.parent.parent.parent.parent.parent
            codegen_version = version(""codegen"")
            _build_docker_image(codegen_root=codegen_root, codegen_version=codegen_version)
        _run_docker_container(repo_config, port, detached)
        # TODO: memory snapshot here
    except Exception as e:
        rich.print(f""[bold red]Error:[/bold red] {e!s}"")
        raise click.Abort()",Starts a local codegen server,"???Initialize and manage a Docker container for a repository, handling existing instances and optional build steps.???"
3409,patch,"def patch(eval_results, dataset):
    
    for pid in range(1, len(dataset) + 1):
        if str(pid) not in eval_results:
            eval_results[str(pid)] = {
                ""sample_id"": 0, 
                ""compiled"": False, 
                ""correctness"": False, 
                ""metadata"": {},
                ""runtime"": -1.0, 
                ""runtime_stats"": {}
            }
    return eval_results",Patch the eval results with the dataset,???Ensure all dataset entries have default evaluation results???
3410,mock_initial_agent,"def mock_initial_agent(self) -> MagicMock:
        
        agent = MagicMock(spec=ConversableAgent)
        agent.name = ""initial_agent""
        agent._function_map = {}
        agent.handoffs = MagicMock()
        agent.handoffs.llm_conditions = []
        agent.handoffs.context_conditions = []
        agent.handoffs.after_work = None
        agent._group_is_established = False
        agent.description = ""Initial agent description""
        agent.llm_config = {""config_list"": [{""model"": ""test-model""}]}
        return agent",Create a mock initial agent for testing.,???Create a mock ConversableAgent with predefined attributes and configurations.???
3411,get_campaigns,"def get_campaigns(
        self,
        chain_id: Optional[str] = None,
        token_address: Optional[str] = None,
        test: bool = False,
        opportunity_id: Optional[str] = None,
        start_timestamp: Optional[int] = None,
        end_timestamp: Optional[int] = None,
        page: int = 0,
        items: int = 20,
    ) -> Dict:
        
        params = {k: v for k, v in locals().items() if v is not None and k != ""self""}

        return self._sync_request(""get"", ""/campaigns/"", params=params)",Get list of reward campaigns with optional filters,???Fetch campaign data with optional filters and pagination settings???
3412,parse_user_message,"def parse_user_message(ai_message: AIMessage) -> dict[str, str]:
            
            extracted_text = ai_message.content
            extracted_thought = ''
            if parsing_mode == 'thought':
                pattern = r""^(.*?)\s*User Response:\s*(.*)""  # The pattern to extract the user response
                match = re.search(pattern, ai_message.content, re.DOTALL)
                if match:
                    extracted_thought = match.group(1).strip()  # Text before ""User Response:""
                    extracted_text = match.group(2).strip()
                else:
                    extracted_text = ai_message.content
            return {'response': extracted_text, 'thought': extracted_thought}",Parse the user message.,???Extracts user response and thought from AI message content based on parsing mode???
3413,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""IndexedDB.disable"",
    }
    json = yield cmd_dict",Disables events from backend.,???Disables IndexedDB by sending a command and yielding a JSON response.???
3414,copy_svg_icon,"def copy_svg_icon(connector_name, connector_docs_dir):
    
    # Look for the icon in the apps directory
    icon_path = None
    for icon_candidate in [f""{connector_name}.svg"", f""{connector_name}-light.svg""]:
        source_path = FRONTEND_ICONS_DIR / icon_candidate
        if source_path.exists():
            icon_path = source_path
            break

    if not icon_path:
        print(f""  Warning: No SVG icon found for {connector_name}"")
        return False

    # Copy icon to the connector directory
    dest_path = connector_docs_dir / ""icon.svg""
    shutil.copy2(icon_path, dest_path)
    print(f""  Copied {icon_path.name} to {dest_path.relative_to(Path.cwd())}"")
    return True",Copy SVG icon to connector documentation directory.,???Copy SVG icon to connector documentation directory if available???
3415,match,"def match(cls, responses, targets) -> float:
        
        if responses is None:
            return 0
        responses = cast_to_set(responses)
        targets = cast_to_set(targets)

        if isinstance(list(targets)[0], str):
            new_responses = {item.lower() if isinstance(item, str) else str(item).lower() for item in responses}
            new_targets = {item.lower() for item in targets}
        elif isinstance(list(targets)[0], tuple):
            new_responses = set()
            new_targets = set()
            try:
                for res in responses:
                    new_res = tuple([item.lower().replace("" "", """").replace(""-"", """").replace(""\n"", """").replace(""\t"", """").replace(""_"", """").replace(""."", """") for item in res])
                    new_responses.add(new_res)
            except:  # the data type of the response might be wrong, return 0 in this case
                return 0
            for tgt in targets:
                new_tgt = tuple([item.lower().replace("" "", """").replace(""-"", """").replace(""\n"", """").replace(""\t"", """").replace(""_"", """").replace(""."", """") for item in tgt])
                new_targets.add(new_tgt)
        else:
            return 0

        return jaccard_index(new_responses, new_targets)",Exact match between targets and responses.,"???Normalize and compare response and target sets, returning their Jaccard similarity index.???"
3416,exec,"def exec(self, search_query):
        
        # Call the search utility function
        print(f""🌐 Searching the web for: {search_query}"")
        results = search_web(search_query)
        return results",Search the web for the given query.,"???  
Defines a method to perform a web search and return the results.  
???"
3417,click_at_coordinates,"def click_at_coordinates(x: int, y: int) -> str:
    
    driver.execute_script(f""window.scrollTo({x}, {y});"")
    driver.find_element(By.TAG_NAME, ""body"").click()",Clicks at the specified coordinates on the screen.,???Simulates a mouse click at specified screen coordinates using a web driver.???
3418,get_album_info,"def get_album_info():
    
    spotify_id = request.args.get(""id"")

    if not spotify_id:
        return Response(
            json.dumps({""error"": ""Missing parameter: id""}),
            status=400,
            mimetype=""application/json"",
        )

    try:
        # Import and use the get_spotify_info function from the utility module.
        from routes.utils.get_info import get_spotify_info

        album_info = get_spotify_info(spotify_id, ""album"")
        return Response(json.dumps(album_info), status=200, mimetype=""application/json"")
    except Exception as e:
        error_data = {""error"": str(e), ""traceback"": traceback.format_exc()}
        return Response(json.dumps(error_data), status=500, mimetype=""application/json"")",Retrieve Spotify album metadata given a Spotify album ID.,"???Fetch and return album details from Spotify API using provided ID, handling errors.???"
3419,basic_openapi_spec,"def basic_openapi_spec(self) -> dict:
        
        return {
            ""openapi"": ""3.0.0"",
            ""info"": {""title"": ""Test API"", ""version"": ""1.0.0""},
            ""paths"": {
                ""/items"": {
                    ""get"": {
                        ""operationId"": ""get_items"",
                        ""summary"": ""Get all items"",
                        ""responses"": {""200"": {""description"": ""Success""}},
                    }
                },
                ""/analytics"": {
                    ""get"": {
                        ""operationId"": ""get_analytics"",
                        ""summary"": ""Get analytics data"",
                        ""responses"": {""200"": {""description"": ""Success""}},
                    }
                },
            },
        }",Create a simple OpenAPI spec for testing.,??? Generates a basic OpenAPI specification for a test API with item and analytics endpoints. ???
3420,get_all_documents,"def get_all_documents(self, user_id: str = None) -> List[SimbaDoc]:
        
        try:
            session = self._Session()
            query = session.query(SQLDocument)
            
            # Filter by user_id if provided
            if user_id:
                query = query.filter(SQLDocument.user_id == user_id)
            
            docs = query.all()
            return [doc.to_simbadoc() for doc in docs]
        except Exception as e:
            logger.error(f""Failed to get all documents: {e}"")
            return []
        finally:
            session.close()",Retrieve all documents using SQLAlchemy ORM.,"???Retrieve and convert documents from database, optionally filtering by user ID.???"
3421,calculate_answer_score,"def calculate_answer_score(answer_text, label, do_print=False, search_api=None, literature_type='trial', pub_date=None):
    
    try:
        data = json.loads(answer_text)
        pred_query = data[""query""]
        
        searched_nctids = run_search_ctgov(pred_query, search_api)
            
        hit_pmids = set(searched_nctids) & set(label)
        recall = len(hit_pmids) / len(label)
        
        if do_print:
            print(f""Recall: {recall}"")
        
        if recall >= 0.7:
            answer_score = 5
        elif recall >= 0.5:
            answer_score = 4
        elif recall >= 0.4:
            answer_score = 3
        elif recall >= 0.3:
            answer_score = 1
        elif recall >= 0.1:
            answer_score = 0.5
        elif recall >= 0.05:
            answer_score = 0.1
        else:
            answer_score = -3.5

    except:
        print(""[Error] Error in evaluation"")
        answer_score = -4
    
    return answer_score",Calculate answer score based on document recall.,???Evaluate answer relevance by calculating recall-based score from search results.???
3422,_get_pool,"def _get_pool(cls):
        
        if cls._pool is None:
            try:
                cls._pool = ThreadedConnectionPool(
                    minconn=3,
                    maxconn=10,
                    user=settings.postgres.user,
                    password=settings.postgres.password,
                    host=settings.postgres.host,
                    port=settings.postgres.port,
                    dbname=settings.postgres.db,
                    sslmode='disable'
                )
                logger.info(""Created PostgreSQL connection pool"")
            except Exception as e:
                logger.error(f""Failed to create connection pool: {str(e)}"")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=""Failed to connect to database""
                )
        return cls._pool",Get or create the connection pool.,"???Establishes a PostgreSQL connection pool if not already initialized, logging success or errors.???"
3423,get_index_config,"def get_index_config(model: BaseEmbeddingModel) -> dict[str, Any]:
        
        base_config = {""dimension"": model.vector_dimensions, ""metric"": ""cosine""}

        # Add model-specific configurations
        if model.model_name == ""openai-text2vec"":
            base_config.update(
                {
                    ""pod_type"": ""p1"",  # Standard pod type
                    ""metadata_config"": {
                        ""indexed"": [""timestamp"", ""source""]  # Example metadata fields
                    },
                }
            )
        elif model.model_name == ""local-text2vec-transformers"":
            base_config.update(
                {
                    ""pod_type"": ""s1"",  # Starter pod type
                    ""metadata_config"": {""indexed"": [""timestamp"", ""source""]},
                }
            )

        return base_config",Get Pinecone index configuration for the model.,???Configure index settings based on embedding model type and metadata.???
3424,load_onnx,"def load_onnx(self, onnx_path:str):
        
        assert os.path.isfile(onnx_path), f""not found onnx file: {onnx_path}"" 
        onnx_graph = onnx.load(onnx_path)
        G_LOGGER.info(f""load onnx file: {onnx_path}"")
        return onnx_graph",Load onnx from file,"???Load and verify ONNX file, returning its graph representation.???"
3425,remove_node,"def remove_node(node_id: NodeId) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""nodeId""] = node_id.to_json()
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DOM.removeNode"",
        ""params"": params,
    }
    json = yield cmd_dict",Removes node with given id.,"???  
Generate a command to remove a DOM node by ID.  
???"
3426,render_path,"def render_path(path_to_item):
    
    result = """"
    for pth in path_to_item:
        if isinstance(pth, int):
            result += ""[{0}]"".format(pth)
        else:
            result += ""['{0}']"".format(pth)
    return result",Returns a string representation of a path,??? Convert a list of path elements into a formatted string representation. ???
3427,tools_schema,"def tools_schema(self):
        
        return {
            ""tools"": [
                {
                    ""name"": ""echo"",
                    ""description"": ""Return whatever text you pass in"",
                    ""parameters"": {
                        ""type"": ""object"",
                        ""properties"": {
                            ""text"": {""type"": ""string""}
                        },
                        ""required"": [""text""]
                    }
                }
            ]
        }",Simple tools JSON schema used across tests.,???Defines a schema for a tool that echoes input text.???
3428,mock_subprocess,"def mock_subprocess():
    
    with (
        patch('subprocess.run') as mock_run,
        patch('subprocess.check_output') as mock_check_output,
    ):
        # Default return values
        mock_run.return_value = MagicMock(returncode=0, stdout='Success', stderr='')
        mock_check_output.return_value = b'Success'

        yield {'run': mock_run, 'check_output': mock_check_output}",Create a mock subprocess module.,???Mock subprocess functions for testing command execution behavior???
3429,write_mtl_file,"def write_mtl_file(material_name: str, texture_path: str, filepath: str) -> None:
    
    with open(filepath, ""w"") as f:
        f.write(f""newmtl {material_name}\n"")
        f.write(f""map_Kd {texture_path}\n"")",Write material library file (.mtl) to link texture with OBJ.,???Generate a material file with specified name and texture path.???
3430,load_index,"def load_index(self):
        
        try:
            self.index = faiss.read_index(self.index_file)
            with open(self.metadata_file, ""rb"") as f:
                self.metadata = pickle.load(f)
            logger.info(f""Loaded FAISS index with {self.index.ntotal} entries"")
        except Exception as e:
            logger.error(f""Error loading FAISS index: {str(e)}"")
            # Create a new index if loading fails
            self.index = faiss.IndexFlatIP(self.dim)
            self.metadata = []
            self.save_index()",Load the FAISS index and metadata from disk.,"???Load or initialize a FAISS index and associated metadata, handling errors gracefully.???"
3431,_darken_color,"def _darken_color(self, hex_color: str, amount: float = 0.7) -> str:
        
        try:
            hex_color = hex_color.lstrip('#')
            if len(hex_color) != 6:
                 if len(hex_color) == 3: hex_color = """".join([c*2 for c in hex_color])
                 else: raise ValueError(""Invalid hex color format"")
            rgb = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
            darker_rgb = tuple(max(0, int(c * amount)) for c in rgb)
            return ""#{:02x}{:02x}{:02x}"".format(*darker_rgb)
        except Exception as e:
            print(f""Warning: Could not darken color {hex_color}: {e}"")
            return ""#808080""",Darkens a hex color by a multiplier (0 to 1).,"???Darken a hex color by a specified amount, returning a new hex color.???"
3432,cleanup_old_audio,"def cleanup_old_audio():
    
    while True:
        try:
            files = glob.glob(f""{SPEECH_DIR}/*.mp3"")
            for file in files:
                os.remove(file)
            print(""Cleaned up old speech files."")
        except Exception as e:
            print(f""Error during cleanup: {e}"")
        time.sleep(300)",Deletes all .mp3 files in the uploads/speech folder every 5 minutes.,??? Periodically delete outdated audio files from a specified directory. ???
3433,_version_callback,"def _version_callback(value: bool) -> None:
    
    if value:
        console.print(f""GolfMCP v{__version__}"")
        raise typer.Exit()",Print version and exit if --version flag is used.,???Display version information and exit if the flag is set.???
3434,get_instruction_args,"def get_instruction_args(self):
        
        return {
            ""num_sentences"": self._num_sentences,
            ""key_sentences"": list(self._key_sentences),
        }",Returns the keyward args of `build_description`.,"??? 
Returns a dictionary with sentence count and key sentences list. 
???"
3435,to_debug_report,"def to_debug_report(self) -> str:
        
        return ""Python SDK Debug Report:\n""\
               ""OS: {env}\n""\
               ""Python Version: {pyversion}\n""\
               ""Version of the API: 0.1.0\n""\
               ""SDK Package Version: 0.1.2"".\
               format(env=sys.platform, pyversion=sys.version)",Gets the essential information for debugging.,??? Generate a formatted debug report with system and SDK details. ???
3436,create_venv,"def create_venv(python_version: str = DEFAULT_PYTHON_VERSION):
    
    if os.path.exists(conf.PATH / VENV_DIR_NAME):
        return  # venv already exists

    RE_VENV_PROGRESS = re.compile(r'^(Using|Creating)')

    def on_progress(line: str):
        if RE_VENV_PROGRESS.match(line):
            log.info(line.strip())

    def on_error(line: str):
        log.error(f""uv: [error]\n {line.strip()}"")

    _wrap_command_with_callbacks(
        [get_uv_bin(), 'venv', '--python', python_version],
        on_progress=on_progress,
        on_error=on_error,
    )",Initialize a virtual environment in the project directory of one does not exist.,"???Create a virtual environment if it doesn't exist, logging progress and errors.???"
3437,get_metadata,"def get_metadata(self, media_id, media_type):
        
        self.logger.debug(f""Retrieving metadata: {media_type} {media_id}"")
        
        query = 
        params = (media_id, media_type)
        result = self.execute_query(query, params)

        if result:
            row = result[0]  # Assuming there's at least one row
            return {
                ""title"": row[0],
                ""overview"": row[1],
                ""release_date"": row[2],
                ""poster_path"": row[3]
            }
        return None",Retrieve metadata for a media item if it exists in the database.,???Retrieve and return media metadata using ID and type parameters.???
3438,_build_caches,"def _build_caches(self) -> None:
        
        if self.spec is None:
            logger.error(""Cannot build caches: OpenAPI spec not loaded"")
            return

        # Build paths cache
        paths_cache: dict[str, dict[str, str]] = {}
        domains_set = set()

        for path, methods in self.spec.get(""paths"", {}).items():
            for method, details in methods.items():
                # Add to paths cache
                if path not in paths_cache:
                    paths_cache[path] = {}
                paths_cache[path][method] = details.get(""operationId"", """")

                # Collect domains (tags)
                for tag in details.get(""tags"", []):
                    domains_set.add(tag)

        self._paths_cache = paths_cache
        self._domains_cache = sorted(list(domains_set))",Build internal caches for faster lookups.,???Initialize caches for API paths and domains from OpenAPI specification???
3439,_load_cached_schemas,"def _load_cached_schemas(self):
        
        for schema_file in self.cache_dir.glob('*.json'):
            if schema_file.name == SCHEMA_METADATA_FILE:
                continue

            try:
                with open(schema_file, 'r') as f:
                    schema = json.load(f)
                    if 'typeName' in schema:
                        resource_type = schema['typeName']
                        self.schema_registry[resource_type] = schema
                        print(f'Loaded schema for {resource_type} from cache')
            except (json.JSONDecodeError, IOError) as e:
                print(f'Error loading schema from {schema_file}: {str(e)}')",Load all cached schemas into the registry.,"???Load and register JSON schemas from cache directory, handling errors gracefully.???"
3440,plot_val_samples,"def plot_val_samples(self, batch, ni):
        
        plot_images(
            images=batch[""img""],
            batch_idx=torch.arange(len(batch[""img""])),
            cls=batch[""cls""].view(-1),  # warning: use .view(), not .squeeze() for Classify models
            fname=self.save_dir / f""val_batch{ni}_labels.jpg"",
            names=self.names,
            on_plot=self.on_plot,
        )",Plot validation image samples.,???Visualize validation batch images with class labels and save output.???
3441,get_router_config,"def get_router_config(self):
        
        config = self.get_config()

        # check if router config exists
        if ""router"" not in config:
            # create default config and save
            router_config = {""host"": DEFAULT_HOST, ""port"": DEFAULT_PORT, ""share_address"": DEFAULT_SHARE_ADDRESS}
            self.set_config(""router"", router_config)
            return router_config

        # get existing config
        router_config = config.get(""router"", {})

        # check if host and port exist, if not, set default values and update config
        # user may only set a customized port while leave host undefined
        updated = False
        if ""host"" not in router_config:
            router_config[""host""] = DEFAULT_HOST
            updated = True
        if ""port"" not in router_config:
            router_config[""port""] = DEFAULT_PORT
            updated = True
        if ""share_address"" not in router_config:
            router_config[""share_address""] = DEFAULT_SHARE_ADDRESS
            updated = True

        # save config if updated
        if updated:
            self.set_config(""router"", router_config)

        return router_config","get router configuration from config file, if not exists, flush default config","???Ensure router configuration exists, applying defaults if necessary.???"
3442,temp_profile_file,"def temp_profile_file():
    
    with tempfile.NamedTemporaryFile(delete=False, suffix="".json"") as f:
        # Create a basic profile config
        config = {
            ""empty_profile"": [],
        }
        f.write(json.dumps(config).encode(""utf-8""))
        temp_path = f.name

    yield temp_path
    # Clean up
    os.unlink(temp_path)",Create a temporary profile config file for testing,??? Generate and manage a temporary JSON profile configuration file. ???
3443,save,"def save(self, directory):
        
        jinx_path = os.path.join(directory, f""{self.jinx_name}.jinx"")
        ensure_dirs_exist(os.path.dirname(jinx_path))
        return write_yaml_file(jinx_path, self.to_dict())",Save jinx to file,???Save object data as a YAML file in the specified directory.???
3444,get_server_config,"def get_server_config(self, server_name: str) -> MCPServerConfig:
        
        if server_name not in self.servers:
            raise ValueError(f""MCP server '{server_name}' not found in config"")
        return self.servers[server_name]",Get configuration for a specific MCP server,"???Retrieve server configuration by name, raising error if not found.???"
3445,_get_google_generativeai_response,"def _get_google_generativeai_response(
    prompt,
    model, 
    temperature,
    max_tokens
    ):
    
    try:
        model = genai.GenerativeModel(model)
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens
            )
        )
        return response.text
    except Exception as e:
        print(f""Error with Google GenerativeAI: {str(e)}"")
        return None",Get response from Google GenerativeAI,???Generate AI-driven text response using specified model parameters.???
3446,validate_provider,"def validate_provider(provider: str) -> Optional[ValidationError]:
        
        try:
            if provider in [""azure_openai_chat_completion_client"", ""AzureOpenAIChatCompletionClient""]:
                provider = ""autogen_ext.models.openai.AzureOpenAIChatCompletionClient""
            elif provider in [""openai_chat_completion_client"", ""OpenAIChatCompletionClient""]:
                provider = ""autogen_ext.models.openai.OpenAIChatCompletionClient""

            module_path, class_name = provider.rsplit(""."", maxsplit=1)
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)

            if not is_component_class(component_class):
                return ValidationError(
                    field=""provider"",
                    error=f""Class {provider} is not a valid component class"",
                    suggestion=""Ensure the class inherits from Component and implements required methods"",
                )
            return None
        except ImportError:
            return ValidationError(
                field=""provider"",
                error=f""Could not import provider {provider}"",
                suggestion=""Check that the provider module is installed and the path is correct"",
            )
        except Exception as e:
            return ValidationError(
                field=""provider"",
                error=f""Error validating provider: {str(e)}"",
                suggestion=""Check the provider string format and class implementation"",
            )",Validate that the provider exists and can be imported,???Validate and resolve provider strings to ensure they map to valid component classes???
3447,mock_roles,"def mock_roles():
    
    return {
        ""Administrator"": {""id"": ""10000"", ""name"": ""Administrator""},
        ""Developer"": {""id"": ""10001"", ""name"": ""Developer""},
    }",Fixture to return mock project roles.,"???  
Create a dictionary mapping role names to their respective identifiers.  
???"
3448,query_perplexity,"def query_perplexity(query: str):
    

    payload = {
        ""model"": ""llama-3.1-sonar-small-128k-online"",
        ""messages"": [
            {""role"": ""system"", ""content"": ""Be precise and concise.""},
            {""role"": ""user"", ""content"": query},
        ],
        # ""max_tokens"": ""Optional"",
        ""temperature"": 0.2,
        ""top_p"": 0.9,
        ""return_citations"": True,
        ""search_domain_filter"": [""perplexity.ai""],
        ""return_images"": False,
        ""return_related_questions"": False,
        ""search_recency_filter"": ""month"",
        ""top_k"": 0,
        ""stream"": False,
        ""presence_penalty"": 0,
        ""frequency_penalty"": 1,
    }
    headers = {""Authorization"": f""Bearer {api_key}"", ""Content-Type"": ""application/json""}

    response = requests.request(""POST"", url, json=payload, headers=headers)
    if response.status_code == 200 and response.text:
        return response.text
    else:
        print(f""{response.status_code} - {response.text}"")
        return ""Failed to query perplexity""",Use Perplexity to concisely search the internet and answer a query with up-to-date information.,???Send a query to an AI model and return the response or an error message.???
3449,shopping_get_latest_order_url,"def shopping_get_latest_order_url() -> str:
    

    header = {
        ""Authorization"": f""Bearer {shopping_get_auth_token()}"",
        ""Content-Type"": ""application/json"",
    }

    params = {
        ""searchCriteria[sortOrders][0][field]"": ""created_at"",
        ""searchCriteria[sortOrders][0][direction]"": ""DESC"",
        ""searchCriteria[pageSize]"": ""1"",
    }

    response = requests.get(
        f""{SHOPPING}/rest/V1/orders"", params=params, headers=header
    )
    assert response.status_code == 200
    response_obj = response.json()[""items""][0]
    order_id = int(response_obj[""increment_id""])
    order_url = f""{SHOPPING}/sales/order/view/order_id/{order_id}/""
    return order_url",Get the latest order url from the shopping website.,???Fetches the URL of the most recent order using API authentication and sorting parameters.???
3450,_format_list_blocks,"def _format_list_blocks(self, block_content: dict, block_type: str, depth: int) -> str:
        
        text = self._extract_rich_text_markdown(block_content.get(""rich_text"", []))
        indent = ""  "" * depth

        if block_type == ""bulleted_list_item"":
            return f""{indent}- {text}""
        elif block_type == ""numbered_list_item"":
            return f""{indent}1. {text}""
        else:  # to_do
            checkbox = ""- [x]"" if block_content.get(""checked"", False) else ""- [ ]""
            return f""{indent}{checkbox} {text}""",Format list and todo blocks.,???Format list blocks into markdown with indentation based on type and depth???
3451,list_collections,"def list_collections(self, *args, **kwargs) -> List[CollectionInfo]:
        
        from azure.core.credentials import AzureKeyCredential
        from azure.search.documents.indexes import SearchIndexClient

        try:
            index_client = SearchIndexClient(
                endpoint=self.endpoint, credential=AzureKeyCredential(self.api_key)
            )

            collections = []
            for index in index_client.list_indexes():
                collections.append(
                    CollectionInfo(
                        collection_name=index.name,
                        description=f""Azure Search Index with {len(index.fields) if hasattr(index, 'fields') else 0} fields"",
                    )
                )
            return collections

        except Exception as e:
            print(f""Collection listing failed: {str(e)}"")
            return []",List all Azure Search indices with metadata,???Retrieve and describe Azure search index collections using client credentials???
3452,set_backend,"def set_backend(backend: Literal['spconv', 'torchsparse']) -> bool:
    
    global BACKEND
    
    backend = backend.lower().strip()
    logger.info(f""Setting sparse backend to: {backend}"")

    if backend == 'spconv':
        try:
            import spconv
            BACKEND = 'spconv'
            os.environ['SPARSE_BACKEND'] = 'spconv'
            return True
        except ImportError:
            logger.warning(""spconv not available"")
            return False
            
    elif backend == 'torchsparse':
        try:
            import torchsparse
            BACKEND = 'torchsparse'
            os.environ['SPARSE_BACKEND'] = 'torchsparse'
            return True
        except ImportError:
            logger.warning(""torchsparse not available"")
            return False
    
    return False",Set sparse backend with validation,"???Configure and validate sparse computation backend, updating environment settings.???"
3453,handle_model_error,"def handle_model_error(e: Exception) -> None:
    
    if isinstance(e, ValueError):
        raise HTTPException(status_code=404, detail=str(e))
    print(f""Error processing request: {str(e)}"")
    raise HTTPException(status_code=500, detail=str(e))",Handle model-related errors and raise appropriate HTTP exceptions,???Handle exceptions by raising HTTP errors with appropriate status codes???
3454,_handle_validation_error,"def _handle_validation_error(self, error, auth_config_class, base_error_message):
        
        from pydantic import ValidationError

        if isinstance(error, ValidationError):
            # Extract the field names and error messages
            error_messages = []
            for err in error.errors():
                field = ""."".join(str(loc) for loc in err.get(""loc"", []))
                msg = err.get(""msg"", """")
                error_messages.append(f""Field '{field}': {msg}"")

            error_detail = f""Invalid configuration for {auth_config_class}:\n"" + ""\n"".join(
                error_messages
            )
            raise HTTPException(
                status_code=422,
                detail=f""Invalid auth fields: {error_detail}. "" + base_error_message,
            ) from error
        else:
            # For other types of errors
            raise HTTPException(
                status_code=422,
                detail=f""Invalid auth fields: {str(error)}. "" + base_error_message,
            ) from error",Handle validation errors for auth fields.,???Handle validation errors by raising HTTP exceptions with detailed messages.???
3455,reset_config,"def reset_config(base_url):
    
    response = requests.get(f""{base_url}/config"")
    original_config = response.json()
    yield
    requests.post(f""{base_url}/config"", json=original_config)",Fixture to reset the main config after a test.,???Restore configuration settings to original state after temporary changes.???
3456,add_token,"def add_token(self, mint, name, symbol, provider, timestamp):
        
        if mint not in self.tokens:
            self.tokens[mint] = {'name': name, 'symbol': symbol, 'detections': {}}
        self.tokens[mint]['detections'][provider] = timestamp
        print(f""[TOKEN] mint={mint} name={name} symbol={symbol} provider={provider} time={timestamp:.3f}"")",Record a token detection event,???Add or update token details with detection timestamp and log the action.???
3457,_build_params,"def _build_params(self, args: GrepAppArguments) -> Dict[str, Any]:
        
        params = {""q"": args.search_query, ""page"": args.page, ""per_page"": args.per_page}
        if args.repository:
            params[""filter[repo][0]""] = args.repository
        if args.regexp:
            params[""regexp""] = ""true""
        if args.case:
            params[""case""] = ""true""
        if args.words:
            params[""words""] = ""true""
        logger.debug(f""Built params: {params}"")
        return params",Build request parameters from arguments,???Constructs a dictionary of search parameters based on user input arguments.???
3458,send_variables,"def send_variables(self, variables: dict):
        
        pickled_vars = base64.b64encode(pickle.dumps(variables)).decode()
        code = f
        self.run_code_raise_errors(code)",Send variables to the kernel namespace using pickle.,???Encode and send serialized variables for remote execution.???
3459,plot_val_samples,"def plot_val_samples(self, batch, ni):
        
        plot_images(
            batch[""img""],
            batch[""batch_idx""],
            batch[""cls""].squeeze(-1),
            batch[""bboxes""],
            kpts=batch[""keypoints""],
            paths=batch[""im_file""],
            fname=self.save_dir / f""val_batch{ni}_labels.jpg"",
            names=self.names,
            on_plot=self.on_plot,
        )",Plots and saves validation set samples with predicted bounding boxes and keypoints.,???Visualize validation samples with annotated images and metadata.???
3460,mock_metrics_guidance,"def mock_metrics_guidance():
    
    return {
        'cluster': {
            'metrics': [
                {
                    'name': 'cluster_failed_node_count',
                    'dimensions': ['ClusterName'],
                    'description': 'Number of failed worker nodes in cluster with node conditions',
                }
            ]
        },
        'node': {
            'metrics': [
                {
                    'name': 'node_cpu_limit',
                    'dimensions': ['ClusterName'],
                    'description': 'Maximum CPU units assignable to a single node',
                }
            ]
        },
    }",Create a mock metrics guidance dictionary.,??? Define mock metrics for cluster and node performance monitoring ???
3461,_init_storage,"def _init_storage(self) -> BaseStorage:
        
        try:
            settings = get_settings()
            if settings.STORAGE_PROVIDER == ""aws-s3"":
                # logger.info(""Initializing S3 storage for multi-vector chunks"")
                return S3Storage(
                    aws_access_key=settings.AWS_ACCESS_KEY,
                    aws_secret_key=settings.AWS_SECRET_ACCESS_KEY,
                    region_name=settings.AWS_REGION,
                    default_bucket=MULTIVECTOR_CHUNKS_BUCKET,
                )
            else:
                # logger.info(""Initializing local storage for multi-vector chunks"")
                storage_path = getattr(settings, ""LOCAL_STORAGE_PATH"", ""./storage"")
                return LocalStorage(storage_path=storage_path)
        except Exception:
            # logger.error(f""Failed to initialize external storage: {e}"")
            return None",Initialize appropriate storage backend based on settings.,"???Initialize storage based on settings, choosing between AWS S3 or local storage.???"
3462,after_collate_config,"def after_collate_config(
    config: Dict[str, Dict[str, Any]]
) -> Dict[str, Dict[str, Any]]:
    
    return {k: v for k, v in config.items() if k not in BEFORE_COLLATE_TRANSFORMS}",Return configs after excluding transformations from `befor_collate_config`.,???Filter configuration dictionary by excluding specific keys.???
3463,_validate_env_vars,"def _validate_env_vars(self) -> None:
        
        missing_vars = [var for var in self.REQUIRED_ENV_VARS if not os.getenv(var)]
        if missing_vars:
            raise ValueError(f""Missing required environment variables: {', '.join(missing_vars)}"")",Validate that all required environment variables are set.,"???  
Checks for missing required environment variables and raises an error if any are absent.  
???"
3464,_make_request,"def _make_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:
        
        url = f""{self.base_url}/{endpoint.lstrip('/')}""
        try:
            response = requests.request(method, url, **kwargs)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise Exception(f""Request failed: {str(e)}"")",Make HTTP request with error handling,"???  
Perform HTTP request and return JSON response or raise error.  
???"
3465,execute,"def execute(
        self,
        symbol: str,
        indicator: str,
        period: int,
        timeframe: str = ""1d"",
        start_date: str = None,
        end_date: str = None,
    ) -> str:
        
        try:
            analysis_container = st.container(border=True)

            with analysis_container:
                period = self._validate_period(period)
                start_date, end_date = self._validate_dates(start_date, end_date)
                symbol = symbol.upper().strip()
                timeframe = self._validate_timeframe(timeframe)

                st.header(f""{symbol} Technical Analysis - {timeframe}"", divider=""rainbow"")
                stock_data = self._fetch_stock_data(symbol=symbol, start=start_date, end=end_date, timeframe=timeframe)

                if stock_data.empty:
                    return f""No data found for {symbol}""

                indicator = indicator.lower()
                result_df = self._calculate_indicator(stock_data, indicator, period)
                self._display_analysis(result_df, symbol, indicator, period, timeframe)

                if indicator in [""liquidity"", ""levels""]:
                    self._show_trading_insights(result_df, symbol, indicator)

                return f""Successfully displayed {indicator.upper()} analysis for {symbol}""

        except Exception as e:
            st.error(f""Technical analysis failed: {str(e)}"")
            return f""Analysis error: {str(e)}""",Execute technical analysis with type-safe operations,???Perform technical analysis on stock data using specified indicators and display results.???
3466,prepopulate,"def prepopulate(self):
        
        if self.verbosity >= 1:
            print(colored(""\nPREPOPULATING MEMORY"", ""light_green""))
        examples = []
        examples.append({""text"": ""When I say papers I mean research papers, which are typically pdfs."", ""label"": ""yes""})
        examples.append({""text"": ""Please verify that each paper you listed actually uses langchain."", ""label"": ""no""})
        examples.append({""text"": ""Tell gpt the output should still be latex code."", ""label"": ""no""})
        examples.append({""text"": ""Hint: convert pdfs to text and then answer questions based on them."", ""label"": ""yes""})
        examples.append({
            ""text"": ""To create a good PPT, include enough content to make it interesting."",
            ""label"": ""yes"",
        })
        examples.append({
            ""text"": ""No, for this case the columns should be aspects and the rows should be frameworks."",
            ""label"": ""no"",
        })
        examples.append({""text"": ""When writing code, remember to include any libraries that are used."", ""label"": ""yes""})
        examples.append({""text"": ""Please summarize the papers by Eric Horvitz on bounded rationality."", ""label"": ""no""})
        examples.append({""text"": ""Compare the h-index of Daniel Weld and Oren Etzioni."", ""label"": ""no""})
        examples.append({
            ""text"": ""Double check to be sure that the columns in a table correspond to what was asked for."",
            ""label"": ""yes"",
        })
        for example in examples:
            self.add_input_output_pair(example[""text""], example[""label""])
        self._save_memos()","Adds a few arbitrary examples to the vector DB, just to make retrieval less trivial.",???Initialize and store labeled text examples for memory prepopulation.???
3467,_handle_streaming_response,"def _handle_streaming_response(self, response: requests.Response) -> Response:
        

        def generate():
            try:
                for chunk in response.iter_content(chunk_size=None):
                    if chunk:
                        yield chunk
            except Exception as e:
                logger.error(f""Error streaming response: {str(e)}"", exc_info=True)
                yield json.dumps({""error"": ""An internal error has occurred.""}).encode()

        response_headers = {
            ""Content-Type"": response.headers.get(""Content-Type"", ""application/json"")
        }

        return Response(
            stream_with_context(generate()),
            status=response.status_code,
            headers=response_headers,
        )",Handle streaming responses from the Ollama API.,???Stream and handle HTTP response chunks with error logging.???
3468,from_otel,"def from_otel(cls, code: Any | None) -> ""StatusCode"":
        
        if code is None:
            return cls.UNSET

        mapping = {""UNSET"": cls.UNSET, ""OK"": cls.OK, ""ERROR"": cls.ERROR}

        if hasattr(code, ""name""):
            return mapping.get(code.name, cls.UNSET)
        return cls.UNSET",Convert from OpenTelemetry StatusCode.,"???  
Convert external status code to internal status representation.  
???"
3469,set_async_call_stack_depth,"def set_async_call_stack_depth(
    max_depth: int,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""maxDepth""] = max_depth
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Debugger.setAsyncCallStackDepth"",
        ""params"": params,
    }
    json = yield cmd_dict",Enables or disables async call stacks tracking.,???Configure asynchronous call stack depth for debugging purposes.???
3470,check_if_processed,"def check_if_processed(model_name, dataset_name):
    
    metrics_file = os.path.join(""results/metrics"", f""{dataset_name}_{model_name}_metrics.txt"")
    generations_file = os.path.join(""results/generations"", f""{dataset_name}_{model_name}_generations.json"")
    
    if os.path.exists(metrics_file) and os.path.exists(generations_file):
        try:
            # Verify the files are valid by trying to read them
            with open(metrics_file, 'r') as f:
                metrics_content = f.read()
            with open(generations_file, 'r') as f:
                json.load(f)
            # If both files exist and are valid, consider it processed
            return True
        except:
            # If there's any error reading the files, consider it not processed
            return False
    return False",Check if this combination has already been processed by looking for both metrics and generations files.,???Check if model and dataset processing results are valid and complete???
3471,add_graph_edge,"def add_graph_edge(self, edge: graph.Edge):
        
        existing_edges: list[ast.Call] = self.get_graph_edge_nodes()
        if len(existing_edges):  # add the new edge after the last existing edge
            _, end = self.get_node_range(existing_edges[-1])
        else:  # find the instantiation of `StateGraph`
            graph_instance = asttools.find_method_calls(self.get_run_method(), 'StateGraph')[0]
            _, end = self.get_node_range(graph_instance)

        source, target = edge.source.name, edge.target.name
        # wrap the node names in quotes if they are not special nodes
        if edge.source.type != graph.NodeType.SPECIAL:
            source = f'""{source}""'
        if edge.target.type != graph.NodeType.SPECIAL:
            target = f'""{target}""'

        code = f
        self.edit_node_range(end, end, code)",Add a new edge to the graph configuration.,???Insert a new graph edge into the existing graph structure.???
3472,image_url_to_mime_and_base64,"def image_url_to_mime_and_base64(url: str) -> tuple[str, str]:
    
    import re

    match = re.match(r""data:(image/\w+);base64,(.*)"", url)
    if not match:
        raise ValueError(f""Invalid image data URI: {url[:30]}..."")
    mime_type, base64_data = match.groups()
    return mime_type, base64_data",Extract mime type and base64 data from ImageUrl,???Extracts MIME type and base64 data from an image URL string.???
3473,no_return_tool_result_factory,"def no_return_tool_result_factory(arg1: str) -> CallToolRequestResult:
    
    return CallToolRequestResult(
        isError=False, content=[], tool=""no_return_tool"", arguments={""arg1"": arg1}
    )",A tool that returns a result based on the input arguments.,???Create a tool request result with no error and empty content.???
3474,_get_items_to_index,"def _get_items_to_index(self) -> list[tuple[str, str]]:
        
        items_to_index = []
        symbols_to_process = [s for s in self.codebase.symbols if s.source]
        logger.info(f""Found {len(symbols_to_process)} symbols to index"")

        # Process each symbol - no need to pre-truncate since _batch_texts_by_tokens handles it
        for symbol in symbols_to_process:
            symbol_id = f""{symbol.file.filepath}::{symbol.name}""
            items_to_index.append((symbol_id, symbol.source))

        logger.info(f""Total symbols to process: {len(items_to_index)}"")
        return items_to_index",Get all symbols and their content to index.,???Extract and log symbols with sources for indexing from a codebase.???
3475,broadcast_obj,"def broadcast_obj(self, obj: Optional[Any], src: int) -> Any:
        
        if self.rank == src:
            self.expire_data()
            key = (f""broadcast_from/{src}/""
                   f""{self.broadcast_send_counter}"")
            self.store.set(key, pickle.dumps(obj))
            self.broadcast_send_counter += 1
            self.entries.append((key, time.perf_counter()))
            return obj
        else:
            key = (f""broadcast_from/{src}/""
                   f""{self.broadcast_recv_src_counter[src]}"")
            recv_obj = pickle.loads(self.store.get(key))
            self.broadcast_recv_src_counter[src] += 1
            return recv_obj",Broadcast an object from a source rank to all other ranks.,???Distribute or retrieve serialized data based on source rank in a network.???
3476,list_executors,"def list_executors() -> None:
    
    console.print(""[bold cyan]Available Executors:[/bold cyan]"")
    for name in plugin_manager.executors.keys():
        console.print(f""- {name}"")",List all available executors.,???Display available executors from plugin manager on console output.???
3477,left_hand_velocity,"def left_hand_velocity(envstate, robot_name: str):
    
    # return envstate[f""{_METASIM_BODY_PREFIX}left_hand""][""left_hand_subtreelinvel""] # Only for mujoco
    return envstate[""robots""][robot_name][""body""][""left_elbow_link""][""vel""]",Returns the velocity of the left hand.,???Retrieve left hand velocity from environment state for a specified robot.???
3478,format_prompt,"def format_prompt(user_query: str) -> str:
    
    system_message = 
    
    user_message = INSTRUCTION +  + user_query + 
    
    return [
        {""role"": ""system"", ""content"": system_message},
        {""role"": ""user"", ""content"": user_message}
    ]",Format the prompt for the model using the same template as make_prefix.,???Formats a user query into a structured message for a system interaction.???
3479,_clone_to_temp,"def _clone_to_temp(self, owner: str, repo: str, sha: str) -> str:
        
        import tempfile

        temp_dir = Path(tempfile.mkdtemp())
        repo_path = temp_dir / f""{owner}-{repo}""

        subprocess.run([""git"", ""clone"", ""--depth"", ""50"", repo_url, str(repo_path)], check=True, capture_output=True)

        self._checkout_sha(repo_path, sha)
        return str(repo_path)",Clone repository to temporary directory (no caching).,???Clone and checkout a specific commit of a repository to a temporary directory.???
3480,_start_iperf_server,"def _start_iperf_server(self) -> None:
        
        try:
            from zeroband.utils.ip import get_ip_address

            iperf_addr = get_ip_address(IPERF_IFNAME)
            iperf_port = IPERF_PORT + self.world_info.global_rank
            cmd: List[str] = [""iperf"", ""-s"", ""-p"", str(iperf_port)]
            self.server_process = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            self.god_store.set(f""iperf_{self.world_info.global_unique_id}"", f""{iperf_addr}:{iperf_port}"")
            self._logger.info(f""Started iperf server on {iperf_addr} with port {iperf_port}"")
        except Exception as e:
            self._logger.error(f""Failed to start iperf server: {str(e)}"")
            raise",Start the iperf server process.,"???Initialize and start an iperf server, logging its address and handling errors.???"
3481,format_observation,"def format_observation(self) -> str:
        
        cube_visualization = str(self.cube)

        # Format previous moves
        moves_made = (
            "", "".join([step[""move""] for step in self.step_history])
            if self.step_history
            else ""None""
        )
        steps_remaining = self.max_steps - len(self.step_history)

        message = (
            f""Current state of the Rubik's cube:\n\n""
            f""```\n{cube_visualization}\n```\n\n""
            f""Previous moves: {moves_made}\n""
            f""Steps remaining: {steps_remaining}\n""
        )

        if self.cube.is_solved():
            message += ""\nCongratulations! The cube is now solved.""

        return message",Format the cube state as a string observation for the LLM,???Generate a formatted status report of a Rubik's cube game progress.???
3482,send_sonic_token,"def send_sonic_token(agent, **kwargs):
    
    try:
        to_address = kwargs.get(""to_address"")
        token_address = kwargs.get(""token_address"")
        amount = float(kwargs.get(""amount""))

        # Direct passthrough to connection method - add your logic before/after this call!
        agent.connection_manager.connections[""sonic""].transfer(
            to_address=to_address,
            amount=amount,
            token_address=token_address
        )
        return

    except Exception as e:
        logger.error(f""Failed to send tokens: {str(e)}"")
        return None",Send tokens on Sonic chain.,???Facilitates token transfer via agent's sonic connection with error handling.???
3483,_get_adapter_name,"def _get_adapter_name(self, adapter_id: str) -> str:
        
        if adapter_id in self.adapter_names:
            return self.adapter_names[adapter_id]
            
        name = adapter_id.replace('.', '_').replace('-', '_')
        name = ''.join(c if c.isalnum() or c == '_' else '' for c in name)
        if name[0].isdigit():
            name = f""adapter_{name}""
            
        self.adapter_names[adapter_id] = name
        return name",Create a valid adapter name from adapter_id.,"???Generate a sanitized adapter name from an identifier, ensuring uniqueness and validity.???"
3484,_get_obs,"def _get_obs(self):
        
        states = self.env.handler.get_states()
        states = [{**state[""robots""], **state[""objects""]} for state in states]  # XXX: compatible with old states format
        joint_pos = np.array([
            [state[self.env.handler.robot.name][""dof_pos""][j] for j in self.env.handler.robot.joint_limits.keys()]
            for state in states
        ])

        # Get end effector positions (assuming 'ee' is the end effector subpath)
        ee_pos = np.array([state[""metasim_body_panda_hand""][""pos""] for state in states])

        return np.concatenate([joint_pos, ee_pos], axis=1)",Get current observations for all environments,???Extracts and combines robot joint and end effector positions from environment states.???
3485,_get_processor_supports,"def _get_processor_supports(self, proc_type: str) -> List[str]:
        
        supports_map = {
            ""image"": [
                ""Image content analysis"",
                ""Visual understanding"",
                ""Image description generation"",
                ""Image entity extraction"",
            ],
            ""table"": [
                ""Table structure analysis"",
                ""Data statistics"",
                ""Trend identification"",
                ""Table entity extraction"",
            ],
            ""equation"": [
                ""Mathematical formula parsing"",
                ""Variable identification"",
                ""Formula meaning explanation"",
                ""Formula entity extraction"",
            ],
            ""generic"": [
                ""General content analysis"",
                ""Structured processing"",
                ""Entity extraction"",
            ],
        }
        return supports_map.get(proc_type, [""Basic processing""])",Get processor supported features,???Retrieve processing capabilities based on specified content type???
3486,set_mode,"def set_mode(self, mode: str):
        
        if mode.lower() not in ['development', 'production', 'testing']:
            raise ValueError(f""Invalid mode: {mode}. Must be 'development', 'production', or 'testing'"")
        self.mode = mode.lower()","Set the application mode (development, production, or testing).","???  
Validate and set application mode to development, production, or testing.  
???"
3487,validate_config,"def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        
        required = [""network""]
        missing = [field for field in required if field not in config]
        if missing:
            raise ValueError(f""Missing config fields: {', '.join(missing)}"")
        
        if config[""network""] not in SONIC_NETWORKS:
            raise ValueError(f""Invalid network '{config['network']}'. Must be one of: {', '.join(SONIC_NETWORKS.keys())}"")
            
        return config",Validate Sonic configuration from JSON,???Ensure configuration includes required fields and valid network type???
3488,_prepare_task,"def _prepare_task(self, task: Dict[str, Any]) -> Tuple[bool, str]:
        
        if task[""file_name""]:
            if isinstance(task[""file_name""], Path):
                task[""file_name""] = str(task[""file_name""])

            file_path = Path(task[""file_name""])
            if not file_path.exists():
                logger.info(f""Skipping task because file not found: {file_path}"")
                return False, f""Skipping task because file not found: {file_path}""
            if file_path.suffix in ["".pdf"", "".docx"", "".doc"", "".txt""]:
                task[""Question""] += (
                    f"" Here are the necessary document files: {file_path}""
                )

            elif file_path.suffix in ["".jpg"", "".jpeg"", "".png""]:
                task[""Question""] += f"" Here are the necessary image files: {file_path}""

            elif file_path.suffix in ["".xlsx"", ""xls"", "".csv""]:
                task[""Question""] += (
                    f"" Here are the necessary table files: {file_path}, for processing excel file, you can write python code and leverage excel toolkit to process the file step-by-step and get the information.""
                )

            elif file_path.suffix in ["".py""]:
                task[""Question""] += f"" Here are the necessary python files: {file_path}""

            else:
                task[""Question""] += f"" Here are the necessary files: {file_path}""

        return True, None",Prepare the task by validating and enriching its data.,???Validate and annotate task based on file existence and type???
3489,list_templates,"def list_templates(self) -> list[ResourceTemplate]:
        
        logger.debug(""Listing templates"", extra={""count"": len(self._templates)})
        return list(self._templates.values())",List all registered templates.,???This function retrieves and logs a list of resource templates.???
3490,on_iteration_start,"def on_iteration_start(self, info: BuildStateInfo) -> None:
        
        self.cot_callable.emitter.emit_thought(""System"", f""📊 Starting iteration {info.iteration + 1}"")",Emit iteration start message.,???Emit a message indicating the start of a new iteration in a process???
3491,add_model_output,"def add_model_output(self, model_output: AgentOutput) -> None:
		
		tool_calls = [
			{
				'name': 'AgentOutput',
				'args': model_output.model_dump(mode='json', exclude_unset=True),
				'id': str(self.state.tool_id),
				'type': 'tool_call',
			}
		]

		msg = AIMessage(
			content='',
			tool_calls=tool_calls,
		)

		self._add_message_with_tokens(msg)
		# empty tool response
		self.add_tool_message(content='')",Add model output as AI message,???Integrate model output into AI message with tool call metadata???
3492,convert_jsonl_for_pyserini,"def convert_jsonl_for_pyserini(input_file, output_file):
    
    docs = []

    with open(input_file, ""r"", encoding=""utf-8"") as f:
        for line in f:
            data = json.loads(line.strip())
            
            # Create JSON document with a clear structure
            doc = {
                ""id"": data[""_id""],  # Unique identifier for search results
                ""contents"": data['title'] + '\n' + data['text'],  # Required field for Pyserini
            }
            
            docs.append(json.dumps(doc))
    
    with open(output_file, ""w"", encoding=""utf-8"") as f:
        for doc in docs:
            f.write(doc + ""\n"")

    print(f""✅ Converted JSONL saved to {output_file}"")",Convert JSONL data to Pyserini-compatible format with a structured 'contents' field,???Convert JSONL data to Pyserini-compatible format with structured documents.???
3493,get_key_limiter,"def get_key_limiter(self, api_name: str, key: str) -> RateLimiter:
        
        rate_limit = self.config.get_api_key_rate_limit(api_name)
        return self.get_or_create_limiter(f""{api_name}_key_{key}"", rate_limit)",Get or create a rate limiter for a specific API key.,"???  
Retrieve or create a rate limiter for a specific API key.  
???"
3494,_handle_error,"def _handle_error(self, task_id, data, task_info):
        
        # Extract error info
        message = data.get(""message"", ""Unknown error"")

        # Log error
        logger.error(f""Task {task_id} error: {message}"")

        # Update task info
        error_count = task_info.get(""error_count"", 0) + 1
        task_info[""error_count""] = error_count
        store_task_info(task_id, task_info)

        # Set status and error message
        # data[""status""] = ProgressState.ERROR
        data[""error""] = message",Handle error status from deezspot,"???Log and update task error details, incrementing error count.???"
3495,_menagerie_exists,"def _menagerie_exists() -> None:
  
  if not _MENAGERIE_PATH.exists():
    print(""mujoco_menagerie not found. Downloading..."")

    try:
      print(""Successfully downloaded mujoco_menagerie"")
    except subprocess.CalledProcessError as e:
      print(f""Error downloading mujoco_menagerie: {e}"", file=sys.stderr)
      raise","Ensure mujoco_menagerie exists, downloading it if necessary.","???Check if a directory exists, download if missing, and handle errors.???"
3496,task_sort_complete_sequence,"def task_sort_complete_sequence(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    # Calculate max possible block size given total array size
    max_size = 1
    total_space = 0
    while total_space + max_size + 1 <= size:
        total_space += max_size + 1
        max_size += 1
    max_size -= 1

    if max_size < 2:
        return None

    color = rng.randint(1, 9)

    # Create sequence of all sizes from 1 to max_size
    blocks = list(range(1, max_size + 1))
    rng.shuffle(blocks)

    # Create input field with shuffled blocks
    question = gen_field(size)
    pos = 0
    for block_size in blocks:
        for i in range(block_size):
            question[pos + i] = color
        pos += block_size + 1

    # Create answer field with sorted blocks
    answer = gen_field(size)
    pos = 0
    for block_size in range(1, max_size + 1):
        for i in range(block_size):
            answer[pos + i] = color
        pos += block_size + 1

    return {""input"": question, ""output"": answer}",Generate a task where a complete sequence of block sizes is sorted.,???Generate a shuffled and sorted block sequence within a specified size.???
3497,_register_default_providers,"def _register_default_providers(self) -> None:
        
        # Register BeeAI provider
        self.provider_registry.register_provider(BeeAIProvider(self.llm_service))
        
        # Additional providers would be registered here
        
        logger.info(f""Registered default providers: {self.provider_registry.list_available_providers()}"")",Register the default set of providers.,???Register default AI service providers and log their availability.???
3498,remove_unittest_inheritance,"def remove_unittest_inheritance(file):
    
    print(f""🔍 Checking file: {file.filepath}"")
    # Iterate through all classes in the file
    for cls in file.classes:
        # Check if the class inherits from unittest.TestCase
        if any(base.source == ""unittest.TestCase"" for base in cls.parent_class_names):
            # Remove the inheritance
            cls.parent_class_names[0].remove()
            print(f""🔧 Removed unittest.TestCase inheritance from: {cls.name}"")",Removes inheritance from unittest.TestCase for classes in a file,???Remove unittest.TestCase inheritance from classes in a given file.???
3499,init_dialog,"def init_dialog(self, experiment_path: str):
        
        self.memory = SqliteSaver(os.path.join(experiment_path, 'memory.db'))
        if self.chatbot is None:
            self.set_agent_tool_chatbot()  # Set the default agent chatbot
        if self.chatbot_initial_messages is None:
            logger = get_logger()
            logger.warning(
                f""{ConsoleColor.RED}Initial messages for the chatbot were not provided. ""
                f""Using empty list{ConsoleColor.RESET}"")
            self.chatbot_initial_messages = []

        self.dialog = Dialog(self.llm_user, self.chatbot, critique=self.llm_critique,
                             intermediate_processing=intermediate_processing,
                             memory=self.memory)
        self.user_prompt = get_prompt_template(self.config['user_prompt'])",Initialize the dialog graph.,"???Initialize a dialog system with memory, chatbot, and user prompt configuration.???"
3500,plot_predictions,"def plot_predictions(self, batch, preds, ni):
        
        pred_kpts = torch.cat([p[:, 6:].view(-1, *self.kpt_shape) for p in preds], 0)
        plot_images(
            batch[""img""],
            *output_to_target(preds, max_det=self.args.max_det),
            kpts=pred_kpts,
            paths=batch[""im_file""],
            fname=self.save_dir / f""val_batch{ni}_pred.jpg"",
            names=self.names,
            on_plot=self.on_plot,
        )",Plots predictions for YOLO model.,???Visualize model predictions with keypoints on batch images and save output.???
