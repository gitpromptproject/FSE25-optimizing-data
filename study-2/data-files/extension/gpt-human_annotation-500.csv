Unnamed: 0.2,Unnamed: 0.1,Unnamed: 0,id,function_name,codeFunctions,originalComment,codeComment,human_conciseness,human_fluency,human_content_adequacy,BLEU_1,ROUGE_1_P,ROUGE_W_R,CodeT5_plus_CS,BERTScore_R,SentenceBERT_CS,InferSent_CS,SIDE_score,C_Coeff,SIDE_emb,model_content_adequacy,model_conciseness,model_fluency
0,0,0,1,register_component_producer,"def register_component_producer(self, component_id: str, atom_name: str):
        
        logger.info(f""[DAG] Registering component producer {component_id=} {atom_name=}"")
        if atom_name in self.atoms:
            self._component_producers[component_id] = atom_name
            if self._current_atom:
                logger.info(f""[DAG] Component registered while atom was active self._current_atom={self._current_atom}"")
        else:
            logger.warning(f""[DAG] Skipping producer registration for unknown atom {atom_name=}"")",Link a component ID to its producing atom for DAG traceability.,"Register a component producer if the atom is recognized, logging the process.",5.0,5.0,4.0,0.6623376623376623,0.3333333333333333,0.3636363636363636,0.6310045719146729,0.25225818,0.65132374,0.71601033,0.8919727802276611,0.5,0.7973032444927464,4.0,5.0,5.0
1,1,1,2,process_in_batches,"def process_in_batches(tx, query, data, batch_size):
    
    for i in range(0, len(data), batch_size):
        batch = data[i : i + batch_size]
        tx.run(query, {""nodes"": batch} if ""nodes"" in query else {""edges"": batch})",Process data in batches and execute the given query.,Execute database queries in specified data batches,5.0,5.0,4.0,0.7686315513218586,0.7142857142857143,0.2222222222222222,0.4729007482528686,0.4238502,0.7193144,0.8215326,0.7848321199417114,0.2857142857142857,0.5190874842878558,4.0,5.0,4.0
2,2,2,3,filter_property,"def filter_property(self, siid: int, piid: int) -> bool:
        
        if (self._cache and 'properties' in self._cache and
            (f'{siid}.{piid}' in self._cache['properties'] or
             f'{siid}.*' in self._cache['properties'])):
            return True
        return False",Filter property by piid.,Check if a property identifier exists in a cached properties list.,5.0,5.0,5.0,0.3181818181818182,0.0909090909090909,0.25,0.6381975412368774,0.014677439,0.39078256,0.46684375,0.8636414408683777,0.1818181818181818,0.4972126613698171,5.0,5.0,5.0
3,3,3,4,_save_ontology_to_db,"def _save_ontology_to_db(self, ontology: ""Ontology"") -> None:  # type: ignore[no-any-unimported]
        
        if self.ontology_table_name in self.falkordb.list_graphs():
            raise ValueError(f""Knowledge graph {self.name} is already created."")
        graph = self.__get_ontology_storage_graph()
        ontology.save_to_graph(graph)",Save graph ontology to a separate table with {graph_name}_ontology,Save ontology data to a database if not already present.,4.0,5.0,3.0,0.7169694062511285,0.4,0.3636363636363636,0.6559760570526123,0.114257924,0.60543215,0.752154,0.8400108814239502,0.4,0.6083726831333309,4.0,5.0,4.0
4,4,4,5,_build_merge_function,"def _build_merge_function(self):
        

        def merge_fn(ctrl_msg: IngestControlMessage):
            do_trace_tagging = ctrl_msg.get_metadata(""config::add_trace_tagging"") is True
            if do_trace_tagging:
                ts_exit = datetime.now()
                ctrl_msg.set_timestamp(f""trace::exit::{self._task_desc}"", ts_exit)
                ctrl_msg.set_timestamp(""latency::ts_send"", ts_exit)
            return ctrl_msg

        return merge_fn",Adds tracing metadata to the control message and marks its completion.,Create a function to add trace timestamps to control messages if configured.,5.0,5.0,5.0,0.8289473684210527,0.4166666666666667,0.4545454545454545,0.5998378992080688,0.25483575,0.64156693,0.7810613,0.1350888907909393,0.1666666666666666,0.5122861707446339,5.0,5.0,5.0
5,5,5,6,filter_paths,"def filter_paths(openapi_schema: Dict[str, Any]) -> Dict[str, Any]:
    
    if ""paths"" not in openapi_schema:
        return openapi_schema

    filtered_paths = {}
    for path, path_item in openapi_schema[""paths""].items():
        include_path = False
        filtered_operations = {}

        for method, operation in path_item.items():
            if method.lower() in [""get"", ""post"", ""put"", ""delete"", ""patch""] and is_included_endpoint(
                path, method
            ):
                filtered_operations[method] = operation
                include_path = True

        if include_path:
            filtered_paths[path] = filtered_operations

    openapi_schema[""paths""] = filtered_paths
    return openapi_schema",Filter OpenAPI paths to only include allowed endpoints.,Filter OpenAPI schema paths to include specific HTTP methods.,5.0,5.0,4.0,0.7213114754098361,0.5555555555555556,0.625,0.5106111764907837,0.64834565,0.64581263,0.772147,0.8323187828063965,0.1111111111111111,0.547954485330062,4.0,5.0,4.0
6,6,6,7,_handle_task_completion,"def _handle_task_completion(self, task: asyncio.Task) -> None:
        
        task_id = getattr(task, ""task_id"", ""unknown"")
        self.pending_tasks.discard(task)

        if task.cancelled():
            logger.warning(f""🚫 WORKER_CANCELLED [{task_id}] Task was cancelled"")
        elif task.exception() is not None:
            logger.error(
                f""💥 WORKER_EXCEPTION [{task_id}] Task completed with exception: {task.exception()}""
            )
        else:
            logger.info(f""🏁 WORKER_CLEANUP [{task_id}] Task cleaned up successfully"")",Handle task completion and clean up.,"Handle task completion by logging cancellation, exceptions, or successful cleanup",5.0,5.0,5.0,0.419753086419753,0.3,0.5,0.5721233487129211,0.6253988,0.7242328,0.6885105,0.964098572731018,0.1,0.598506914267946,5.0,5.0,5.0
7,7,7,8,_get_mime_type,"def _get_mime_type(self) -> str:
        
        if self._format:
            return f""image/{self._format.lower()}""

        if self.path:
            suffix = self.path.suffix.lower()
            return {
                "".png"": ""image/png"",
                "".jpg"": ""image/jpeg"",
                "".jpeg"": ""image/jpeg"",
                "".gif"": ""image/gif"",
                "".webp"": ""image/webp"",
            }.get(suffix, ""application/octet-stream"")
        return ""image/png""",Get MIME type from format or guess from file extension.,Determine the MIME type of an image based on its format or file extension.,5.0,5.0,5.0,0.6891891891891891,0.4285714285714285,0.6,0.6272716522216797,0.51242065,0.7909616,0.8087659,0.8961490988731384,0.0714285714285714,0.5684769679577544,5.0,5.0,5.0
8,8,8,9,__init__,"def __init__(self, servers: list[dict[str, Any]], title: Optional[str] = None, **kwargs: Any) -> None:
        
        self._servers = servers
        self._title = title or ""MCP Proxy""
        self._kwargs = kwargs
        self._registered_funcs: list[Callable[..., Any]] = []
        self._globals: dict[str, Any] = {}

        self._security: dict[str, list[BaseSecurity]] = {}
        self._security_params: dict[Optional[str], BaseSecurityParameters] = {}
        self._tags: set[str] = set()

        self._function_group: dict[str, list[str]] = {}",Proxy class to generate client from OpenAPI schema.,"Initialize proxy configuration with server details, title, and security settings.",5.0,5.0,4.0,0.5432098765432098,0.1,0.125,0.49542897939682,0.2673516,0.4459738,0.652683,0.8704290986061096,0.3,0.5274969763272273,3.0,4.0,5.0
9,9,9,10,check_and_install_dependencies,"def check_and_install_dependencies():
    
    required_packages = [
        ""gunicorn"",
        ""tiktoken"",
        ""psutil"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")",Check and install required dependencies,"Ensure required software packages are installed, installing if necessary.",5.0,5.0,4.0,0.4794520547945205,0.2222222222222222,0.2,0.629093587398529,0.3499059,0.7228939,0.74307513,0.9805846214294434,0.4444444444444444,0.5033793549581349,5.0,5.0,5.0
10,10,10,11,get,"def get(cls, name: str, raise_on_missing: bool = True) -> Optional[T]:
        

        matches = [registered for registered in cls._get_storage() if re.match(registered, name)]

        if len(matches) > 1:
            raise ValueError(f""Multiple taggers match {name}: {', '.join(matches)}"")

        elif len(matches) == 0:
            if raise_on_missing:
                tagger_names = "", "".join([tn for tn, _ in cls.items()])
                raise ValueError(f""Unknown tagger {name}; available taggers: {tagger_names}"")
            return None

        else:
            name = matches[0]
            t, _ = cls._get_storage()[name]
            return t",Get a tagger from the registry; raise ValueError if it doesn't exist.,Retrieve or raise error for a specific tagger based on name matching,4.0,5.0,4.0,0.782524896105431,0.25,0.1538461538461538,0.6117190718650818,0.2761558,0.6588984,0.5861549,0.6138187646865845,0.5833333333333334,0.5543397337125364,5.0,5.0,4.0
11,11,11,12,get_single_param_type_from_schema,"def get_single_param_type_from_schema(param_schema: Dict[str, Any]) -> str:
    
    if ""anyOf"" in param_schema:
        types = {schema.get(""type"") for schema in param_schema[""anyOf""] if schema.get(""type"")}
        if ""null"" in types:
            types.remove(""null"")
        if types:
            return next(iter(types))
        return ""string""
    return param_schema.get(""type"", ""string"")",Get the type of a parameter from the schema.,"Determine parameter type from schema, defaulting to ""string"" if unspecified",5.0,5.0,5.0,0.5333333333333333,0.4,0.3333333333333333,0.5704011917114258,0.50727504,0.63853765,0.6835121,0.9669222831726074,0.4,0.5902924854072706,5.0,5.0,5.0
12,12,12,13,start,"def start(self):
        
        try:
            self.reset_trace()
            self.current_trace[""start_time""] = datetime.now()
            self._active = True
            self._monkey_patch()

            if self.save_interval:
                loop = asyncio.get_event_loop()
                self._save_task = loop.create_task(self._periodic_save())

            logger.info(""Tracing started"")
        except Exception as e:
            logger.error(f""Error starting tracer: {e}"")
            self.on_error(e, context=""start"")
            raise",Start tracing with enhanced error handling and async support,Initialize and start a tracing process with error handling and periodic saving.,5.0,5.0,5.0,0.7088607594936709,0.5,0.6666666666666666,0.5422117710113525,0.494651,0.6526789,0.83310884,0.9435657262802124,0.0833333333333333,0.5721286492942541,5.0,5.0,5.0
13,13,13,14,generate_audio_from_phonemes,"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella""):
    
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    if response.status_code != 200:
        print(f""Error: {response.text}"")
        return None
    return response.content",Generate audio from phonemes,Convert phonetic input into audio using specified voice settings.,5.0,5.0,3.0,0.3538461538461538,0.1111111111111111,0.25,0.5578997135162354,0.26801553,0.6922981,0.6455563,0.6973874568939209,0.1111111111111111,0.5513172395343857,4.0,5.0,5.0
14,14,14,15,_build_conversation_context,"def _build_conversation_context(self) -> str:
        
        context_parts = []

        # Include recent messages (last 5 for now)
        recent_messages = (
            self.state.messages[-5:]
            if len(self.state.messages) > 5
            else self.state.messages
        )

        for msg in recent_messages:
            if msg.role != ""system"":  # Skip system message in context
                role_label = ""User"" if msg.role == ""user"" else ""Assistant""
                context_parts.append(f""{role_label}: {msg.content}"")

        return (
            ""\n"".join(context_parts)
            if context_parts
            else ""This is the start of our conversation.""
        )",Build context string from conversation history,Constructs a conversation context string from recent non-system messages.,5.0,5.0,5.0,0.547945205479452,0.4,0.5,0.5860644578933716,0.63557476,0.7247748,0.81894696,0.9284523725509644,0.4444444444444444,0.6770032146248859,5.0,5.0,5.0
15,15,15,16,to_api_params,"def to_api_params(self) -> Dict[str, JsonValue]:
        
        params: Dict[str, JsonValue] = {
            ""includeResolvedCases"": cast(JsonValue, self.include_resolved_cases),
            ""includeCommunications"": cast(JsonValue, self.include_communications),
            ""language"": cast(JsonValue, self.language),
        }

        if self.case_id_list:
            params[""caseIdList""] = cast(JsonValue, self.case_id_list)
        if self.display_id:
            params[""displayId""] = cast(JsonValue, self.display_id)
        if self.after_time:
            params[""afterTime""] = cast(JsonValue, self.after_time)
        if self.before_time:
            params[""beforeTime""] = cast(JsonValue, self.before_time)
        if self.max_results:
            params[""maxResults""] = cast(JsonValue, self.max_results)
        if self.next_token:
            params[""nextToken""] = cast(JsonValue, self.next_token)

        return params",Convert to AWS API parameters.,Convert object attributes into a dictionary of API query parameters.,5.0,5.0,4.0,0.3970588235294117,0.3,0.6,0.5273870825767517,0.5332565,0.60154617,0.68275666,0.907254695892334,0.2,0.4677241930969377,5.0,5.0,5.0
16,16,16,17,interpreter_feedback,"def interpreter_feedback(self, output: str) -> str:
        
        if not output:
            raise ValueError(""No output to interpret."")
        return f",Not really needed for this tool (use return of execute() directly),Function checks for empty output and raises an error if none is found.,4.0,4.0,3.0,0.6714285714285714,0.0769230769230769,0.0909090909090909,0.6477795243263245,0.10408587,0.34901917,0.6012193,0.3561413288116455,0.3076923076923077,0.3298709852845292,2.0,4.0,2.0
17,17,17,18,_load_model,"def _load_model(self):
        
        try:
            # Check if adaptive-classifier is installed
            try:
                import adaptive_classifier
            except ImportError:
                logger.info(""Installing adaptive-classifier library..."")
                os.system(f""{sys.executable} -m pip install adaptive-classifier"")
                import adaptive_classifier
            
            # Import the AdaptiveClassifier class
            from adaptive_classifier import AdaptiveClassifier
            
            logger.info(f""Loading complexity classifier model: {self.model_name}"")
            self.classifier = AdaptiveClassifier.from_pretrained(self.model_name)
            logger.info(""Classifier loaded successfully"")
            
        except Exception as e:
            logger.error(f""Error loading complexity classifier: {e}"")
            # Fallback to basic classification if model fails to load
            self.classifier = None",Load the classification model using adaptive-classifier library.,"Load or install and initialize an adaptive complexity classifier model, with error handling.",5.0,5.0,5.0,0.6195652173913043,0.3076923076923077,0.375,0.6894853115081787,0.54608214,0.6878934,0.61172414,0.917072296142578,0.4615384615384615,0.737604894603453,5.0,5.0,5.0
18,18,18,19,load_optillm_bench,"def load_optillm_bench() -> datasets.Dataset:
    
    try:
        dataset = load_dataset(""codelion/optillmbench"")
        return dataset[""test""]  # We use the test split for evaluation
    except Exception as e:
        logger.error(f""Error loading dataset: {e}"")
        raise",Load the OptiLLM Bench dataset.,Load and return the test split of the OptiLLM benchmark dataset for evaluation.,5.0,5.0,5.0,0.379746835443038,0.3076923076923077,0.8,0.7099065184593201,0.7695815,0.5308567,0.30372655,0.9509202837944032,0.6153846153846154,0.672825727966652,5.0,5.0,5.0
19,19,19,20,__init__,"def __init__(self, agent: SwarmAgent, context_variables: Dict[str, str] = None):
        
        super().__init__(agent=agent)
        self.context_variables = defaultdict(str, context_variables or {})
        self.instruction = (
            agent.instruction(self.context_variables)
            if isinstance(agent.instruction, Callable)
            else agent.instruction
        )
        logger.debug(
            f""Swarm initialized with agent {agent.name}"",
            data={
                ""context_variables"": self.context_variables,
                ""instruction"": self.instruction,
            },
        )","Initialize the LLM planner with an agent, which will be used as the starting point for the workflow.",Initialize a swarm agent with context variables and log the setup details.,5.0,5.0,5.0,0.637166787313692,0.3333333333333333,0.1666666666666666,0.5834614634513855,0.33464658,0.58393157,0.70781654,0.8972458243370056,0.3333333333333333,0.6913398163290179,5.0,5.0,5.0
20,20,20,21,auto_discover_processors,"def auto_discover_processors():
    
    # get processors directory path
    processors_path = Path(__file__).parent.parent / ""processors""

    # iterate over all subdirectories in processors directory
    for _, name, _ in pkgutil.iter_modules([str(processors_path)]):
        # if it is a directory and contains processor.py
        processor_file = processors_path / name / ""processor.py""
        if processor_file.exists():
            module_path = f""lpm_kernel.file_data.processors.{name}.processor""
            try:
                importlib.import_module(module_path)
            except ImportError as e:
                print(f""Failed to load processor module {module_path}: {e}"")",Automatically discover and import all processors,Automatically discover and import processor modules from a specified directory path.,5.0,5.0,5.0,0.5595238095238095,0.4545454545454545,0.8333333333333334,0.644162118434906,0.700723,0.80168045,0.85148174,0.8572423458099365,0.5454545454545454,0.6336818878843322,5.0,5.0,5.0
21,21,21,22,__init__,"def __init__(self):
        
        super().__init__()
        self._request_times = []
        self._lock = asyncio.Lock()
        self._stats = {
            ""api_calls"": 0,
            ""rate_limit_waits"": 0,
        }",Initialize the LinearSource with rate limiting state.,Initialize asynchronous API call tracker with rate limit monitoring.,5.0,5.0,4.0,0.6911764705882353,0.4444444444444444,0.5714285714285714,0.5249898433685303,0.4812933,0.5328562,0.7538631,0.7288120985031128,0.0,0.3554406452367062,4.0,5.0,5.0
22,22,22,23,infer_on_single_gpu,"def infer_on_single_gpu(model_path, device_id, chunk_of_tested_messages, batch_size, results=None):
    
    model, processor = init_model(model_path, device_id)
    
    ### split batch
    responses = []
    batch_messages_list = [chunk_of_tested_messages[start: start + batch_size] 
               for start in range(0, len(chunk_of_tested_messages), batch_size)]

    for batch_messages in tqdm.auto.tqdm(batch_messages_list, desc=f""GPU {device_id} progress"", position=device_id, leave=False):
        batch_output_text = answer_a_batch_question_qwen(batch_messages, model, processor)
        
        responses.extend(batch_output_text)
    
    results[device_id] = responses
    return",init model on this single gpu and let it answer asign chunk of questions,Process test messages in batches using a model on a single GPU and store results.,4.0,4.0,5.0,0.7160493827160493,0.3333333333333333,0.3571428571428571,0.6779584288597107,0.29898164,0.6911922,0.7704054,0.7166670560836792,0.3333333333333333,0.6414218243273868,4.0,5.0,5.0
23,23,23,24,download_youtube_video,"def download_youtube_video(video_url, output_path):
    
    # video_url, output_path = info
    try:
        if not os.path.exists(os.path.dirname(output_path)):
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # download command
        command = ['yt-dlp', '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', '--merge-output-format',
               'mp4', '--output', output_path , video_url]
        # subprocess.run
        result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')

        if result.returncode == 0:
            print('Download {:s} successfully!'.format(video_url))
        else:
            print(""Fail to download {:s}, error info:\n{:s}"".format(video_url, result.stderr))
    except Exception as e:
        print(f""error: {e}"")",:param video_url: youtube video url :param output_dir: file path to save,Download YouTube video to specified path using yt-dlp command.,5.0,5.0,4.0,0.6314204524642643,0.4,0.2307692307692307,0.6256189942359924,0.0024223044,0.56188124,0.8140942,0.9851990938186646,0.3333333333333333,0.5762428770008979,5.0,5.0,5.0
24,24,24,25,sub_device_state,"def sub_device_state(
        self, did: str, handler: Callable[[str, MIoTDeviceState, Any], None],
        handler_ctx: Any = None
    ) -> bool:
        
        if did not in self._device_list_cache:
            raise MIoTClientError(f'did not exist, {did}')
        self._sub_device_state[did] = MipsDeviceState(
            did=did, handler=handler, handler_ctx=handler_ctx)
        _LOGGER.debug('client sub device state, %s', did)
        return True",Call callback handler in main loop,"Subscribe to device state updates, raising error if device ID is invalid.",5.0,5.0,5.0,0.3150684931506849,0.0,0.0,0.633482813835144,0.1605049,0.22649904,0.60647964,0.9445496797561646,0.5,0.4521410078268968,5.0,5.0,5.0
25,25,25,26,generate_audio_from_phonemes,"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella"") -> Optional[bytes]:
    
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    
    print(f""Response status: {response.status_code}"")
    print(f""Response headers: {dict(response.headers)}"")
    print(f""Response content type: {response.headers.get('Content-Type')}"")
    print(f""Response length: {len(response.content)} bytes"")
    
    if response.status_code != 200:
        print(f""Error response: {response.text}"")
        return None
        
    if not response.content:
        print(""Error: Empty response content"")
        return None
        
    return response.content",Generate audio from phonemes.,Convert phonemes to audio using a specified voice and return the audio data.,5.0,5.0,4.0,0.3552631578947369,0.1538461538461538,0.25,0.5651586651802063,0.4589733,0.70464134,0.6665226,0.9033184051513672,0.3076923076923077,0.6352531203324381,4.0,5.0,5.0
26,26,26,27,_setup_user_dir,"def _setup_user_dir(self):
        
        if '--user-data-dir' not in [
            arg.split('=')[0] for arg in self.options.arguments
        ]:
            # For all browsers, use a temporary directory
            temp_dir = self._temp_directory_manager.create_temp_dir()
            self.options.arguments.append(f'--user-data-dir={temp_dir.name}')",Prepares the user data directory if necessary.,Ensure user data directory is set for browser options,5.0,5.0,4.0,0.6981132075471698,0.3333333333333333,0.4285714285714285,0.611697793006897,0.2899489,0.5814559,0.8363986,0.9001234769821167,0.4444444444444444,0.6461576793853099,5.0,5.0,4.0
27,27,27,28,__post_init__,"def __post_init__(
        self,
    ) -> ""BaseSecurity"":  # dataclasses uses __post_init__ instead of model_validator
        
        valid_in_values = {
            ""apiKey"": [""header"", ""query"", ""cookie""],
            ""http"": [""bearer"", ""basic""],
            ""oauth2"": [""bearer""],
            ""openIdConnect"": [""bearer""],
            ""mutualTLS"": [""tls""],
            ""unsupported"": [""unsupported""],
        }
        if self.in_value not in valid_in_values[self.type]:
            raise ValueError(f""Invalid in_value '{self.in_value}' for type '{self.type}'"")
        return self",Validate the in_value based on the type.,Validate security configuration based on type and location constraints.,5.0,5.0,5.0,0.4788732394366197,0.4444444444444444,0.5,0.494299829006195,0.49535736,0.47264707,0.6523142,0.5673559308052063,0.2222222222222222,0.4123953815338927,5.0,5.0,5.0
28,28,28,29,to_mcp_prompt,"def to_mcp_prompt(self, **overrides: Any) -> MCPPrompt:
        
        arguments = [
            MCPPromptArgument(
                name=arg.name,
                description=arg.description,
                required=arg.required,
            )
            for arg in self.arguments or []
        ]
        kwargs = {
            ""name"": self.name,
            ""description"": self.description,
            ""arguments"": arguments,
        }
        return MCPPrompt(**kwargs | overrides)",Convert the prompt to an MCP prompt.,Convert object attributes and overrides into an MCPPrompt instance.,5.0,5.0,5.0,0.4626865671641791,0.2222222222222222,0.2857142857142857,0.5231387615203857,0.51263505,0.5818064,0.6898851,0.6777635216712952,0.3333333333333333,0.6399363631354856,5.0,5.0,5.0
29,29,29,30,__getitem__,"def __getitem__(self, key: str) -> Any:
        
        try:
            return self.data[key]
        except KeyError:
            raise KeyError(f""Context variable '{key}' not found"")",Get a value using dictionary syntax: context[key],Retrieve value from data dictionary or raise key error if missing,5.0,5.0,5.0,0.5538461538461539,0.2727272727272727,0.375,0.6225908398628235,0.22858092,0.5976056,0.6961133,0.9575483798980712,0.1818181818181818,0.4425939958288319,5.0,5.0,4.0
30,30,30,31,__call__,"def __call__(
      self, query_sequence: str, chain_polymer_type: str
  ) -> tuple[msa.Msa, MsaErrors]:
    
    if chain_polymer_type != self._chain_polymer_type:
      raise ValueError(
          f'EmptyMsaProvider of type {self._chain_polymer_type} called with '
          f'sequence of {chain_polymer_type=}, {query_sequence=}.'
      )
    return (
        msa.Msa.from_empty(
            query_sequence=query_sequence,
            chain_poly_type=self._chain_polymer_type,
        ),
        (),
    )","Returns an MSA containing just the query sequence, never errors.",Validate polymer type and return empty MSA for a query sequence.,5.0,5.0,5.0,0.734375,0.3636363636363636,0.4,0.58845055103302,0.30030137,0.747931,0.7170675,0.5088006258010864,0.2727272727272727,0.5875971601955554,5.0,5.0,5.0
31,31,31,32,interpreter_feedback,"def interpreter_feedback(self, output: str) -> str:
        
        if self.execution_failure_check(output):
            return f""Web search failed: {output}""
        return f""Web search result:\n{output}""",Feedback of web search to agent.,The function evaluates execution success and formats web search feedback.,5.0,5.0,5.0,0.410958904109589,0.3,0.3333333333333333,0.6181093454360962,0.39352438,0.5686712,0.619412,0.971869707107544,0.1,0.5518530627060924,4.0,5.0,4.0
32,32,32,33,map_your_entity_network,"def map_your_entity_network(self)->bool:
        
        try:
            # Mark step as in progress
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.IN_PROGRESS)
            logger.info(""Starting entity network mapping..."")
        
            # Get or prepare L2 data
            self._prepare_l2_data()

            l2_generator = L2Generator(
                data_path=os.path.join(os.getcwd(), ""resources"")
            )
            l2_generator.data_preprocess(self.l2_data[""notes""], self.l2_data[""basic_info""])
            
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.COMPLETED)
            logger.info(""Entity network mapping completed successfully"")
            return True
            
        except Exception as e:
            logger.error(f""Map entity network failed: {str(e)}"")
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.FAILED)
            self._cleanup_resources()
            return False",Map entity network using notes and basic info,"Map and preprocess entity network data, updating progress status accordingly.",4.0,5.0,4.0,0.5324675324675324,0.4,0.375,0.5184218287467957,0.3321362,0.54468745,0.79668623,0.7896491289138794,0.4,0.6592091092129774,5.0,5.0,5.0
33,33,33,34,initialize_cache,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")",Initialize the KV cache in all workers.,"Initialize cache with specified GPU and CPU blocks, logging memory usage.",4.0,5.0,4.0,0.4931506849315068,0.1818181818181818,0.2857142857142857,0.5479017496109009,0.3779757,0.3900354,0.7386474,0.9473244547843932,0.4545454545454545,0.6774899800467978,5.0,5.0,5.0
34,34,34,35,_build_service_characteristics,"def _build_service_characteristics(self) -> Dict[str, Any]:
        
        characteristics: Dict[str, Any] = {
            'write_throughput_limitation': 'Amazon Keyspaces has specific throughput characteristics that differ from self-managed Cassandra',
            'implementation_notes': 'The service architecture imposes a 1MB item size limit and throughput constraints different from open-source Cassandra',
        }

        response_guidance = {
            'do_not_mention': ['DynamoDB', 'underlying implementation', 'AWS storage layer'],
            'preferred_terminology': [
                'Keyspaces architecture',
                'managed service design',
                'AWS distributed systems',
            ],
        }

        characteristics['response_guidance'] = response_guidance

        return characteristics",Build service characteristics for Amazon Keyspaces.,Constructs a dictionary detailing Amazon Keyspaces service characteristics and response guidelines.,5.0,5.0,5.0,0.494949494949495,0.3636363636363636,0.3333333333333333,0.6414053440093994,0.6862263,0.7881756,0.6933336,0.9088215827941896,0.5454545454545454,0.5939055774050171,5.0,5.0,5.0
35,35,35,36,__str__,"def __str__(self) -> str:
        
        info = """"
        info += ""Split Task:\n""
        info += f""  tokenizer: {self._tokenizer}\n""
        info += f""  chunk_size: {self._chunk_size}\n""
        info += f""  chunk_overlap: {self._chunk_overlap}\n""
        for key, value in self._params.items():
            info += f""  {key}: {value}\n""
        return info",Returns a string with the object's config and run time state,Generate a formatted string summarizing tokenizer and chunk parameters.,5.0,5.0,5.0,0.676056338028169,0.3333333333333333,0.25,0.5975320339202881,0.35610846,0.5742216,0.6541387,0.8396241068840027,0.2222222222222222,0.5307542198998907,5.0,5.0,5.0
36,36,36,37,_get_camoufox_options,"def _get_camoufox_options(self):
        
        return {
            ""geoip"": self.geoip,
            ""proxy"": self.proxy,
            ""enable_cache"": True,
            ""addons"": self.addons,
            ""exclude_addons"": [] if self.disable_ads else [DefaultAddons.UBO],
            ""headless"": self.headless,
            ""humanize"": self.humanize,
            ""i_know_what_im_doing"": True,  # To turn warnings off with the user configurations
            ""allow_webgl"": self.allow_webgl,
            ""block_webrtc"": self.block_webrtc,
            ""block_images"": self.block_images,  # Careful! it makes some websites doesn't finish loading at all like stackoverflow even in headful
            ""os"": None if self.os_randomize else get_os_name(),
            **self.additional_arguments
        }",Return consistent browser options dictionary for both sync and async methods,Configure browser options for privacy and customization settings.,5.0,5.0,3.0,0.7014301248748563,0.5,0.3636363636363636,0.5036596059799194,0.21040976,0.4477197,0.7563239,0.5727200508117676,0.0,0.451281034867008,4.0,5.0,5.0
37,37,37,38,validate_dimensions,"def validate_dimensions(dimension: Union[int, float], attribute_name: str) -> int:
    
    dimension = int(dimension)
    if dimension <= 0 or dimension % 64 != 0:
        raise ValueError(f""The '{attribute_name}' must be a multiple of 64."")
    return dimension",Dimensions must be a multiple of 64 otherwise a GGML_ASSERT error is encountered.,"Ensure dimension is a positive multiple of 64, else raise error.",5.0,5.0,5.0,0.6828658746255741,0.6363636363636364,0.4285714285714285,0.5965253114700317,0.21112534,0.826843,0.6772045,0.8926339745521545,0.6363636363636364,0.6102250257880175,5.0,5.0,4.0
38,38,38,39,context_with_data,"def context_with_data() -> CodeExecutorContext:
  
  state_data = {
      ""_code_execution_context"": {
          ""execution_session_id"": ""session123"",
          ""processed_input_files"": [""file1.csv"", ""file2.txt""],
      },
      ""_code_executor_input_files"": [
          {""name"": ""input1.txt"", ""content"": ""YQ=="", ""mime_type"": ""text/plain""}
      ],
      ""_code_executor_error_counts"": {""invocationA"": 2},
  }
  state = State(state_data, {})
  return CodeExecutorContext(state)",Fixture for a CodeExecutorContext with some pre-populated data.,Initialize a code execution context with session data and input files.,5.0,5.0,4.0,0.6714285714285714,0.2727272727272727,0.3333333333333333,0.5636090040206909,0.29490674,0.72554874,0.39865044,0.7597153186798096,0.0909090909090909,0.6173933291237788,4.0,5.0,5.0
39,39,39,40,_format_comment_body,"def _format_comment_body(self, comment: GitHubPRComment) -> str:
        
        body = comment.comment_body

        # Add code snippet reference if provided
        if comment.code_snippet:
            body += f""\n\n```\n{comment.code_snippet}\n```""

        # Add suggestion if provided
        if comment.suggestion:
            body += f""\n\n```suggestion\n{comment.suggestion}\n```""

        return body",Format a comment body with code snippet and suggestion if provided.,Formats GitHub PR comment with optional code and suggestion snippets.,5.0,5.0,5.0,0.8405797101449275,0.7,0.5454545454545454,0.5850586891174316,0.357673,0.7028179,0.7921821,0.9725518226623536,0.4,0.658653354733308,5.0,5.0,5.0
40,40,40,41,_save_credentials,"def _save_credentials(self) -> None:
        
        if not self.credentials:
            logger.error(f""Attempted to save null credentials for user {self.user_morphik_id}."")
            return

        creds_path = self._get_user_credentials_path()
        try:
            with open(creds_path, ""w"") as creds_file:
                json.dump(self.credentials, creds_file)
            logger.info(f""Successfully saved Zotero credentials for user {self.user_morphik_id}"")
        except Exception as e:
            logger.error(f""Failed to save Zotero credentials for user {self.user_morphik_id}: {e}"")",Save credentials to file.,"Safely store user credentials to a file, logging success or failure.",4.0,5.0,4.0,0.3529411764705882,0.2727272727272727,0.75,0.6602627038955688,0.4854393,0.5905467,0.56222093,0.9464243054389954,0.4545454545454545,0.5201482964974512,5.0,5.0,5.0
41,41,41,42,extract_json_from_response,"def extract_json_from_response(content: str) -> dict | None:
    
    try:
        json_start = content.find(""```json"")
        if json_start != -1:
            json_text = content[json_start + 7 :]  # Skip past ```json
            json_end = json_text.find(""```"")
            if json_end != -1:
                json_text = json_text[:json_end].strip()
                return json.loads(json_text)
    except Exception as e:
        print(f""Error extracting JSON from response: {e}"")
    return None",Extracts JSON from markdown-formatted response.,Extract JSON data from a string response containing markdown code blocks.,5.0,5.0,5.0,0.6027397260273972,0.4545454545454545,0.6666666666666666,0.6307748556137085,0.49628696,0.82693005,0.6716777,0.9638209939002992,0.3636363636363636,0.4718762402668815,5.0,5.0,5.0
42,42,42,43,extract_target_uri_s3,"def extract_target_uri_s3(bucket, key, s3_client, head_bytes=1048576):
    
    target_uri = None
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key, Range=f""bytes=0-{head_bytes-1}"")
        stream = response[""Body""]
        for record in ArchiveIterator(stream):
            for name, value in record.rec_headers.headers:
                if name == ""WARC-Target-URI"":
                    target_uri = value
                    break
            if target_uri:
                break  # Only use the first valid response record
    except Exception as e:
    return target_uri","Retrieves the first head_bytes bytes (1 MB by default) from the S3 object using a range request, and extracts the first response record's target URI from the HTTP headers.",Extracts the target URI from an S3 object using WARC headers.,5.0,5.0,4.0,0.1539540211877631,0.8181818181818182,0.1935483870967742,0.625724196434021,0.18415633,0.63304275,0.8132727,0.9505800008773804,0.1818181818181818,0.6443321870255861,4.0,5.0,5.0
43,43,43,44,get_weather,"def get_weather(city: str) -> str:
    
    print(f""[debug] get_weather called with city: {city}"")
    choices = [""sunny"", ""cloudy"", ""rainy"", ""snowy""]
    return f""The weather in {city} is {random.choice(choices)}.""",Get the weather for a given city.,Simulate random weather conditions for a specified city.,5.0,5.0,5.0,0.5178571428571429,0.5,0.5714285714285714,0.5994858145713806,0.54462516,0.60782087,0.71986926,0.8641899228096008,0.375,0.5313542050779096,5.0,5.0,5.0
44,44,44,45,evaluate,"def evaluate(self, system_config, num_examples=1, output_dir=None):
        
        print(f""[SIM] Running {self.name} benchmark simulation..."")
        print(f""[SIM] System config: {system_config}"")

        # Return simulated results
        return {
            ""quality_score"": self.quality_score,
            ""speed_score"": self.speed_score,
            ""component_timing"": {
                ""search"": 0.5,
                ""processing"": 0.3,
                ""llm"": 1.2,
                ""total"": 2.0,
            },
            ""resource_usage"": {""memory_mb"": 500, ""cpu_percent"": 30},
        }",Simulate benchmark evaluation with predefined scores.,Simulate benchmark evaluation and return performance metrics.,5.0,5.0,4.0,0.7868852459016393,0.4285714285714285,0.5,0.5199834108352661,0.5422761,0.7343447,0.8357723,0.9233013391494752,0.4285714285714285,0.6339178101764806,5.0,5.0,5.0
45,45,45,46,define_log_level,"def define_log_level(print_level=""INFO"", logfile_level=""DEBUG"", name: str = None):
    
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime(""%Y%m%d%H%M%S"")
    log_name = (
        f""{name}_{formatted_date}"" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()
    _logger.add(sys.stderr, level=print_level)
    _logger.add(PROJECT_ROOT / f""logs/{log_name}.log"", level=logfile_level)
    return _logger",Adjust the log level to above level,Configure logging levels and file naming for application logs.,5.0,5.0,5.0,0.4032258064516129,0.2222222222222222,0.2857142857142857,0.5967375040054321,0.25886327,0.44154313,0.6525589,0.8795120716094971,0.0,0.5273947429107113,4.0,5.0,5.0
46,46,46,47,normalize_pandas,"def normalize_pandas(obj):
    
    if isinstance(obj, pd.Series):
        return obj.tolist()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict(""records"")
    elif isinstance(obj, dict):
        return {k: normalize_pandas(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [normalize_pandas(item) for item in obj]
    return obj",Convert pandas Series/DataFrames to primitive Python types,Convert pandas objects to native Python data structures,5.0,5.0,5.0,0.740315728172764,0.5,0.5,0.4890637993812561,0.58156604,0.8366064,0.8709719,0.645873486995697,0.0,0.5190215122442946,5.0,5.0,5.0
47,47,47,48,format_solution,"def format_solution(self, solution: str) -> str:
        
        logger.info(""Starting solution formatting phase..."")
        logger.info(f""Formatting solution of length {len(solution)} characters..."")
        
        chat_request = ChatRequest(
            message=solution,
            system_prompt="""",  # Will be set by strategy
            temperature=0.3  # Lower temperature for more consistent formatting
        )
        
        logger.info(""Calling chat service with SolutionFormatterStrategy..."")
        response = chat_service.chat(
            request=chat_request,
            strategy_chain=[BasePromptStrategy, SolutionFormatterStrategy],
            stream=False,
            json_response=False
        )
        
        formatted_solution = response.choices[0].message.content
        logger.info(f""Formatting completed. Result length: {len(formatted_solution)} characters"")
        logger.info(f""First 100 characters of formatted solution: {formatted_solution[:100]}..."")
        return formatted_solution",Format the final solution,Formats a solution string using a chat service with specific strategies for consistent output.,5.0,5.0,4.0,0.2553191489361702,0.1428571428571428,0.5,0.5966878533363342,0.23803811,0.5231451,0.53082263,0.8970614671707153,0.5714285714285714,0.6128141882169161,5.0,5.0,5.0
48,48,48,49,get_client_and_project,"def get_client_and_project():
        
        is_dev_mode = os.getenv(""isDevelopmentMode"", ""enabled"") == ""enabled""
        if is_dev_mode:
            return None, None

        project_id = os.environ.get(""GCP_PROJECT"")
        if not project_id:
            raise HTTPException(
                status_code=500, detail=""GCP_PROJECT environment variable is not set""
            )

        try:
            client = secretmanager.SecretManagerServiceClient()
            return client, project_id
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f""Failed to initialize Secret Manager client: {str(e)}"",
            )",Get Secret Manager client and project ID based on environment.,"Determine client and project ID, handling development mode and errors",5.0,5.0,4.0,0.782608695652174,0.4,0.4,0.5244566202163696,0.3806375,0.4620447,0.6434586,0.8851891160011292,0.1,0.4368005368283489,5.0,5.0,4.0
49,49,49,50,get_dataset_info,"def get_dataset_info(cls) -> Dict[str, str]:
        
        return {
            ""id"": ""custom"",  # Unique identifier for the dataset
            ""name"": ""Custom Dataset"",  # Human-readable name
            ""description"": ""Template for a custom benchmark dataset"",  # Description
            ""url"": cls.get_default_dataset_path(),  # Default URL or path
        }",Get basic information about the dataset.,"Provide metadata for a custom benchmark dataset including ID, name, description, and URL.",5.0,5.0,5.0,0.4269662921348314,0.0769230769230769,0.1666666666666666,0.7469145655632019,0.37699625,0.4970373,0.50728506,0.8969338536262512,0.6153846153846154,0.4957478595458968,5.0,5.0,5.0
50,50,50,51,as_data_dict,"def as_data_dict(self) -> features.BatchDict:
    
    output = {
        **self.msa.as_data_dict(),
        **self.templates.as_data_dict(),
        **self.token_features.as_data_dict(),
        **self.ref_structure.as_data_dict(),
        **self.predicted_structure_info.as_data_dict(),
        **self.polymer_ligand_bond_info.as_data_dict(),
        **self.ligand_ligand_bond_info.as_data_dict(),
        **self.pseudo_beta_info.as_data_dict(),
        **self.atom_cross_att.as_data_dict(),
        **self.convert_model_output.as_data_dict(),
        **self.frames.as_data_dict(),
    }
    return output",Converts batch object to dictionary.,Aggregate multiple data sources into a unified dictionary representation.,5.0,5.0,5.0,0.3972602739726027,0.1111111111111111,0.2,0.5137850046157837,0.33332402,0.5520625,0.5223012,0.8721462488174438,0.1111111111111111,0.3789562584272248,4.0,5.0,5.0
51,51,51,52,is_fastTrack_eligible,"def is_fastTrack_eligible(self):
        
        if (self.handler.context_url != ''):
            logger.debug(""Fast track not eligible: context_url present"")
            return False
        if (len(self.handler.prev_queries) > 0):
            logger.debug(f""Fast track not eligible: {len(self.handler.prev_queries)} previous queries present"")
            return False
        logger.info(""Query is eligible for fast track"")
        return True",Check if query is eligible for fast track processing,Determine fast track eligibility based on context URL and previous queries.,5.0,5.0,4.0,0.6,0.3636363636363636,0.2222222222222222,0.6178785562515259,0.3562358,0.6855313,0.7706809,0.9605579376220704,0.3636363636363636,0.6941360139531296,5.0,5.0,5.0
52,52,52,53,get_price_per_token,"def get_price_per_token(model: str) -> float:
		
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]",Get the price per token for a specified model.,Determine token price based on specified AI model type.,5.0,5.0,5.0,0.7636363636363637,0.4444444444444444,0.3333333333333333,0.5376671552658081,0.519967,0.7480092,0.70014954,0.9305668473243712,0.1111111111111111,0.5687237016045407,5.0,5.0,5.0
53,53,53,54,get_research_tools,"def get_research_tools(config: RunnableConfig):
    
    search_tool = get_search_tool(config)
    tool_list = [search_tool, Section]
    return tool_list, {tool.name: tool for tool in tool_list}",Get research tools based on configuration,Create and return a list and dictionary of research tools based on configuration.,5.0,5.0,4.0,0.4938271604938271,0.3846153846153846,0.8333333333333334,0.6480822563171387,0.5559436,0.7344494,0.7827232,0.9727948904037476,0.3076923076923077,0.6653694018003325,4.0,5.0,5.0
54,54,54,55,_generate_subqueries,"def _generate_subqueries(
        self, query: str, initial_results: List[Dict], current_knowledge: str
    ) -> List[str]:
        
        try:
            # Format context for question generation
            context = f

            # Generate sub-queries using the question generator
            return self.question_generator.generate_questions(
                query, context, int(get_db_setting(""search.questions_per_iteration""))
            )
        except Exception:
            logger.exception(""Error generating sub-queries"")
            return []",Generate sub-queries based on initial search results and the main query.,Generate sub-queries from a query using a question generator with error handling.,4.0,5.0,4.0,0.8024691358024691,0.3076923076923077,0.3333333333333333,0.6146490573883057,0.46510333,0.7687638,0.7062889,0.9436618685722352,0.6666666666666666,0.6256963693246164,4.0,5.0,4.0
55,55,55,56,load_server_config,"def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if not config_file:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig(
                        type=server_config[""type""],
                        url=server_config.get(""url""),
                        command=server_config.get(""command""),
                        args=server_config.get(""args"", []),
                    )
                return servers
        except Exception as e:
            raise ValueError(f""Failed to load MCP server config: {e}"")",Load MCP server configuration from JSON file,Load and parse MCP server configurations from a JSON file into a dictionary.,5.0,5.0,4.0,0.5789473684210527,0.5384615384615384,1.0,0.6111332774162292,0.72705215,0.8647013,0.93165386,0.8849640488624573,0.3846153846153846,0.6840858131150334,5.0,5.0,5.0
56,56,56,57,api_get_types,"def api_get_types():
    
    try:
        # Get all setting types
        types = [t.value for t in SettingType]
        return jsonify({""types"": types})
    except Exception as e:
        logger.exception(""Error getting types"")
        return jsonify({""error"": str(e)}), 500",Get all setting types,"Retrieve and return setting types as JSON, handling errors gracefully.",5.0,5.0,5.0,0.2857142857142857,0.2,0.5,0.6012499332427979,0.4319091,0.28960657,0.5209676,0.9120646119117736,0.4,0.6432262579581726,5.0,5.0,5.0
57,57,57,58,set_default_attn_processor,"def set_default_attn_processor(self):
        
        if all(
            proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnAddedKVProcessor()
        elif all(
            proc.__class__ in CROSS_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnProcessor()
        else:
            raise ValueError(
                f""Cannot call `set_default_attn_processor` when attention processors are of type {next(iter(self.attn_processors.values()))}""
            )

        self.set_attn_processor(processor, _remove_lora=True)",Disables custom attention processors and sets the default attention implementation.,Configure default attention processor based on current processor types.,4.0,5.0,3.0,0.6898707960666994,0.3333333333333333,0.2,0.548798680305481,0.30107215,0.5908129,0.76069266,0.9540863037109376,0.4444444444444444,0.6622053788840095,5.0,5.0,5.0
58,58,58,59,state,"def state(self, value: JobStateEnum) -> None:
        
        if self._state in _TERMINAL_STATES:
            logger.error(f""Attempt to change state from {self._state.name} to {value.name} denied."")
            raise ValueError(f""Cannot change state from {self._state.name} to {value.name}."")
        if value.value < self._state.value:
            logger.error(f""Invalid state transition attempt from {self._state.name} to {value.name}."")
            raise ValueError(f""State can only transition forward, from {self._state.name} to {value.name} not allowed."")
        self._state = value",Sets the current state of the job with transition constraints.,"Enforces valid state transitions for a job, logging errors on invalid attempts.",5.0,5.0,5.0,0.6329113924050633,0.25,0.2,0.6063122749328613,0.36740252,0.4964505,0.56244373,0.698340654373169,0.3333333333333333,0.5306255289461469,5.0,5.0,5.0
59,59,59,60,_get_hash,"def _get_hash(identifier: str) -> str:
        
        identifier = identifier.lower().strip()
        if isinstance(identifier, str):
            # Hash functions have to take bytes
            identifier = identifier.encode('utf-8')

        hash_value = sha256(identifier).hexdigest()
        return f""{hash_value}_{len(identifier)}""","If you want to hash identifier in your storage system, use this safer",Generate a unique hash for a given string identifier.,5.0,5.0,4.0,0.6138595812104514,0.2222222222222222,0.1538461538461538,0.6642206311225891,0.13931717,0.5401976,0.68317443,0.980426788330078,0.4444444444444444,0.6148611744684118,4.0,4.0,5.0
60,60,60,61,extract_question_only,"def extract_question_only(task: str) -> str:
    
    question_only = task.replace('\n## Question: \n\n', '')
    question_only = question_only.replace('\n\n\n## Instruction \n\nPlease answer this question by first reasoning and then providing your answer.\nPresent your reasoning and solution in the following json format. \nPlease show your final answer in the `answer` field, e.g.,`""answer"": ""42""`.\n\n```json\n{\n    ""reasoning"": ""___"",\n    ""answer"": ""___""\n}\n```\n', '')
    return question_only","We noticed that sometimes if the task includes specific formatting instructions, they may interfere with the reasoning flow.",Function removes instructional text to isolate the question from a task string.,4.0,5.0,5.0,0.5227726231179484,0.25,0.1111111111111111,0.6585320830345154,0.27463055,0.4090799,0.73163533,0.820459246635437,0.25,0.5477364953697197,5.0,5.0,5.0
61,61,61,62,is_valid_tool_call_item,"def is_valid_tool_call_item(call_item: dict) -> bool:
    
    if ""name"" not in call_item or not isinstance(call_item[""name""], str):
        return False

    if set(call_item.keys()) - {""name"", ""arguments""}:  # noqa: SIM103
        return False

    return True","Check that a dictionary item has at least 'name', optionally 'arguments' and no other keys to match a tool call JSON",Check if a dictionary represents a valid tool call item.,5.0,5.0,5.0,0.3180532225863994,0.7,0.2857142857142857,0.6221709847450256,0.07982356,0.6998186,0.8086007,0.9621331095695496,0.3,0.7055186977321775,4.0,5.0,5.0
62,62,62,63,update_dispatch_mode,"def update_dispatch_mode(dispatch_mode, dispatch_fn, collect_fn):
    
    _check_dispatch_mode(dispatch_mode)
    assert dispatch_mode in DISPATCH_MODE_FN_REGISTRY, f""dispatch_mode {dispatch_mode} not found""
    DISPATCH_MODE_FN_REGISTRY[dispatch_mode] = {""dispatch_fn"": dispatch_fn, ""collect_fn"": collect_fn}",Update the dispatch mode.,Update dispatch mode registry with new dispatch and collect functions,5.0,5.0,5.0,0.3478260869565218,0.3,0.75,0.5309322476387024,0.64987725,0.7176591,0.653303,0.9163486957550048,0.0,0.7015184183653889,5.0,5.0,5.0
63,63,63,64,_handle_content,"def _handle_content(self, content):
        
        if content.get(""type"") == ""text"":
            text = content.get(""text"", """")
            if text == ""<DONE>"":
                return
            logger.info(f""Assistant: {text}"")",Handle content updates from the assistant.,Process content to log text messages unless marked as done,5.0,5.0,4.0,0.5862068965517241,0.1,0.1666666666666666,0.5584776997566223,0.20790079,0.32868004,0.66514194,0.8917325735092163,0.1,0.4839497560025712,4.0,5.0,4.0
64,64,64,65,add_prompt,"def add_prompt(self, prompt: Prompt) -> Prompt:
        
        logger.debug(f""Adding prompt: {prompt.name}"")
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing
        self._prompts[prompt.name] = prompt
        return prompt",Add a prompt to the manager.,Add or log existing prompt in collection with optional warning,5.0,5.0,4.0,0.4032258064516129,0.2,0.3333333333333333,0.4705334305763244,0.36079496,0.36878997,0.6232124,0.8543099761009216,0.3,0.6152392969344594,5.0,5.0,4.0
65,65,65,66,_create_visualizations,"def _create_visualizations(self):
        
        if not PLOTTING_AVAILABLE:
            logger.warning(""Matplotlib not available, skipping visualization creation"")
            return

        if not self.study or len(self.study.trials) < 2:
            logger.warning(""Not enough trials to create visualizations"")
            return

        # Create directory for visualizations
        viz_dir = os.path.join(self.output_dir, ""visualizations"")
        os.makedirs(viz_dir, exist_ok=True)

        # Create Optuna visualizations
        self._create_optuna_visualizations(viz_dir)

        # Create custom visualizations
        self._create_custom_visualizations(viz_dir)

        logger.info(f""Visualizations saved to {viz_dir}"")",Create and save comprehensive visualizations of the optimization results.,Generate and save visualizations if conditions are met and dependencies are available.,4.0,5.0,5.0,0.7093023255813954,0.25,0.3333333333333333,0.6237382292747498,0.55977905,0.5042895,0.74800694,0.9693006277084352,0.3333333333333333,0.514492890523901,5.0,5.0,5.0
66,66,66,67,compute_similarity_scores,"def compute_similarity_scores(
    embedding_a: FloatArray, embedding_b: FloatArray
) -> Dict[str, float]:
    
    # Reshape for sklearn's cosine_similarity
    a_emb = embedding_a.reshape(1, -1)
    b_emb = embedding_b.reshape(1, -1)

    cosine_sim = float(cosine_similarity(a_emb, b_emb)[0, 0])

    # Could add other similarity metrics here
    return {
        ""cosine"": cosine_sim,
        # ""euclidean"": float(euclidean_similarity),
        # ""dot_product"": float(dot_product)
    }",Compute different similarity metrics between embeddings,Calculate cosine similarity between two vector embeddings.,5.0,5.0,4.0,0.7931034482758621,0.4285714285714285,0.5,0.5536909103393555,0.43573853,0.62430525,0.75632095,0.8902888894081116,0.1428571428571428,0.6139958626332525,4.0,5.0,5.0
67,67,67,68,delete_all_output_files,"def delete_all_output_files() -> bool:
    
    try:
        for filename in os.listdir(OUTPUTS_DIR):
            if any(filename.endswith(ext) for ext in AUDIO_FORMATS):
                file_path = os.path.join(OUTPUTS_DIR, filename)
                os.remove(file_path)
        return True
    except Exception as e:
        print(f""Error deleting output files: {e}"")
        return False",Delete all audio files from the outputs directory.,"Remove all audio files from the specified output directory, handling errors.",5.0,5.0,4.0,0.631578947368421,0.6363636363636364,0.875,0.6468192338943481,0.8979604,0.906772,0.9104203,0.9766420125961304,0.1818181818181818,0.6423313422426198,5.0,5.0,5.0
68,68,68,69,setup_logger,"def setup_logger(session_id, user_log_dir):
    
    logger = logging.getLogger(session_id)
    formatter = logging.Formatter(""%(message)s"")
    file_handler = logging.FileHandler(user_log_dir / f""{session_id}.jsonl"", mode=""w"")
    file_handler.setFormatter(formatter)
    logger.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    return logger",Creates a log file and logging object for the corresponding session ID,Initialize a session-specific logger with JSONL file output.,5.0,5.0,5.0,0.6066452361716067,0.3333333333333333,0.1666666666666666,0.5639523863792419,0.2708464,0.79211164,0.6866676,0.9615910649299622,0.25,0.6424066044340414,4.0,5.0,5.0
69,69,69,70,run,"def run() -> None:
    
    for p_file in glob.glob(f""agent/prompts/raw/*.py""):
        # import the file as a module
        base_name = os.path.basename(p_file).replace("".py"", """")
        module = importlib.import_module(f""agent.prompts.raw.{base_name}"")
        prompt = module.prompt
        # save the prompt as a json file
        os.makedirs(""agent/prompts/jsons"", exist_ok=True)
        with open(f""agent/prompts/jsons/{base_name}.json"", ""w+"") as f:
            json.dump(prompt, f, indent=2)
    print(f""Done convert python files to json"")",Convert all python files in agent/prompts to json files in agent/prompts/jsons,Convert Python prompt modules to JSON files in a directory.,5.0,5.0,4.0,0.5772823790030223,0.7,0.5,0.6207281351089478,0.123509936,0.73877454,0.87302434,0.9090123772621156,0.9,0.6225120530413616,5.0,5.0,5.0
70,70,70,71,text,"def text(markdown_str: str, size: float = 1.0, component_id: str | None = None, **kwargs) -> ComponentReturn:
    
    component = {
        ""type"": ""text"",
        ""id"": component_id,
        ""markdown"": markdown_str,
        ""value"": markdown_str,
        ""size"": size,
    }

    logger.info(f""[text] ID = {component_id}, content = {markdown_str}"")
    return ComponentReturn(markdown_str, component)",Create a text/markdown component.,Create a text component with markdown content and optional size and ID.,5.0,5.0,4.0,0.4507042253521127,0.4166666666666667,0.8,0.6258566975593567,0.8058347,0.79770553,0.34247473,0.9128740429878236,0.4166666666666667,0.6546795264264218,5.0,5.0,5.0
71,71,71,72,setUp,"def setUp(self):
        
        # Patch the environment variable
        self.env_patcher = patch.dict('os.environ', {'FIRECRAWL_API_KEY': 'fake-api-key'})
        self.env_patcher.start()
        
        # Create a mock for the FirecrawlApp
        self.firecrawl_app_patcher = patch('deepsearcher.loader.web_crawler.firecrawl_crawler.FirecrawlApp')
        self.mock_firecrawl_app = self.firecrawl_app_patcher.start()
        
        # Set up mock instances
        self.mock_app_instance = MagicMock()
        self.mock_firecrawl_app.return_value = self.mock_app_instance
        
        # Create the crawler
        self.crawler = FireCrawlCrawler()",Set up test fixtures.,Set up test environment by mocking environment variables and FirecrawlApp instance.,5.0,5.0,5.0,0.216867469879518,0.2727272727272727,0.75,0.6489642858505249,0.42507482,0.38422796,0.51547825,0.927176594734192,0.6363636363636364,0.688887150205233,5.0,5.0,5.0
72,72,72,73,_respect_rate_limit,"def _respect_rate_limit(self):
        
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if time_since_last_request < self.delay_between_requests:
            wait_time = self.delay_between_requests - time_since_last_request
            logger.info(f""Rate limiting: waiting {wait_time:.2f} seconds"")
            time.sleep(wait_time)

        self.last_request_time = time.time()",Apply self-imposed rate limiting between requests,Enforces API rate limits by delaying requests if needed.,5.0,5.0,4.0,0.7321428571428571,0.3333333333333333,0.4285714285714285,0.5441099405288696,0.38716158,0.630422,0.7807641,0.930011749267578,0.1111111111111111,0.5410700301732168,5.0,5.0,5.0
73,73,73,74,_cleanup_existing_alembic,"def _cleanup_existing_alembic(self) -> None:
        
        # logger.info(""Cleaning up existing Alembic configuration..."")

        # Remove entire alembic directory if it exists
        if self.alembic_dir.exists():
            import shutil

            shutil.rmtree(self.alembic_dir)
            logger.info(f""Removed alembic directory: {self.alembic_dir}"")

        # Remove alembic.ini if it exists
        if self.alembic_ini_path.exists():
            self.alembic_ini_path.unlink()
            logger.info(""Removed alembic.ini"")",Completely remove existing Alembic configuration including versions.,Remove existing Alembic configuration by deleting directory and configuration file.,5.0,5.0,5.0,0.7349397590361446,0.4,0.5714285714285714,0.6162247061729431,0.4484804,0.8367286,0.79651225,0.9615591764450072,0.4,0.7559770441231995,5.0,5.0,5.0
74,74,74,75,create_custom_calendar,"def create_custom_calendar(calendar_name, description=""""):
    
    service = get_calendar_service()
    
    calendar = {
        'summary': calendar_name,
        'description': description,
        'timeZone': TIMEZONE
    }

    created_calendar = service.calendars().insert(body=calendar).execute()
    return created_calendar",Creates a new custom calendar in Google Calendar.,Create a personalized calendar with specified name and description.,5.0,5.0,3.0,0.6417910447761194,0.3333333333333333,0.375,0.5706765055656433,0.28757542,0.589574,0.7463275,0.9731405973434448,0.3333333333333333,0.6153777318243809,5.0,5.0,5.0
75,75,75,76,_separate_create_config,"def _separate_create_config(self, config: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        
        create_config = {k: v for k, v in config.items() if k not in self.extra_kwargs}
        extra_kwargs = {k: v for k, v in config.items() if k in self.extra_kwargs}
        return create_config, extra_kwargs",Separate the config into create_config and extra_kwargs.,Split configuration into main settings and additional parameters.,5.0,5.0,3.0,0.6461538461538462,0.25,0.2222222222222222,0.5355520844459534,0.111544855,0.53011435,0.7383702,0.7733139395713806,0.0,0.4213962382056724,5.0,5.0,5.0
76,76,76,77,_format_server_info,"def _format_server_info(self, server_name: str) -> str:
        
        server_config = self.server_registry.get_server_config(server_name)
        server_str = f""Server Name: {server_name}""
        if not server_config:
            return server_str

        description = server_config.description
        if description:
            server_str = f""{server_str}\nDescription: {description}""

        return server_str",Format server information for display to planners,Format server details into a descriptive string output,5.0,5.0,3.0,0.7037037037037037,0.25,0.2857142857142857,0.4812843799591064,0.3756719,0.5069506,0.6942953,0.9443361759185792,0.125,0.566838130984344,4.0,5.0,5.0
77,77,77,78,format_question,"def format_question(category: str, question: str, answer: str) -> Dict[str, Any]:
    
    # Basic sanity checks
    if not question or not answer:
        raise ValueError(f""Empty question or answer in {category}"")
        
    return {
        ""id"": f""{category}_{random.getrandbits(32):08x}"",
        ""category"": category,
        ""question"": clean_text(question),
        ""answer"": clean_text(answer),
        ""metadata"": {
            ""source"": SOURCES[category][""name""],
            ""type"": category,
            ""difficulty"": ""challenging""  # All examples are chosen to be challenging
        }
    }",Format a question for the benchmark dataset,Generate a structured question-answer dictionary with metadata validation.,5.0,5.0,3.0,0.5,0.2222222222222222,0.2857142857142857,0.562790036201477,0.46760875,0.6522499,0.64117694,0.8598877191543579,0.125,0.5620309366051134,3.0,4.0,4.0
78,78,78,79,get_tool,"def get_tool(self, key: str) -> Tool:
        
        if key in self._tools:
            return self._tools[key]
        raise NotFoundError(f""Unknown tool: {key}"")",Get tool by key.,Retrieve a tool by key or raise an error if not found.,5.0,5.0,5.0,0.2777777777777778,0.25,0.75,0.6757820844650269,0.6802855,0.59029645,0.5047437,0.9144129157066344,0.4166666666666667,0.6309123801355253,5.0,5.0,5.0
79,79,79,80,stable_serialize,"def stable_serialize(obj: Any) -> Any:
    
    if isinstance(obj, dict):
        return {k: stable_serialize(v) for k, v in sorted(obj.items())}
    elif isinstance(obj, (list, tuple)):
        return [stable_serialize(x) for x in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:
        return str(obj)",Serialize object in a stable way for hashing.,"Recursively serialize data structures into a stable, sorted format.",5.0,5.0,4.0,0.5522388059701493,0.3333333333333333,0.375,0.5843435525894165,0.32168648,0.46333307,0.5738357,0.8064797520637512,0.2222222222222222,0.485009803035935,5.0,5.0,5.0
80,80,80,81,register_mcp_tools,"def register_mcp_tools(self):
        
        if self.mcp_client:
            for server_name in self.mcp_client.server_name_to_tools:
                for tool in self.mcp_client.server_name_to_tools[server_name]:
                    tool_name = f""mcp.{server_name}.{tool.name}""
                    self.registry.registry.actions[tool_name] = RegisteredAction(
                        name=tool_name,
                        description=tool.description,
                        function=tool,
                        param_model=create_tool_param_model(tool),
                    )
                    logger.info(f""Add mcp tool: {tool_name}"")
                logger.debug(
                    f""Registered {len(self.mcp_client.server_name_to_tools[server_name])} mcp tools for {server_name}"")
        else:
            logger.warning(f""MCP client not started."")",Register the MCP tools used by this controller.,Register and log MCP tools in the action registry if the MCP client is active.,4.0,5.0,4.0,0.5769230769230769,0.2666666666666666,0.375,0.6646566390991211,0.4047293,0.7506815,0.7934586,0.9081419706344604,0.4666666666666667,0.7297568630575801,5.0,5.0,5.0
81,81,81,82,_ms_to_ass_ts,"def _ms_to_ass_ts(ms: int) -> str:
        
        total_seconds, milliseconds = divmod(ms, 1000)
        minutes, seconds = divmod(total_seconds, 60)
        hours, minutes = divmod(minutes, 60)
        centiseconds = int(milliseconds / 10)
        return f""{int(hours):01}:{int(minutes):02}:{int(seconds):02}.{centiseconds:02}""",Convert milliseconds to ASS timestamp format (H:MM:SS.cc),"Convert milliseconds to formatted timestamp string in hours, minutes, seconds.",5.0,5.0,4.0,0.5641025641025641,0.5,0.4,0.6861414909362793,0.19935976,0.6404227,0.8508172,0.9446961283683776,0.4,0.4739626742387803,4.0,4.0,4.0
82,82,82,83,get_model_config,"def get_model_config(version: str):
    
    if version not in MODEL_CONFIGS:
        raise ValueError(f""Unsupported model version '{version}'. Supported versions are: {list(MODEL_CONFIGS.keys())}"")
    return MODEL_CONFIGS[version]",Retrieve the configuration for a given model version.,"Retrieve configuration for specified model version, raising error if unsupported.",5.0,5.0,5.0,0.6296296296296297,0.5,0.625,0.6966838836669922,0.6830785,0.6615194,0.8431364,0.9756333827972412,0.3,0.6630233496013811,5.0,5.0,5.0
83,83,83,84,get_cortex_search_service,"def get_cortex_search_service(cfg: RetrievalProviderConfig) -> Tuple[str,str,str]:
    
    if not cfg:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake configuration"")
    index_name = cfg.index_name
    if not index_name:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake Cortex Search Service, is SNOWFLAKE_CORTEX_SEARCH_SERVICE set?"")
    parts = index_name.split(""."")
    if len(parts) != 3:
        raise snowflake.ConfigurationError(f""Invalid SNOWFLAKE_CORTEX_SEARCH_SERVICE, expected format:<database>.<schema>.<service>, got {index_name}"")
    return (parts[0], parts[1], parts[2])","Retrieve the Cortex Search Service (database, schema, service) to use from the configuration, or raise an error.",Validate and parse configuration for Snowflake Cortex Search Service details.,5.0,5.0,4.0,0.5358164575469911,0.4,0.1764705882352941,0.5937928557395935,0.19352119,0.48306617,0.77883494,0.5896837711334229,0.4,0.5837131426175782,5.0,5.0,5.0
84,84,84,85,_save_faiss_index,"def _save_faiss_index(self):
        
        faiss.write_index(self._index, self._faiss_index_file)

        # Save metadata dict to JSON. Convert all keys to strings for JSON storage.
        # _id_to_meta is { int: { '__id__': doc_id, '__vector__': [float,...], ... } }
        # We'll keep the int -> dict, but JSON requires string keys.
        serializable_dict = {}
        for fid, meta in self._id_to_meta.items():
            serializable_dict[str(fid)] = meta

        with open(self._meta_file, ""w"", encoding=""utf-8"") as f:
            json.dump(serializable_dict, f)",Save the current Faiss index + metadata to disk so it can persist across runs.,"Persist FAISS index and metadata to files, converting keys for JSON compatibility.",5.0,5.0,4.0,0.7439024390243902,0.4166666666666667,0.2857142857142857,0.6284849047660828,0.35843244,0.55429554,0.7310251,0.8709020614624023,0.4166666666666667,0.6897546615188352,5.0,5.0,5.0
85,85,85,86,_jsonl_to_csv,"def _jsonl_to_csv(self, jsonl_file, csv_file):
        
        with open(jsonl_file, 'r', encoding='utf-8') as infile:
            data = [json.loads(line) for line in infile]
        
        if not data:
            print(""Empty JSONL file."")
            return
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as outfile:
            writer = csv.DictWriter(outfile, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        
        print(f""Converted {jsonl_file} to {csv_file}"")",Convert a JSONL file to a CSV file.,Convert JSONL data to CSV format and save to file,5.0,5.0,3.0,0.6530612244897959,0.5,0.625,0.5569971203804016,0.60814947,0.8024382,0.8365302,0.963988482952118,0.4,0.6843290007780463,5.0,5.0,5.0
86,86,86,87,recursive_mask_values,"def recursive_mask_values(obj, parent_key=None):
            
            if isinstance(obj, dict):
                return {k: recursive_mask_values(v, k) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [recursive_mask_values(item, parent_key) for item in obj]
            elif isinstance(obj, str):
                # List of keys that should NOT be masked
                excluded_keys = {
                    'start_time', 'end_time', 'name', 'id', 
                    'hash_id', 'parent_id', 'source_hash_id',
                    'cost', 'type', 'feedback', 'error', 'ctx','telemetry.sdk.version',
                    'telemetry.sdk.language','service.name'
                }
                # Apply masking only if the key is NOT in the excluded list
                if parent_key and parent_key.lower() not in excluded_keys:
                    return masking_func(obj)
                return obj
            else:
                return obj",Apply masking to all values in nested structure.,"Recursively mask string values in nested structures, excluding specified keys.",4.0,5.0,3.0,0.5512820512820513,0.5,0.625,0.5013484358787537,0.56021166,0.67335904,0.7493304,0.8719656467437744,0.2,0.5380163259630287,5.0,5.0,5.0
87,87,87,88,deprecated,"def deprecated(replacement: str = """"):
    
    import functools
    import warnings

    def decorator(func):
        qualified_name = _get_qualified_name(func)
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            msg = f""Warning: API '{qualified_name}' is deprecated.""
            if replacement:
                msg += f"" Please use '{replacement}' instead.""
            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        return wrapped
    return decorator",Decorator to mark APIs as deprecated.,"Decorator issues deprecation warnings for outdated functions, suggesting replacements.",5.0,5.0,5.0,0.3837209302325581,0.2222222222222222,0.3333333333333333,0.5735589265823364,0.4651366,0.48514912,0.5421926,0.9095858335494996,0.2222222222222222,0.6280773934996161,5.0,5.0,5.0
88,88,88,89,__init__,"def __init__(self):
        
        super().__init__()

        # Client connections (virtual)
        self.websocket_connections = {}

        # In browser, set dummy branding manager
        self.branding_manager = type(
            ""DummyBrandingManager"",
            (),
            {
                ""static_dir"": """",
                ""get_branding_config"": lambda *args: {
                    ""name"": ""Preswald"",
                    ""favicon"": ""/favicon.ico"",
                },
            },
        )()

        # Register with JavaScript
        self._register_js_handlers()",Initialize the service with browser-compatible components,Initialize a virtual client manager with dummy branding and JavaScript handlers.,4.0,5.0,5.0,0.5375,0.1818181818181818,0.2857142857142857,0.6397623419761658,0.49790242,0.4265189,0.6205429,0.9482810497283936,0.6363636363636364,0.571380435198905,5.0,5.0,5.0
89,89,89,90,_format_timestamp,"def _format_timestamp(self, timestamp: str) -> str:
        
        if len(timestamp) < 14:
            return timestamp

        try:
            year = timestamp[0:4]
            month = timestamp[4:6]
            day = timestamp[6:8]
            hour = timestamp[8:10]
            minute = timestamp[10:12]
            second = timestamp[12:14]
            return f""{year}-{month}-{day} {hour}:{minute}:{second}""
        except Exception:
            return timestamp",Format Wayback Machine timestamp into readable date,Convert a string timestamp into a formatted datetime string if valid,5.0,5.0,5.0,0.5735294117647058,0.2727272727272727,0.2857142857142857,0.5007442235946655,0.3045719,0.65847194,0.7131714,0.9511062502861024,0.3636363636363636,0.5883471867825532,5.0,5.0,4.0
90,90,90,91,get_skill_states,"def get_skill_states(skill_category: str) -> Set[str]:
    
    if skill_category in _skill_states_cache:
        return _skill_states_cache[skill_category]

    try:
        # Import the skill category module
        skill_module = importlib.import_module(f""skills.{skill_category}"")

        # Look for the SkillStates TypedDict class
        if hasattr(skill_module, ""SkillStates""):
            skill_states_class = getattr(skill_module, ""SkillStates"")
            # Get the annotations which contain the state names
            if hasattr(skill_states_class, ""__annotations__""):
                states = set(skill_states_class.__annotations__.keys())
                _skill_states_cache[skill_category] = states
                return states

        logger.warning(f""Could not find SkillStates for {skill_category}"")
        return set()

    except ImportError as e:
        logger.warning(f""Could not import skill category {skill_category}: {e}"")
        return set()",Get the actual skill states for a given skill category by importing its module.,Retrieve and cache skill states from a module based on a given category.,5.0,5.0,3.0,0.7687307682653286,0.4615384615384615,0.3571428571428571,0.5503517389297485,0.42723006,0.7048328,0.7321508,0.929148256778717,0.5384615384615384,0.663288430055073,5.0,5.0,5.0
91,91,91,92,_progress_callback,"def _progress_callback(self, message: str, progress: int, metadata: dict) -> None:
        
        logger.info(f""Progress: {progress}% - {message}"")
        if hasattr(self, ""progress_callback""):
            self.progress_callback(message, progress, metadata)",Handle progress updates from the strategy.,Logs progress updates and invokes a custom callback if available.,5.0,5.0,5.0,0.5230769230769231,0.2,0.3333333333333333,0.6031084060668945,0.33781385,0.41130927,0.6389815,0.9597938060760498,0.3,0.5079193943531103,5.0,5.0,5.0
92,92,92,93,custom_get_evaluation_llm,"def custom_get_evaluation_llm(custom_config=None):
            
            if custom_config is None:
                custom_config = evaluation_config

            print(f""Getting evaluation LLM with config: {custom_config}"")
            return get_llm(**custom_config)",Override that uses the local get_llm with database access.,Retrieve and configure a language model for evaluation using specified or default settings.,4.0,5.0,4.0,0.5274725274725275,0.0769230769230769,0.1,0.6250969767570496,0.14593464,0.18711802,0.6683308,0.5353959202766418,0.1538461538461538,0.403840751561113,5.0,5.0,5.0
93,93,93,94,to_dict,"def to_dict(self):
        
        result = {
            ""id"": self.id,
            ""name"": self.name,
            ""type"": self.type,
            ""path"": self.path,
            ""created_at"": self.created_at.isoformat() if self.created_at else None,
            ""meta_data"": self.meta_data,
        }
        if self.document_id:
            result[""document_id""] = self.document_id
        return result","Convert to dictionary, including document_id",Convert object attributes to a dictionary representation with optional fields.,5.0,5.0,5.0,0.4615384615384615,0.3,0.5,0.503998339176178,0.31442347,0.54296505,0.5726886,0.8746833801269531,0.1,0.4334451981770331,5.0,5.0,5.0
94,94,94,95,mcp_tool,"def mcp_tool(
    name: str | None = None,
    description: str | None = None,
    tags: set[str] | None = None,
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        call_args = {
            ""name"": name or func.__name__,
            ""description"": description,
            ""tags"": tags,
        }
        call_args = {k: v for k, v in call_args.items() if v is not None}
        setattr(func, _MCP_REGISTRATION_TOOL_ATTR, call_args)
        return func

    return decorator",Decorator to mark a method as an MCP tool for later registration.,Decorator function to register metadata attributes to another function.,5.0,5.0,5.0,0.7323943661971831,0.2222222222222222,0.1666666666666666,0.5183284282684326,0.38158265,0.59458315,0.7249837,0.7823668718338013,0.1111111111111111,0.3348833452077395,5.0,5.0,5.0
95,95,95,96,trace_id_to_uuid,"def trace_id_to_uuid(trace_id: str) -> str:
    
    trace_id = str(trace.format_trace_id(trace_id))
    if len(trace_id) != 32:
        raise ValueError(""Trace ID must be a 32-character hexadecimal string"")
    return f""{trace_id[:8]}-{trace_id[8:12]}-{trace_id[12:16]}-{trace_id[16:20]}-{trace_id[20:]}""",Convert a 32-character OpenTelemetry trace ID to a UUID-like format.,Convert a 32-character trace ID into a UUID format.,5.0,5.0,5.0,0.7165313105737893,0.9,0.75,0.6791566610336304,0.696277,0.9417016,0.9648318,0.9265343546867372,0.4444444444444444,0.7227866488689232,5.0,5.0,5.0
96,96,96,97,_get_endpoint_config,"def _get_endpoint_config(self):
        
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is an Azure AI Search endpoint
        if endpoint_config.db_type != ""azure_ai_search"":
            error_msg = f""Endpoint {self.endpoint_name} is not an Azure AI Search endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config",Get the Azure Search endpoint configuration from CONFIG,Validate and retrieve Azure AI Search endpoint configuration,5.0,5.0,4.0,0.75,0.5,0.5,0.508787989616394,0.54038507,0.8494432,0.8461905,0.882710874080658,0.75,0.7160080282629026,5.0,5.0,5.0
97,97,97,98,save_results,"def save_results(results, output_dir):
    
    output_path = Path(output_dir) / ""rich_autoscan_results.json""

    # Convert results to serializable format
    serializable_results = []
    for result in results:
        if result is None:
            continue
        serializable_results.append(result)

    with open(output_path, ""w"") as f:
        json.dump(serializable_results, f, indent=2, default=lambda o: o.value if isinstance(o, Enum) else o)

    print(f""Results saved to {output_path}"")",Save the full results to a JSON file for analysis.,Serialize and save processed results to a JSON file in the specified directory.,5.0,5.0,4.0,0.5822784810126582,0.5384615384615384,0.6,0.65278160572052,0.48755103,0.8583209,0.86804307,0.8183108568191528,0.3846153846153846,0.5826009579448094,5.0,5.0,5.0
98,98,98,99,mock_client,"def mock_client():
    
    client = AsyncMock(spec=httpx.AsyncClient)
    # Set up a mock response
    mock_response = MagicMock()
    mock_response.json.return_value = {""result"": ""success""}
    mock_response.raise_for_status.return_value = None
    client.request.return_value = mock_response
    return client",Create a mock httpx.AsyncClient.,Create a mock asynchronous HTTP client with predefined response behavior.,5.0,5.0,5.0,0.3698630136986301,0.3,0.6,0.619011640548706,0.627565,0.55994904,0.6251376,0.9279735684394836,0.4,0.6817763401282759,5.0,5.0,5.0
99,99,99,100,log_workflow_step,"def log_workflow_step(
    logger, conversation_id: str, step: str, details: Optional[Dict[str, Any]] = None
):
    
    log_data = {
        ""conversation_id"": conversation_id,
        ""workflow_step"": step,
        **(details or {}),
    }
    logger.debug(f""Workflow step: {step}"", data=log_data)",Log workflow execution steps for debugging,Log a workflow step with details using a debug logger.,5.0,5.0,4.0,0.6851851851851852,0.4,0.6666666666666666,0.5435489416122437,0.45564508,0.8027861,0.85159564,0.9764623641967772,0.5,0.6414198192364479,5.0,5.0,5.0
100,100,100,101,generate_app_embedding,"def generate_app_embedding(
    app: AppEmbeddingFields,
    openai_client: OpenAI,
    embedding_model: str,
    embedding_dimension: int,
) -> list[float]:
    
    logger.debug(f""Generating embedding for app: {app.name}..."")
    # generate app embeddings based on app config's name, display_name, provider, description, categories
    text_for_embedding = app.model_dump_json()
    logger.debug(f""Text for app embedding: {text_for_embedding}"")
    return generate_embedding(
        openai_client, embedding_model, embedding_dimension, text_for_embedding
    )",Generate embedding for app.,Generate app-specific embeddings using OpenAI model based on app metadata.,5.0,5.0,5.0,0.3513513513513513,0.2727272727272727,0.75,0.6248908042907715,0.5184131,0.7800825,0.5997329,0.9732546210289,0.6,0.6947997362831135,5.0,5.0,5.0
101,101,101,102,get_os_name,"def get_os_name() -> Union[str, None]:
    
    #
    os_name = platform.system()
    return {
        'Linux': 'linux',
        'Darwin': 'macos',
        'Windows': 'windows',
        # For the future? because why not
        'iOS': 'ios',
    }.get(os_name)",Get the current OS name in the same format needed for browserforge :return: Current OS name or `None` otherwise,Determine operating system name and return standardized identifier.,5.0,5.0,5.0,0.4179368264653403,0.25,0.1052631578947368,0.5349263548851013,0.030113263,0.5263712,0.6595871,0.9634859561920166,0.125,0.5209162543810059,5.0,5.0,5.0
102,102,102,103,__exit__,"def __exit__(self, exc_type, exc_value, exc_tb):
        
        if os.environ.get('HY3DGEN_DEBUG', '0') == '1':
            self.end.record()
            torch.cuda.synchronize()
            self.time = self.start.elapsed_time(self.end)
            if self.name is not None:
                logger.info(f'{self.name} takes {self.time} ms')",Context manager exit: stop timing and log results.,Log execution time if debugging is enabled in environment settings.,5.0,5.0,5.0,0.6119402985074627,0.2,0.125,0.5895932912826538,0.23311462,0.5087478,0.62898195,0.8680874109268188,0.3,0.4115389465344617,4.0,5.0,5.0
103,103,103,104,create_turn_dir,"def create_turn_dir(self) -> None:
        
        if not self.run_dir:
            logger.warning(""Cannot create turn directory: run_dir not set"")
            return

        # Increment turn counter
        self.turn_count += 1

        # Create turn directory with padded number
        turn_name = f""turn_{self.turn_count:03d}""
        self.current_turn_dir = os.path.join(self.run_dir, turn_name)
        os.makedirs(self.current_turn_dir, exist_ok=True)
        logger.info(f""Created turn directory: {self.current_turn_dir}"")",Create a new directory for the current turn.,"Create a new directory for each turn in a sequence, logging the process.",5.0,5.0,5.0,0.5833333333333334,0.5384615384615384,0.75,0.656099796295166,0.70385987,0.708372,0.8639486,0.6924443244934082,0.4615384615384615,0.6567114084684216,5.0,5.0,5.0
104,104,104,105,copy_and_update,"def copy_and_update(self, *args, **kwargs):
        
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if len(kwargs) > 0:
            new_info.update(kwargs)

        return StreamInfo(**new_info)",Copy the StreamInfo object and update it with the given StreamInfo instance and/or other keyword arguments.,Create a new StreamInfo by merging attributes from instances and keyword arguments,5.0,5.0,5.0,0.6832708383601913,0.4166666666666667,0.2941176470588235,0.571214497089386,0.29416,0.7539537,0.7691348,0.7249249219894409,0.1666666666666666,0.5241895457484741,5.0,5.0,5.0
105,105,105,106,write_output,"def write_output(
    inference_result: model.InferenceResult,
    output_dir: os.PathLike[str] | str,
    terms_of_use: str | None = None,
    name: str | None = None,
) -> None:
  
  processed_result = post_process_inference_result(inference_result)

  prefix = f'{name}_' if name is not None else ''

  with open(os.path.join(output_dir, f'{prefix}model.cif'), 'wb') as f:
    f.write(processed_result.cif)

  with open(
      os.path.join(output_dir, f'{prefix}summary_confidences.json'), 'wb'
  ) as f:
    f.write(processed_result.structure_confidence_summary_json)

  with open(os.path.join(output_dir, f'{prefix}confidences.json'), 'wb') as f:
    f.write(processed_result.structure_full_data_json)

  if terms_of_use is not None:
    with open(os.path.join(output_dir, 'TERMS_OF_USE.md'), 'wt') as f:
      f.write(terms_of_use)",Writes processed inference result to a directory.,Save processed model results and optional terms of use to specified directory.,4.0,5.0,4.0,0.6025641025641025,0.3333333333333333,0.5714285714285714,0.5507960915565491,0.45730194,0.64804363,0.6743275,0.558897852897644,0.0833333333333333,0.4633952582510494,5.0,5.0,5.0
106,106,106,107,xpath_attr_functional_pseudo_element,"def xpath_attr_functional_pseudo_element(
            xpath: OriginalXPathExpr, function: FunctionalPseudoElement
    ) -> XPathExpr:
        
        if function.argument_types() not in ([""STRING""], [""IDENT""]):
            raise ExpressionError(
                f""Expected a single string or ident for ::attr(), got {function.arguments!r}""
            )
        return XPathExpr.from_xpath(xpath, attribute=function.arguments[0].value)",Support selecting attribute values using ::attr() pseudo-element,Convert functional pseudo-element to XPath expression with attribute handling.,5.0,5.0,4.0,0.6923076923076923,0.3,0.25,0.5883283019065857,0.35811585,0.53120685,0.68010426,0.946877121925354,0.1111111111111111,0.6804367929887238,4.0,5.0,4.0
107,107,107,108,get_torch_device,"def get_torch_device() -> any:
    
    device_name = get_device_name()
    try:
        return getattr(torch, device_name)
    except AttributeError:
        logger.warning(f""Device namespace '{device_name}' not found in torch, try to load torch.cuda."")
        return torch.cuda",Return the corresponding torch attribute based on the device type string.,"Determine and return the appropriate PyTorch device, defaulting to CUDA if unavailable.",5.0,5.0,3.0,0.7011494252873564,0.25,0.2727272727272727,0.6993991136550903,0.3042429,0.4061062,0.5912478,0.986851930618286,0.25,0.5586071798000839,4.0,5.0,5.0
108,108,108,109,initialize_cache,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )",Initialize the KV cache in all workers.,"Initialize cache with specified GPU and CPU blocks, logging memory usage and configuration.",5.0,5.0,4.0,0.4065934065934066,0.1538461538461538,0.2857142857142857,0.5625091791152954,0.37777162,0.40860155,0.7271948,0.9657549858093262,0.3846153846153846,0.6694623800707158,5.0,5.0,5.0
109,109,109,110,__str__,"def __str__(self) -> str:
        
        info = """"
        info += ""Filter Task:\n""
        info += f""  content_type: {self._content_type}\n""
        info += f""  min_size: {self._min_size}\n""
        info += f""  max_aspect_ratio: {self._max_aspect_ratio}\n""
        info += f""  min_aspect_ratio: {self._min_aspect_ratio}\n""
        info += f""  filter: {self._filter}\n""
        return info",Returns a string with the object's config and run time state,Generate a string representation of filter task attributes for display.,5.0,5.0,5.0,0.6619718309859155,0.2,0.1666666666666666,0.5584713220596313,0.32903582,0.5935398,0.6631979,0.6720615029335022,0.2,0.5972001064929123,5.0,5.0,5.0
110,110,110,111,to_dict,"def to_dict(self) -> Dict[str, Any]:
    
    return {
      ""response"": self.response,
      ""context"": {
        ""entities"": [(e.to_dict(e, include_fields=e.F_TO_CONTEXT), float(s)) for e, s in self.context.entities],
        ""relations"": [(r.to_dict(r, include_fields=r.F_TO_CONTEXT), float(s)) for r, s in self.context.relations],
        ""chunks"": [(c.to_dict(c, include_fields=c.F_TO_CONTEXT), float(s)) for c, s in self.context.chunks],
      },
    }",Convert the query response to a dictionary.,Convert object attributes into a structured dictionary format for serialization.,5.0,5.0,5.0,0.4875,0.3,0.4285714285714285,0.5803871154785156,0.49559543,0.6657618,0.71727043,0.9651486873626708,0.2,0.3777610275067966,5.0,5.0,5.0
111,111,111,112,extract_urls_from_jsonl,"def extract_urls_from_jsonl(file_path):
    
    urls = set()
    url_to_data = {}
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if ""url"" in data and data[""url""]:
                    url = data[""url""]
                    urls.add(url)
                    # Store minimal context for each URL
                    url_to_data[url] = {""id"": data.get(""id"", """"), ""type"": data.get(""type"", """"), ""page"": data.get(""page"", """")}
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON from line in {file_path}"")
                continue
    return urls, url_to_data",Extract URLs from a JSONL file.,Extract unique URLs and associated metadata from a JSONL file,5.0,5.0,4.0,0.4918032786885246,0.6,1.0,0.5610396265983582,0.8092725,0.803039,0.7522142,0.9813823699951172,0.5,0.5336415761272352,5.0,5.0,5.0
112,112,112,113,get_status_html,"def get_status_html(is_available: bool) -> str:
    
    color = ""green"" if is_available else ""red""
    status = ""Available"" if is_available else ""Unavailable""
    return f",Generate HTML for status indicator.,Generate HTML status message with color based on availability,5.0,5.0,5.0,0.5245901639344263,0.3333333333333333,0.6,0.5357668399810791,0.66768837,0.69395924,0.79054856,0.8196170330047607,0.2222222222222222,0.6855696626662574,2.0,4.0,4.0
113,113,113,114,get_port,"def get_port():
    
    if 'PORT' in os.environ:
        port = int(os.environ['PORT'])
        print(f""Using PORT from environment variable: {port}"")
        return port
    elif 'WEBSITE_SITE_NAME' in os.environ:
        # Running in Azure App Service
        print(""Running in Azure App Service, using default port 8000"")
        return 8000  # Azure will redirect requests to this port
    else:
        # Use configured port
        print(f""Using configured port {CONFIG.port}"")
        return CONFIG.port","Get the port to listen on, using config or environment.",Determine the appropriate server port based on environment variables or configuration.,5.0,5.0,5.0,0.6046511627906976,0.4545454545454545,0.4,0.6534931659698486,0.4563582,0.7006399,0.7294375,0.9748663306236268,0.3636363636363636,0.5518951457166854,5.0,5.0,5.0
114,114,114,115,parse_rules_file,"def parse_rules_file(file_path):
    
    pdf_rules = defaultdict(list)

    with open(file_path, ""r"") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rule = json.loads(line)
                if ""pdf"" in rule:
                    pdf_rules[rule[""pdf""]].append(rule)
            except json.JSONDecodeError:
                print(f""Warning: Could not parse line as JSON: {line}"")

    return pdf_rules",Parse the rules file and organize rules by PDF.,Parse a file to extract and organize PDF-related rules into a dictionary.,5.0,5.0,4.0,0.589041095890411,0.4615384615384615,0.5555555555555556,0.5692266225814819,0.63174886,0.81696737,0.8553833,0.9814173579216005,0.3333333333333333,0.6836451842253679,5.0,5.0,5.0
115,115,115,116,_download_file,"def _download_file(self, url: str):
        
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            file_name = url.split(""/"")[-1]

            file_path = os.path.join(self.cache_dir, file_name)

            with open(file_path, ""wb"") as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)

            return file_path

        except requests.exceptions.RequestException as e:
            print(f""Error downloading the file: {e}"")",Download a file from a URL and save it to the cache directory.,Download and save a file from a URL to a specified cache directory.,5.0,5.0,4.0,0.8805970149253731,0.8461538461538461,0.6923076923076923,0.6002515554428101,0.7769004,0.9503407,0.93344367,0.9535748958587646,0.3846153846153846,0.5895143577878708,5.0,5.0,5.0
116,116,116,117,get_predefined_execute_fn,"def get_predefined_execute_fn(execute_mode: Execute):
    
    predefined_execute_mode_fn = {
        Execute.ALL: {""execute_fn_name"": ""execute_all""},
        Execute.RANK_ZERO: {""execute_fn_name"": ""execute_rank_zero""},
    }
    return predefined_execute_mode_fn[execute_mode]",Note that here we only asks execute_all and execute_rank_zero to be implemented,Map execution modes to corresponding function names for execution.,4.0,5.0,3.0,0.5723623309045445,0.3333333333333333,0.1333333333333333,0.5361078977584839,-0.071412705,0.5845408,0.5594679,0.8424433469772339,0.0,0.5049845764049434,5.0,5.0,5.0
117,117,117,118,__init__,"def __init__(self):
            
            self.domain = ""mock-domain.auth0.com""
            self.algorithms = [""RS256""]
            self.jwks = {""keys"": []}
            self.auth0_user_model = Auth0User",Initialize the mock Auth0 instance.,"Initialize authentication settings with domain, algorithms, and user model.",5.0,5.0,5.0,0.4133333333333333,0.1111111111111111,0.2,0.52587890625,0.3335458,0.6256562,0.54163134,0.9756479263305664,0.0,0.6072813092215541,4.0,5.0,5.0
118,118,118,119,recreate_collection,"def recreate_collection(collection_name, vector_size):
    
    if client.collection_exists(collection_name):
        print(f""Dropping existing collection '{collection_name}'"")
        client.delete_collection(collection_name)

    print(f""Creating collection '{collection_name}' with vector size {vector_size}"")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
    )",Recreate a collection in Qdrant,Recreate a vector collection by deleting and then creating it anew.,5.0,5.0,5.0,0.4477611940298507,0.2727272727272727,0.6,0.546495795249939,0.35757712,0.55548215,0.5768194,0.8134258389472961,0.3636363636363636,0.6958458428032451,5.0,5.0,5.0
119,119,119,120,to_dict,"def to_dict(self):
        
        return {
            ""retrieved_task"": self.retrieved_task,
            ""sub_question"": [item.to_dict() for item in self.sub_retrieved_set],
            ""graph_data"": (
                [str(spo) for spo in self.graph_data.get_all_spo()]
                if self.graph_data
                else []
            ),
            ""chunk_datas"": [item.to_dict() for item in self.chunk_datas],
            ""summary"": self.summary,
        }",Convert response to dictionary format,Convert object attributes to a structured dictionary format.,5.0,5.0,5.0,0.5833333333333334,0.5,0.8,0.5349759459495544,0.36607254,0.6803988,0.8067733,0.8335951566696167,0.125,0.4224690824918821,5.0,5.0,5.0
120,120,120,121,get_random_fact,"def get_random_fact() -> str:
    

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()

        return f""Random Fact: {data['text']}""

    except requests.exceptions.RequestException as e:
        return f""Error fetching random fact: {str(e)}""","Fetches a random fact from the ""uselessfacts.jsph.pl"" API.",Fetches and returns a random fact or error message from an API.,5.0,5.0,5.0,0.6666666666666666,0.5,0.6,0.6200567483901978,0.35128754,0.67156476,0.6857387,0.5690366625785828,0.4166666666666667,0.633917705586844,5.0,5.0,5.0
121,121,121,122,_fetch_max_batch_size,"def _fetch_max_batch_size(self, model_name, model_version: str = """") -> int:
        
        if model_name in self._max_batch_sizes:
            return self._max_batch_sizes[model_name]

        with self._lock:
            # Double check, just in case another thread set the value while we were waiting
            if model_name in self._max_batch_sizes:
                return self._max_batch_sizes[model_name]

            if not self._grpc_endpoint:
                self._max_batch_sizes[model_name] = 1
                return 1

            try:
                client = self.client if self.client else grpcclient.InferenceServerClient(url=self._grpc_endpoint)
                model_config = client.get_model_config(model_name=model_name, model_version=model_version)
                self._max_batch_sizes[model_name] = model_config.config.max_batch_size
                logger.debug(f""Max batch size for model '{model_name}': {self._max_batch_sizes[model_name]}"")
            except Exception as e:
                self._max_batch_sizes[model_name] = 1
                logger.warning(f""Failed to retrieve max batch size: {e}, defaulting to 1"")

            return self._max_batch_sizes[model_name]",Fetch the maximum batch size from the Triton model configuration in a thread-safe manner.,Retrieve and cache the maximum batch size for a given model using gRPC client,5.0,5.0,5.0,0.7334507439951521,0.4285714285714285,0.3333333333333333,0.566831111907959,0.43798786,0.39507365,0.8490704,0.8667746186256409,0.5714285714285714,0.7079063704507712,4.0,5.0,4.0
122,122,122,123,create_text_response,"def create_text_response(text, stop_reason=""end_turn"", usage=None):
        
        return {
            ""output"": {
                ""message"": {
                    ""role"": ""assistant"",
                    ""content"": [{""text"": text}],
                },
            },
            ""stopReason"": stop_reason,
            ""usage"": usage
            or {
                ""inputTokens"": 150,
                ""outputTokens"": 100,
                ""totalTokens"": 250,
            },
        }",Creates a text response for testing.,"Generate a structured response object with text, stop reason, and token usage.",5.0,5.0,5.0,0.4358974358974359,0.25,0.3333333333333333,0.6154306530952454,0.33593097,0.5940799,0.68150926,0.8714321851730347,0.1666666666666666,0.6228598118842894,5.0,5.0,5.0
123,123,123,124,from_dict,"def from_dict(cls, data: dict) -> ""L1GenerationResult"":
        
        return cls(
            bio=data.get(""bio""),
            clusters=data.get(""clusters"", {""clusterList"": []}),
            chunk_topics=data.get(""chunk_topics"", {}),
            generate_time=datetime.fromisoformat(data[""generate_time""])
            if ""generate_time"" in data
            else datetime.now(),
        )",Create instance from dictionary,Convert dictionary data into an L1GenerationResult object instance.,5.0,5.0,5.0,0.4328358208955223,0.25,0.25,0.6070578098297119,0.3985119,0.6546912,0.7392017,0.774622917175293,0.25,0.6420349167439648,5.0,5.0,5.0
124,124,124,125,extract_json_from_string,"def extract_json_from_string(s: str) -> Optional[Any]:
    
    # Regex to find JSON objects (greedy, matches first { to last })
    match = re.search(r""\{.*\}"", s, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None","Searches for a JSON object within the string and returns the loaded JSON if found, otherwise returns None.",Extracts and parses JSON object from a given string using regex.,5.0,5.0,4.0,0.4458378767338111,0.4545454545454545,0.1666666666666666,0.65976881980896,0.18696526,0.6082343,0.8190204,0.9818214178085328,0.3636363636363636,0.5882923144834427,5.0,5.0,5.0
125,125,125,126,create_gpt_vector_store,"def create_gpt_vector_store(client: ""OpenAI"", name: str, fild_ids: list[str]) -> Any:
    
    try:
        vector_store = client.vector_stores.create(name=name)
    except Exception as e:
        raise AttributeError(f""Failed to create vector store, please install the latest OpenAI python package: {e}"")

    # poll the status of the file batch for completion.
    batch = client.vector_stores.file_batches.create_and_poll(vector_store_id=vector_store.id, file_ids=fild_ids)

    if batch.status == ""in_progress"":
        time.sleep(1)
        logging.debug(f""file batch status: {batch.file_counts}"")
        batch = client.vector_stores.file_batches.poll(vector_store_id=vector_store.id, batch_id=batch.id)

    if batch.status == ""completed"":
        return vector_store

    raise ValueError(f""Failed to upload files to vector store {vector_store.id}:{batch.status}"")",Create a openai vector store for gpt assistant,Create and monitor a vector store for file processing using OpenAI client,5.0,5.0,3.0,0.589041095890411,0.5,0.625,0.551328718662262,0.3008065,0.6101993,0.7978921,0.7559491395950317,0.5833333333333334,0.6824731796194277,4.0,5.0,4.0
126,126,126,127,get_config_for_model,"def get_config_for_model(model_string: str) -> Dict[str, Any]:
    
    if model_string in MODEL_CONFIG_MAP:
        return MODEL_CONFIG_MAP[model_string]
    # If model not found, use default configuration based on provider
    provider, _ = parse_model_string(model_string)
    return {
        ""provider"": provider,
        ""default_params"": {""temperature"": 0.3},
    }","Get configuration for a specific model, with fallback to defaults.",Retrieve model configuration or default settings based on model identifier.,5.0,5.0,5.0,0.6933333333333334,0.3,0.2,0.5754616260528564,0.29855362,0.66188025,0.7195423,0.8226314783096313,0.7,0.5432817165277863,5.0,5.0,5.0
127,127,127,128,_validate_resolution_format,"def _validate_resolution_format(resolution: str):
    
    pattern = r""^\d+x\d+$""  # Matches a pattern of digits, ""x"", and digits
    matched_resolution = re.match(pattern, resolution)
    if matched_resolution is None:
        raise ValueError(f""Invalid resolution format: {resolution}"")","Checks if a string is in a valid resolution format (e.g., ""1024x768"").","Validate if a resolution string matches the ""widthxheight"" format.",5.0,5.0,5.0,0.7130257122157823,0.6666666666666666,0.3076923076923077,0.725395917892456,0.3692769,0.6980825,0.78420806,0.7842662930488586,0.5555555555555556,0.578389822713938,5.0,5.0,5.0
128,128,128,129,create_item,"def create_item(
        name: str, value: int, state: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        
        if state:
            # state was read
            pass
        return {""name"": name, ""value"": value}",Create a new item.,Create a dictionary representing an item with optional state handling.,5.0,5.0,3.0,0.2571428571428571,0.3,0.75,0.6297817230224609,0.6347757,0.3269531,0.45174834,0.9083526730537416,0.2,0.6577634295649046,3.0,5.0,5.0
129,129,129,130,create_named_schedule_sampler,"def create_named_schedule_sampler(name, diffusion):
    
    if name == ""uniform"":
        return UniformSampler(diffusion)
    elif name == ""loss-second-moment"":
        return LossSecondMomentResampler(diffusion)
    else:
        raise NotImplementedError(f""unknown schedule sampler: {name}"")",Create a ScheduleSampler from a library of pre-defined samplers.,Selects and returns a schedule sampler based on the given name.,5.0,5.0,4.0,0.7186604069965188,0.1818181818181818,0.2,0.6082570552825928,0.35391536,0.67774355,0.57468194,0.8376789093017578,0.3636363636363636,0.5925012675835246,5.0,5.0,5.0
130,130,130,131,validate_window_size,"def validate_window_size(configured: dict[str, Any], actual: dict[str, Any]) -> None:
	
	# Allow for small differences due to browser chrome, scrollbars, etc.
	width_diff = abs(configured['width'] - actual['width'])
	height_diff = abs(configured['height'] - actual['height'])

	# Tolerance of 5% or 20px, whichever is greater
	width_tolerance = max(configured['width'] * 0.05, 20)
	height_tolerance = max(configured['height'] * 0.05, 20)

	if width_diff > width_tolerance or height_diff > height_tolerance:
		print(f'⚠️  WARNING: Significant difference between expected and actual page size! ±{width_diff}x{height_diff}px')
		raise Exception('Window size validation failed')
	else:
		print('✅ Window size validation passed: actual size matches configured size within tolerance')",Compare configured window size with actual size and report differences,Validate if actual window size is within tolerance of configured dimensions.,5.0,5.0,5.0,0.8026315789473685,0.3636363636363636,0.2,0.6238316297531128,0.41468805,0.53264475,0.783546,0.8816694617271423,0.8181818181818182,0.699221251284379,5.0,5.0,5.0
131,131,131,132,cleanup_memory,"def cleanup_memory(self):
        
        # Clear CUDA cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

            # Log memory usage if in verbose mode
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            logger.info(f""GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved"")

        # Collect Python garbage
        import gc
        gc.collect()",Clean up GPU and CPU memory to prevent VRAM overflow during multiple generations.,Release GPU resources and collect garbage to optimize memory usage.,5.0,5.0,5.0,0.6539896216370912,0.4,0.2307692307692307,0.5914961099624634,0.3481638,0.63154584,0.78070974,0.8564508557319641,0.4,0.583054480451667,5.0,5.0,5.0
132,132,132,133,copy_to_shm,"def copy_to_shm(src:str):
    
    shm_model_root = '/dev/shm/verl-cache/'
    src_abs = os.path.abspath(os.path.normpath(src))
    dest = os.path.join(shm_model_root, hashlib.md5(src_abs.encode('utf-8')).hexdigest())
    os.makedirs(dest, exist_ok=True)
    dest = os.path.join(dest, os.path.basename(src_abs))
    if os.path.exists(dest) and verify_copy(src, dest):
        # inform user and depends on him
        print(f""[WARNING]: The memory model path {dest} already exists. If it is not you want, please clear it and restart the task."")
    else:
        if os.path.isdir(src):
            shutil.copytree(src, dest, symlinks=False, dirs_exist_ok=True)
        else:
            shutil.copy2(src, dest)
    return dest",Load the model into   /dev/shm   to make the process of loading the model multiple times more efficient.,"Copy a source file or directory to a shared memory location, ensuring uniqueness.",4.0,4.0,3.0,0.5762202073543689,0.0769230769230769,0.0555555555555555,0.6113086342811584,0.08565648,0.15800193,0.63380146,0.9419630765914916,0.3076923076923077,0.5284269556880318,4.0,4.0,4.0
133,133,133,134,parse_hedge_fund_response,"def parse_hedge_fund_response(response):
    
    try:
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f""JSON decoding error: {e}\nResponse: {repr(response)}"")
        return None
    except TypeError as e:
        print(f""Invalid response type (expected string, got {type(response).__name__}): {e}"")
        return None
    except Exception as e:
        print(f""Unexpected error while parsing response: {e}\nResponse: {repr(response)}"")
        return None",Parses a JSON string and returns a dictionary.,"Safely parse JSON from hedge fund API response, handling errors gracefully.",5.0,5.0,5.0,0.5333333333333333,0.1818181818181818,0.25,0.6289844512939453,0.25956365,0.4587295,0.65534705,0.9928993582725524,0.1818181818181818,0.6553081804285804,5.0,5.0,5.0
134,134,134,135,from_pretrained,"def from_pretrained(self, repo_id: str) -> ""Vocos"":
        
        config_path = hf_hub_download(repo_id=repo_id, filename=""config.yaml"")
        model_path = hf_hub_download(repo_id=repo_id, filename=""pytorch_model.bin"")
        model = self.from_hparams(config_path)
        state_dict = torch.load(model_path, map_location=""cpu"")
        if isinstance(model.feature_extractor, EncodecFeatures):
            encodec_parameters = {
                ""feature_extractor.encodec."" + key: value
                for key, value in model.feature_extractor.encodec.state_dict().items()
            }
            state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model",Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.,Load and initialize a pre-trained Vocos model from a specified repository.,5.0,5.0,4.0,0.5175259949286448,0.5833333333333334,0.238095238095238,0.6335539817810059,0.41898227,0.76430297,0.72678745,0.8887142539024353,0.2727272727272727,0.6347537423568741,5.0,5.0,5.0
135,135,135,136,add_legal_comment,"def add_legal_comment(cif: str) -> str:
  
  # fmt: off
  # pylint: disable=line-too-long
  comment = (
      '# By using this file you agree to the legally binding terms of use found at\n'
      f'# {_LICENSE_URL}.\n'
      '# To request access to the AlphaFold 3 model parameters, follow the process set\n'
      '# received directly from Google. Use is subject to terms of use available at\n'
  )
  # pylint: enable=line-too-long
  # fmt: on
  return f'{comment}\n{cif}'",Adds legal comment at the top of the mmCIF.,Appends legal disclaimer to a given text content.,5.0,5.0,5.0,0.6530612244897959,0.125,0.1111111111111111,0.6128603219985962,0.20315027,0.4775139,0.61012113,0.8639936447143555,0.25,0.3536146778071459,4.0,5.0,5.0
136,136,136,137,_parse_datetime_field,"def _parse_datetime_field(self, dt_obj: Optional[Dict]) -> Optional[datetime]:
        
        if not dt_obj or not dt_obj.get(""dateTime""):
            return None
        try:
            dt_str = dt_obj[""dateTime""]
            if ""T"" in dt_str:
                # Handle timezone info
                if dt_str.endswith(""Z""):
                    dt_str = dt_str.replace(""Z"", ""+00:00"")
                elif ""+"" not in dt_str and ""-"" not in dt_str[-6:]:
                    # If no timezone info, assume UTC
                    dt_str += ""+00:00""
                return datetime.fromisoformat(dt_str)
        except (ValueError, TypeError) as e:
            logger.warning(f""Error parsing datetime: {str(e)}"")
        return None",Parse datetime from Microsoft Graph API format.,Parse and convert a datetime string with timezone handling from a dictionary.,5.0,5.0,5.0,0.5064935064935064,0.25,0.4285714285714285,0.5715253353118896,0.4401838,0.4504282,0.7396402,0.9772558212280272,0.4166666666666667,0.5233196853767713,5.0,5.0,5.0
137,137,137,138,_get_auth_headers,"def _get_auth_headers(self) -> Dict[str, str]:
        
        headers = {
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json""
        }
        
        if ':' in self.credentials:
            # Basic authentication (username:password)
            encoded_credentials = base64.b64encode(self.credentials.encode()).decode()
            headers[""Authorization""] = f""Basic {encoded_credentials}""
        else:
            # API key authentication
            headers[""Authorization""] = f""Bearer {self.credentials}""
        
        return headers",Get authentication headers for OpenSearch requests.,Generate HTTP headers for authentication using credentials.,5.0,5.0,5.0,0.7796610169491526,0.4285714285714285,0.3333333333333333,0.5171244144439697,0.5199497,0.7102736,0.87740266,0.9747763872146606,0.2857142857142857,0.5752748673490143,4.0,5.0,5.0
138,138,138,139,setup_logger,"def setup_logger():
    
    config = {
        ""handlers"": [
            {
                ""sink"": sys.stdout,
                ""format"": ""<fg #2E8B57>{time:hh:mm:ss A}</fg #2E8B57> | ""
                ""{level: <8} | ""
                ""<fg #4169E1>{module}:{line}</fg #4169E1> | ""
                ""{message}"",
                ""colorize"": True,
                ""level"": ""DEBUG"",
            },
        ],
    }
    logger.remove()
    logger.configure(**config)
    logger.level(""ERROR"", color=""<red>"")",Configure loguru logger with custom formatting,Configure a logger with custom formatting and error level settings.,5.0,5.0,4.0,0.6567164179104478,0.5,0.8333333333333334,0.5738050937652588,0.59869015,0.8128484,0.88787246,0.951991081237793,0.1,0.6040949586324713,4.0,5.0,5.0
139,139,139,140,to_dict,"def to_dict(self) -> Dict:
        

        task_properties = {
            ""params"": {},
        }

        return {""type"": ""infographic_data_extract"", ""task_properties"": task_properties}",Convert to a dict for submission to redis,Create a dictionary for infographic data extraction task properties.,5.0,5.0,3.0,0.5147058823529411,0.2222222222222222,0.25,0.6601969003677368,0.17510276,0.32679337,0.62581366,0.8445760607719421,0.1111111111111111,0.6663705052892419,4.0,5.0,4.0
140,140,140,141,get_topic_map,"def get_topic_map() -> Dict[str, str]:
    
    return {
        'index': 'Overview and table of contents',
        'logging': 'Structured logging implementation',
        'tracing': 'Tracing implementation',
        'metrics': 'Metrics implementation',
        'cdk': 'CDK integration patterns',
        'dependencies': 'Dependencies management',
        'insights': 'Lambda Insights integration',
        'bedrock': 'Bedrock Agent integration',
    }",Get a dictionary mapping topic names to their descriptions.,Map topic identifiers to descriptive summaries for documentation.,5.0,5.0,5.0,0.7846153846153846,0.5,0.4444444444444444,0.5981739163398743,0.33141637,0.56321156,0.76195836,0.6831042766571045,0.0,0.4640735802074549,5.0,5.0,5.0
141,141,141,142,hotkey,"def hotkey(self, keys: List):
        
        # add quotes around the keys
        keys = [f""'{key}'"" for key in keys]
        return f""import pyautogui; pyautogui.hotkey({', '.join(keys)})""",Press a hotkey combination,Formats key inputs for PyAutoGUI hotkey automation script.,5.0,5.0,4.0,0.4310344827586206,0.125,0.25,0.6500111818313599,0.31422466,0.5455332,0.6369528,0.6858975887298584,0.375,0.6301069588087524,5.0,5.0,5.0
142,142,142,143,close_idle_connections,"def close_idle_connections(cls) -> None:
        
        now = datetime.now()
        idle_threshold = now - timedelta(minutes=cls._idle_timeout)

        idle_connections = [
            conn_id
            for conn_id, info in cls._connections.items()
            if info.last_used < idle_threshold
        ]

        for conn_id in idle_connections:
            logger.info(f'Closing idle DocumentDB connection {conn_id}')
            cls._connections[conn_id].client.close()
            del cls._connections[conn_id]",Close connections that have been idle for longer than the timeout.,Close and remove idle database connections exceeding timeout threshold.,5.0,5.0,5.0,0.8169014084507042,0.4444444444444444,0.2727272727272727,0.5293023586273193,0.35733095,0.483303,0.7212634,0.9958439469337464,0.2222222222222222,0.547206097968878,5.0,5.0,5.0
143,143,143,144,build_agent_tree,"def build_agent_tree(parent_tree, agent_obj):
            
            parent_tree.add(create_tools_section(agent_obj.tools))

            if agent_obj.managed_agents:
                agents_branch = parent_tree.add(""🤖 [italic #1E90FF]Managed agents:"")
                for name, managed_agent in agent_obj.managed_agents.items():
                    agent_tree = agents_branch.add(get_agent_headline(managed_agent, name))
                    if managed_agent.__class__.__name__ == ""CodeAgent"":
                        agent_tree.add(
                            f""✅ [italic #1E90FF]Authorized imports:[/italic #1E90FF] {managed_agent.additional_authorized_imports}""
                        )
                    agent_tree.add(f""📝 [italic #1E90FF]Description:[/italic #1E90FF] {managed_agent.description}"")
                    build_agent_tree(agent_tree, managed_agent)",Recursively builds the agent tree.,Constructs a hierarchical tree structure for agents and their managed sub-agents.,5.0,4.0,5.0,0.3703703703703703,0.1666666666666666,0.2,0.575882077217102,0.2807779,0.5701103,0.45400316,0.6968140006065369,0.1818181818181818,0.6129274345819742,4.0,5.0,5.0
144,144,144,145,get_weight_buffer_meta_from_module,"def get_weight_buffer_meta_from_module(module: nn.Module) -> Dict[str, Dict]:
    
    weight_buffer_meta = {}
    for name, param in sorted(module.named_parameters()):
        weight_buffer_meta[name] = {'shape': param.shape, 'dtype': param.dtype}
    return weight_buffer_meta",Return a dictionary containing name to a shape and dtype.,Extracts metadata of weights from a neural network module.,5.0,5.0,5.0,0.6896551724137931,0.1111111111111111,0.1,0.6160984039306641,0.12540166,0.27653316,0.628651,0.6122538447380066,0.1111111111111111,0.5463612938529075,5.0,5.0,5.0
145,145,145,146,_read_file_safe,"def _read_file_safe(file_path: str) -> Optional[str]:
    
    if not os.path.exists(file_path):
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f""Error reading file {file_path}: {e}"")
        return None","Safely read a file, returning None if it doesn't exist or on error.","Safely read file content, handling errors and non-existent paths.",5.0,5.0,5.0,0.7608410080809173,0.5,0.2857142857142857,0.603369951248169,0.3559553,0.67373925,0.73096263,0.9840000867843628,0.1111111111111111,0.4929605852551674,5.0,5.0,5.0
146,146,146,147,__init__,"def __init__(
        self,
        exit_stack: contextlib.AsyncExitStack | None = None,
        component_name_hook: _ComponentNameHook | None = None,
    ) -> None:
        

        self._tools = {}
        self._resources = {}
        self._prompts = {}

        self._sessions = {}
        self._tool_to_session = {}
        if exit_stack is None:
            self._exit_stack = contextlib.AsyncExitStack()
            self._owns_exit_stack = True
        else:
            self._exit_stack = exit_stack
            self._owns_exit_stack = False
        self._session_exit_stacks = {}
        self._component_name_hook = component_name_hook",Initializes the MCP client.,Initialize resource management with optional exit stack and component hook,5.0,5.0,4.0,0.3108108108108108,0.1,0.25,0.5180572867393494,0.43198553,0.2625113,0.22082503,0.8709725737571716,0.0,0.5775839281127783,5.0,5.0,4.0
147,147,147,148,_fix_chrome_permissions,"def _fix_chrome_permissions(self, user_data_dir):
        
        try:
            if sys.platform == 'darwin':  # macOS
                import subprocess
                import pwd
                
                # Get current user
                current_user = pwd.getpwuid(os.getuid()).pw_name
                
                # Fix permissions for Chrome directory
                chrome_dir = os.path.expanduser('~/Library/Application Support/Google/Chrome')
                if os.path.exists(chrome_dir):
                    subprocess.run(['chmod', '-R', 'u+rwX', chrome_dir])
                    subprocess.run(['chown', '-R', f'{current_user}:staff', chrome_dir])
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('oauth.chrome_permissions_fixed') if self.translator else 'Fixed Chrome user data directory permissions'}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.YELLOW}{EMOJI['WARNING']} {self.translator.get('oauth.chrome_permissions_fix_failed', error=str(e)) if self.translator else f'Failed to fix Chrome permissions: {str(e)}'}{Style.RESET_ALL}"")",Fix permissions for Chrome user data directory,"Adjusts macOS Chrome directory permissions for the current user, handling exceptions.",5.0,5.0,3.0,0.5294117647058824,0.4545454545454545,0.4285714285714285,0.6819310188293457,0.51661193,0.69897914,0.8585978,0.939630389213562,0.6363636363636364,0.6493749154139807,5.0,5.0,5.0
148,148,148,149,init_progressive,"def init_progressive(self, noise_scale=0.01):
        
        self._copy_non_transformer_params()

        # copy pretrained layers
        for i in range(self.pretrained_layers):
            for key in self.pretrained_state:
                if f""blocks.{i}."" in key:
                    new_key = key.replace(f""blocks.{i}."", f""blocks.{i}."")
                    self.target_state[new_key] = self.pretrained_state[key]

        # progressive init new layers
        for i in range(self.pretrained_layers, self.target_layers):
            prev_layer = i - 1
            for key in self.target_state:
                if f""blocks.{i}."" in key:
                    prev_key = key.replace(f""blocks.{i}."", f""blocks.{prev_layer}."")
                    # add random noise
                    noise = torch.randn_like(self.target_state[prev_key]) * noise_scale
                    self.target_state[key] = self.target_state[prev_key] + noise

        self.target_model.load_state_dict(self.target_state)
        return self.target_model",progressive init strategy (with noise),Progressively initialize model layers with noise based on pretrained state,4.0,5.0,3.0,0.4729729729729729,0.3,0.6,0.4672189354896545,0.35973024,0.45461994,0.48960477,0.930448055267334,0.4,0.6855511363544493,5.0,5.0,4.0
149,149,149,150,_build_refinement_prompt,"def _build_refinement_prompt(
        self,
        original_request: str,
        current_response: str,
        feedback: EvaluationResult,
        iteration: int,
    ) -> str:
        
        return f",Build the refinement prompt for the optimizer,"Constructs a refinement prompt using request, response, feedback, and iteration.",5.0,5.0,5.0,0.4875,0.2,0.2857142857142857,0.5774227380752563,0.38746932,0.62229043,0.71762943,0.9478259086608888,0.3,0.6566731845441963,1.0,5.0,5.0
150,150,150,151,extract_code_blocks,"def extract_code_blocks(text: str) -> List[str]:
    
    pattern = r'```python\s*(.*?)\s*```'
    matches = re.findall(pattern, text, re.DOTALL)
    blocks = [m.strip() for m in matches]
    logger.info(f""Extracted {len(blocks)} code blocks"")
    for i, block in enumerate(blocks):
        logger.info(f""Code block {i+1}:\n{block}"")
    return blocks",Extract Python code blocks from text.,Extracts and logs Python code blocks from a given text input.,5.0,5.0,5.0,0.6065573770491803,0.5454545454545454,1.0,0.6078158020973206,0.7178645,0.8488442,0.85480565,0.7660861015319824,0.3636363636363636,0.6499518627960919,5.0,5.0,5.0
151,151,151,152,check_and_install_dependencies,"def check_and_install_dependencies():
    
    required_packages = [
        ""uvicorn"",
        ""tiktoken"",
        ""fastapi"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")",Check and install required dependencies,"Ensure required packages are installed, installing if necessary.",5.0,5.0,5.0,0.546875,0.25,0.2,0.6009362936019897,0.37653002,0.85099816,0.7614869,0.979375422000885,0.5,0.5327683301568199,5.0,5.0,5.0
152,152,152,153,_analyze_contrarian_sentiment,"def _analyze_contrarian_sentiment(news):
    

    max_score = 1
    score = 0
    details: list[str] = []

    if not news:
        details.append(""No recent news"")
        return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}

    # Count negative sentiment articles
    sentiment_negative_count = sum(
        1 for n in news if n.sentiment and n.sentiment.lower() in [""negative"", ""bearish""]
    )
    
    if sentiment_negative_count >= 5:
        score += 1  # The more hated, the better (assuming fundamentals hold up)
        details.append(f""{sentiment_negative_count} negative headlines (contrarian opportunity)"")
    else:
        details.append(""Limited negative press"")

    return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}",Very rough gauge: a wall of recent negative headlines can be a *positive* for a contrarian.,Evaluate contrarian sentiment in news articles to identify potential investment opportunities.,5.0,5.0,4.0,0.7127659574468085,0.0909090909090909,0.0625,0.6596799492835999,0.057867184,0.5201252,0.62774503,0.9642515182495116,0.4545454545454545,0.6493595712945733,4.0,5.0,5.0
153,153,153,154,opt_repetitions,"def opt_repetitions(up_to_n, prefix_with_sep=False):
        

        content = (
            f""{separator_rule} {item_rule}""
            if prefix_with_sep and separator_rule
            else item_rule
        )
        if up_to_n == 0:
            return """"
        elif up_to_n == 1:
            return f""({content})?""
        elif separator_rule and not prefix_with_sep:
            return f""({content} {opt_repetitions(up_to_n - 1, prefix_with_sep=True)})?""
        else:
            return (f""({content} "" * up_to_n).rstrip() + ("")?"" * up_to_n)","n=4, no sep:             '(a (a (a (a)?)?)?)?' - n=4, sep=',', prefix:    '("","" a ("","" a ("","" a ("","" a)?)?)?)?' - n=4, sep=',', no prefix: '(a ("","" a ("","" a ("","" a)?)?)?)?'",Generate optional repetition patterns with customizable separators.,5.0,5.0,5.0,0.1058749843443322,0.0,0.0,0.5146265029907227,-0.78950214,0.14783727,0.22957063,0.953663170337677,0.0,0.5466841439704639,4.0,5.0,5.0
154,154,154,155,comment_magic_commands,"def comment_magic_commands(script_content: str) -> str:
    
    lines = script_content.splitlines()
    commented_lines = []
    for line in lines:
        # Check for magic commands, shell commands, or direct execution commands
        if re.match(r'^\s*(!|%|pip|apt-get|curl|conda)', line.strip()):
            commented_lines.append(f""# {line}"")  # Comment the line
        else:
            commented_lines.append(line)  # Keep the line unchanged
    return ""\n"".join(commented_lines)","Comment out magic commands, shell commands, and direct execution commands in the script content.",Transform script lines by commenting out shell or magic commands.,5.0,5.0,5.0,0.5442976238950071,0.6,0.2857142857142857,0.58391273021698,0.4439874,0.86037636,0.7686696,0.8316172361373901,0.5,0.67255946293041,5.0,5.0,5.0
155,155,155,156,to_native_types,"def to_native_types(obj: Any, resolve: bool = True, throw_on_missing: bool = True, enum_to_str: bool = True) -> Any:
    

    # convert dataclass to structured config
    if hasattr(obj, ""to_dict""):
        # huggingface objects have a to_dict method, we prefer that
        obj = obj.to_dict()
    elif is_dataclass(obj):
        # we go through structured config instead and hope for the best
        obj = om.to_container(obj)

    if isinstance(obj, DictConfig) or isinstance(obj, ListConfig):
        obj = om.to_container(obj, resolve=resolve, throw_on_missing=throw_on_missing, enum_to_str=enum_to_str)

    if isinstance(obj, dict):
        return {k: to_native_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [to_native_types(v) for v in obj]
    else:
        return obj","Converts an OmegaConf object to native types (dicts, lists, etc.)","Convert complex objects to native Python types, handling dataclasses and configurations.",4.0,4.0,5.0,0.6818181818181818,0.4545454545454545,0.5,0.6492756605148315,0.2831659,0.09230583,0.7701802,0.9525415301322936,0.3636363636363636,0.6699186232174464,5.0,5.0,5.0
156,156,156,157,list_fail_condition,"def list_fail_condition(self):
        
        payload = {}
        headers = {
                'Authorization': f'Bearer {os.getenv(""RAGAAI_CATALYST_TOKEN"")}',
                'X-Project-Id': str(self.project_id)
                }
        response = requests.request(""GET"", f""{self.base_url}/guardrail/deployment/configurations"", headers=headers, data=payload, timeout=self.timeout)
        return response.json()[""data""]",List all fail conditions for the current project's deployments.,Fetch deployment configurations using authorization token and project ID,5.0,5.0,3.0,0.7361111111111112,0.2222222222222222,0.1,0.5259191989898682,0.3517481,0.5135745,0.5301595,0.4899637103080749,0.0,0.4215606854371674,4.0,5.0,5.0
157,157,157,158,detect_mutating_keywords,"def detect_mutating_keywords(sql: str) -> list[str]:
    
    matched = []

    if DDL_REGEX.search(sql):
        matched.append('DDL')

    if PERMISSION_REGEX.search(sql):
        matched.append('PERMISSION')

    if SYSTEM_REGEX.search(sql):
        matched.append('SYSTEM')

    # Match individual keywords from MUTATING_KEYWORDS
    keyword_matches = MUTATING_PATTERN.findall(sql)
    if keyword_matches:
        # Deduplicate and normalize casing
        matched.extend(sorted({k.upper() for k in keyword_matches}))

    return matched",Return a list of mutating keywords found in the SQL (excluding comments).,Detects and categorizes SQL mutating keywords into predefined groups.,5.0,5.0,5.0,0.7658830260915016,0.3333333333333333,0.1666666666666666,0.597510814666748,0.21029621,0.6520616,0.74479294,0.8771010041236877,0.2222222222222222,0.6803969704696102,5.0,5.0,5.0
158,158,158,159,_get_trace_filename,"def _get_trace_filename(self) -> str:
        
        path_pattern = self.path_settings.path_pattern
        unique_id_type = self.path_settings.unique_id

        if unique_id_type == ""session_id"":
            unique_id = self.session_id
        elif unique_id_type == ""timestamp"":
            now = datetime.now()
            time_format = self.path_settings.timestamp_format
            unique_id = now.strftime(time_format)
        else:
            raise ValueError(
                f""Invalid unique_id type: {unique_id_type}. Expected 'session_id' or 'timestamp'.""
            )

        return path_pattern.replace(""{unique_id}"", unique_id)",Generate a trace filename based on the path settings.,Generate a trace filename using a pattern with a session ID or timestamp,5.0,5.0,4.0,0.6805555555555556,0.3076923076923077,0.4444444444444444,0.5676968097686768,0.5757536,0.71805125,0.8187255,0.8672105073928833,0.3846153846153846,0.60669655733767,5.0,5.0,5.0
159,159,159,160,_load_prompt_template,"def _load_prompt_template(self) -> None:
		
		try:
			# This works both in development and when installed as a package
			with importlib.resources.files('browser_use.agent').joinpath('system_prompt.md').open('r') as f:
				self.prompt_template = f.read()
		except Exception as e:
			raise RuntimeError(f'Failed to load system prompt template: {e}')",Load the prompt template from the markdown file.,"Load a system prompt template from a file, handling exceptions.",5.0,5.0,4.0,0.6984126984126984,0.5,0.625,0.6199244260787964,0.58761644,0.60609484,0.8683823,0.9705812931060792,0.6,0.6637479102207317,5.0,5.0,5.0
160,160,160,161,model_dump,"def model_dump(self, **kwargs) -> dict[str, Any]:
		
		return {
			'history': [h.model_dump(**kwargs) for h in self.history],
		}",Custom serialization that properly uses AgentHistory's model_dump,Converts object history into a dictionary format using specified parameters.,5.0,5.0,4.0,0.6842105263157895,0.1,0.1111111111111111,0.5971282720565796,0.14125735,0.55932885,0.6029849,0.7413274049758911,0.1,0.4804267527662392,5.0,5.0,5.0
161,161,161,162,should_ignore_directory,"def should_ignore_directory(dirname: str) -> bool:
    
    ignore_dirs = {
        ""venv"",
        ""env"",
        "".venv"",
        ""virtualenv"",
        ""__pycache__"",
        "".pytest_cache"",
        "".mypy_cache"",
        "".tox"",
        "".git"",
        ""build"",
        ""dist"",
        ""node_modules"",
        "".next"",
        ""storage"",
    }
    return dirname in ignore_dirs",Check if directory should be ignored.,Determine if a directory name should be ignored based on a predefined list.,5.0,5.0,5.0,0.44,0.3846153846153846,0.8333333333333334,0.5022757053375244,0.758315,0.7872844,0.6604177,0.9595803022384644,0.3076923076923077,0.5597116086373976,5.0,5.0,5.0
162,162,162,163,read_audio,"def read_audio(file_path):
    
    
    try:
        audio = AudioSegment.from_file(file_path)
        return audio
    except Exception as e:
        print(f""Error loading file: {e}"")
        return None",Use AudioSegment to load audio from all supported audio input format,This function attempts to load an audio file and returns the audio data or an error message.,5.0,5.0,5.0,0.6413043478260869,0.2352941176470588,0.3636363636363636,0.6618176698684692,0.18067497,0.5566351,0.80778164,0.9695034623146056,0.3529411764705882,0.5412688437718152,4.0,4.0,4.0
163,163,163,164,print_curl,"def print_curl(self, method: str, path: str, data: Optional[Dict[str, Any]] = None) -> None:
        
        curl_cmd = f
        
        if data:
            curl_cmd += f"" \\\n  -H 'Content-Type: application/json' \\\n  -d '{json.dumps(data)}'""
        
        print(""\nEquivalent curl command:"")
        print(curl_cmd)
        print()",Print equivalent curl command for debugging.,Generate and display a curl command for HTTP requests with optional JSON data.,4.0,5.0,3.0,0.4871794871794871,0.2307692307692307,0.5,0.647333562374115,0.4703717,0.46170816,0.6795322,0.9527227878570556,0.2307692307692307,0.5587564624995027,4.0,5.0,5.0
164,164,164,165,_log_available_capabilities,"def _log_available_capabilities(self):
        
        capabilities = []
        if self.mistral_client:
            capabilities.append(""Mistral OCR"")
        if self.exiftool_available:
            capabilities.append(""Exiftool metadata extraction"")
        if self.openai_client:
            capabilities.append(""OpenAI image description"")

        if capabilities:
            logger.info(f""Image converter initialized with: {', '.join(capabilities)}"")
        else:
            logger.warning(""Image converter initialized without any processing capabilities"")",Log which conversion capabilities are available.,Log initialized image processing capabilities based on available tools,5.0,5.0,4.0,0.5857142857142857,0.3333333333333333,0.5,0.5080145597457886,0.37947834,0.4829878,0.71760964,0.8662787079811096,0.4444444444444444,0.488580004479053,5.0,5.0,5.0
165,165,165,166,load_openai_mappings,"def load_openai_mappings() -> Dict:
    
    api_dir = os.path.dirname(os.path.dirname(__file__))
    mapping_path = os.path.join(api_dir, ""core"", ""openai_mappings.json"")
    try:
        with open(mapping_path, ""r"") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f""Failed to load OpenAI mappings: {e}"")
        return {""models"": {}, ""voices"": {}}",Load OpenAI voice and model mappings from JSON,"Load OpenAI configuration from JSON file, handle errors gracefully.",4.0,4.0,4.0,0.5970149253731343,0.4444444444444444,0.5,0.6237952709197998,0.44725403,0.63060355,0.716529,0.6950181126594543,0.2222222222222222,0.5078546583015733,5.0,5.0,5.0
166,166,166,167,get_pydoc_string,"def get_pydoc_string(self) -> str:
    
    pydoc_params = [param.to_pydoc_string() for param in self._params]
    pydoc_description = (
        self._operation.summary or self._operation.description or ''
    )
    pydoc_return = PydocHelper.generate_return_doc(
        self._operation.responses or {}
    )
    pydoc_arg_list = chr(10).join(
        f'        {param_doc}' for param_doc in pydoc_params
    )
    return dedent(f).strip()",Returns the generated PyDoc string.,Generate a formatted Python docstring from operation details and parameters.,5.0,5.0,4.0,0.4078947368421052,0.1,0.2,0.597265362739563,0.5972615,0.43839544,0.41834578,0.8624430894851685,0.1,0.6346718443309856,4.0,5.0,5.0
167,167,167,168,_run,"def _run(self, **kwargs) -> str:
        
        if not self.server_manager.active_server:
            return (
                ""No MCP server is currently active. ""
                ""Use connect_to_mcp_server to connect to a server.""
            )
        return f""Currently active MCP server: {self.server_manager.active_server}""",Get the currently active MCP server.,Check if an MCP server is active and return its status message.,5.0,5.0,5.0,0.5238095238095238,0.25,0.3333333333333333,0.6644248366355896,0.5839232,0.7133286,0.6586721,0.8623805642127991,0.6666666666666666,0.5773359161728165,5.0,5.0,5.0
168,168,168,169,load_images_from_directory,"def load_images_from_directory(image_dir, valid_extensions=('.jpg', '.jpeg', '.png', '.webp')):
    
    images_dict = {}

    if not os.path.exists(image_dir):
        raise ValueError(f""Directory {image_dir} does not exist"")

    for filename in os.listdir(image_dir):
        if filename.lower().endswith(valid_extensions):
            image_path = os.path.join(image_dir, filename)
            try:
                image = Image.open(image_path).convert(""RGB"")
                images_dict[image_path] = image
            except Exception as e:
                print(f""Failed to load image {filename}: {str(e)}"")

    if not images_dict:
        raise ValueError(f""No valid image files found in {image_dir}"")

    return images_dict",Load images from directory,Load and return valid images from a specified directory as a dictionary.,5.0,5.0,5.0,0.3611111111111111,0.3333333333333333,1.0,0.612419605255127,0.30415097,0.7438549,0.73134613,0.9428887367248536,0.5833333333333334,0.6742865966591659,4.0,5.0,5.0
169,169,169,170,get_sample_document_chunks,"def get_sample_document_chunks(num_chunks=3, num_vectors=3, dim=128):
    
    chunks = []
    for i in range(num_chunks):
        embeddings = get_sample_embeddings(num_vectors, dim)
        chunk = DocumentChunk(
            document_id=f""doc_{i}"",
            content=f""Test content {i}"",
            embedding=embeddings,
            chunk_number=i,
            metadata={""test_key"": f""test_value_{i}""},
        )
        chunks.append(chunk)
    return chunks",Create sample document chunks for testing,Generate document chunks with embeddings and metadata for testing purposes.,5.0,5.0,4.0,0.52,0.4,0.6666666666666666,0.5308202505111694,0.6860293,0.69508123,0.89125824,0.7180107235908508,0.3,0.6475474658825139,5.0,5.0,5.0
170,170,170,171,_update_component_states,"def _update_component_states(self, states: dict[str, Any]):
        
        with self._lock:
            logger.info(""[STATE] Updating states"")
            for component_id, new_value in states.items():
                old_value = self._component_states.get(component_id)

                cleaned_new_value = clean_nan_values(new_value)
                cleaned_old_value = clean_nan_values(old_value)

                if cleaned_old_value != cleaned_new_value:
                    self._component_states[component_id] = cleaned_new_value
                    logger.info(f""[STATE] State changed for {component_id=}"")
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f""[STATE]  - {cleaned_old_value=}\n  - {cleaned_new_value=}"")",Update internal state dictionary with cleaned component values.,"Safely update component states, logging changes and handling NaN values.",5.0,5.0,5.0,0.7222222222222222,0.4,0.375,0.5329651832580566,0.49065638,0.5278961,0.6706538,0.9038686156272888,0.2,0.6019471459093024,5.0,5.0,5.0
171,171,171,172,main,"def main(model_name: str = ""gpt-4o-mini"", provider: str = ""openai""):
    
    model = LiteLLMModel(
        model_id=f""{provider}/{model_name}"",
        api_key=os.environ.get(""OPENAI_API_KEY""),
    )
    
    agent = CodeAgent(
        tools=[
            get_hugging_face_top_daily_paper,
            get_paper_id_by_title,
            download_paper_by_id,
            read_pdf_file
        ],
        model=model,
        add_base_tools=True
    )

    agent.run(
        ""Summarize today's top paper on Hugging Face daily papers by reading it.""
    )",Initialize and run the paper summarization agent.,Initialize and execute an AI agent to summarize top daily papers using a specified language model.,4.0,5.0,4.0,0.4795918367346938,0.3125,0.4285714285714285,0.6634957194328308,0.60576856,0.6115832,0.5779604,0.914143681526184,0.5625,0.6052934190362425,5.0,5.0,5.0
172,172,172,173,safe_save_model_for_hf_trainer,"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    

    if trainer.deepspeed:
        torch.cuda.synchronize()
        trainer.save_model(output_dir)
        return

    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,"Safely save a model using a trainer, handling deepspeed and CPU state.",5.0,4.0,4.0,0.4857142857142857,0.1666666666666666,0.125,0.6591126322746277,0.24319187,0.19330531,0.5254405,0.9847497940063475,0.1666666666666666,0.7185434837625608,4.0,5.0,4.0
173,173,173,174,load_model,"def load_model(self) -> None:
        
        # TODO: add mps (apple metal) support, currently cant benchmark mps device accurately for energy
        if self.config.device == ""cuda"" or self.config.device == ""mps"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""gpu"", **self.config.model_kwargs)
        elif self.config.device == ""cpu"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""cpu"", **self.config.model_kwargs)
        else:
            raise ValueError(f""Invalid device: {self.config.device}"")
        
        self.pretrained_model = nexa_model.model","Load the model from the given model path (normally GGUF, GGML)",Load a text inference model based on specified device configuration,5.0,5.0,4.0,0.5970149253731343,0.2,0.1818181818181818,0.5911457538604736,0.10865281,0.4448483,0.6742146,0.8984009027481079,0.3,0.6053183068586183,5.0,5.0,5.0
174,174,174,175,switch_applications,"def switch_applications(self, app_code):
        
        if self.platform == ""darwin"":
            return f""import pyautogui; import time; pyautogui.hotkey('command', 'space', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""
        elif self.platform == ""linux"":
            return UBUNTU_APP_SETUP.replace(""APP_NAME"", app_code)
        elif self.platform == ""windows"":
            return f""import pyautogui; import time; pyautogui.hotkey('win', 'd', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""",Switch to a different application that is already open,Generate platform-specific scripts to switch applications using automation tools.,5.0,5.0,3.0,0.6049382716049383,0.3,0.2222222222222222,0.550551176071167,-0.11969891,0.57499945,0.65224314,0.8979500532150269,0.0,0.5260813846333081,5.0,5.0,5.0
175,175,175,176,_merge_data,"def _merge_data(self, existing: list[dict] | None, new_data: list[dict], key_field: str) -> list[dict]:
        
        if not existing:
            return new_data

        # Create a set of existing keys for O(1) lookup
        existing_keys = {item[key_field] for item in existing}

        # Only add items that don't exist yet
        merged = existing.copy()
        merged.extend([item for item in new_data if item[key_field] not in existing_keys])
        return merged","Merge existing and new data, avoiding duplicates based on a key field.",Merge new data into existing list by unique key field,5.0,5.0,4.0,0.6708388515873592,0.6,0.4166666666666667,0.571632444858551,0.36997658,0.72208273,0.76393914,0.9628711938858032,0.3,0.6377070789268644,5.0,5.0,4.0
176,176,176,177,check_hub_revision_exists,"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id,
                    revision=training_args.hub_model_revision,
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )",Checks if a given Hub revision exists.,Check if a specific model revision exists on the hub and validate its README presence.,5.0,5.0,5.0,0.4186046511627907,0.4,0.7142857142857143,0.6223115921020508,0.6280203,0.7298037,0.7177168,0.9351970553398132,0.6666666666666666,0.5859919888757807,5.0,5.0,4.0
177,177,177,178,__update_session_state,"def __update_session_state(self, session: Session, event: Event) -> None:
    
    if not event.actions or not event.actions.state_delta:
      return
    for key, value in event.actions.state_delta.items():
      if key.startswith(State.TEMP_PREFIX):
        continue
      session.state.update({key: value})",Updates the session state based on the event.,"Update session state with event-driven changes, ignoring temporary keys.",5.0,5.0,4.0,0.6111111111111112,0.4,0.5,0.622566282749176,0.5372005,0.6882211,0.746368,0.9583477973937988,0.1111111111111111,0.6069751254415441,5.0,5.0,5.0
178,178,178,179,restart,"def restart(self) -> None:
        
        self._container.restart()
        if self._container.status != ""running"":
            raise ValueError(f""Failed to restart container. Logs: {self._container.logs()}"")",(Experimental) Restart the code executor.,"Restart a container and verify its running status, raising an error if unsuccessful.",4.0,4.0,5.0,0.369047619047619,0.0769230769230769,0.2,0.7280546426773071,0.2650945,0.4580414,0.48046502,0.9016104936599731,0.2307692307692307,0.6561729362555878,5.0,5.0,5.0
179,179,179,180,save_fid_stats,"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers)

    np.savez_compressed(paths[1], mu=m1, sigma=s1)",Calculates the FID of two paths,Compute and save statistical data from a specified path using a neural model.,5.0,5.0,3.0,0.3376623376623376,0.0769230769230769,0.1666666666666666,0.6431866884231567,0.20523688,0.28167576,0.5374057,0.8246628046035767,0.3076923076923077,0.5400574177132041,4.0,4.0,4.0
180,180,180,181,reorder,"def reorder(self, indices):
        
        indices_np = indices.detach().numpy()
        self.batch = self.batch[indices]
        self.non_tensor_batch = {key: val[indices_np] for key, val in self.non_tensor_batch.items()}",Note that this operation is in-place,Reorder batch and non-tensor data using specified indices.,5.0,5.0,5.0,0.5172413793103449,0.0,0.0,0.5914613604545593,0.16342348,0.20912707,0.51825523,0.9600040912628174,0.0,0.7348562354063121,5.0,5.0,5.0
181,181,181,182,_update_macos_platform_uuid,"def _update_macos_platform_uuid(self, new_ids):
        
        try:
            uuid_file = ""/var/root/Library/Preferences/SystemConfiguration/com.apple.platform.uuid.plist""
            if os.path.exists(uuid_file):
                # Use sudo to execute plutil command
                cmd = f'sudo plutil -replace ""UUID"" -string ""{new_ids[""telemetry.macMachineId""]}"" ""{uuid_file}""'
                result = os.system(cmd)
                if result == 0:
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('reset.macos_platform_uuid_updated')}{Style.RESET_ALL}"")
                else:
                    raise Exception(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.failed_to_execute_plutil_command')}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.update_macos_platform_uuid_failed', error=str(e))}{Style.RESET_ALL}"")
            raise",Update macOS Platform UUID,Update macOS platform UUID using a new telemetry ID with error handling.,4.0,5.0,4.0,0.3472222222222222,0.3333333333333333,1.0,0.5796307325363159,0.5984181,0.7732788,0.42548296,0.958748459815979,0.1666666666666666,0.6394686475844787,5.0,5.0,5.0
182,182,182,183,to_query_params,"def to_query_params(self, is_stream: bool) -> ""QueryParam"":
        
        # Use Pydantic's `.model_dump(exclude_none=True)` to remove None values automatically
        request_data = self.model_dump(exclude_none=True, exclude={""query""})

        # Ensure `mode` and `stream` are set explicitly
        param = QueryParam(**request_data)
        param.stream = is_stream
        return param",Converts a QueryRequest instance into a QueryParam instance.,"Converts model data to query parameters, excluding None values, and sets stream mode.",5.0,5.0,4.0,0.5882352941176471,0.0769230769230769,0.125,0.6984382271766663,0.359593,0.5393255,0.47506887,0.659371554851532,0.3846153846153846,0.6516250357606446,5.0,5.0,5.0
183,183,183,184,process_audio_file,"def process_audio_file(audio_path, text, polyphone):
    
    if not Path(audio_path).exists():
        print(f""audio {audio_path} not found, skipping"")
        return None
    try:
        audio_duration = get_audio_duration(audio_path)
        if audio_duration <= 0:
            raise ValueError(f""Duration {audio_duration} is non-positive."")
        return (audio_path, text, audio_duration)
    except Exception as e:
        print(f""Warning: Failed to process {audio_path} due to error: {e}. Skipping corrupt file."")
        return None",Process a single audio file by checking its existence and extracting duration.,"Validate and extract audio file duration, handling errors gracefully.",4.0,4.0,4.0,0.6869063999599688,0.5555555555555556,0.25,0.539467453956604,0.38682377,0.68129385,0.7801636,0.6302531957626343,0.1111111111111111,0.4716997603391834,4.0,5.0,5.0
184,184,184,185,create_dataset_if_not_exists,"def create_dataset_if_not_exists():
    
    # Construct a BigQuery client object.
    dataset_id = f""{client.project}.{DATASET_ID}""
    dataset = bigquery.Dataset(dataset_id)
    dataset.location = ""US""
    client.delete_dataset(
        dataset_id, delete_contents=True, not_found_ok=True
    )  # Make an API request.
    dataset = client.create_dataset(dataset)  # Make an API request.
    print(""Created dataset {}.{}"".format(client.project, dataset.dataset_id))
    return dataset",Creates a BigQuery dataset if it does not already exist.,Create or replace a BigQuery dataset in the US location.,5.0,5.0,3.0,0.8214285714285714,0.4,0.4,0.6000614166259766,0.21672621,0.71113455,0.63267505,0.8996552228927612,0.4,0.5769969608323233,5.0,5.0,5.0
185,185,185,186,_get_node_color,"def _get_node_color(self, node_type: str) -> str:
        
        color_map = {
            ""person"": ""#4f46e5"",  # Indigo
            ""organization"": ""#06b6d4"",  # Cyan
            ""location"": ""#10b981"",  # Emerald
            ""date"": ""#f59e0b"",  # Amber
            ""concept"": ""#8b5cf6"",  # Violet
            ""event"": ""#ec4899"",  # Pink
            ""product"": ""#ef4444"",  # Red
            ""entity"": ""#4f46e5"",  # Indigo (for generic entities)
            ""attribute"": ""#f59e0b"",  # Amber
            ""relationship"": ""#ec4899"",  # Pink
            ""high_level_element"": ""#10b981"",  # Emerald
            ""semantic_unit"": ""#8b5cf6"",  # Violet
        }
        return color_map.get(node_type.lower(), ""#6b7280"")",Get color for a node type to match the UI color scheme.,Map node types to specific color codes for visualization.,5.0,5.0,4.0,0.6842105263157895,0.5555555555555556,0.3333333333333333,0.4228203892707824,0.41856417,0.6938298,0.79520136,0.8031348586082458,0.1111111111111111,0.6173146813450634,5.0,5.0,5.0
186,186,186,187,dummy_chunks,"def dummy_chunks():
    
    return [
        FakeChunk(content=""text1"", metadata={}, score=0.5),
        FakeChunk(content=""data:image/png;base64,AAA"", metadata={""is_image"": True}, score=0.9),
        FakeChunk(content=""AAA"", metadata={""is_image"": True}, score=0.8),
    ]",Sample fake chunks for testing retrieve_chunks,"Generate a list of mock data chunks with content, metadata, and scores",5.0,5.0,4.0,0.5285714285714286,0.0833333333333333,0.1428571428571428,0.6279017329216003,0.1560646,0.51218975,0.70129186,0.700901985168457,0.0833333333333333,0.6498433641612279,5.0,5.0,5.0
187,187,187,188,_search_by_url_sync,"def _search_by_url_sync(self, url: str, collection_name: str) -> Optional[List[str]]:
        
        client = self._get_milvus_client()
        
        logger.debug(f""Querying collection: {collection_name} for URL: {url}"")
        res = client.query(
            collection_name=collection_name,
            filter=f""url == '{url}'"",
            limit=1,
            output_fields=[""url"", ""text"", ""name"", ""site""],
        )
        
        if len(res) == 0:
            logger.warning(f""No item found for URL: {url}"")
            return None
        
        item = res[0]
        txt = json.dumps(item[""text""])
        logger.info(f""Successfully retrieved item for URL: {url}"")
        return [item[""url""], txt, item[""name""], item[""site""]]",Synchronous implementation of search_by_url for thread execution,Retrieve and log data from a collection by matching a given URL.,5.0,5.0,4.0,0.671875,0.1666666666666666,0.2222222222222222,0.5902922749519348,-0.012625838,0.54526234,0.50474334,0.8622453212738037,0.4166666666666667,0.53977369678052,4.0,5.0,4.0
188,188,188,189,get_all_pages,"def get_all_pages(s3_client, document_files):
    
    file_contents = {}

    # First, collect all file paths and their document info
    for file_path in tqdm(document_files, desc=""Loading document files""):
        lines = load_document_file(s3_client, file_path)
        if not lines:
            logger.warning(f""Empty or invalid file: {file_path}"")
            continue

        # Parse each line for document info
        documents = []
        for i, line in enumerate(lines):
            doc_info = get_document_info_from_line(line, file_path, i)
            # Always add an entry for each line, even if None, to preserve line alignment
            documents.append(doc_info)

        # Store all documents for this file
        file_contents[file_path] = documents
        logger.info(f""Loaded {len(documents)} documents from {file_path}"")

    logger.info(f""Loaded documents from {len(file_contents)} files"")
    return file_contents","Get all pages from the document files for processing, preserving file and line order.","Load and parse document files from S3, storing structured information for each line.",5.0,5.0,4.0,0.811707637373334,0.4615384615384615,0.2857142857142857,0.6280035972595215,0.31614646,0.6746283,0.82442534,0.940503716468811,0.6153846153846154,0.5076215084875848,5.0,5.0,5.0
189,189,189,190,decrypt,"def decrypt(ciphertext_b64: str, password: str) -> str:
    
    try:
        encrypted = base64.b64decode(ciphertext_b64)
        key = derive_key(password, len(encrypted))
        decrypted = bytes(a ^ b for a, b in zip(encrypted, key))
        return decrypted.decode()
    except Exception as e:
        logger.error(f""Error decrypting data: {str(e)}"")
        return f""Error: Could not decrypt data - {str(e)}""",Decrypt base64-encoded ciphertext with XOR.,Decrypts base64-encoded text using a password-derived key.,5.0,5.0,3.0,0.6206896551724138,0.3333333333333333,0.5,0.6283596158027649,0.66903424,0.8934822,0.3760056,0.9822689294815063,0.4285714285714285,0.609964887631638,4.0,5.0,5.0
190,190,190,191,_store_conversation_turn,"def _store_conversation_turn(
        self, prompt: str, response_content: Dict[str, Any], call_type: str
    ):
        
        if self.request_id not in _conversation_history:
            _conversation_history[self.request_id] = []

        # Add user message
        _conversation_history[self.request_id].append(
            {""role"": ""user"", ""content"": prompt}
        )

        # Add AI response based on call type
        ai_content = self._format_ai_response(response_content, call_type)
        if ai_content:
            _conversation_history[self.request_id].append(
                {""role"": ""assistant"", ""content"": ai_content}
            )

        # Update project metadata
        if self.request_id in _project_metadata:
            _project_metadata[self.request_id][""last_activity""] = time.time()",Store a conversation turn in memory.,Store and update conversation history and metadata for user interactions.,5.0,5.0,4.0,0.4794520547945205,0.2,0.3333333333333333,0.5086919069290161,0.22424532,0.5163798,0.6199913,0.9401211738586426,0.5,0.6439122857567897,5.0,5.0,5.0
191,191,191,192,get_vocab,"def get_vocab():
    
    _pad = ""$""
    _punctuation = ';:,.!?隆驴鈥斺��""芦禄"""" '
    _letters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz""
    _letters_ipa = ""蓱蓯蓲忙蓳蕶尾蓴蓵莽蓷蓶冒胜蓹蓸蓺蓻蓽蓾蔀蔁蕜伞蔂散蕸搔骚魔丧蕼扫瑟蕽森涩色僧薀杀莎砂艐沙刹纱酶傻筛胃艙啥蕵晒珊删苫蕗蕘山蕚蕛蕡失蕢蕣蕥獗笔屔Ｉな嵪囀幨徥懯愂捠斒∈暿⑶�莵莻莾藞藢藧藨始蚀拾时什史藸摔藶鈫撯啈鈫掆啑鈫�'泰'岬�""

    # Create vocabulary dictionary
    symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)
    return {symbol: i for i, symbol in enumerate(symbols)}",Get the vocabulary dictionary mapping characters to token IDs,Create a vocabulary dictionary mapping symbols to unique indices.,5.0,5.0,5.0,0.7692307692307693,0.4444444444444444,0.4444444444444444,0.4870209097862243,0.693483,0.59099466,0.86150444,0.8573278188705444,0.5555555555555556,0.5619454122231444,5.0,5.0,5.0
192,192,192,193,save_settings,"def save_settings(settings: Dict[str, Any]):
    
    # Ensure sensitive keys are not saved
    settings.pop(""provider_api_key"", None)
    try:
        with open(SETTINGS_FILE, ""w"") as f:
            json.dump(settings, f, indent=4)
        print(f""DEBUG - Saved settings to {SETTINGS_FILE}"")
    except IOError as e:
        print(f""Warning: Could not save settings to {SETTINGS_FILE}: {e}"")",Saves settings to the JSON file.,Remove sensitive data before saving configuration to a file.,5.0,5.0,3.0,0.4333333333333333,0.3333333333333333,0.5,0.6208500266075134,0.47034445,0.37254626,0.6560548,0.8616822361946106,0.3333333333333333,0.4891935821279539,5.0,5.0,5.0
193,193,193,194,_create_branch_ctx_for_sub_agent,"def _create_branch_ctx_for_sub_agent(
    agent: BaseAgent,
    sub_agent: BaseAgent,
    invocation_context: InvocationContext,
) -> InvocationContext:
  
  invocation_context = invocation_context.model_copy()
  branch_suffix = f""{agent.name}.{sub_agent.name}""
  invocation_context.branch = (
      f""{invocation_context.branch}.{branch_suffix}""
      if invocation_context.branch
      else branch_suffix
  )
  return invocation_context",Create isolated branch for every sub-agent.,Create a new invocation context with updated branch information for a sub-agent.,5.0,5.0,4.0,0.5,0.3846153846153846,0.7142857142857143,0.5920286178588867,0.5072089,0.55527455,0.6032087,0.9571699500083924,0.1666666666666666,0.764488826332457,5.0,5.0,5.0
194,194,194,195,build_list_tables_context,"def build_list_tables_context(keyspace_name: str, tables: List[TableInfo]) -> str:
    
    context = {
        'cassandra_knowledge': build_cassandra_knowledge(),
        'amazon_keyspaces_knowledge': build_amazon_keyspaces_knowledge(),
    }

    # Add table-specific guidance
    tables_guidance = {
        'data_modeling': 'In Cassandra, tables are containers for related data, similar to tablesin relational databases. '
        'However, Cassandra tables  are optimized for specific access patterns based on their primary key '
        'design. The primary key determines how data is distributed physically in the database, and the '
        'attributes that can be specified for efficient query execution. Primary keys consist of a '
        'partition key (which determines data distribution) and optional cluster columns which determine '
        'how data is ordered within a partition.',
        'naming_conventions': 'Table names typically use snake_case and should be descriptive of the entity they represent.',
    }
    context['tables_guidance'] = tables_guidance

    return dict_to_markdown(context)",Provide LLM context for tables.,Generate a markdown context for Cassandra and Amazon Keyspaces table guidance.,5.0,5.0,4.0,0.3333333333333333,0.2727272727272727,0.6,0.6506649255752563,0.48723763,0.2777018,0.42957553,0.8234027028083801,0.6363636363636364,0.6370450342038069,4.0,4.0,4.0
195,195,195,196,_generate_summary,"def _generate_summary(self) -> Dict[str, Any]:
        
        correct = sum(result[""score""] for result in self._results)
        return {
            ""total"": len(self._results),
            ""correct"": correct,
            ""results"": self._results,
            ""accuracy"": correct / len(self._results) if len(self._results) > 0 else 0,
        }",Generate and return a summary of the benchmark results.,Calculate and return performance metrics from results data.,5.0,5.0,4.0,0.7796610169491526,0.375,0.3333333333333333,0.542232871055603,0.50009304,0.667117,0.76116216,0.7851554155349731,0.25,0.4163271798382519,4.0,5.0,4.0
196,196,196,197,run_cmd,"def run_cmd(cmd: str, *, assert_success: bool = False, capture_output: bool = False) -> subprocess.CompletedProcess:
    
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            text=True,
            capture_output=capture_output,
            check=assert_success,
        )
        return result
    except subprocess.CalledProcessError as e:
        print(f""Command failed: {cmd}\nExit code: {e.returncode}"")
        sys.exit(e.returncode)",run a shell command and optionally assert success.,Execute shell command with options for success assertion and output capture,5.0,5.0,3.0,0.6,0.5454545454545454,0.5,0.4960106313228607,0.54595375,0.7957481,0.7213309,0.9793962240219116,0.0,0.4781498208922448,4.0,5.0,5.0
197,197,197,198,resolve_ref,"def resolve_ref(ref_string, current_doc):
      
      parts = ref_string.split(""/"")
      if parts[0] != ""#"":
        raise ValueError(f""External references not supported: {ref_string}"")

      current = current_doc
      for part in parts[1:]:
        if part in current:
          current = current[part]
        else:
          return None  # Reference not found
      return current",Resolves a single $ref string.,Resolve JSON reference paths within a document to retrieve nested values.,5.0,5.0,4.0,0.3698630136986301,0.1818181818181818,0.4,0.5323185324668884,0.29341862,0.21385354,0.043363787,0.9374070167541504,0.1818181818181818,0.3192508498384629,5.0,5.0,5.0
198,198,198,199,toggle_light,"def toggle_light(light_name: str, state: bool) -> dict[str, Any]:
    
    if not (bridge := _get_bridge()):
        return {""error"": ""Bridge not connected"", ""success"": False}
    try:
        result = bridge.set_light(light_name, ""on"", state)
        return {
            ""light"": light_name,
            ""set_on_state"": state,
            ""success"": True,
            ""phue2_result"": result,
        }
    except (KeyError, PhueException, Exception) as e:
        return handle_phue_error(light_name, ""toggle_light"", e)",Turns a specific light on (true) or off (false) using phue2.,Toggle a smart light's state and handle potential errors,5.0,5.0,4.0,0.6484187215795874,0.2,0.1818181818181818,0.4856508374214172,0.038861927,0.5179118,0.5614528,0.9019696712493896,0.2222222222222222,0.4563274640418993,5.0,5.0,5.0
199,199,199,200,list_files,"def list_files(relative_path: str = """") -> List[str]:
    
    path = (CONTEXT_PATH / relative_path).resolve()
    if not str(path).startswith(str(CONTEXT_PATH)):
        return [f""Access denied: {relative_path}""]
    if not path.exists() or not path.is_dir():
        return [f""Not a directory: {relative_path}""]
    return os.listdir(path)",List files and directories under CONTEXT_PATH/relative_path.,"Function lists directory contents, ensuring path security and validity.",5.0,4.0,4.0,0.5915492957746479,0.4444444444444444,0.3333333333333333,0.565467357635498,0.07177916,0.56760484,0.68559045,0.9842764735221864,0.2222222222222222,0.471552690736099,5.0,5.0,5.0
200,200,200,201,process_query,"def process_query(ner_query):
            
            candidate_entities = self.ner.invoke(ner_query, **kwargs)
            for candidate_entity in candidate_entities:
                query_type = candidate_entity.get_entity_first_type_or_un_std()

                ner_id = f""{candidate_entity.entity_name}_{query_type}""
                if ner_id not in ner_maps:
                    ner_maps[ner_id] = {
                        ""candidate"": candidate_entity,
                        ""query"": ner_query,
                        ""query_type"": query_type,
                    }",Process a single query in parallel.,Process named entity recognition queries and map unique entities.,5.0,5.0,3.0,0.4769230769230769,0.2222222222222222,0.3333333333333333,0.5163118839263916,0.3295855,0.23308684,0.60206103,0.8512675762176514,0.0,0.458735943772864,4.0,5.0,4.0
201,201,201,202,_create_project_entity,"def _create_project_entity(self, project_data):
        
        logger.debug(
            f""Creating project entity for: {project_data.get('key')} - {project_data.get('name')}""
        )
        # Use a composite ID format that includes the entity type for uniqueness
        entity_id = f""project-{project_data['id']}""

        return JiraProjectEntity(
            entity_id=entity_id,  # Modified to use unique ID
            breadcrumbs=[],  # top-level object, no parent
            project_key=project_data[""key""],
            name=project_data.get(""name""),
            description=project_data.get(""description""),
        )",Transform raw project data into a JiraProjectEntity.,"Create a unique project entity with ID, key, name, and description.",4.0,5.0,4.0,0.582089552238806,0.1818181818181818,0.1428571428571428,0.6842272281646729,0.3371025,0.550446,0.63702524,0.9795205593109132,0.4545454545454545,0.6850439041177648,4.0,4.0,4.0
202,202,202,203,_generate_context,"def _generate_context(self) -> str:
        
        context_parts = []

        for idx, intent in enumerate(self.intents.values(), 1):
            description = (
                f""{idx}. Intent: {intent.name}\nDescription: {intent.description}""
            )

            if intent.examples:
                examples = ""\n"".join(f""- {example}"" for example in intent.examples)
                description += f""\nExamples:\n{examples}""

            if intent.metadata:
                metadata = ""\n"".join(
                    f""- {key}: {value}"" for key, value in intent.metadata.items()
                )
                description += f""\nAdditional Information:\n{metadata}""

            context_parts.append(description)

        return ""\n\n"".join(context_parts)",Generate a formatted context string describing all intents,"Generate a formatted string detailing intents with descriptions, examples, and metadata.",5.0,5.0,5.0,0.6363636363636364,0.4545454545454545,0.625,0.6719428300857544,0.70348525,0.81918657,0.89381623,0.8939715623855591,0.3636363636363636,0.616872828954281,5.0,5.0,5.0
203,203,203,204,from_orm_with_collection_mapping,"def from_orm_with_collection_mapping(cls, obj):
        
        # Convert to dict and filter out SQLAlchemy internal attributes
        obj_dict = {k: v for k, v in obj.__dict__.items() if not k.startswith(""_"")}

        # Map the readable_collection_id to collection if needed
        if hasattr(obj, ""readable_collection_id""):
            obj_dict[""collection""] = obj.readable_collection_id

        return cls.model_validate(obj_dict)",Create a SourceConnection from a source_connection ORM model.,Convert ORM object to validated model dictionary with collection mapping.,5.0,5.0,4.0,0.6301369863013698,0.2,0.2222222222222222,0.6137136220932007,0.2735181,0.44516397,0.6369351,0.920685887336731,0.3,0.6814380528294784,5.0,5.0,4.0
204,204,204,205,_replace_agentlist_placeholder,"def _replace_agentlist_placeholder(cls: Type[""GroupManagerSelectionMessageContextStr""], v: Any) -> Union[str, Any]:  # noqa: N805
        
        if isinstance(v, str):
            if ""{agentlist}"" in v:
                return v.replace(""{agentlist}"", ""<<agent_list>>"")  # Perform the replacement
            else:
                return v  # If no replacement is needed, return the original value
        return """"",Replace {agentlist} placeholder before validation/assignment.,Replace placeholder in string with agent list identifier,5.0,5.0,3.0,0.7512656145937356,0.25,0.3333333333333333,0.562315821647644,0.3096008,0.64205843,0.6622602,0.6811630129814148,0.125,0.6364304349718717,4.0,5.0,4.0
205,205,205,206,from_pretrained0802,"def from_pretrained0802(self, config_path, model_path):
        
        model = self.from_hparams0802(config_path)
        state_dict_raw = torch.load(model_path, map_location=""cpu"")['state_dict']
        state_dict = dict()
        for k, v in state_dict_raw.items():
            if k.startswith('backbone.') or k.startswith('head.') or k.startswith('feature_extractor.'):
                state_dict[k] = v
        # if isinstance(model.feature_extractor, EncodecFeatures):
        #     encodec_parameters = {
        #         ""feature_extractor.encodec."" + key: value
        #         for key, value in model.feature_extractor.encodec.state_dict().items()
        #     }
        #     state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model",Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.,Load and initialize a model with filtered pretrained weights from specified paths.,5.0,5.0,4.0,0.5836426400330711,0.25,0.1428571428571428,0.5637432336807251,0.24883448,0.37848595,0.7370055,0.7646595239639282,0.1666666666666666,0.495783335616575,5.0,5.0,5.0
206,206,206,207,from_mcp_tool_result,"def from_mcp_tool_result(
        cls, result: CallToolResult, tool_use_id: str
    ) -> types.Content:
        
        if result.isError:
            function_response = {""error"": str(result.content)}
        else:
            function_response_parts = mcp_content_to_google_parts(result.content)
            function_response = {""result"": function_response_parts}

        function_response_part = types.Part.from_function_response(
            name=tool_use_id,
            response=function_response,
        )

        function_response_content = types.Content(
            role=""tool"", parts=[function_response_part]
        )

        return function_response_content",Convert an MCP tool result to an LLM input type,Convert tool result into structured content with error handling.,5.0,5.0,4.0,0.5625,0.3333333333333333,0.3,0.5680272579193115,0.28882444,0.28536493,0.81906056,0.9415189623832704,0.1111111111111111,0.4688857306644936,4.0,5.0,5.0
207,207,207,208,load_user_config,"def load_user_config():
    
    if Path(f'{args.model_dir}/config-user.yaml').exists():
        file_content = open(f'{args.model_dir}/config-user.yaml', 'r').read().strip()

        if file_content:
            user_config = yaml.safe_load(file_content)
        else:
            user_config = {}
    else:
        user_config = {}

    for model_name in user_config:
        user_config[model_name] = transform_legacy_kv_cache_options(user_config[model_name])

    return user_config",Loads custom model-specific settings,Load and transform user configuration from a YAML file if it exists,5.0,4.0,4.0,0.4626865671641791,0.0833333333333333,0.2,0.6087386012077332,0.23672155,0.46267548,0.5018576,0.9216140508651732,0.25,0.6712909392158426,4.0,5.0,4.0
208,208,208,209,shopping_get_sku_latest_review_author,"def shopping_get_sku_latest_review_author(sku: str) -> str:
    
    header = {
        ""Authorization"": f""Bearer {shopping_get_auth_token()}"",
        ""Content-Type"": ""application/json"",
    }
    response = requests.get(
        f""{SHOPPING}/rest/V1/products/{sku}/reviews"", headers=header
    )
    assert response.status_code == 200
    response_obj = response.json()
    if len(response_obj) == 0:
        return """"
    author: str = response_obj[-1][""nickname""]
    return author",Get the latest review for shopping admin.,Retrieve the latest review author's name for a product SKU from an API.,5.0,5.0,4.0,0.5211267605633803,0.2857142857142857,0.5714285714285714,0.5822599530220032,0.1288625,0.46369028,0.7038093,0.9212318658828736,0.0769230769230769,0.7199730166770995,5.0,5.0,5.0
209,209,209,210,__init__,"def __init__(self):
        
        self.reader = None
        # Determine best available device
        self.device = ""cpu""
        if torch.cuda.is_available():
            self.device = ""cuda""
        elif (
            hasattr(torch, ""backends"")
            and hasattr(torch.backends, ""mps"")
            and torch.backends.mps.is_available()
        ):
            self.device = ""mps""
        logger.info(f""OCR processor initialized with device: {self.device}"")",Initialize the OCR processor.,Initialize OCR processor with optimal device selection for execution.,5.0,5.0,4.0,0.4202898550724637,0.3333333333333333,0.75,0.557198703289032,0.78440076,0.7602045,0.5845032,0.9757722020149232,0.4444444444444444,0.6072989101598469,5.0,5.0,5.0
210,210,210,211,pkce_challenge,"def pkce_challenge():
    
    import base64
    import hashlib
    import secrets

    # Generate a code verifier
    code_verifier = secrets.token_urlsafe(64)[:128]

    # Create code challenge using S256 method
    code_verifier_bytes = code_verifier.encode(""ascii"")
    sha256 = hashlib.sha256(code_verifier_bytes).digest()
    code_challenge = base64.urlsafe_b64encode(sha256).decode().rstrip(""="")

    return {""code_verifier"": code_verifier, ""code_challenge"": code_challenge}",Create a PKCE challenge with code_verifier and code_challenge.,Generate PKCE code verifier and challenge for secure OAuth2 authentication.,5.0,5.0,5.0,0.72,0.5,0.5,0.6086394786834717,0.3453259,0.6593424,0.6565579,0.8571827411651611,0.4,0.594721164405873,5.0,5.0,5.0
211,211,211,212,add_tool,"def add_tool(
        self,
        fn: Callable[..., Any],
        name: str | None = None,
        description: str | None = None,
        annotations: ToolAnnotations | None = None,
    ) -> Tool:
        
        tool = Tool.from_function(
            fn, name=name, description=description, annotations=annotations
        )
        existing = self._tools.get(tool.name)
        if existing:
            if self.warn_on_duplicate_tools:
                logger.warning(f""Tool already exists: {tool.name}"")
            return existing
        self._tools[tool.name] = tool
        return tool",Add a tool to the server.,"Add a new tool to a collection, checking for duplicates and logging warnings.",5.0,5.0,4.0,0.3116883116883117,0.3076923076923077,0.6666666666666666,0.5510417222976685,0.72574335,0.467737,0.7071909,0.8799254298210144,0.2307692307692307,0.5811916222610386,5.0,5.0,5.0
212,212,212,213,cleanup_temp_files,"def cleanup_temp_files(self):
        
        try:
            for part_index in list(self._part_working_dirs.keys()):
                self.cleanup_part_working_dir(part_index)
            if self._is_temp_dir:
                logger.info(f""cleanup temp files: {self.working_dir}"")
                shutil.rmtree(self.working_dir)
        except Exception:
            logger.exception(""Error cleaning up temporary files"")",Clean up all temporary files including part working directories,Clean up temporary directories and log any errors encountered,5.0,5.0,4.0,0.8090980933366198,0.4444444444444444,0.4444444444444444,0.5941485166549683,0.36796603,0.7403151,0.82831967,0.9770691394805908,0.2222222222222222,0.523662231369325,4.0,5.0,5.0
213,213,213,214,print_validation_result,"def print_validation_result(result: dict, rel_path: Path):
    
    print(f""\nValidating: {rel_path}"")
    if ""error"" in result:
        print(f""Error: {result['error']}"")
    else:
        print(f""Duration: {result['duration']}"")
        print(f""Sample Rate: {result['sample_rate']} Hz"")
        print(f""Peak Amplitude: {result['peak_amplitude']}"")
        print(f""RMS Level: {result['rms_level']}"")
        print(f""DC Offset: {result['dc_offset']}"")

        if result[""issues""]:
            print(""\nIssues Found:"")
            for issue in result[""issues""]:
                print(f""- {issue}"")
        else:
            print(""\nNo issues found"")",Print full validation details for a single file.,"Display audio validation results, highlighting errors, metrics, and potential issues.",5.0,4.0,5.0,0.5058823529411764,0.1,0.125,0.5358006358146667,0.37156835,0.32670456,0.63814557,0.8727839589118958,0.1,0.5118864515891971,5.0,5.0,5.0
214,214,214,215,get_api_key,"def get_api_key(self, context: SkillContext) -> Optional[str]:
        
        # Check agent config first
        agent_api_key = context.config.get(""api_key"")
        if agent_api_key:
            logger.debug(f""Using agent-specific Venice API key for skill {self.name}"")
            return agent_api_key

        # Fallback to system config
        system_api_key = self.skill_store.get_system_config(""venice_api_key"")
        if system_api_key:
            logger.debug(f""Using system Venice API key for skill {self.name}"")
            return system_api_key

        logger.warning(
            f""No Venice API key found in agent or system config for skill {self.name}""
        )
        return None","Get the API key, prioritizing agent config then system config.",Retrieve API key from agent or system configuration for a specific skill,5.0,5.0,4.0,0.6944444444444444,0.3333333333333333,0.4,0.5975427627563477,0.3197693,0.574795,0.80620027,0.9576973915100098,0.6666666666666666,0.6937493190388476,5.0,5.0,5.0
215,215,215,216,analyze_text,"def analyze_text(self, text_to_analyze, analysis_instructions):
        
        # Assemble the message.
        text_to_analyze = ""# TEXT\n"" + text_to_analyze + ""\n""
        analysis_instructions = ""# INSTRUCTIONS\n"" + analysis_instructions + ""\n""
        msg_text = ""\n"".join([
            analysis_instructions,
            text_to_analyze,
            analysis_instructions,
        ])  # Repeat the instructions.
        # Generate and return the analysis string.
        return self.generate_oai_reply([{""role"": ""user"", ""content"": msg_text}], None, None)[1]","Analyzes the given text as instructed, and returns the analysis.",Constructs a message for text analysis based on user instructions and processes it.,5.0,4.0,3.0,0.6746987951807228,0.3076923076923077,0.3,0.6268907785415649,0.3528995,0.6696366,0.5618218,0.8667186498641968,0.3846153846153846,0.5957220314764488,4.0,5.0,4.0
216,216,216,217,_get_save_path,"def _get_save_path(self, description: str) -> str:
        
        timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
        output_dir = os.path.join(os.path.dirname(__file__), ""results"")
        os.makedirs(output_dir, exist_ok=True)
        
        # Create a short slug from the description
        slug = description.lower()[:30].replace("" "", ""_"")
        return os.path.join(output_dir, f""red_teaming_{slug}_{timestamp}.csv"")",Generate a path for saving the final DataFrame.,Generate a timestamped file path for saving results based on a description.,5.0,5.0,4.0,0.5733333333333334,0.4166666666666667,0.625,0.6332532167434692,0.6561665,0.67034376,0.8239442,0.8577625751495361,0.25,0.5909414585972103,5.0,5.0,5.0
217,217,217,218,process_revision_directives,"def process_revision_directives(context, revision, directives):
    
    for directive in directives:
        if isinstance(directive, ops.MigrationScript):
            # Get the current timestamp
            timestamp = time.strftime(""%Y%m%d%H%M%S"")
            # Modify the revision ID to include the timestamp
            directive.rev_id = f""{timestamp}_{directive.rev_id}""",Automatically prepend timestamp to migration filenames.,Update migration script IDs with timestamps in revision directives.,5.0,5.0,4.0,0.6417910447761194,0.2222222222222222,0.1666666666666666,0.5645009279251099,0.27880624,0.73754287,0.73978966,0.9045027494430542,0.5555555555555556,0.6727326310311015,5.0,5.0,5.0
218,218,218,219,get_profile_features,"def get_profile_features(
    msa: np.ndarray, deletion_matrix: np.ndarray
) -> FeatureDict:
  
  num_restypes = residue_names.POLYMER_TYPES_NUM_WITH_UNKNOWN_AND_GAP
  profile = msa_profile.compute_msa_profile(
      msa=msa, num_residue_types=num_restypes
  )

  return {
      'profile': profile.astype(np.float32),
      'deletion_mean': np.mean(deletion_matrix, axis=0),
  }",Returns the MSA profile and deletion_mean features.,Extracts profile and deletion mean features from sequence alignment data.,5.0,5.0,4.0,0.6164383561643836,0.5,0.625,0.545566976070404,0.30228683,0.5497097,0.5447094,0.8269079923629761,0.1,0.5355937419713156,5.0,5.0,5.0
219,219,219,220,delete_subscription_by_stripe_id,"def delete_subscription_by_stripe_id(db_session: Session, stripe_subscription_id: str) -> None:
    
    subscription = get_subscription_by_stripe_id(db_session, stripe_subscription_id)
    if not subscription:
        logger.warning(
            f""Subscription not found for stripe_subscription_id={stripe_subscription_id} during delete attempt.""
        )
        return

    db_session.delete(subscription)
    db_session.flush()
    logger.info(
        ""Deleted subscription"",
        extra={""stripe_subscription_id"": stripe_subscription_id},
    )",Mark a subscription as deleted by Stripe subscription ID.,Remove subscription from database using Stripe ID if it exists,5.0,5.0,4.0,0.7580645161290323,0.3,0.3333333333333333,0.5498908758163452,0.45243397,0.9032295,0.7588775,0.9944998025894164,0.4,0.6504941892841272,5.0,5.0,5.0
220,220,220,221,_safe_save,"def _safe_save(self, output_dir: str):
        
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,Safely save model state to specified directory if saving is enabled.,4.0,5.0,4.0,0.5147058823529411,0.1818181818181818,0.25,0.5801029205322266,0.30564615,0.39441395,0.5964811,0.908378839492798,0.1818181818181818,0.5719508162948574,4.0,5.0,5.0
221,221,221,222,__str__,"def __str__(self):
        
        error_message = ""({0})\n"" ""Reason: {1}\n"".format(self.status, self.reason)
        if self.headers:
            error_message += ""HTTP response headers: {0}\n"".format(self.headers)

        if self.body:
            error_message += ""HTTP response body: {0}\n"".format(self.body)

        return error_message",Custom error messages for exception,Generate a formatted error message from HTTP response details.,5.0,5.0,5.0,0.5,0.2222222222222222,0.4,0.6286871433258057,0.46263927,0.41100034,0.7688943,0.96890789270401,0.3333333333333333,0.5024166685080709,5.0,5.0,5.0
222,222,222,223,_save_image,"def _save_image(self, image_data_base64: str) -> str:
        
        image_data = base64.b64decode(image_data_base64)
        # Randomly generate a filename.
        filename = f""{uuid.uuid4().hex}.png""
        path = os.path.join(self.output_dir, filename)
        with open(path, ""wb"") as f:
            f.write(image_data)
        return os.path.abspath(path)",Save image data to a file.,"Decode base64 image data, save as PNG, and return file path.",5.0,5.0,5.0,0.4166666666666667,0.3636363636363636,0.5,0.6121618151664734,0.345559,0.4825201,0.71408606,0.8274030685424805,0.2727272727272727,0.612986058624291,5.0,5.0,5.0
223,223,223,224,setUp,"def setUp(self):
        
        # Create mock modules
        self.mock_qdrant = MagicMock()
        self.mock_models = MagicMock()
        self.mock_qdrant.models = self.mock_models
        
        # Create the module patcher
        self.module_patcher = patch.dict('sys.modules', {
            'qdrant_client': self.mock_qdrant,
            'qdrant_client.models': self.mock_models
        })
        self.module_patcher.start()
        
        # Import after mocking
        from deepsearcher.vector_db import Qdrant
        from deepsearcher.loader.splitter import Chunk
        from deepsearcher.vector_db.base import RetrievalResult
        
        self.Qdrant = Qdrant
        self.Chunk = Chunk
        self.RetrievalResult = RetrievalResult",Set up test fixtures.,Set up test environment with mocked modules for deepsearcher components.,5.0,5.0,5.0,0.2777777777777778,0.3,0.75,0.641305148601532,0.47809386,0.46644062,0.5598432,0.9522305727005004,0.1,0.5589583843976219,5.0,5.0,5.0
224,224,224,225,execute_method,"def execute_method(self, method: Union[str, bytes], *args, **kwargs):
        
        if self.vllm_tp_rank == 0 and method != ""execute_model"":
            print(f""[DP={self.vllm_dp_rank},TP={self.vllm_tp_rank}] execute_method: {method if isinstance(method, str) else 'Callable'}"")
        return self.rollout.execute_method(method, *args, **kwargs)",Called by ExternalRayDistributedExecutor collective_rpc.,Execute a method with logging for specific conditions and delegate execution,5.0,4.0,5.0,0.5131578947368421,0.0,0.0,0.6200000643730164,-0.09058739,0.38071597,0.029290428,0.8561122417449951,0.2727272727272727,0.4466629552074488,4.0,5.0,4.0
225,225,225,226,visualize_scroll,"def visualize_scroll(self, direction: str, clicks: int, img_base64: str) -> None:
        
        if (
            not self.agent.save_trajectory
            or not hasattr(self.agent, ""experiment_manager"")
            or not self.agent.experiment_manager
        ):
            return

        try:
            # Use the visualization utility
            img = visualize_scroll(direction, clicks, img_base64)

            # Save the visualization
            self.agent.experiment_manager.save_action_visualization(
                img, ""scroll"", f""{direction}_{clicks}""
            )
        except Exception as e:
            logger.error(f""Error visualizing scroll: {str(e)}"")",Visualize a scroll action by drawing arrows on the screenshot.,Visualize and save scroll action using base64 image data if conditions are met.,5.0,5.0,4.0,0.6708860759493671,0.2307692307692307,0.3,0.5937885046005249,0.49340162,0.37457907,0.6659483,0.9632166624069214,0.2307692307692307,0.5524738831265316,5.0,5.0,5.0
226,226,226,227,_passages2string,"def _passages2string(retrieval_result):
    
    format_reference = """"
    for idx, doc_item in enumerate(retrieval_result):
        content = doc_item[""document""][""contents""]
        title = content.split(""\n"")[0]
        text = ""\n"".join(content.split(""\n"")[1:])
        format_reference += f""Doc {idx + 1} (Title: {title})\n{text}\n\n""
    return format_reference.strip()",Convert retrieval results to formatted string.,Convert retrieved documents into formatted string summaries.,5.0,5.0,5.0,0.7333333333333333,0.5714285714285714,0.6666666666666666,0.5484346151351929,0.6742938,0.8667185,0.7922747,0.7806447744369507,0.0,0.4706825015705319,5.0,5.0,5.0
227,227,227,228,format_search_results,"def format_search_results(self, results: list[tuple[BaseTool, str, float]]) -> str:
        

        # Only show top_k results
        results = results

        formatted_output = ""Search results\n\n""

        for i, (tool, server_name, score) in enumerate(results):
            # Format score as percentage
            if i < 5:
                score_pct = f""{score * 100:.1f}%""
                logger.info(f""{i}: {tool.name} ({score_pct} match)"")
            formatted_output += f""[{i + 1}] Tool: {tool.name} ({score_pct} match)\n""
            formatted_output += f""    Server: {server_name}\n""
            formatted_output += f""    Description: {tool.description}\n\n""

        # Add footer with information about how to use the results
        formatted_output += (
            ""\nTo use a tool, connect to the appropriate server first, then invoke the tool.""
        )

        return formatted_output",Format search results in a consistent format.,Format and display top search results with tool details and usage instructions.,5.0,5.0,4.0,0.5443037974683544,0.25,0.4285714285714285,0.5410774350166321,0.42846173,0.5394616,0.76158,0.8821830749511719,0.6666666666666666,0.6755074055942557,4.0,4.0,5.0
228,228,228,229,session_init_data,"def session_init_data(self) -> list[dict[str, Any]]:
        
        session_update = {
            ""turn_detection"": {""type"": ""server_vad""},
            ""voice"": self._voice,
            ""modalities"": [""audio"", ""text""],
            ""temperature"": self._temperature,
        }
        return [{""type"": ""session.update"", ""session"": session_update}]",Control initial session with OpenAI.,"Initialize session data with voice, modalities, and detection settings",5.0,5.0,4.0,0.4285714285714285,0.3333333333333333,0.6,0.5726958513259888,0.27901566,0.49110156,0.6421318,0.9114428758621216,0.0,0.5959901881857749,5.0,5.0,5.0
229,229,229,230,_is_supported,"def _is_supported(self, file_path: str) -> bool:
        
        ext = Path(file_path).suffix.lower()
        if ext not in self.SUPPORTED_EXTENSIONS:
            logger.warning(f""Unsupported file extension: {ext} for file: {file_path}"")
            return False
        return True",Check if the file extension is supported.,"Check if a file's extension is within supported types, logging warnings otherwise.",5.0,5.0,5.0,0.5,0.4615384615384615,0.8571428571428571,0.645439863204956,0.63941336,0.7399711,0.70738417,0.9263256788253784,0.3333333333333333,0.5696209956100418,5.0,5.0,5.0
230,230,230,231,generate_diff_html,"def generate_diff_html(a, b):
    
    seq_matcher = SequenceMatcher(None, a, b)
    output_html = """"
    for opcode, a0, a1, b0, b1 in seq_matcher.get_opcodes():
        if opcode == ""equal"":
            output_html += a[a0:a1]
        elif opcode == ""insert"":
            output_html += f""<span class='added'>{b[b0:b1]}</span>""
        elif opcode == ""delete"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span>""
        elif opcode == ""replace"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span><span class='added'>{b[b0:b1]}</span>""
    return output_html",Generates HTML with differences between strings a and b.,Generate HTML highlighting differences between two text sequences.,5.0,5.0,5.0,0.7424242424242424,0.5,0.4444444444444444,0.5216941833496094,0.46231917,0.7205079,0.819109,0.9717472791671752,0.0,0.5305307900152759,5.0,5.0,5.0
231,231,231,232,get_dataset_info,"def get_dataset_info(cls) -> Dict[str, str]:
        
        return {
            ""id"": ""simpleqa"",
            ""name"": ""SimpleQA"",
            ""description"": ""Simple question-answering evaluation dataset"",
            ""url"": cls.get_default_dataset_path(),
        }",Get basic information about the dataset.,Returns metadata for a SimpleQA question-answering dataset.,5.0,5.0,5.0,0.576271186440678,0.125,0.1666666666666666,0.6259832382202148,0.43015537,0.5792612,0.50919163,0.9286969304084778,0.4285714285714285,0.5037796392799716,5.0,5.0,5.0
232,232,232,233,_build_docker_image,"def _build_docker_image(self):
    
    if not self.docker_path:
      raise ValueError('Docker path is not set.')
    if not os.path.exists(self.docker_path):
      raise FileNotFoundError(f'Invalid Docker path: {self.docker_path}')

    logger.info('Building Docker image...')
    self._client.images.build(
        path=self.docker_path,
        tag=self.image,
        rm=True,
    )
    logger.info('Docker image: %s built.', self.image)",Builds the Docker image.,"Builds a Docker image from a specified path, logging progress and errors.",5.0,5.0,5.0,0.3287671232876712,0.25,0.75,0.6221504211425781,0.7561332,0.5782639,0.2851833,0.968301236629486,0.4166666666666667,0.6027684124193151,4.0,5.0,5.0
233,233,233,234,_create_relevant_node,"def _create_relevant_node(
        self, node: Dict[str, Any], relevance_score: float, reason: str
    ) -> Dict[str, Any]:
        
        relevant_node = {
            ""id"": node[""id""],
            ""name"": node[""name""],
            ""type"": node[""type""],
            ""relevance_score"": relevance_score,
            ""reason"": reason,
            ""children"": [],
        }

        for field in [""file_path"", ""start_line"", ""end_line"", ""relationship""]:
            if field in node:
                relevant_node[field] = node[field]

        return relevant_node",Create a node with relevance information,Constructs a node dictionary with relevance metadata and optional fields.,5.0,5.0,3.0,0.547945205479452,0.4,0.6666666666666666,0.510909378528595,0.50453615,0.6327576,0.8031275,0.7930824756622314,0.2,0.6251225728177408,5.0,5.0,5.0
234,234,234,235,get_speaker_selection_result,"def get_speaker_selection_result(self, groupchat: ""GroupChat"") -> Optional[Union[Agent, str]]:
        
        if self.agent_name is not None:
            # Find the agent by name in the groupchat
            for agent in groupchat.agents:
                if agent.name == self.agent_name:
                    return agent
            raise ValueError(f""Agent '{self.agent_name}' not found in groupchat."")
        elif self.speaker_selection_method is not None:
            return self.speaker_selection_method
        elif self.terminate is not None and self.terminate:
            return None
        else:
            raise ValueError(
                ""Unable to establish speaker selection result. No terminate, agent, or speaker selection method provided.""
            )",Get the speaker selection result.,Determine speaker selection in group chat based on agent name or method,5.0,5.0,4.0,0.4225352112676056,0.1666666666666666,0.4,0.6121364831924438,0.5041849,0.5601335,0.5572481,0.8931816816329956,0.6666666666666666,0.6533703398912142,4.0,5.0,4.0
235,235,235,236,file_uri_to_path,"def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    
    parsed = urlparse(file_uri)
    if parsed.scheme != ""file"":
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path",Convert a file URI to a local file path,Convert file URI to local file path and network location.,5.0,5.0,5.0,0.6842105263157895,0.7,0.7777777777777778,0.5959070324897766,0.6893345,0.8139022,0.93888545,0.9470927119255066,0.3,0.5190100709128689,5.0,5.0,5.0
236,236,236,237,_is_video_file,"def _is_video_file(self, file: bytes, filename: str) -> bool:
        
        try:
            kind = filetype.guess(file)
            return kind is not None and kind.mime.startswith(""video/"")
        except Exception as e:
            logging.error(f""Error detecting file type: {str(e)}"")
            return False",Check if the file is a video file.,Determine if a file is a video based on its MIME type.,5.0,5.0,4.0,0.5,0.4166666666666667,0.625,0.6581978797912598,0.57059383,0.54481447,0.8052004,0.9037581086158752,0.5833333333333334,0.5993697641751092,5.0,5.0,5.0
237,237,237,238,context,"def context(self) -> ""Context"":
        
        # First try instance context
        if self._context is not None:
            return self._context

        try:
            # Fall back to global context if available
            from mcp_agent.core.context import get_current_context

            return get_current_context()
        except Exception as e:
            raise RuntimeError(
                f""No context available for {self.__class__.__name__}. ""
                ""Either initialize MCPApp first or pass context explicitly.""
            ) from e","Get context, with graceful fallback to global context if needed.","Retrieve instance or global context, raising error if unavailable.",5.0,5.0,5.0,0.7272727272727273,0.3333333333333333,0.3,0.6537988185882568,0.19658479,0.52651256,0.7022137,0.8917464017868042,0.5555555555555556,0.5030192863002184,5.0,5.0,5.0
238,238,238,239,get_system_message,"def get_system_message() -> Dict[str, str]:
    
    return {
        ""role"": ""system"",
        ""content"": ,
    }",Return the standard system message for Morphik's query agent.,Define a function to return a dictionary with system role and content keys.,5.0,5.0,4.0,0.64,0.1538461538461538,0.2,0.6193315386772156,0.25120372,0.41373017,0.71994483,0.186603844165802,0.2307692307692307,0.5316806919303404,2.0,5.0,4.0
239,239,239,240,format_step_result,"def format_step_result(step_result: StepResult) -> str:
    
    tasks_str = ""\n"".join(
        f""  - {format_task_result(task)}"" for task in step_result.task_results
    )
    return STEP_RESULT_TEMPLATE.format(
        step_description=step_result.step.description,
        step_result=step_result.result,
        tasks_str=tasks_str,
    )",Format a step result for display to planners,Format step results into a structured string using a template.,5.0,5.0,4.0,0.6290322580645161,0.4,0.375,0.5543288588523865,0.18498546,0.5572624,0.68857646,0.9494355916976928,0.2,0.5923024073985095,4.0,5.0,5.0
240,240,240,241,update_presigned_url,"def update_presigned_url(self, presigned_url, base_url):
        
        #To Do: If Proxy URL has domain name how do we handle such cases
        
        presigned_parts = urlparse(presigned_url)
        base_parts = urlparse(base_url)
        # Check if base_url contains localhost or an IP address
        if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
            new_netloc = base_parts.hostname  # Extract domain from base_url
            if base_parts.port:  # Add port if present in base_url
                new_netloc += f"":{base_parts.port}""
            updated_parts = presigned_parts._replace(netloc=new_netloc)
            return urlunparse(updated_parts)
        return presigned_url","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL only if the base URL contains 'localhost' or an IP address.",Update presigned URL's domain to match base URL if it's localhost or IP.,5.0,5.0,4.0,0.3246208629772402,0.6,0.2857142857142857,0.6869399547576904,0.27119163,0.7424176,0.7741761,0.965293288230896,0.5384615384615384,0.6780948285290945,5.0,5.0,4.0
241,241,241,242,get_audio_files,"def get_audio_files(df):
    
    audios = []
    for index, row in df.iterrows():
        number = row['number']
        line_count = len(eval(row['lines']) if isinstance(row['lines'], str) else row['lines'])
        for line_index in range(line_count):
            temp_file = OUTPUT_FILE_TEMPLATE.format(f""{number}_{line_index}"")
            audios.append(temp_file)
    return audios",Generate a list of audio file paths,Generate audio file paths based on data frame content,5.0,5.0,4.0,0.6226415094339622,0.4444444444444444,0.5714285714285714,0.4967616200447082,0.59399587,0.9106453,0.87222916,0.7767612934112549,0.2222222222222222,0.4924014555744831,4.0,5.0,4.0
242,242,242,243,resolve,"def resolve(self, override: ModelSettings | None) -> ModelSettings:
        
        if override is None:
            return self

        changes = {
            field.name: getattr(override, field.name)
            for field in fields(self)
            if getattr(override, field.name) is not None
        }
        return replace(self, **changes)",Produce a new ModelSettings by overlaying any non-None values from the override on top of this instance.,Merge default settings with overrides to produce updated configuration.,5.0,5.0,4.0,0.5663259256864519,0.2222222222222222,0.0555555555555555,0.5835825800895691,0.038757578,0.58475316,0.6739266,0.6403449177742004,0.1111111111111111,0.5038901708519377,5.0,5.0,5.0
243,243,243,244,generate_new_ids,"def generate_new_ids(self):
        
        # Generate new UUID
        dev_device_id = str(uuid.uuid4())

        # Generate new machineId (64 characters of hexadecimal)
        machine_id = hashlib.sha256(os.urandom(32)).hexdigest()

        # Generate new macMachineId (128 characters of hexadecimal)
        mac_machine_id = hashlib.sha512(os.urandom(64)).hexdigest()

        # Generate new sqmId
        sqm_id = ""{"" + str(uuid.uuid4()).upper() + ""}""

        self.update_machine_id_file(dev_device_id)

        return {
            ""telemetry.devDeviceId"": dev_device_id,
            ""telemetry.macMachineId"": mac_machine_id,
            ""telemetry.machineId"": machine_id,
            ""telemetry.sqmId"": sqm_id,
            ""storage.serviceMachineId"": dev_device_id,  # Add storage.serviceMachineId
        }",Generate new machine ID,Generate unique identifiers for telemetry and storage purposes.,5.0,5.0,4.0,0.2857142857142857,0.125,0.25,0.5866208076477051,0.44805062,0.26086345,0.62705594,0.6816763281822205,0.25,0.5969943909255622,4.0,5.0,5.0
244,244,244,245,_format_child_blocks,"def _format_child_blocks(
        self, block_content: dict, block: dict, block_type: str, page_breadcrumbs: List[Breadcrumb]
    ) -> str:
        
        if block_type == ""child_page"":
            title = block_content.get(""title"", ""Untitled Page"")
            return f""馃搫 **[{title}]** (Child Page)""
        else:  # child_database
            return self._format_child_database_block(block_content, block, page_breadcrumbs)",Format child page and database blocks.,Formats child blocks as either a page or database based on type.,5.0,5.0,4.0,0.59375,0.4166666666666667,0.6666666666666666,0.6003019213676453,0.5919199,0.70857245,0.7849834,0.9199937582015992,0.25,0.6326762013325415,4.0,5.0,5.0
245,245,245,246,load_ioi_tests_for_year,"def load_ioi_tests_for_year(year: int) -> dict[str, dict[str, tuple[str, str]]]:
    
    tests_dataset = load_dataset(""open-r1/ioi-test-cases"", name=f""{year}"", split=""train"")
    test_cases = defaultdict(dict)
    for test_case in tests_dataset:
        test_cases[test_case[""problem_id""]][test_case[""test_name""]] = test_case[""test_input""], test_case[""test_output""]
    return test_cases",Load IOI tests for a given year.,Load and organize IOI test cases by year into a structured dictionary.,5.0,5.0,4.0,0.4285714285714285,0.4166666666666667,0.5714285714285714,0.6362426280975342,0.43331128,0.73031557,0.6598391,0.9641126990318298,0.1666666666666666,0.7290733477341932,5.0,5.0,5.0
246,246,246,247,get_api_version,"def get_api_version(cls) -> str:
        
        logger.debug(""Retrieving DeepSeek Azure API version from config"")
        provider_config = CONFIG.llm_endpoints.get(""deepseek_azure"")
        if provider_config and provider_config.api_version:
            logger.debug(f""DeepSeek Azure API version: {provider_config.api_version}"")
            return provider_config.api_version
        logger.warning(""DeepSeek Azure API version not found in config"")
        return None",Get DeepSeek Azure API version from config,Retrieve and log the DeepSeek Azure API version from configuration settings.,5.0,5.0,4.0,0.5394736842105263,0.4545454545454545,0.7142857142857143,0.6996840238571167,0.75332075,0.671474,0.8743998,0.946639120578766,0.5454545454545454,0.7201298809747719,5.0,5.0,5.0
247,247,247,248,_update_analyze_status_failed,"def _update_analyze_status_failed(self, doc_id: int) -> None:
        
        try:
            with self._repository._db.session() as session:
                document = session.get(self._repository.model, doc_id)
                if document:
                    document.analyze_status = ProcessStatus.FAILED
                    session.commit()
                    logger.debug(f""Updated analyze status for document {doc_id} to FAILED"")
                else:
                    logger.warning(f""Document not found with id: {doc_id}"")
        except Exception as e:
            logger.error(f""Error updating document analyze status: {str(e)}"")",update status as failed,Update document status to FAILED in database and log the outcome,5.0,5.0,4.0,0.3437499999999999,0.2727272727272727,0.75,0.4761110842227936,0.3085,0.49165905,0.68657094,0.8944357633590698,0.3636363636363636,0.6363931016883307,5.0,5.0,5.0
248,248,248,249,matches,"def matches(self, uri: str) -> dict[str, Any] | None:
        
        # Convert template to regex pattern
        pattern = self.uri_template.replace(""{"", ""(?P<"").replace(""}"", "">[^/]+)"")
        match = re.match(f""^{pattern}$"", uri)
        if match:
            return match.groupdict()
        return None",Check if URI matches template and extract parameters.,Extracts parameters from a URI using a template-based regex pattern.,5.0,5.0,4.0,0.6911764705882353,0.3636363636363636,0.25,0.6771364212036133,0.3383041,0.7787325,0.7268159,0.6529864072799683,0.5,0.5752199062036881,5.0,5.0,5.0
249,249,249,250,_retry_if_not_cancelled_and_failed,"def _retry_if_not_cancelled_and_failed(retry_state):
    
    if retry_state.outcome.failed:
        exception = retry_state.outcome.exception()
        # Don't retry on CancelledError
        if isinstance(exception, asyncio.CancelledError):
            logger.debug(""Operation was cancelled, not retrying"")
            return False
        # Retry on network related errors
        if isinstance(
            exception, httpx.HTTPError | ConnectionError | ValueError | TimeoutError
        ):
            logger.warning(f""Network error occurred: {exception}, will retry"")
            return True
    # Don't retry on success
    return False",Only retry if the exception is not CancelledError and the attempt failed.,Determine retry eligibility based on operation failure and exception type,5.0,5.0,3.0,0.8082191780821918,0.3,0.1666666666666666,0.620893657207489,0.18624967,0.68502855,0.67622316,0.930624544620514,0.3,0.57527958700641,5.0,5.0,5.0
250,250,250,251,validate_config_values,"def validate_config_values(self):
        
        for key, value in self.__dict__.items():
            if isinstance(value, dict):
                raise ValueError(f""Value for '{key}' must not be a dictionary (depth 0 only)"")
        return self",Validate that no values are dictionaries (depth 0).,Ensure configuration values are not dictionaries at the top level.,5.0,5.0,4.0,0.6515151515151515,0.3,0.375,0.648651123046875,0.15031764,0.58480895,0.7574088,0.8302600383758545,0.3,0.5697123269122899,5.0,5.0,5.0
251,251,251,252,prompt_fn,"def prompt_fn(line, task_name: str = None):
    
    return Doc(
        task_name=task_name,
        query=line[""problem""],
        choices=[line[""solution""]],
        gold_index=0,
    )",Assumes the model is either prompted to emit \boxed{answer} or does so automatically,Create a document object with task details and solution choices.,5.0,5.0,3.0,0.617300686923729,0.0,0.0,0.5356211066246033,-0.16235983,0.26552764,0.59188193,0.739607572555542,0.1,0.5099188999543666,4.0,4.0,4.0
252,252,252,253,_get_gradio_app_code,"def _get_gradio_app_code(self, tool_module_name: str = ""tool"") -> str:
        
        class_name = self.__class__.__name__
        return textwrap.dedent(
            f
        )",Get the Gradio app code.,Generates Gradio app code using the class name and tool module.,5.0,5.0,5.0,0.3809523809523809,0.3636363636363636,0.6,0.6020796895027161,0.5876586,0.7424404,0.59924257,0.8684122562408447,0.0,0.6284228781213512,1.0,4.0,4.0
253,253,253,254,to_dict,"def to_dict(self) -> dict:
        
        return {
            ""id"": self.id,
            ""space_id"": self.space_id,
            ""sender_endpoint"": self.sender_endpoint,
            ""content"": self.content,
            ""message_type"": self.message_type,
            ""round"": self.round,
            ""create_time"": self.create_time.isoformat(),
            ""role"": self.role
        }",Convert DTO to dictionary,Convert object attributes to a dictionary representation with formatted timestamps.,5.0,5.0,4.0,0.2650602409638554,0.3,0.75,0.5157296657562256,0.35180223,0.642046,0.7053882,0.8324585556983948,0.1,0.4390717287303995,5.0,4.0,5.0
254,254,254,255,from_runnable_config,"def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        values: dict[str, Any] = {
            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))
            for f in fields(cls)
            if f.init
        }
        return cls(**{k: v for k, v in values.items() if v})",Create a Configuration instance from a RunnableConfig.,Create a configuration object from environment variables and optional settings.,5.0,5.0,4.0,0.620253164556962,0.4,0.5714285714285714,0.5754202008247375,0.45856297,0.53206575,0.7278217,0.942884087562561,0.2,0.3978423401282401,5.0,5.0,5.0
255,255,255,256,get_equation_hash,"def get_equation_hash(equation, bg_color=""white"", text_color=""black"", font_size=24):
    
    params_str = f""{equation}|{bg_color}|{text_color}|{font_size}""
    return hashlib.sha1(params_str.encode(""utf-8"")).hexdigest()",Calculate SHA1 hash of the equation string and rendering parameters.,Generate a unique hash for an equation with customizable display settings.,5.0,5.0,4.0,0.7837837837837838,0.1818181818181818,0.2,0.5873761773109436,0.42715472,0.6197639,0.6986042,0.8870594501495361,0.0909090909090909,0.5682171323709322,5.0,5.0,5.0
256,256,256,257,q_posterior_mean_variance,"def q_posterior_mean_variance(self, x_start, x_t, t):
        
        assert x_start.shape == x_t.shape
        posterior_mean = (
            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start
            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t
        )
        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)
        posterior_log_variance_clipped = _extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)
        assert (
            posterior_mean.shape[0]
            == posterior_variance.shape[0]
            == posterior_log_variance_clipped.shape[0]
            == x_start.shape[0]
        )
        return posterior_mean, posterior_variance, posterior_log_variance_clipped","Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)",Calculate posterior mean and variance for a given time step in a sequence.,5.0,5.0,3.0,0.6820525207492915,0.3076923076923077,0.1764705882352941,0.6301222443580627,-0.12169816,0.61417353,0.6565222,0.946980595588684,0.1538461538461538,0.5533033273164744,4.0,5.0,5.0
257,257,257,258,convert_to_srt_format,"def convert_to_srt_format(start_time, end_time):
    
    def seconds_to_hmsm(seconds):
        seconds = seconds % 60
        milliseconds = int(seconds * 1000) % 1000
        return f""{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}""

    start_srt = seconds_to_hmsm(start_time)
    end_srt = seconds_to_hmsm(end_time)
    return f""{start_srt} --> {end_srt}""","Convert time (in seconds) to the format: hours:minutes:seconds,milliseconds","Convert time intervals to SRT subtitle format with hours, minutes, seconds, and milliseconds.",5.0,5.0,4.0,0.7311827956989247,0.6153846153846154,0.7272727272727273,0.7123023867607117,0.32022774,0.7586349,0.6067091,0.896510124206543,0.1538461538461538,0.5675011849374364,4.0,5.0,5.0
258,258,258,259,save_report,"def save_report(report, filename=None):
    
    if filename is None:
        filename = f""test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt""
    with open(filename, 'w') as file:
        file.write(report)
    print(f""Report saved to {os.path.abspath(filename)}"")",Save the report to a file.,Automatically save a report to a timestamped text file if no filename is provided.,5.0,5.0,4.0,0.2926829268292683,0.3571428571428571,0.8333333333333334,0.6864638924598694,0.3532493,0.62993777,0.5726719,0.9629654288291932,0.6428571428571429,0.6269167841559333,5.0,5.0,5.0
259,259,259,260,initialize_cache,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f'before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f'after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )",Initialize the KV cache in all workers.,Initialize and log cache configuration for GPU and CPU blocks in a distributed system.,5.0,5.0,3.0,0.4069767441860465,0.2142857142857142,0.4285714285714285,0.5928723812103271,0.41068673,0.37268606,0.7160021,0.8999134302139282,0.5,0.6806958903182607,5.0,5.0,5.0
260,260,260,261,get_db_context,"def get_db_context():
    
    if not _db_manager:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database manager not initialized"",
        )
    try:
        yield _db_manager
    except Exception as e:
        logger.error(f""Database operation failed: {str(e)}"")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database operation failed"",
        ) from e",Provide a transactional scope around a series of operations.,Ensure database manager is initialized and handle operation errors,5.0,5.0,3.0,0.7272727272727273,0.1111111111111111,0.1111111111111111,0.5497061610221863,0.22004768,0.2716055,0.559126,0.7543903589248657,0.3333333333333333,0.5122402879192983,5.0,5.0,4.0
261,261,261,262,get_processor,"def get_processor(cls, file_type: FileType) -> Type[BaseFileProcessor]:
        
        if not cls._initialized:
            cls.init()
        print(f""Current registered processors: {cls._processors}"")
        if file_type not in cls._processors:
            raise ValueError(f""No processor found for {file_type}"")
        return cls._processors[file_type]",Get processor before ensuring initialization,"Retrieve appropriate file processor based on file type, initializing if necessary.",5.0,5.0,4.0,0.5121951219512195,0.1818181818181818,0.4,0.6205538511276245,0.22455448,0.44723865,0.7779138,0.856975257396698,0.2727272727272727,0.6116776433788843,5.0,5.0,5.0
262,262,262,263,tf_roll_out,"def tf_roll_out(
    agent: Agent, env: ScriptBrowserEnv, config_file: str
) -> list[Any]:
    
    obs, state_info = env.reset(options={""config_file"": config_file})

    trajectory: list[Any] = [{""observation"": obs, ""info"": state_info}]
    while True:
        action = agent.next_action(
            trajectory=trajectory, intent="""", meta_data={}
        )
        trajectory.append(action)
        if action[""action_type""] == ActionTypes.STOP:
            break

        # preceed to next action
        obs, reward, terminated, truncated, info = env.step(action)
        state_info = {""observation"": obs, ""info"": info}
        trajectory.append(state_info)

    return trajectory",Roll out the agent using teacher forcing actions,Simulate agent interactions in environment using configuration file,5.0,5.0,3.0,0.6567164179104478,0.25,0.25,0.4943174123764038,0.30655983,0.38011318,0.60336375,0.5527456402778625,0.25,0.391739104753363,4.0,5.0,4.0
263,263,263,264,show_permission_dialog,"def show_permission_dialog(code: str, action_description: str):
    
    if platform.system() == ""Darwin"":
        result = os.system(
            f'osascript -e \'display dialog ""Do you want to execute this action?\n\n{code} which will try to {action_description}"" with title ""Action Permission"" buttons {{""Cancel"", ""OK""}} default button ""OK"" cancel button ""Cancel""\''
        )
        return result == 0
    elif platform.system() == ""Linux"":
        result = os.system(
            f'zenity --question --title=""Action Permission"" --text=""Do you want to execute this action?\n\n{code}"" --width=400 --height=200'
        )
        return result == 0
    return False",Show a platform-specific permission dialog and return True if approved.,Display a permission dialog for executing actions on macOS and Linux,4.0,4.0,3.0,0.7457734295245487,0.3636363636363636,0.3636363636363636,0.6089715957641602,0.3077027,0.5553686,0.73174876,0.8857569694519043,0.2727272727272727,0.7038454598534359,4.0,5.0,5.0
264,264,264,265,__launch_kwargs,"def __launch_kwargs(self):
        
        launch_kwargs = {'headless': self.headless, 'ignore_default_args': self.harmful_default_args, 'channel': 'chrome' if self.real_chrome else 'chromium'}
        if self.stealth:
            launch_kwargs.update({'args': self.__set_flags(), 'chromium_sandbox': True})

        return launch_kwargs",Creates the arguments we will use while launching playwright's browser,Configure browser launch settings based on user preferences and stealth mode.,5.0,5.0,3.0,0.7272727272727273,0.1818181818181818,0.0909090909090909,0.604828953742981,0.08575234,0.32503104,0.73977506,0.8483110070228577,0.0,0.4624545618692256,4.0,5.0,5.0
265,265,265,266,clean_response,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            raise ValueError(""No JSON object found in response"")
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,"Extract and parse JSON from a string, handling errors if parsing fails.",5.0,5.0,4.0,0.6056338028169014,0.25,0.2222222222222222,0.665818989276886,0.3325147,0.6018596,0.67609394,0.6968168020248413,0.4166666666666667,0.3348860019528871,5.0,5.0,5.0
266,266,266,267,make_request,"def make_request(prompt):
    
    payload = {""input"": prompt}
    
    try:
        response = requests.post(
            API_URL,
            json=payload,
            stream=True
        )
        
        print(f""\nMaking request with prompt: '{prompt}'\n"")
        print(""Raw response:"")
        for line in response.iter_lines():
            if line:
                print(line.decode('utf-8'))
                
    except Exception as e:
        print(f""Error making request: {e}"")",Make request and print raw response,Send input data to API and print streaming response.,5.0,5.0,3.0,0.5576923076923077,0.3333333333333333,0.5,0.5934405326843262,0.5174563,0.61055833,0.68191224,0.9003278613090515,0.1111111111111111,0.4231215626065294,5.0,5.0,5.0
267,267,267,268,_merge_audio,"def _merge_audio(files, output: str) -> bool:
    
    try:
        # Create an empty audio segment
        combined = AudioSegment.empty()
        silence = AudioSegment.silent(duration=100)  # 100ms silence
        
        # Add audio files one by one
        for file in files:
            audio = AudioSegment.from_wav(file)
            combined += audio + silence
        combined += silence
        combined.export(output, format=""wav"", parameters=[""-acodec"", ""pcm_s16le"", ""-ar"", ""16000"", ""-ac"", ""1""])
        
        if os.path.getsize(output) == 0:
            rprint(f""[red]Output file size is 0"")
            return False
            
        rprint(f""[green]Successfully merged audio files"")
        return True
        
    except Exception as e:
        rprint(f""[red]Failed to merge audio: {str(e)}"")
        return False","Merge audio files, add a brief silence",Merge multiple audio files with silence into a single WAV output.,5.0,5.0,3.0,0.4769230769230769,0.4545454545454545,0.5714285714285714,0.5937188267707825,0.45669493,0.7324443,0.8117528,0.9261584877967834,0.5454545454545454,0.6224462175967184,5.0,5.0,5.0
268,268,268,269,pop_context,"def pop_context():
    
    if _context_stack:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f""Popping context for atom {ctx.atom_name}"")
        _context_stack.pop()
    else:
        logger.warning(""[DAG] Attempted to pop context, but stack was empty"")","Pop the topmost context off the stack, ending dependency tracking for the current atom.","Remove the top context from the stack, logging actions and warnings.",5.0,5.0,4.0,0.6561393912318161,0.3636363636363636,0.2857142857142857,0.6716957092285156,0.47263873,0.49135426,0.69825244,0.7699052095413208,0.2727272727272727,0.5332052491214325,5.0,5.0,5.0
269,269,269,270,remove_obsolete_ckpt,"def remove_obsolete_ckpt(path: str, global_step: int, save_limit: int = -1, directory_format: str = ""global_step_{}""):
    
    if save_limit <= 0:
        return

    if not os.path.exists(path):
        return

    pattern = re.escape(directory_format).replace(r""\{\}"", r""(\d+)"")
    ckpt_folders = []
    for folder in os.listdir(path):
        if match := re.match(pattern, folder):
            step = int(match.group(1))
            if step < global_step:
                ckpt_folders.append((step, folder))

    ckpt_folders.sort(reverse=True)
    for _, folder in ckpt_folders[save_limit - 1 :]:
        folder_path = os.path.join(path, folder)
        shutil.rmtree(folder_path, ignore_errors=True)
        print(f""Removed obsolete checkpoint: {folder_path}"")",Remove the obsolete checkpoints that exceed the save_limit.,Remove outdated checkpoints based on step threshold and retention limit.,5.0,5.0,4.0,0.7222222222222222,0.3,0.3333333333333333,0.5118671655654907,0.4082637,0.8312186,0.82550645,0.6554567813873291,0.3,0.5549066800257318,4.0,5.0,5.0
270,270,270,271,_get_endpoint_config,"def _get_endpoint_config(self):
        
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is a Qdrant endpoint
        if endpoint_config.db_type != ""qdrant"":
            error_msg = f""Endpoint {self.endpoint_name} is not a Qdrant endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config",Get the Qdrant endpoint configuration from CONFIG,Retrieve and validate Qdrant endpoint configuration by name,5.0,5.0,4.0,0.6610169491525424,0.375,0.4285714285714285,0.572891116142273,0.5603291,0.8072423,0.78411686,0.9566904306411744,0.375,0.6836664112486331,5.0,5.0,5.0
271,271,271,272,_safe_save,"def _safe_save(self, output_dir: str):
        
        if self.deepspeed:
            torch.cuda.synchronize()
            self.save_model(output_dir)
            return
    
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,"Safely save model state to directory, handling GPU synchronization.",4.0,5.0,4.0,0.5223880597014925,0.2222222222222222,0.25,0.5532246828079224,0.3270875,0.4170001,0.5848268,0.9471116065979004,0.0,0.6195380239312884,4.0,4.0,4.0
272,272,272,273,validate_fold_input,"def validate_fold_input(fold_input: folding_input.Input):
  
  for i, chain in enumerate(fold_input.protein_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing unpaired MSA.')
    if chain.paired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing paired MSA.')
    if chain.templates is None:
      raise ValueError(f'Protein chain {i + 1} is missing Templates.')
  for i, chain in enumerate(fold_input.rna_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'RNA chain {i + 1} is missing unpaired MSA.')",Validates the fold input contains MSA and templates for featurisation.,Validate presence of MSA and templates in protein and RNA chains,5.0,5.0,3.0,0.7824698418109669,0.3636363636363636,0.4,0.57237309217453,0.19949134,0.5308516,0.7453879,0.7805728912353516,0.2727272727272727,0.5398910595076383,5.0,5.0,4.0
273,273,273,274,get_unrealspeech_skill,"def get_unrealspeech_skill(
    name: str,
    store: SkillStoreABC,
) -> UnrealSpeechBaseTool:
    
    if name == ""text_to_speech"":
        if name not in _cache:
            _cache[name] = TextToSpeech(
                skill_store=store,
            )
        return _cache[name]
    else:
        raise ValueError(f""Unknown UnrealSpeech skill: {name}"")",Get an UnrealSpeech skill by name.,Retrieve or cache text-to-speech skill from UnrealSpeech tool repository.,4.0,5.0,3.0,0.3972602739726027,0.1818181818181818,0.1666666666666666,0.6506311893463135,0.46615812,0.5149741,0.47782582,0.7691370248794556,0.2222222222222222,0.6855923244866984,4.0,5.0,4.0
274,274,274,275,handle_money,"def handle_money(m: re.Match[str]) -> str:
    

    bill = ""dollar"" if m.group(2) == ""$"" else ""pound""
    coin = ""cent"" if m.group(2) == ""$"" else ""pence""
    number = m.group(3)

    multiplier = m.group(4)
    try:
        number = float(number)
    except:
        return m.group()

    if m.group(1) == ""-"":
        number *= -1

    if number % 1 == 0 or multiplier != """":
        text_number = f""{INFLECT_ENGINE.number_to_words(conditional_int(number))}{multiplier} {INFLECT_ENGINE.plural(bill, count=number)}""
    else:
        sub_number = int(str(number).split(""."")[-1].ljust(2, ""0""))

        text_number = f""{INFLECT_ENGINE.number_to_words(int(round(number)))} {INFLECT_ENGINE.plural(bill, count=number)} and {INFLECT_ENGINE.number_to_words(sub_number)} {INFLECT_ENGINE.plural(coin, count=sub_number)}""

    return text_number",Convert money expressions to spoken form,"Converts currency amounts from symbols to words, handling dollars and pounds.",5.0,5.0,4.0,0.4415584415584415,0.1818181818181818,0.3333333333333333,0.6041728258132935,0.20016024,0.6836287,0.6721291,0.8118353486061096,0.0909090909090909,0.4855563525719367,4.0,5.0,4.0
275,275,275,276,hotkey,"def hotkey(self, keys: List):
        
        # Normalize any 'cmd' to 'command'
        keys = [_normalize_key(k) for k in keys]
        # add quotes around the keys
        keys = [f""'{key}'"" for key in keys]
        return f""import pyautogui; pyautogui.hotkey({', '.join(keys)}, interval=1)""",Press a hotkey combination,Generate a PyAutoGUI hotkey command with normalized key inputs.,5.0,5.0,4.0,0.3809523809523809,0.2222222222222222,0.5,0.6290686130523682,0.481462,0.64454126,0.69033843,0.6591266393661499,0.4444444444444444,0.6393120552590349,5.0,5.0,5.0
276,276,276,277,print_model_size,"def print_model_size(model: nn.Module, name: Optional[str] = None) -> None:
    
    if is_rank0():
        n_params, scale = _get_model_size(model, scale=""auto"")
        if name is None:
            name = model.__class__.__name__

        print(f""{name} contains {n_params:.2f}{scale} parameters."")",Print the model size.,Display neural network model size if on primary process,5.0,5.0,4.0,0.3090909090909091,0.2222222222222222,0.5,0.5692728757858276,0.525888,0.42120034,0.47096992,0.9047294855117798,0.1111111111111111,0.3837208951658257,5.0,5.0,4.0
277,277,277,278,read_pdf_file,"def read_pdf_file(file_path):
    
    try:
        doc = fitz.open(file_path)
        # Read only the first few pages to speed up processing
        num_pages_to_read = 3  # Adjust as needed
        full_text = []
        for page_num in range(min(num_pages_to_read, len(doc))):
            page = doc.load_page(page_num)
            full_text.append(page.get_text())
        pdf_content = '\n'.join(full_text)
        return pdf_content
    except Exception as e:
        print(f""Error reading PDF file {file_path}: {e}"")
        return None",Read text content from a PDF file.,Extracts text from the first few pages of a PDF file for quick processing.,5.0,5.0,4.0,0.4189189189189189,0.3571428571428571,0.7142857142857143,0.670591413974762,0.18557017,0.52047396,0.72458214,0.9096412658691406,0.6428571428571429,0.4623862044791492,5.0,5.0,5.0
278,278,278,279,_sanitize_input,"def _sanitize_input(self, args: tuple, kwargs: dict) -> dict:
            

            def sanitize_value(value):
                if isinstance(value, (int, float, bool, str)):
                    return value
                elif isinstance(value, list):
                    return [sanitize_value(item) for item in value]
                elif isinstance(value, dict):
                    return {key: sanitize_value(val) for key, val in value.items()}
                else:
                    return str(value)  # Convert non-standard types to string

            return {
                ""args"": [sanitize_value(arg) for arg in args],
                ""kwargs"": {key: sanitize_value(val) for key, val in kwargs.items()},
            }","Sanitize and format input data, including handling of nested lists and dictionaries.",Sanitize and convert input arguments to standard data types,5.0,5.0,3.0,0.5991266839040348,0.4444444444444444,0.3333333333333333,0.4910480380058288,0.46096757,0.7162839,0.7694523,0.8373197317123413,0.3333333333333333,0.5468993635109068,5.0,5.0,4.0
279,279,279,280,cache_s3_files,"def cache_s3_files(dataset: Dataset, pdf_cache_location: str, num_proc: int = 32) -> Dataset:
    

    # Define the download function to use in parallel processing
    def cache_file(example):
        s3_path = example[""s3_path""]
        if s3_path:
            # Download the file and cache it locally
            local_path = _cache_s3_file(s3_path, pdf_cache_location)
            return {""local_pdf_path"": local_path}
        return {""local_pdf_path"": None}

    # Map the caching function to the dataset (with parallelism if needed)
    dataset = dataset.map(cache_file, num_proc=num_proc, load_from_cache_file=False)

    return dataset",Caches all S3 paths in the dataset to the local cache directory.,Parallelly cache S3 files locally from a dataset using specified processors.,4.0,5.0,3.0,0.6973684210526315,0.3636363636363636,0.25,0.5787105560302734,0.40751663,0.6286882,0.80245817,0.8745176196098328,0.4545454545454545,0.6312577248687724,5.0,5.0,4.0
280,280,280,281,mock_response,"def mock_response(self) -> list[dict[str, Any]]:
        
        return [
            {
                ""title"": ""Test Result"",
                ""body"": ""This is a test snippet."",  # Use 'body' as per duckduckgo-search
            }
        ]",Provide a mock response fixture for testing.,Returns a mock list of dictionaries with test data for response simulation.,5.0,5.0,4.0,0.5333333333333333,0.4166666666666667,0.4285714285714285,0.6660517454147339,0.7059369,0.71417475,0.6615566,0.7526968121528625,0.25,0.6627258323889567,5.0,5.0,5.0
281,281,281,282,create_query_fasta_file,"def create_query_fasta_file(sequence: str, path: str, linewidth: int = 80):
  
  with open(path, 'w') as f:
    f.write('>query\n')

    i = 0
    while i < len(sequence):
      f.write(f'{sequence[i:(i + linewidth)]}\n')
      i += linewidth",Creates a fasta file with the sequence with line width limit.,Write a FASTA file with a sequence split by line width.,5.0,5.0,3.0,0.7499245347600448,0.6363636363636364,0.6363636363636364,0.5897536277770996,0.5870247,0.8414195,0.82620955,0.5428547263145447,0.2727272727272727,0.5672660812196048,5.0,5.0,4.0
282,282,282,283,start,"def start(self) -> str:
        
        try:
            self.logger.info(f""Started new session with Session ID: {self.session_id}"")
        except Exception as e:
            logger.error(f""[file_logger] Failed to create logging file: {e}"")
        finally:
            return self.session_id",Start the logger and return the session_id.,Initialize session logging and return session identifier,5.0,5.0,3.0,0.625,0.4285714285714285,0.375,0.46146160364151,0.51007265,0.68460715,0.6958918,0.9340585470199584,0.5714285714285714,0.581511470653759,4.0,5.0,5.0
283,283,283,284,load_configs,"def load_configs(prefix: str) -> Dict[str, Any]:
    
    keys = [key for key in os.environ.keys() if key.startswith(f""{prefix}_"")]
    configs = {}
    for key in keys:
        config_key = key.removeprefix(f""{prefix}_"").lower()
        value = os.getenv(key)
        try:
            configs[config_key] = json.loads(value) if value.startswith(""["") else value
        except json.JSONDecodeError:
            configs[config_key] = value
    return configs",load configurations based on the given prefix.,Extracts and parses environment variables with a specified prefix into a configuration dictionary.,5.0,5.0,3.0,0.4591836734693877,0.1538461538461538,0.1428571428571428,0.6348539590835571,0.40444207,0.3690332,0.62484145,0.9087013006210328,0.1538461538461538,0.5124492299999986,5.0,5.0,5.0
284,284,284,285,write_to_file,"def write_to_file(filename: str, content: str) -> str:
    
    with open(filename, ""w"") as f:
        f.write(content)
    return f""Content successfully written to {filename}""",Write content to a file with the specified filename.,This function saves specified text to a file and confirms success.,5.0,5.0,4.0,0.6666666666666666,0.3636363636363636,0.3333333333333333,0.6205293536186218,0.41165912,0.6538548,0.7679546,0.9223913550376892,0.1818181818181818,0.3794681215926857,5.0,5.0,5.0
285,285,285,286,get_usage,"def get_usage(response) -> dict:
        
        # ...  # pragma: no cover
        return {
            ""prompt_tokens"": response.usage.prompt_tokens,
            ""completion_tokens"": response.usage.completion_tokens,
            ""total_tokens"": response.usage.total_tokens,
            ""cost"": response.cost,
            ""model"": response.model,
        }",Return usage summary of the response using RESPONSE_USAGE_KEYS.,Extracts and returns token usage and cost details from a response object.,5.0,5.0,4.0,0.5205479452054794,0.25,0.3,0.6209690570831299,0.11810573,0.4651939,0.72206163,0.7938169240951538,0.1666666666666666,0.5731103010298145,5.0,5.0,5.0
286,286,286,287,delete_integration_credential,"def delete_integration_credential(mapper, connection, target):
    
    if target.integration_credential_id:
        # Get the session
        session = Session.object_session(target)
        if session:
            # If we're in a session, use the session to delete the IntegrationCredential
            from airweave.models.integration_credential import IntegrationCredential

            credential = session.get(IntegrationCredential, target.integration_credential_id)
            if credential:
                session.delete(credential)
        else:
            # If we're not in a session, use the connection directly
            connection.execute(
                f""DELETE FROM integration_credential WHERE id = '{target.integration_credential_id}'""  # noqa: E501
            )","When a Connection is deleted, also delete its IntegrationCredential if present.",Delete integration credentials using session or direct connection based on context.,5.0,5.0,3.0,0.7831325301204819,0.1818181818181818,0.0909090909090909,0.5377298593521118,0.12544693,0.72567534,0.6206017,0.9769359230995178,0.4545454545454545,0.5200913115301262,5.0,5.0,4.0
287,287,287,288,analyze_schema_types,"def analyze_schema_types(filename: str) -> Counter:
    
    all_types = Counter()
    
    with open(filename) as f:
        for line in f:
            items = line.strip().split('\t')
            if (len(items) < 2):
                continue
            js = json.loads(items[1])
            try:
                all_types.update(extract_types(js))
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON line: {line[:100]}..."")
                continue
            
    return all_types",Analyze a JSONL file containing  markup and return all types found with counts,"Analyze and count JSON schema types from a file, handling parsing errors.",5.0,5.0,3.0,0.8058820674818595,0.5,0.2307692307692307,0.5853202939033508,0.2765205,0.63421386,0.80254704,0.9488661885261536,0.1666666666666666,0.590964519159557,5.0,5.0,5.0
288,288,288,289,create_default_config,"def create_default_config(self) -> bool:
        
        try:
            if self.config_path.exists():
                return True
                
            default_config = {
                ""mcpServers"": {},
                ""log_level"": ""INFO""
            }
            
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w') as f:
                json.dump(default_config, f, indent=2)
                
            logger.info(f""Created default configuration at {self.config_path}"")
            return True
            
        except Exception as e:
            logger.error(f""Error creating default configuration: {e}"")
            return False",Create a default configuration file if none exists,"Initialize default configuration file if it doesn't exist, logging success or errors.",5.0,5.0,4.0,0.5647058823529412,0.3846153846153846,0.625,0.6139714121818542,0.6090985,0.7463651,0.8426431,0.954960823059082,0.3333333333333333,0.5539482762560998,5.0,5.0,5.0
289,289,289,290,teardown,"def teardown(self) -> None:
        
        logger.info(""Stopping services"")
        try:
            # Don't remove volumes by default to enable reuse
            self.docker.stop(remove_volumes=False)
        except Exception as e:
            logger.error(f""Error during service teardown: {e}"")
            raise",Stop all services after tests.,The code logs and handles errors while stopping services without removing volumes.,4.0,5.0,4.0,0.3292682926829268,0.1666666666666666,0.4,0.6268819570541382,0.3613252,0.4319418,0.49869227,0.942370057106018,0.1666666666666666,0.5320590016215343,5.0,5.0,5.0
290,290,290,291,save_fid_stats,"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers, flag=""ref"")
    np.savez_compressed(paths[1], mu=m1, sigma=s1)",Calculates the FID of two paths,Compute and save statistical data from a specified path using a neural model.,5.0,5.0,3.0,0.3376623376623376,0.0769230769230769,0.1666666666666666,0.6425248384475708,0.20523688,0.28167576,0.5374057,0.825316309928894,0.3076923076923077,0.5392887200831649,4.0,4.0,4.0
291,291,291,292,pytest_sessionfinish,"def pytest_sessionfinish(session, exitstatus):
    
    cov_dir = os.path.abspath(""."")
    if exitstatus == 0 and os.environ.get(""COVERAGE_PROCESS_START""):
        try:
            cov = coverage.Coverage()
            cov.combine(data_paths=[cov_dir], strict=True)
            cov.save()
        except Exception as e:
            print(f""Error combining coverage data: {e}"", file=sys.stderr)",Combine coverage data from subprocesses at the end of the test session.,Finalize test session by merging coverage data if tests pass.,5.0,5.0,3.0,0.7096505184409414,0.4,0.25,0.5576198101043701,0.341893,0.7261844,0.7772752,0.8290847539901733,0.3,0.5637233349666513,5.0,5.0,5.0
292,292,292,293,_build_param_buffer,"def _build_param_buffer(self, pp_rank):
        
        if pp_rank == self._pp_rank:
            from verl.utils.memory_buffer import MemoryBuffer

            # The code here is very hard-coded, based on the following assumptions:
            # 1. `len(_this_rank_models) == 1`
            # 2. `_this_rank_models[0]` is a instance of `DistributedDataParallel` and `use_distributed_optimizer=True`
            # 3. Only bfloat16 data type is used in parameters
            source = self._this_rank_models[0].buffers[0].param_data
            self.memory_buffers[pp_rank] = {torch.bfloat16: MemoryBuffer(source.numel(), source.numel(), torch.bfloat16, source)}
        else:
            model = self.pp_models[pp_rank]
            weight_buffer_meta = get_weight_buffer_meta_from_module(model)
            self.memory_buffers[pp_rank] = build_memory_buffer(weight_buffer_meta)",Build the parameter buffer in each pp rank,Constructs memory buffers for model parameters based on rank and data type assumptions.,5.0,5.0,3.0,0.4367816091954023,0.2307692307692307,0.25,0.6013541221618652,0.27856502,0.5890261,0.7413234,0.8187046051025391,0.6153846153846154,0.6480189305025742,4.0,5.0,4.0
293,293,293,294,MMMU_preproc,"def MMMU_preproc(data):
    
    print(""Preprocessing MMMU dataset..."")
    cnt = 0
    As, Bs, Ans = list(data['A']), list(data['B']), list(data['answer'])
    lt = len(data)
    for i in range(lt):
        if pd.isna(As[i]):
            As[i] = Ans[i]
            Bs[i] = 'Other Answers'
            cnt += 1
    print(f'During MMMU_preproc in Evaluation, {cnt} open questions are re-formulated to multi-choice ones.')
    data['A'] = As
    data['B'] = Bs
    return data",Preprocess MMMU dataset to reformulate open questions to multi-choice ones.,Refine dataset by converting open-ended questions to multiple-choice format,5.0,5.0,3.0,0.8,0.4545454545454545,0.4545454545454545,0.5965592861175537,0.4627967,0.7557534,0.66397345,0.5511632561683655,0.3333333333333333,0.5204441242159076,4.0,5.0,4.0
294,294,294,295,parse_expected_output,"def parse_expected_output(cls, v):
        
        if isinstance(v, str):
            try:
                return json.loads(v)
            except json.JSONDecodeError as e:
                logger.error(f""Error parsing expected_output: {str(e)}"")
                raise ValueError(""Invalid JSON format for expected_output"")
        return v",Ensure expected_output is a dictionary,"Convert string input to JSON, logging errors if parsing fails.",5.0,5.0,4.0,0.4193548387096774,0.0,0.0,0.6408408880233765,0.09610337,0.34567443,0.4293589,0.9538061618804932,0.3,0.3926882807981022,5.0,5.0,5.0
295,295,295,296,get_latest_files,"def get_latest_files(directory: str, file_types: list = ['.webm', '.zip']) -> Dict[str, Optional[str]]:
    
    latest_files: Dict[str, Optional[str]] = {ext: None for ext in file_types}

    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        return latest_files

    for file_type in file_types:
        try:
            matches = list(Path(directory).rglob(f""*{file_type}""))
            if matches:
                latest = max(matches, key=lambda p: p.stat().st_mtime)
                # Only return files that are complete (not being written)
                if time.time() - latest.stat().st_mtime > 1.0:
                    latest_files[file_type] = str(latest)
        except Exception as e:
            print(f""Error getting latest {file_type} file: {e}"")

    return latest_files",Get the latest recording and trace files,Retrieve the most recent files of specified types from a directory.,5.0,5.0,3.0,0.5074626865671642,0.1818181818181818,0.2857142857142857,0.5929088592529297,0.34550664,0.60703015,0.7368208,0.9759311676025392,0.2727272727272727,0.6182123955583186,4.0,5.0,5.0
296,296,296,297,run_code,"def run_code(
        self,
        code: str,
    ) -> dict[str, Any]:
        
        with Sandbox(api_key=self.api_key) as sandbox:
            execution = sandbox.run_code(code)
            return {""text"": execution.text}",Execute code in E2B sandbox and return the result.,Execute code in a secure environment and return the output text.,5.0,5.0,4.0,0.671875,0.5454545454545454,0.6666666666666666,0.6363511085510254,0.67082024,0.6153882,0.7885855,0.9319831132888794,0.2727272727272727,0.4911057019675079,4.0,5.0,5.0
297,297,297,298,_trajectory_to_text,"def _trajectory_to_text(trajectory: Dict[str, Any]) -> str:
    
    text = ""AGENT TRAJECTORY\n"" + ""=""*50 + ""\n\n""
    
    # Add answers/responses
    if ""answer"" in trajectory and isinstance(trajectory[""answer""], list):
        for i, answer in enumerate(trajectory[""answer""]):
            text += f""STEP {i+1}:\n""
            text += f""Agent Response:\n{answer}\n\n""
    
    # Add parsed responses if available
    if ""parsed_response"" in trajectory and isinstance(trajectory[""parsed_response""], list):
        text += ""\nPARSED RESPONSES\n"" + ""-""*50 + ""\n\n""
        for i, parsed in enumerate(trajectory[""parsed_response""]):
            text += f""Step {i+1} Parsed:\n""
            for key, value in parsed.items():
                if value:
                    text += f""  {key}: {value}\n""
            text += ""\n""
    
    return text",Convert a trajectory to formatted text.,Converts agent trajectory data into a formatted text report.,5.0,5.0,3.0,0.65,0.5555555555555556,0.6666666666666666,0.5593206882476807,0.55129784,0.75359464,0.81144404,0.8845508098602295,0.4444444444444444,0.5933922896749895,5.0,5.0,5.0
298,298,298,299,get_dataset_info,"def get_dataset_info(cls) -> Dict[str, str]:
        
        return {
            ""id"": ""browsecomp"",
            ""name"": ""BrowseComp"",
            ""description"": ""Web browsing comprehension evaluation dataset"",
            ""url"": cls.get_default_dataset_path(),
        }",Get basic information about the dataset.,Return metadata for a web browsing comprehension evaluation dataset.,5.0,5.0,4.0,0.5735294117647058,0.1111111111111111,0.1666666666666666,0.6411733031272888,0.36926806,0.39684778,0.59464866,0.9189961552619934,0.6666666666666666,0.4631298228336677,5.0,5.0,5.0
299,299,299,300,_run,"def _run(self, **kwargs) -> str:
        
        if not self.server_manager.active_server:
            return ""No MCP server is currently active, so there's nothing to disconnect from.""

        server_name = self.server_manager.active_server
        try:
            # Clear the active server
            self.server_manager.active_server = None

            # Note: We're not actually closing the session here, just 'deactivating'
            # This way we keep the session cache without requiring reconnection if needed again

            return f""Successfully disconnected from MCP server '{server_name}'.""
        except Exception as e:
            logger.error(f""Error disconnecting from server '{server_name}': {e}"")
            return f""Failed to disconnect from server '{server_name}': {str(e)}""",Disconnect from the currently active MCP server.,"Disconnects from active server, handling errors and preserving session cache.",5.0,4.0,4.0,0.5324675324675324,0.4,0.5714285714285714,0.6251689195632935,0.55356693,0.5981575,0.5470549,0.9099602699279784,0.6,0.4539452079619545,5.0,5.0,5.0
300,300,300,301,on_signal,"def on_signal(self, signal_name: str) -> Callable:
        

        def decorator(func: Callable) -> Callable:
            unique_name = f""{signal_name}_{uuid.uuid4()}""

            async def wrapped(value: SignalValueT):
                try:
                    if asyncio.iscoroutinefunction(func):
                        await func(value)
                    else:
                        func(value)
                except Exception as e:
                    # Log the error but don't fail the entire signal handling
                    print(f""Error in signal handler {signal_name}: {str(e)}"")

            self._handlers.setdefault(signal_name, []).append((unique_name, wrapped))
            return wrapped

        return decorator",Register a handler for a signal.,Decorator registers async or sync signal handlers with error logging.,5.0,5.0,4.0,0.4347826086956521,0.3,0.3333333333333333,0.578642725944519,0.41947287,0.6009072,0.545032,0.930742621421814,0.6,0.3604300555689391,5.0,5.0,4.0
301,301,301,302,download_model,"def download_model(model_name):
    
    assert model_name in pretrained_models
    local_path = f""output/pretrained_models/{model_name}""
    if not os.path.isfile(local_path):
        hf_endpoint = os.environ.get(""HF_ENDPOINT"")
        if hf_endpoint is None:
        os.makedirs(""output/pretrained_models"", exist_ok=True)
        web_path = f""""
        download_url(web_path, ""output/pretrained_models/"")
    model = torch.load(local_path, map_location=lambda storage, loc: storage)
    return model",Downloads a pre-trained Sana model from the web.,Download and load a pretrained model from a specified source if not locally available.,4.0,4.0,4.0,0.5116279069767442,0.2857142857142857,0.4444444444444444,0.6404028534889221,0.43903774,0.55664,0.62650037,0.9224565029144288,0.3571428571428571,0.640479671156995,3.0,4.0,4.0
302,302,302,303,_save_search_results_to_json,"def _save_search_results_to_json(results: List[Dict[str, Any]], output_dir: str):
    
    search_file = os.path.join(output_dir, SEARCH_INFO_FILENAME)
    try:
        # Simple overwrite for now, could be append
        with open(search_file, ""w"", encoding=""utf-8"") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f""Search results saved to {search_file}"")
    except Exception as e:
        logger.error(f""Failed to save search results to {search_file}: {e}"")",Appends or overwrites search results to a JSON file.,"Save search results as a JSON file in the specified directory, logging success or failure.",5.0,5.0,3.0,0.5444444444444444,0.4,0.5555555555555556,0.6975820064544678,0.34237617,0.55831146,0.7905059,0.8974007964134216,0.4,0.6565508420592252,5.0,5.0,5.0
303,303,303,304,add_knowledge_base,"def add_knowledge_base(request: KnowledgeBaseRequest):
    
    # In a real implementation, this might validate the KB ID with AWS
    return {
        'message': f'Knowledge base {request.kb_id} added successfully',
        'kb_id': request.kb_id,
    }",Add a new knowledge base ID.,Add a knowledge base entry and return a success message with its ID.,5.0,5.0,4.0,0.4117647058823529,0.3846153846153846,0.8333333333333334,0.6261066794395447,0.6756036,0.70879054,0.75622106,0.741939902305603,0.4615384615384615,0.6339156098820227,4.0,5.0,5.0
304,304,304,305,get_branding_config_with_data_urls,"def get_branding_config_with_data_urls(
        self, script_path: str | None = None
    ) -> dict[str, Any]:
        
        branding = self.get_branding_config(script_path)

        # Convert logo and favicon to data URLs
        branding[""logo""] = self._convert_to_data_url(branding[""logo""])
        branding[""favicon""] = self._convert_to_data_url(
            branding[""favicon""].split(""?"")[0]
        )  # Remove timestamp query param

        logger.info(f""Actual final branding configuration: {branding}"")
        return branding",Get branding config with logo and favicon as data URLs,Convert branding assets to data URLs and log final configuration,5.0,5.0,4.0,0.78125,0.4,0.3,0.5748664140701294,0.35932523,0.5031723,0.8114245,0.8446837067604065,0.9,0.6416464553664987,5.0,5.0,5.0
305,305,305,306,analyze_voice_file,"def analyze_voice_file(file_path):
    
    try:
        tensor = torch.load(file_path, map_location=""cpu"")
        logger.info(f""\nAnalyzing {os.path.basename(file_path)}:"")
        logger.info(f""Shape: {tensor.shape}"")
        logger.info(f""Mean: {tensor.mean().item():.4f}"")
        logger.info(f""Std: {tensor.std().item():.4f}"")
        logger.info(f""Min: {tensor.min().item():.4f}"")
        logger.info(f""Max: {tensor.max().item():.4f}"")
        return tensor.shape
    except Exception as e:
        logger.error(f""Error analyzing {file_path}: {e}"")
        return None",Analyze dimensions and statistics of a voice tensor.,Analyzes audio file statistics and logs tensor details or errors.,5.0,5.0,5.0,0.7230769230769231,0.4,0.375,0.5212042927742004,0.38483867,0.60734034,0.649594,0.7858411073684692,0.1,0.6378161396704546,5.0,5.0,5.0
306,306,306,307,pdf_path,"def pdf_path():
    
    pdf_path = Path(PDF_FILE_PATH)
    if not pdf_path.exists():
        pytest.skip(f""Test PDF file not found at {pdf_path}"")
    return pdf_path",Fixture to provide the path to the test PDF file,Check if a PDF file exists and skip test if not found,5.0,5.0,5.0,0.6415094339622641,0.25,0.2,0.6405273675918579,0.2412761,0.4656672,0.749964,0.7284715175628662,0.5833333333333334,0.5800542080600134,5.0,5.0,5.0
307,307,307,308,download_dataset,"def download_dataset(self) -> None:
        
        assert self.data_dir is not None
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir, exist_ok=True)

        logging.info(f""[WebGames] Downloading dataset into '{self.data_dir}'..."")

        # Use pandas to download and save the dataset
        df = pd.read_json(  # type: ignore
        )
        output_path = os.path.join(self.data_dir, self.TEST_FILE)
        df.to_json(output_path, orient=""records"", lines=True)  # type: ignore

        logging.info(""[WebGames] Dataset downloaded."")",Download the dataset into self.data_dir using huggingface datasets.,Download and save dataset to specified directory using pandas.,4.0,5.0,4.0,0.78860664379003,0.3333333333333333,0.3,0.6000578999519348,0.34541148,0.4631257,0.7624007,0.9508174657821656,0.6666666666666666,0.5781102006581796,4.0,5.0,5.0
308,308,308,309,process_task,"def process_task(input_data):
    
    print(f""Processing: '{input_data[:50]}...'"")
    
    # Simulate work
    time.sleep(2)

    processed_result = f""Processed: {input_data}""
    print(f""Finished processing."")
    return processed_result",Minimal simulation of processing the input data.,Simulate task processing and return modified input data.,5.0,5.0,4.0,0.7678571428571429,0.5,0.5714285714285714,0.57619309425354,0.37794244,0.5583893,0.7254267,0.8374173641204834,0.25,0.5624471826528498,5.0,5.0,5.0
309,309,309,310,get_ratelimit_middleware_instance,"def get_ratelimit_middleware_instance(fastapi_app: FastAPI) -> RateLimitMiddleware:
    
    layer = fastapi_app.middleware_stack
    while layer is not None:
        if layer.__class__.__name__ == RateLimitMiddleware.__name__:
            return cast(RateLimitMiddleware, layer)
        layer = getattr(layer, ""app"", None)

    assert False, f""{RateLimitMiddleware.__name__} instance not found""",find the RateLimitMiddleware instance in the middleware stack,Retrieve existing rate limit middleware from FastAPI app's middleware stack.,5.0,5.0,4.0,0.6842105263157895,0.1818181818181818,0.25,0.6633028984069824,0.50740415,0.5857825,0.6864532,0.8698525428771973,0.1,0.7315244047075308,5.0,5.0,5.0
310,310,310,311,post,"def post(self, shared, prep_res, exec_res):
        
        tools = exec_res
        shared[""tools""] = tools
        
        # Format tool information for later use
        tool_info = []
        for i, tool in enumerate(tools, 1):
            properties = tool.inputSchema.get('properties', {})
            required = tool.inputSchema.get('required', [])
            
            params = []
            for param_name, param_info in properties.items():
                param_type = param_info.get('type', 'unknown')
                req_status = ""(Required)"" if param_name in required else ""(Optional)""
                params.append(f""    - {param_name} ({param_type}): {req_status}"")
            
            tool_info.append(f""[{i}] {tool.name}\n  Description: {tool.description}\n  Parameters:\n"" + ""\n"".join(params))
        
        shared[""tool_info""] = ""\n"".join(tool_info)
        return ""decide""",Store tools and process to decision node,Store and format tool details for shared access and decision-making.,5.0,5.0,4.0,0.5588235294117647,0.3636363636363636,0.5714285714285714,0.5042599439620972,0.22900045,0.6772877,0.75811636,0.799521803855896,0.4,0.5147762191203455,4.0,5.0,5.0
311,311,311,312,plot_total_conversation_time,"def plot_total_conversation_time(results, title, filename):
    
    df = pd.DataFrame(results)
    df = df[(df['elapsed'].notnull()) & (df['turn'] == 'ALL')]
    if df.empty:
        print(f""No successful results to plot for {title}."")
        return
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    df.boxplot(column='elapsed', by=['provider'], ax=ax)
    plt.title(title)
    plt.suptitle("""")
    plt.ylabel('Total Conversation Time (s)')
    plt.xlabel('Provider')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(filename)
    plt.show()",Plot total conversation time per provider and save to file.,Visualize and save a boxplot of conversation times by provider from given data.,5.0,5.0,4.0,0.6962025316455697,0.3846153846153846,0.3,0.6146883964538574,0.3949329,0.5846975,0.7701321,0.9381960034370422,0.3076923076923077,0.5464469918371196,5.0,5.0,5.0
312,312,312,313,_extract_tool_calls,"def _extract_tool_calls(self, trajectory: dict) -> list:
        
        tool_calls = []
        for message in trajectory.get(""messages"", []):
            if message.get(""role"") == ""assistant"" and message.get(""tool_calls""):
                for tool_call in message.get(""tool_calls"", []):
                    if tool_call.get(""function"", {}).get(""name"") and tool_call.get(""function"", {}).get(""arguments""):
                        tool_calls.append({
                            ""name"": tool_call[""function""][""name""],
                            ""arguments"": json.loads(tool_call[""function""][""arguments""])
                        })
        return tool_calls",Extract tool calls from a trajectory,Extracts and returns assistant tool call details from a message trajectory.,5.0,5.0,4.0,0.48,0.5454545454545454,1.0,0.5663740634918213,0.5255973,0.6797162,0.7336477,0.5478984117507935,0.4545454545454545,0.7087342353199064,5.0,5.0,5.0
313,313,313,314,close,"def close(self):
        
        try:
            # Milvus client doesn't have an explicit close method, but we can release the collection
            if hasattr(self.client, ""release_collection""):
                self.client.release_collection(collection_name=self.collection_name)
        except Exception as e:
            logger.error(f""Error closing Milvus connection: {e}"")",Close the Milvus connection.,"Safely release Milvus collection resources, logging errors if encountered.",5.0,5.0,5.0,0.3513513513513513,0.1111111111111111,0.25,0.698684573173523,0.5297107,0.501986,0.03962406,0.8719179630279541,0.4444444444444444,0.6096662357362079,5.0,5.0,5.0
314,314,314,315,log_generations_to_mlflow,"def log_generations_to_mlflow(self, samples, step):
        

        import json
        import tempfile

        import mlflow

        try:
            with tempfile.TemporaryDirectory() as tmp_dir:
                validation_gen_step_file = Path(tmp_dir, f""val_step{step}.json"")
                row_data = []
                for sample in samples:
                    data = {""input"": sample[0], ""output"": sample[1], ""score"": sample[2]}
                    row_data.append(data)
                with open(validation_gen_step_file, ""w"") as file:
                    json.dump(row_data, file)
                mlflow.log_artifact(validation_gen_step_file)
        except Exception as e:
            print(f""WARNING: save validation generation file to mlflow failed with error {e}"")",Log validation generation to mlflow as artifacts,Log sample data to MLflow as JSON artifacts for each step,5.0,5.0,3.0,0.6666666666666666,0.4545454545454545,0.7142857142857143,0.5274660587310791,0.29483527,0.77089554,0.7921201,0.7914230227470398,0.6363636363636364,0.6972816773817448,4.0,5.0,4.0
315,315,315,316,register_error_handlers,"def register_error_handlers(app):
    

    @app.errorhandler(404)
    def not_found(error):
        return make_response(jsonify({""error"": ""Not found""}), 404)

    @app.errorhandler(500)
    def server_error(error):
        return make_response(jsonify({""error"": ""Server error""}), 500)",Register error handlers with the Flask app.,Configure application to handle 404 and 500 errors with JSON responses.,5.0,5.0,5.0,0.5492957746478874,0.1818181818181818,0.2857142857142857,0.6816931962966919,0.45058858,0.40757,0.7006719,0.8482216596603394,0.1818181818181818,0.4924684642524547,5.0,5.0,5.0
316,316,316,317,print_summary,"def print_summary(self):
        
        summary = self.get_summary()
        total = summary.get(""total_duration"", 0)
        
        print(""\n===== SPEED PROFILE SUMMARY ====="")
        print(f""Total execution time: {total:.2f} seconds"")
        print(""\n--- Component Breakdown ---"")
        
        # Print each component's timing
        for name, data in self.timings.items():
            if name != ""total"":
                percent = data[""total""] / total * 100 if total > 0 else 0
                print(f""{name}: {data['total']:.2f}s ({percent:.1f}%) - ""
                      f""{data['count']} calls, avg {data['total'] / data['count']:.3f}s per call"")
        
        print(""\n=============================="")",Print a formatted summary of timing information.,Generate and display a detailed execution time summary for various components.,5.0,5.0,5.0,0.5512820512820513,0.2727272727272727,0.2857142857142857,0.5999189615249634,0.3456361,0.7081555,0.715554,0.9400704503059388,0.4545454545454545,0.6503093735999341,5.0,5.0,5.0
317,317,317,318,generate_cursor_checksum,"def generate_cursor_checksum(token: str, translator=None) -> str:
    
    try:
        # Clean the token
        clean_token = token.strip()
        
        # Generate machineId and macMachineId
        machine_id = generate_hashed64_hex(clean_token, 'machineId')
        mac_machine_id = generate_hashed64_hex(clean_token, 'macMachineId')
        
        # Get timestamp and convert to byte array
        byte_array = bytearray(struct.pack('>Q', timestamp)[-6:])  # Take last 6 bytes
        
        # Obfuscate bytes and encode as base64
        obfuscated_bytes = obfuscate_bytes(byte_array)
        encoded_checksum = base64.b64encode(obfuscated_bytes).decode('utf-8')
        
        # Combine final checksum
        return f""{encoded_checksum}{machine_id}/{mac_machine_id}""
    except Exception as e:
        print(f""{Fore.RED}{EMOJI['ERROR']} {translator.get('auth_check.error_generating_checksum', error=str(e)) if translator else f'Error generating checksum: {str(e)}'}{Style.RESET_ALL}"")
        return """"",Generate Cursor checksum from token using the algorithm,Generate a secure checksum by hashing and encoding a token with error handling.,5.0,5.0,4.0,0.6075949367088608,0.2307692307692307,0.375,0.6276453733444214,0.50614524,0.8381489,0.8070509,0.9589205980300904,0.4615384615384615,0.6189491211952616,4.0,4.0,4.0
318,318,318,319,get_screen_size,"def get_screen_size(self) -> Tuple[int, int]:
        
        try:
            size = pyautogui.size()
            return size.width, size.height
        except Exception as e:
            logger.warning(f""Failed to get screen size with pyautogui: {e}"")
        
        logger.info(""Getting screen size (simulated)"")
        return 1920, 1080",Get the screen size.,"Retrieve screen dimensions using pyautogui, with fallback to default values.",5.0,5.0,4.0,0.2368421052631578,0.1,0.25,0.6859939098358154,0.479039,0.4169344,0.42989296,0.8340169191360474,0.4,0.6583407100831402,5.0,5.0,5.0
319,319,319,320,generate_sub_questions,"def generate_sub_questions(state: ResearchState) -> ResearchState:
    
    prompt = PromptTemplate(
        input_variables=[""topic""],
        template=""Given the topic '{topic}', generate 3 specific sub-questions to guide research.""
    )
    response = llm.invoke(prompt.format(topic=state[""topic""]))
    questions = [q.strip() for q in response.content.split(""\n"") if q.strip()]
    return {""sub_questions"": questions, ""status"": ""generated_questions""}",Generate sub-questions based on the topic.,Generate sub-questions for research topics using a language model.,5.0,5.0,3.0,0.6060606060606061,0.4,0.5714285714285714,0.6352014541625977,0.6815391,0.7234361,0.6160554,0.6019594073295593,0.5555555555555556,0.6020991171213803,4.0,5.0,5.0
320,320,320,321,hub_status,"def hub_status() -> str:
    
    try:
        bridge = Bridge(
            ip=str(settings.hue_bridge_ip),
            username=settings.hue_bridge_username,
            save_config=False,
        )
        bridge.connect()
        return ""Hub OK. Hue Bridge Connected (via phue2).""
    except Exception as e:
        return f""Hub Warning: Hue Bridge connection failed or not attempted: {e}""",Checks the status of the main hub and connections.,Check Hue Bridge connection status and return success or warning message.,5.0,4.0,3.0,0.6027397260273972,0.3636363636363636,0.3333333333333333,0.6203847527503967,0.3379256,0.39771104,0.65304494,0.9299401640892028,0.5454545454545454,0.6892350014610734,5.0,5.0,5.0
321,321,321,322,create_tool_result_message,"def create_tool_result_message(tool_result, tool_name, status=""success""):
        
        from google.genai import types

        if status == ""success"":
            function_response = {""result"": tool_result}
        else:
            function_response = {""error"": tool_result}

        return types.Content(
            role=""tool"",
            parts=[
                types.Part.from_function_response(
                    name=tool_name, response=function_response
                )
            ],
        )",Creates a tool result message for testing in Google's format.,Generate a structured message based on tool execution outcome and status.,4.0,5.0,3.0,0.6986301369863014,0.2727272727272727,0.1818181818181818,0.5672374367713928,0.29594845,0.5936706,0.80262005,0.8801465034484863,0.1818181818181818,0.5147656262199479,5.0,5.0,5.0
322,322,322,323,_merge_by_placement,"def _merge_by_placement(self, tensors: list[torch.Tensor], placement: Placement) -> torch.Tensor:
        
        if placement.is_replicate():
            return tensors[0]
        elif placement.is_partial():
            raise NotImplementedError(""Partial placement is not supported yet"")
        elif placement.is_shard():
            return torch.cat(tensors, dim=placement.dim).contiguous()

        raise NotImplementedError(f""Unsupported placement: {placement}"")",Merges a list of tensors based on their DTensor placement,"Merge tensors based on placement type, handling replication and sharding.",5.0,4.0,3.0,0.6986301369863014,0.5,0.5,0.5856990814208984,0.2001613,0.73874533,0.6688217,0.935633897781372,0.2,0.6082167855091618,4.0,5.0,5.0
323,323,323,324,analyze_news_sentiment,"def analyze_news_sentiment(news_items: list) -> str:
    
    if not news_items or len(news_items) == 0:
        return ""No news data available""
    
    # Just return a simple count for now - in a real implementation, this would use NLP
    return f""Qualitative review of {len(news_items)} recent news items would be needed""",Simple qualitative analysis of recent news.,Determine sentiment analysis necessity for given news items list,4.0,4.0,4.0,0.546875,0.2222222222222222,0.3333333333333333,0.7009215354919434,0.169585,0.4273178,0.6469126,0.857394278049469,0.4444444444444444,0.634389604962121,3.0,4.0,4.0
324,324,324,325,_encode_image_to_base64,"def _encode_image_to_base64(self, image_path: str) -> str:
        
        try:
            with open(image_path, ""rb"") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode(""utf-8"")
            return encoded_string
        except Exception as e:
            logger.error(f""Failed to encode image {image_path}: {e}"")
            return """"",Encode image to base64,Convert image file to Base64 string with error handling,5.0,5.0,4.0,0.3454545454545454,0.3333333333333333,0.75,0.565281867980957,0.532202,0.71822417,0.3357357,0.9616125226020812,0.3333333333333333,0.6015212957373063,5.0,5.0,5.0
325,325,325,326,save_raw_response,"def save_raw_response(filename: str, problem_id: int, response_data: Dict):
    
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # Create a timestamped ID for this response
    timestamp = int(time.time())
    response_id = f""{problem_id}_{timestamp}""
    
    # Create or update the raw responses file
    try:
        with open(filename, 'r') as f:
            raw_responses = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        raw_responses = {}
    
    # Add this response to the collection
    raw_responses[response_id] = response_data
    
    # Save the updated collection
    with open(filename, 'w') as f:
        json.dump(raw_responses, f)
    
    return response_id",Save raw response data (including logprobs) to a separate file.,"Store timestamped problem responses in a JSON file, ensuring directory creation.",5.0,5.0,5.0,0.6625,0.2727272727272727,0.3,0.5747789144515991,0.27324286,0.5151614,0.68531555,0.8960137367248535,0.4545454545454545,0.4883585417498338,5.0,5.0,5.0
326,326,326,327,get_predefined_execute_fn,"def get_predefined_execute_fn(execute_mode):
    
    predefined_execute_mode_fn = {
        Execute.ALL: {
            'execute_fn_name': 'execute_all'
        },
        Execute.RANK_ZERO: {
            'execute_fn_name': 'execute_rank_zero'
        }
    }
    return predefined_execute_mode_fn[execute_mode]",Note that here we only asks execute_all and execute_rank_zero to be implemented,Map execution mode to corresponding function name.,5.0,4.0,3.0,0.4255227585897055,0.2857142857142857,0.1333333333333333,0.5446978807449341,-0.07982038,0.5386136,0.5314522,0.8355151414871216,0.0,0.4605727334174401,5.0,5.0,5.0
327,327,327,328,deepseek_fn,"def deepseek_fn(self, history, verbose=False):
        
        if self.is_local:
            raise Exception(""Deepseek (API) is not available for local use. Change config.ini"")
        try:
            response = client.chat.completions.create(
                model=""deepseek-chat"",
                messages=history,
                stream=False
            )
            thought = response.choices[0].message.content
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""Deepseek API error: {str(e)}"") from e",Use deepseek api to generate text.,Function interfaces with Deepseek API to generate chat responses from message history.,4.0,5.0,3.0,0.3604651162790697,0.3333333333333333,0.6666666666666666,0.6191310882568359,0.7021825,0.6386422,0.519018,0.8875738382339478,0.3333333333333333,0.6271147960522461,4.0,5.0,5.0
328,328,328,329,copy_old_config_names_to_new,"def copy_old_config_names_to_new(self) -> Self:
		
		if self.window_width or self.window_height:
			self.window_size = self.window_size or {}
			self.window_size['width'] = (self.window_size or {}).get('width') or self.window_width or 1280
			self.window_size['height'] = (self.window_size or {}).get('height') or self.window_height or 1100
		return self",Copy old config window_width & window_height to window_size.,Update window size configuration with default values if necessary.,4.0,5.0,4.0,0.6212121212121212,0.2222222222222222,0.2,0.6212711334228516,0.2218722,0.487368,0.5899062,0.6319907307624817,0.1111111111111111,0.4898116985224907,4.0,5.0,4.0
329,329,329,330,_prepare_batch_get_documents_request,"def _prepare_batch_get_documents_request(
        self, document_ids: List[str], folder_name: Optional[Union[str, List[str]]], end_user_id: Optional[str]
    ) -> Dict[str, Any]:
        
        if folder_name or end_user_id:
            request = {""document_ids"": document_ids}
            if folder_name:
                request[""folder_name""] = folder_name
            if end_user_id:
                request[""end_user_id""] = end_user_id
            return request
        return document_ids",Prepare request for batch_get_documents endpoint,Constructs a document retrieval request with optional folder and user identifiers.,5.0,5.0,4.0,0.5121951219512195,0.1818181818181818,0.1428571428571428,0.5645453929901123,0.19737607,0.52035713,0.60771716,0.8806248903274536,0.1818181818181818,0.5723101620791264,5.0,5.0,5.0
330,330,330,331,start_cleanup_task,"def start_cleanup_task(self) -> None:
        

        async def cleanup_loop():
            while not self._is_shutting_down:
                try:
                    await self._cleanup_idle_sandboxes()
                except Exception as e:
                    logger.error(f""Error in cleanup loop: {e}"")
                await asyncio.sleep(self.cleanup_interval)

        self._cleanup_task = asyncio.create_task(cleanup_loop())",Starts automatic cleanup task.,Initialize an asynchronous task to periodically clean up idle resources.,5.0,5.0,4.0,0.3611111111111111,0.1,0.25,0.591677188873291,0.5406691,0.49992642,0.5622555,0.8908498883247375,0.1,0.5090903088013953,5.0,5.0,5.0
331,331,331,332,__init__,"def __init__(self, name, prompt_path, provider, verbose=False):
        
        super().__init__(name, prompt_path, provider, verbose, None)
        self.tools = {
            ""file_finder"": FileFinder(),
            ""bash"": BashInterpreter()
        }
        self.work_dir = self.tools[""file_finder""].get_work_dir()
        self.role = ""files""
        self.type = ""file_agent""
        self.memory = Memory(self.load_prompt(prompt_path),
                        recover_last_session=False, # session recovery in handled by the interaction class
                        memory_compression=False,
                        model_provider=provider.get_model_name())",The file agent is a special agent for file operations.,"Initialize a file agent with tools, memory, and configuration settings.",5.0,5.0,4.0,0.676056338028169,0.3,0.2,0.5977201461791992,0.2870933,0.4289738,0.7650878,0.8552001714706421,0.1,0.5032821269939672,4.0,5.0,5.0
332,332,332,333,download_pdf_from_url,"def download_pdf_from_url(url, output_path):
  
  print(f""Downloading PDF from {url}..."")
  response = requests.get(url, stream=True)
  response.raise_for_status()  # Raise an exception for HTTP errors
  
  with open(output_path, 'wb') as f:
    for chunk in response.iter_content(chunk_size=8192):
      f.write(chunk)
  
  print(f""PDF downloaded successfully to {output_path}"")
  return output_path",Downloads a PDF file from the specified URL.,Download and save a PDF file from a specified URL to a local path.,5.0,5.0,5.0,0.6666666666666666,0.5,0.875,0.6352116465568542,0.76912767,0.6783972,0.843443,0.9797418117523192,0.5,0.6151264179493705,5.0,5.0,5.0
333,333,333,334,load_jsonl_files,"def load_jsonl_files(input_dir):
    
    jsonl_files = list(Path(input_dir).glob(""*.jsonl""))
    if not jsonl_files:
        print(f""No JSONL files found in {input_dir}"")
        return []

    print(f""Found {len(jsonl_files)} JSONL files: {[f.name for f in jsonl_files]}"")
    return jsonl_files",Load all JSONL files from the input directory.,"Load and list JSONL files from a specified directory, handling absence gracefully.",5.0,5.0,4.0,0.5487804878048781,0.4166666666666667,0.625,0.6852376461029053,0.65155745,0.59035563,0.81691295,0.935133695602417,0.25,0.60594773882626,5.0,5.0,5.0
334,334,334,335,tagged_resources_server,"def tagged_resources_server():
    
    server = FastMCP(""TaggedResourcesServer"")

    # Add a resource with tags
    @server.resource(
    )
    async def get_tagged_data():
        return {""type"": ""tagged_data""}

    # Add a resource template with tags
    @server.resource(
        tags={""template"", ""parameterized""},
        description=""A tagged template"",
    )
    async def get_template_data(id: str):
        return {""id"": id, ""type"": ""template_data""}

    return server",Fixture that creates a FastMCP server with tagged resources and templates.,Define a server with resources tagged for data retrieval and templates.,5.0,5.0,3.0,0.810107085327375,0.6363636363636364,0.5454545454545454,0.5530534982681274,0.40266,0.67453533,0.8264066,0.6214887499809265,0.5454545454545454,0.6442361478250731,4.0,4.0,4.0
335,335,335,336,_load_environment,"def _load_environment(self) -> None:
        
        env_file = self.app_root / "".env""
        if env_file.exists():
            # logger.info(f""Loading environment variables from {env_file}"")
            load_dotenv(str(env_file))",Load environment variables from .env file if it exists,Load environment variables from a .env file if it exists.,5.0,5.0,5.0,0.9473684210526316,0.9,1.0,0.7056882381439209,0.9353215,0.98009753,0.9697701,0.970173716545105,0.6,0.6238910140194741,5.0,5.0,5.0
336,336,336,337,add_tool,"def add_tool(self, tool: BaseTool):
        
        if tool.name in self.tool_map:
            logger.warning(f""Tool {tool.name} already exists in collection, skipping"")
            return self

        self.tools += (tool,)
        self.tool_map[tool.name] = tool
        return self",Add a single tool to the collection.,Add a tool to the collection if it doesn't already exist,5.0,5.0,5.0,0.6071428571428571,0.5,0.8571428571428571,0.6423040628433228,0.8206752,0.7761501,0.7291714,0.9736942052841188,0.6363636363636364,0.7119760515182946,5.0,5.0,5.0
337,337,337,338,_generate_batch_response,"def _generate_batch_response(self, text, system_message, provider, model_config, api_key, api_base):
        
        MAX_RETRIES = 3
        
        for attempt in range(MAX_RETRIES):
            try:
                if provider == ""gemini"" and api_base:
                    messages = [{'role': 'user', 'content': system_message + text}]
                    response = proxy_api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    # response = proxy_call.api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    return pd.DataFrame(ast.literal_eval(response[0]))
                else:
                    return self._generate_llm_response(text, system_message, model_config, api_key)
            except (json.JSONDecodeError, ValueError) as e:
                if attempt == MAX_RETRIES - 1:
                    raise Exception(f""Failed to generate valid response after {MAX_RETRIES} attempts: {str(e)}"")
                continue",Generate a batch of responses using the specified provider.,"Attempt to generate a batch response using a specified provider, retrying on failure up to three times.",5.0,5.0,4.0,0.5631067961165048,0.4117647058823529,0.7777777777777778,0.6567654609680176,0.6580023,0.4181391,0.76546,0.8115363121032715,0.5294117647058824,0.6196084572664269,4.0,5.0,5.0
338,338,338,339,model_kwargs,"def model_kwargs():
    
    inference_url = settings.TEXT2VEC_INFERENCE_URL
    if not inference_url:
        raise ValueError(""TEXT2VEC_INFERENCE_URL environment variable is not set"")
    return {""inference_url"": inference_url}",Return kwargs for model initialization.,Retrieve and validate the inference URL for text-to-vector conversion.,5.0,5.0,5.0,0.4428571428571428,0.0909090909090909,0.2,0.6672911643981934,0.37393618,0.24998733,0.48276547,0.3789377808570862,0.0,0.513334979907309,5.0,5.0,5.0
339,339,339,340,validate_adapter,"def validate_adapter(self, adapter_id: str) -> bool:
        
        try:
            config = PeftConfig.from_pretrained(
                adapter_id,
                trust_remote_code=True,
                use_auth_token=os.getenv(""HF_TOKEN"")
            )
            return True
        except Exception as e:
            logger.error(f""Error validating adapter {adapter_id}: {str(e)}"")
            return False",Validate if adapter exists and is compatible,Checks adapter configuration validity using a pre-trained model and logs errors.,5.0,5.0,4.0,0.5125,0.25,0.2857142857142857,0.5924780368804932,0.103179626,0.6530648,0.71077794,0.7396312355995178,0.1818181818181818,0.4965187357841338,5.0,5.0,5.0
340,340,340,341,_get_litellm_response,"def _get_litellm_response(
    prompt,
    model, 
    temperature,
    max_tokens
    ):
    
    try:
        response = completion(
            model=model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f""Error with LiteLLM: {str(e)}"")
        return None",Get response using LiteLLM,Fetch AI-generated text using specified model parameters and handle errors.,5.0,5.0,3.0,0.28,0.0909090909090909,0.25,0.5421004891395569,0.062863894,0.2098979,0.50288904,0.3477227389812469,0.1,0.2770748094271691,5.0,5.0,5.0
341,341,341,342,extract_python_code,"def extract_python_code(text: str) -> List[str]:
    
    # print(f""Extracting code: {text}"")
    pattern = r'```python\s*(.*?)\s*```'
    return re.findall(pattern, text, re.DOTALL)",Extract Python code blocks from text.,Extracts Python code blocks from a given text using regex pattern matching.,5.0,5.0,5.0,0.4933333333333333,0.5,1.0,0.6762152910232544,0.74729717,0.8567368,0.8425834,0.806896984577179,0.3333333333333333,0.6726381949756827,5.0,5.0,5.0
342,342,342,343,handle_js_message,"def handle_js_message(client_id, message_type, data):
        
        if _service:
            asyncio.create_task(  # noqa: RUF006
                _service.handle_client_message(
                    client_id, {""type"": message_type, ""data"": data}
                )
            )
        return True",Handle message from JavaScript,Handle asynchronous client messages based on type and data.,4.0,5.0,3.0,0.4067796610169492,0.2222222222222222,0.5,0.5868548154830933,0.27048665,0.29332018,0.6722092,0.948314130306244,0.1111111111111111,0.5680373033672214,4.0,5.0,5.0
343,343,343,344,rename_state_dict,"def rename_state_dict(
    old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]
):
    
    # need this list not to break the dict iterator
    old_keys = [k for k in state_dict if k.startswith(old_prefix)]
    if len(old_keys) > 0:
        logging.warning(f""Rename: {old_prefix} -> {new_prefix}"")
    for k in old_keys:
        v = state_dict.pop(k)
        new_k = k.replace(old_prefix, new_prefix)
        state_dict[new_k] = v",Replace keys of old prefix with new prefix in state dict.,Rename dictionary keys by replacing old prefix with new prefix in a state dictionary.,5.0,5.0,5.0,0.6588235294117647,0.6428571428571429,0.7272727272727273,0.6068882942199707,0.59299767,0.8695022,0.8818289,0.8332676291465759,0.1428571428571428,0.7136778115631136,5.0,5.0,5.0
344,344,344,345,get_note_name,"def get_note_name(midi):
    
    notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    note = notes[midi % 12]
    return f""{note}{octave}""",Convert MIDI note number to note name,Convert MIDI number to musical note with octave.,5.0,5.0,5.0,0.7083333333333334,0.625,0.7142857142857143,0.6052982807159424,0.6286828,0.5120305,0.8829241,0.8726406097412109,0.125,0.632061708692565,1.0,5.0,4.0
345,345,345,346,sample,"def sample(self, n) -> None:
        
        assert (
            n <= len(self.docs)
        ), f""Error: number of fewshot samples requested exceeds the {len(self.docs)} that are available.""
        return self.docs[:n]",Draw the first `n` samples in order from the specified split.,"Retrieve a specified number of documents from a collection, ensuring availability.",5.0,5.0,4.0,0.6097560975609756,0.1818181818181818,0.0909090909090909,0.6764933466911316,0.078600384,0.13423164,0.64879346,0.6901090145111084,0.3636363636363636,0.4223288727242034,4.0,5.0,5.0
346,346,346,347,_get_loggable_messages,"def _get_loggable_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        
        loggable_messages = []
        for msg in messages:
            if isinstance(msg.get(""content""), list):
                new_content = []
                for content in msg[""content""]:
                    if content.get(""type"") == ""image"":
                        new_content.append(
                            {""type"": ""image"", ""image_url"": {""url"": ""[BASE64_IMAGE_DATA]""}}
                        )
                    else:
                        new_content.append(content)
                loggable_messages.append({""role"": msg[""role""], ""content"": new_content})
            else:
                loggable_messages.append(msg)
        return loggable_messages",Create a loggable version of messages with image data truncated.,Filter and sanitize messages for logging by replacing image data with placeholders.,5.0,5.0,3.0,0.7228915662650602,0.3333333333333333,0.3,0.5662745237350464,0.34612355,0.5159899,0.7218764,0.589016318321228,0.1666666666666666,0.3289541743805195,5.0,5.0,5.0
347,347,347,348,_build_split_documents,"def _build_split_documents(row, chunks: List[str]) -> List[dict[str, Any]]:
    
    documents: List[dict] = []

    for i, text in enumerate(chunks):
        if text is None or not text.strip():
            continue

        metadata = row.metadata if hasattr(row, ""metadata"") and isinstance(row.metadata, dict) else {}
        metadata = copy.deepcopy(metadata)

        metadata[""content""] = text

        documents.append({""document_type"": ContentTypeEnum.TEXT.value, ""metadata"": metadata, ""uuid"": str(uuid.uuid4())})

    return documents",Build documents from text chunks,Transform text chunks into structured document dictionaries with metadata and unique identifiers.,5.0,5.0,5.0,0.3092783505154639,0.25,0.4,0.5717374086380005,0.5626973,0.6876032,0.7459598,0.8910351991653442,0.4166666666666667,0.5266469035172993,5.0,5.0,5.0
348,348,348,349,_extract_variable_names,"def _extract_variable_names(self, expr: str) -> list[str]:
        
        # Find all patterns like ${var_name}
        matches = re.findall(r""\${([^}]*)}"", expr)
        return matches",Extract all variable references ${var_name} from the expression.,Extract variable names from a string pattern using regex,5.0,5.0,1.8,0.7275582372903311,0.4444444444444444,0.4444444444444444,0.6699832081794739,0.21264423,0.57699656,0.73291564,0.8635849952697754,0.2222222222222222,0.5638047576191048,5.0,5.0,5.0
349,349,349,350,_format_repository_preview,"def _format_repository_preview(self, repo: Dict[str, Any]) -> Dict[str, Any]:
        
        return {
            ""id"": str(repo.get(""id"", """")),
            ""title"": repo.get(""full_name"", """"),
            ""link"": repo.get(""html_url"", """"),
            ""snippet"": repo.get(""description"", ""No description provided""),
            ""stars"": repo.get(""stargazers_count"", 0),
            ""forks"": repo.get(""forks_count"", 0),
            ""language"": repo.get(""language"", """"),
            ""updated_at"": repo.get(""updated_at"", """"),
            ""created_at"": repo.get(""created_at"", """"),
            ""topics"": repo.get(""topics"", []),
            ""owner"": repo.get(""owner"", {}).get(""login"", """"),
            ""is_fork"": repo.get(""fork"", False),
            ""search_type"": ""repository"",
            ""repo_full_name"": repo.get(""full_name"", """"),
        }",Format repository search result as preview,Transforms repository data into a structured preview dictionary with key attributes.,5.0,5.0,5.0,0.4761904761904761,0.1818181818181818,0.3333333333333333,0.4864364266395569,0.28860265,0.5420122,0.74127406,0.8111491203308105,0.0909090909090909,0.5862178712992074,5.0,5.0,5.0
350,350,350,351,save_chat_history,"def save_chat_history(chat_history: List[dict[Any, Any]], company_name: str) -> None:
    

    # 保存聊天历史记录到文件
    chat_history_file = os.path.join(LOG_DIR, f""{company_name}_chat_history.json"")
    with open(chat_history_file, ""w"", encoding=""utf-8"") as f:
        json.dump(chat_history, f, ensure_ascii=False, indent=2)

    print(f""{Fore.GREEN}Records saved to {chat_history_file}{Style.RESET_ALL}"")",Analyze chat history and extract tool call information.,Save chat history to a JSON file named after the company.,5.0,5.0,4.0,0.7192982456140351,0.1818181818181818,0.25,0.5951187610626221,0.34845033,0.4061514,0.6894953,0.9534354209899902,0.2727272727272727,0.6603233668821661,5.0,5.0,5.0
351,351,351,352,save_log,"def save_log(logger_name, task_name, output_dir):
    
    log_dir = os.path.join(output_dir, 'trajectory')
    os.makedirs(log_dir, exist_ok=True)
    log_file_name = f'{task_name}.log'
    log_file_path = os.path.join(log_dir, log_file_name)
    logger = AgentLogger(logger_name, filepath=log_file_path)
    return logger",Creates a log file and logging object for the corresponding task Name,Create a logger to save task-specific logs in a designated directory.,2.6,1.0,1.9,0.8115942028985508,0.3333333333333333,0.25,0.595979630947113,0.26653698,0.8537327,0.76491296,0.937122106552124,0.2727272727272727,0.616286186358867,4.0,5.0,5.0
352,352,352,353,read_jsonl,"def read_jsonl(paths):
    
    for path in paths:
        try:
            with smart_open.smart_open(path, ""r"", encoding=""utf-8"") as f:
                for line in f:
                    yield line.strip()
        except Exception as e:
            print(f""Error reading {path}: {e}"")",Generator that yields lines from multiple JSONL files.,"Iterate and yield lines from multiple JSONL files, handling errors.",5.0,5.0,5.0,0.7761194029850746,0.6,0.75,0.6029077768325806,0.62618554,0.6512234,0.8139707,0.8880095481872559,0.2,0.4762002448269099,5.0,5.0,5.0
353,353,353,354,try_add_cuda_lib_path,"def try_add_cuda_lib_path():
    
    required_submodules = [""cuda_runtime"", ""cublas""]
    cuda_versions = [""11"", ""12""]

    module_spec = find_spec(""nvidia"")
    if not module_spec:
        return

    nvidia_lib_root = Path(module_spec.submodule_search_locations[0])

    for submodule in required_submodules:
        for ver in cuda_versions:
            try:
                package_name = f""nvidia_{submodule}_cu{ver}""
                _ = distribution(package_name)

                lib_path = nvidia_lib_root / submodule / ""bin""
                os.add_dll_directory(str(lib_path))
                os.environ[""PATH""] = str(lib_path) + \
                    os.pathsep + os.environ[""PATH""]
                logging.debug(f""Added {lib_path} to PATH"")
            except PackageNotFoundError:
                logging.debug(f""{package_name} not found"")",Try to add the CUDA library paths to the system PATH.,Add CUDA library paths to system environment if NVIDIA modules are available,5.0,5.0,4.0,0.5657894736842105,0.5,0.5454545454545454,0.5849383473396301,0.5696002,0.7445632,0.8531614,0.9115989208221436,0.1666666666666666,0.6429705279943704,4.0,5.0,5.0
354,354,354,355,format_page_entry,"def format_page_entry(page_loc: str, indent: str, keywords: dict[str, str], mkdocs_docs_dir: Path) -> str:
    
    file_path_str = f""{page_loc}.md""
    title = format_title(file_path_str, keywords, mkdocs_docs_dir)
    return f""{indent}    - [{title}]({file_path_str})""",Format a single page entry as either a parenthesized path or a markdown link.,Formats a markdown entry with title and link for a documentation page.,5.0,5.0,5.0,0.762648680916023,0.5833333333333334,0.2857142857142857,0.5943244695663452,0.21827199,0.60938656,0.71397036,0.890710711479187,0.25,0.6392196469504865,5.0,5.0,5.0
355,355,355,356,parse_size,"def parse_size(size):
    
    units = {""KB"": 1024, ""MB"": 1024**2, ""GB"": 1024**3}
    if size.isdigit():
        return int(size)
    unit = size[-2:].upper()
    number = size[:-2]
    if unit in units and number.isdigit():
        return int(number) * units[unit]
    raise ValueError(""Size must be in the format <number>[KB|MB|GB]."")","Parse the size string with optional suffix (KB, MB) into bytes.","Convert size string to bytes, supporting KB, MB, GB units.",5.0,5.0,5.0,0.7434139134045505,0.5,0.3636363636363636,0.6565361022949219,0.3029377,0.7104743,0.7125397,0.94853538274765,0.1,0.5136331795625305,5.0,5.0,5.0
356,356,356,357,signal_handler,"def signal_handler(signum, frame):
            
            logger.info(f""Received signal {signum}. Terminating pipeline subprocess group..."")
            _terminate_subprocess(process)
            sys.exit(0)",Handle signals to ensure clean subprocess termination.,Handle termination signal to safely stop subprocess and exit.,5.0,5.0,5.0,0.8032786885245902,0.5555555555555556,0.5714285714285714,0.6581170558929443,0.6582457,0.8328916,0.7576148,0.9727314114570618,0.2222222222222222,0.6589072176193504,4.0,5.0,5.0
357,357,357,358,print_optimization_results,"def print_optimization_results(params: Dict[str, Any], score: float):
    
    print(""\n"" + ""="" * 50)
    print("" OPTIMIZATION RESULTS "")
    print(""="" * 50)
    print(f""SCORE: {score:.4f}"")
    print(""\nBest Parameters:"")
    for param, value in params.items():
        print(f""  {param}: {value}"")
    print(""="" * 50 + ""\n"")",Print optimization results in a nicely formatted way.,Display formatted optimization results including score and parameters.,5.0,5.0,5.0,0.7142857142857143,0.375,0.25,0.5225721597671509,0.41191342,0.5488277,0.8027232,0.9089109897613524,0.375,0.6026887784789382,5.0,5.0,5.0
358,358,358,359,run_stateless_http_server,"def run_stateless_http_server(server_port: int) -> None:
    
    _, app = make_fastmcp_stateless_http_app()
    server = uvicorn.Server(
        config=uvicorn.Config(
            app=app, host=""127.0.0.1"", port=server_port, log_level=""error""
        )
    )
    print(f""Starting stateless StreamableHTTP server on port {server_port}"")
    server.run()",Run the stateless StreamableHTTP server.,Initialize and run a stateless HTTP server on a specified port using Uvicorn.,5.0,5.0,4.0,0.4545454545454545,0.2307692307692307,0.6,0.6731281280517578,0.5928986,0.7682148,0.5452369,0.9864522218704224,0.4615384615384615,0.6357217032699127,5.0,5.0,5.0
359,359,359,360,package_template_features,"def package_template_features(
    *,
    hit_features: Sequence[Mapping[str, Any]],
    include_ligand_features: bool,
) -> Mapping[str, Any]:
  

  features_to_include = set(_POLYMER_FEATURES)
  if include_ligand_features:
    features_to_include.update(_LIGAND_FEATURES)

  features = {
      feat: [single_hit_features[feat] for single_hit_features in hit_features]
      for feat in features_to_include
  }

  stacked_features = {}
  for k, v in features.items():
    if k in _POLYMER_FEATURES:
      v = np.stack(v, axis=0) if v else np.array([], dtype=_POLYMER_FEATURES[k])
    stacked_features[k] = v

  return stacked_features","Stacks polymer features, adds empty and keeps ligand features unstacked.",Aggregate and stack selected polymer and ligand features from input data.,5.0,5.0,4.0,0.8356164383561644,0.4545454545454545,0.5,0.5029513835906982,0.27868775,0.5799666,0.76396114,0.8322729468345642,0.0909090909090909,0.5476979658990443,4.0,5.0,4.0
360,360,360,361,rewrite_schema_for_sqlglot,"def rewrite_schema_for_sqlglot(
        cls, schema: str | SQLGlotSchemaType | BirdSampleType
    ) -> SQLGlotSchemaType:
        
        schema_dict = None
        if schema:
            if isinstance(schema, str):
                schema = cls.extract_schema_from_ddls(schema)
                schema_dict = cls.format_schema(schema)
            elif _isinstance_sqlglot_schema_type(schema):
                schema_dict = schema
            elif _isinstance_bird_sample_type(schema):
                schema_dict = cls._get_schema_from_bird_sample(schema)
            elif _isinstance_ddl_schema_type(schema):
                schema_dict = cls.format_schema(schema)
            else:
                raise TypeError(f""Unsupported schema type: {type(schema)}"")
        return schema_dict",Rewrites the schema for use in SQLGlot.,Convert various schema formats into a standardized SQLGlot schema type.,5.0,5.0,3.0,0.5211267605633803,0.2,0.2857142857142857,0.5876084566116333,0.38864332,0.7495739,0.6308392,0.7808799147605896,0.4,0.6409265317504705,5.0,5.0,5.0
361,361,361,362,get_config_defaults,"def get_config_defaults() -> Dict[str, Any]:
    
    defaults = {
        'site': 'all',
        'model': 'gpt-4o-mini',
        'generate_mode': 'list', 
        'retrieval_backend': CONFIG.preferred_retrieval_endpoint,
        'prev': []
    }
    
    # Try to get preferred model from LLM config
    if hasattr(CONFIG, 'preferred_llm_provider') and CONFIG.preferred_llm_provider:
        llm_provider = CONFIG.get_llm_provider()
        if llm_provider and llm_provider.models:
            # Use the 'low' model as default for testing
            defaults['model'] = llm_provider.models.low or defaults['model']
    
    return defaults",Get default values from config files.,Initialize default configuration settings with potential overrides from a preferred model provider.,5.0,5.0,3.0,0.3636363636363636,0.1666666666666666,0.3333333333333333,0.5510074496269226,0.5819261,0.6249538,0.7529687,0.8107295632362366,0.4166666666666667,0.4636532759640249,5.0,5.0,5.0
362,362,362,363,write_fold_input_json,"def write_fold_input_json(
    fold_input: folding_input.Input,
    output_dir: os.PathLike[str] | str,
) -> None:
  
  os.makedirs(output_dir, exist_ok=True)
  path = os.path.join(output_dir, f'{fold_input.sanitised_name()}_data.json')
  print(f'Writing model input JSON to {path}')
  with open(path, 'wt') as f:
    f.write(fold_input.to_json())",Writes the input JSON to the output directory.,Serialize folding input to JSON file in specified directory.,5.0,5.0,4.0,0.6,0.4444444444444444,0.375,0.5673105120658875,0.49522755,0.70679784,0.784914,0.872035026550293,0.3333333333333333,0.6690716466884506,5.0,5.0,5.0
363,363,363,364,_generate_audio_segments,"def _generate_audio_segments(self, text: str, temp_dir: str) -> List[str]:
        
        qa_pairs = self.provider.split_qa(
            text, self.ending_message, self.provider.get_supported_tags()
        )
        audio_files = []
        provider_config = self._get_provider_config()

        for idx, (question, answer) in enumerate(qa_pairs, 1):
            for speaker_type, content in [(""question"", question), (""answer"", answer)]:
                temp_file = os.path.join(
                    temp_dir, f""{idx}_{speaker_type}.{self.audio_format}""
                )
                voice = provider_config.get(""default_voices"", {}).get(speaker_type)
                model = provider_config.get(""model"")

                audio_data = self.provider.generate_audio(content, voice, model)
                with open(temp_file, ""wb"") as f:
                    f.write(audio_data)
                audio_files.append(temp_file)

        return audio_files",Generate audio segments for each Q&A pair.,Convert text into audio segments using a provider's configuration and save them as files.,4.0,5.0,4.0,0.4269662921348314,0.2,0.375,0.5431609153747559,0.17805423,0.5375642,0.76808167,0.8994141221046448,0.2142857142857142,0.6018753751774707,4.0,5.0,5.0
364,364,364,365,format,"def format(self) -> str:
        
        if len(self.ingestions) == 0 and len(self.queries) == 0:
            return ""There were no ingestion or query tasks detected.""

        instructions = ""Tasks:\n\n""
        order = 1

        if len(self.ingestions) > 0:
            instructions += ""Ingestions:\n""
            for ingestion in self.ingestions:
                instructions += f""{order}: {ingestion.path_or_url}\n""
                order += 1

            instructions += ""\n""

        if len(self.queries) > 0:
            instructions += ""Queries:\n""
            for query in self.queries:
                instructions += f""{order}: {query.query}\n""
                order += 1

        return instructions",Format the DocumentTask as a string for the TaskManager to work with.,Generate a formatted task list from ingestion and query data.,5.0,5.0,3.0,0.7045463862276964,0.2,0.0833333333333333,0.6042914390563965,0.3077671,0.5457603,0.58106095,0.6484639048576355,0.5,0.5943902609379789,5.0,5.0,5.0
365,365,365,366,sort_agent_signals,"def sort_agent_signals(signals):
    
    # Create order mapping from ANALYST_ORDER
    analyst_order = {display: idx for idx, (display, _) in enumerate(ANALYST_ORDER)}
    analyst_order[""Risk Management""] = len(ANALYST_ORDER)  # Add Risk Management at the end

    return sorted(signals, key=lambda x: analyst_order.get(x[0], 999))",Sort agent signals in a consistent order.,Sorts signals based on predefined analyst priority order.,5.0,5.0,4.0,0.6491228070175439,0.375,0.4285714285714285,0.6118775606155396,0.46404994,0.7020937,0.6918474,0.8801886439323425,0.25,0.648164685339319,5.0,5.0,5.0
366,366,366,367,_log_debug,"def _log_debug(self, message: str, **kwargs) -> None:
        
        if self.server.debug:
            print(f""DEBUG: {message}"")
            if kwargs:
                print(json.dumps(kwargs, indent=2))",Log debug information if debug mode is enabled.,Logs debug messages and optional data if debugging is enabled.,5.0,5.0,5.0,0.6935483870967742,0.6,0.75,0.6029514074325562,0.6196317,0.76404035,0.87238,0.9962113499641418,0.3,0.4933598308242719,5.0,5.0,5.0
367,367,367,368,load_and_preprocess_image,"def load_and_preprocess_image(image_path, target_img_size):
    
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f""Failed to load image from path: {image_path}"")

    # Resize and pad the image to the target size
    resized_image = resize_image(image, target_img_size)

    # Normalize the image (assuming model expects normalized input)
    normalized_image = resized_image.astype(np.float32) / 255.0

    # Expand dimensions to match the model's input shape
    input_data = np.expand_dims(normalized_image, axis=0)  # Add batch dimension
    return resized_image, input_data","Load and preprocess an image from the specified path, resizing and padding it to the target size.","Load, resize, normalize, and prepare image for model input processing.",5.0,5.0,4.0,0.6022545137413059,0.4,0.1764705882352941,0.5987998247146606,0.26856947,0.60497963,0.6027533,0.9761310815811156,0.7,0.6217335779648492,5.0,5.0,5.0
368,368,368,369,_generate_greedy_sample,"def _generate_greedy_sample(self, prompt: str) -> str:
        
        logger.debug(""Generating greedy sample"")
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            max_tokens=self.max_tokens,
            temperature=0.0  # Greedy decoding
        )
        
        self.completion_tokens += response.usage.completion_tokens
        
        return response.choices[0].message.content.strip()",Generate a single greedy sample with temperature=0.,Generate a deterministic text response using a language model with zero randomness.,4.0,5.0,4.0,0.5662650602409639,0.25,0.375,0.6175727844238281,0.3143453,0.36136746,0.6441729,0.1107418835163116,0.25,0.3893278382178133,4.0,5.0,5.0
369,369,369,370,_prepare_toolset,"def _prepare_toolset(self) -> None:
    
    # For each API, get the first version and the first spec of that version.
    spec_str = self._apihub_client.get_spec_content(self._apihub_resource_name)
    spec_dict = yaml.safe_load(spec_str)
    if not spec_dict:
      return

    self.name = self.name or _to_snake_case(
        spec_dict.get('info', {}).get('title', 'unnamed')
    )
    self.description = self.description or spec_dict.get('info', {}).get(
        'description', ''
    )
    self._openapi_toolset = OpenAPIToolset(
        spec_dict=spec_dict,
        auth_credential=self._auth_credential,
        auth_scheme=self._auth_scheme,
        tool_filter=self.tool_filter,
    )",Fetches the spec from API Hub and generates the toolset.,Initialize API toolset with specifications and authentication details.,4.0,5.0,3.0,0.6,0.375,0.2,0.5507110357284546,0.3009692,0.67932975,0.76684654,0.909214973449707,0.25,0.5077833738563514,4.0,5.0,5.0
370,370,370,371,add_message_pair,"def add_message_pair(self, user_message: str, assistant_message: str):
        
        if not any(entry[""user""] == user_message for entry in self.cache):
            self.cache.append({""user"": user_message, ""assistant"": assistant_message})
            self._save()",Add a user/assistant pair to the cache if not present.,Add unique user-assistant message pairs to cache and save changes.,5.0,5.0,4.0,0.696969696969697,0.5454545454545454,0.5454545454545454,0.6726023554801941,0.37344733,0.61044633,0.77085197,0.930237591266632,0.0,0.7192227015335896,5.0,5.0,5.0
371,371,371,372,clean_response,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            return {}
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,"Extract and parse JSON from a string, logging errors if parsing fails.",5.0,5.0,3.0,0.5857142857142857,0.25,0.2222222222222222,0.682233452796936,0.33434805,0.6042968,0.69515693,0.6984511613845825,0.4166666666666667,0.3451086623453399,5.0,5.0,5.0
372,372,372,373,_create_session,"def _create_session(self) -> requests.Session:
        
        session = requests.Session()

        # Configure automatic retries with exponential backoff
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=self.retry_backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods={""HEAD"", ""GET"", ""POST"", ""OPTIONS""},
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)

        # Set up headers
        headers = {""Accept"": ""application/json""}
        if self.api_key:
            headers[""x-api-key""] = self.api_key

        session.headers.update(headers)

        return session",Create and configure a requests session with retry capabilities,Establishes a configured HTTP session with retry logic and custom headers.,5.0,5.0,4.0,0.7297297297297297,0.5454545454545454,0.4444444444444444,0.6065587997436523,0.44205782,0.58865964,0.8297593,0.9609264135360718,0.5454545454545454,0.5202676747143278,5.0,5.0,5.0
373,373,373,374,_snowflake_to_iso,"def _snowflake_to_iso(snowflake: str) -> str:
        
        if not DiscordRetrieveTool._is_snowflake(snowflake):
            raise ValueError(f""Invalid snowflake ID: {snowflake}"")

        # Discord epoch (2015-01-01)
        discord_epoch = 1420070400000

        # Convert ID to int and shift right 22 bits to get timestamp
        timestamp_ms = (int(snowflake) >> 22) + discord_epoch

        # Convert to datetime and format as ISO string
        dt = datetime.fromtimestamp(timestamp_ms / 1000.0, tz=timezone.utc)
        return dt.isoformat()",Convert a Discord snowflake ID to ISO timestamp string.,Convert Discord snowflake ID to ISO 8601 timestamp format.,5.0,5.0,5.0,0.8793103448275862,0.7777777777777778,0.7777777777777778,0.6413384079933167,0.738245,0.8087938,0.98072255,0.8697537779808044,0.8888888888888888,0.8125941544302078,5.0,5.0,5.0
374,374,374,375,wait_for_health,"def wait_for_health(url: str, timeout: int = 300) -> bool:
    
    print(f""Waiting for {url} to be healthy..."")
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            response = requests.get(f""{url}/health"", timeout=5)
            if response.status_code == 200:
                print(""✓ Service is healthy"")
                return True
        except requests.exceptions.RequestException:
            pass

        time.sleep(2)
        print(""."", end="""", flush=True)

    print(""\n✗ Service health check timed out"")
    return False",Wait for service to be healthy.,Check if a service is healthy within a specified timeout period.,5.0,5.0,3.0,0.453125,0.1818181818181818,0.3333333333333333,0.5387248396873474,0.29192907,0.66982013,0.4973394,0.9140611886978148,0.7272727272727273,0.5765512753146598,5.0,5.0,5.0
375,375,375,376,create,"def create() -> None:
    
    adk_app = AdkApp(agent=root_agent, enable_tracing=True)

    remote_agent = agent_engines.create(
        adk_app,
        display_name=root_agent.name,
        requirements=[
            ""google-adk (>=0.0.2)"",
            ""google-cloud-aiplatform[agent_engines] (>=1.91.0,!=1.92.0)"",
            ""google-genai (>=1.5.0,<2.0.0)"",
            ""pydantic (>=2.10.6,<3.0.0)"",
            ""absl-py (>=2.2.1,<3.0.0)"",
        ],
        #        extra_packages=[""""],
    )
    print(f""Created remote agent: {remote_agent.resource_name}"")",Creates an agent engine for Academic Research.,Initialize and configure a remote agent with specified dependencies and tracing enabled.,5.0,5.0,4.0,0.4886363636363636,0.0833333333333333,0.1428571428571428,0.5399994254112244,0.24629013,0.44975972,0.596666,0.9128068089485168,0.25,0.4896523671958637,5.0,5.0,5.0
376,376,376,377,configure,"def configure(cls, **kwargs):
        
        for key, value in kwargs.items():
            key = key.strip().lower()
            if hasattr(cls, key):
                if key in cls.parser_keywords:
                    setattr(cls, key, value)
                else:
                    # Yup, no fun allowed LOL
                    raise AttributeError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')
            else:
                raise ValueError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')

        if not kwargs:
            raise AttributeError(f'You must pass a keyword to configure, current keywords: {cls.parser_keywords}?')","Set multiple arguments for the parser at once globally :param kwargs: The keywords can be any arguments of the following: huge_tree, keep_comments, keep_cdata, auto_match, storage, storage_args, automatch_domain","Configure class attributes using validated keyword arguments, raising errors for unknown keys.",5.0,5.0,4.0,0.2696477452886903,0.25,0.0606060606060606,0.6449633240699768,-0.1918799,0.36657578,0.76044977,0.8811193108558655,0.25,0.4098010753805887,5.0,5.0,5.0
377,377,377,378,__init__,"def __init__(
        self,
        config,
        actor_module: nn.Module,
        actor_optimizer: torch.optim.Optimizer = None,
    ):
        
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer
        self.use_remove_padding = self.config.get('use_remove_padding', False)
        print(f'Actor use_remove_padding={self.use_remove_padding}')
        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = torch.compile(verl_F.entropy_from_logits, dynamic=True)","When optimizer is None, it is Reference Policy","Initialize actor with configuration, module, optimizer, and entropy computation settings.",5.0,5.0,3.0,0.449438202247191,0.1,0.125,0.5904783606529236,0.05578516,0.26716545,0.47215283,0.9561812877655028,0.0,0.5836473361468871,4.0,5.0,5.0
378,378,378,379,list_available,"def list_available(cls) -> List[str]:
        
        keys = list(Registrable._registry[cls].keys())
        default = cls.default_implementation

        if default is None:
            return keys
        elif default not in keys:
            raise ConfigurationError(
                f""Default implementation {default} is not registered""
            )
        else:
            return [default] + [k for k in keys if k != default]",List default first if it exists,Retrieve and prioritize registered implementations for a class,5.0,5.0,3.0,0.4193548387096774,0.0,0.0,0.4560204446315765,-0.07872419,0.2203991,0.54546565,0.8393629193305969,0.5,0.4210840248980387,5.0,5.0,4.0
379,379,379,380,mock_session,"def mock_session():
    
    from unittest.mock import AsyncMock, MagicMock

    # Create mock connector
    connector = MagicMock()
    connector.connect = AsyncMock()
    connector.disconnect = AsyncMock()
    connector.initialize = AsyncMock(return_value={""session_id"": ""test_session""})
    connector.tools = [{""name"": ""test_tool""}]
    connector.call_tool = AsyncMock(return_value={""result"": ""success""})

    return connector",Return a mock session object for testing.,Create a mock asynchronous session connector for testing purposes.,5.0,5.0,5.0,0.5757575757575758,0.5555555555555556,0.7142857142857143,0.5583035945892334,0.7941862,0.60905474,0.76490086,0.8600949645042419,0.4444444444444444,0.7351416483112261,4.0,5.0,5.0
380,380,380,381,load_json_file,"def load_json_file(self, path: str) -> dict:
        
        json_memory = {}
        try:
            with open(path, 'r') as f:
                json_memory = json.load(f)
        except FileNotFoundError:
            self.logger.warning(f""File not found: {path}"")
            return {}
        except json.JSONDecodeError:
            self.logger.warning(f""Error decoding JSON from file: {path}"")
            return {}
        except Exception as e:
            self.logger.warning(f""Error loading file {path}: {e}"")
            return {}
        return json_memory",Load a JSON file.,"Load JSON data from a file, handling errors gracefully.",5.0,5.0,5.0,0.3090909090909091,0.4444444444444444,0.75,0.58458411693573,0.73002875,0.62568545,0.67445165,0.974480390548706,0.4444444444444444,0.4766360373987901,5.0,5.0,5.0
381,381,381,382,__str__,"def __str__(self) -> str:
        
        plan_str = """"
        if self.task is not None:
            plan_str += f""Task: {self.task}\n""
        for i, step in enumerate(self.steps):
            plan_str += f""{i}. {step.agent_name}: {step.title}\n   {step.details}\n""
        return plan_str",Return the string representation of the plan.,Formats and returns a string representation of a task plan with steps.,5.0,5.0,5.0,0.6,0.4166666666666667,0.7142857142857143,0.6404599547386169,0.5827609,0.7809757,0.6866857,0.5845521092414856,0.3333333333333333,0.6487484672867436,5.0,5.0,5.0
382,382,382,383,get_current_date,"def get_current_date() -> dict:
    
    return {""current_date"": datetime.now().strftime(""%Y-%m-%d"")}",Get the current date in the format YYYY-MM-DD,Returns a dictionary with the current date formatted as a string.,3.0,5.0,4.0,0.5230769230769231,0.3636363636363636,0.4,0.6927019357681274,0.16969714,0.44392312,0.77373743,0.8267186284065247,0.0909090909090909,0.6924901652553814,5.0,5.0,5.0
383,383,383,384,fetch_presigned_datastore,"def fetch_presigned_datastore(presigned_url):
    
    try:
        # Clean up the presigned URL (sometimes the signature may need re-encoding)
        url_parts = urlsplit(presigned_url)
        query_params = parse_qs(url_parts.query)
        encoded_query = urlencode(query_params, doseq=True)
        cleaned_url = urlunsplit((url_parts.scheme, url_parts.netloc, url_parts.path, encoded_query, url_parts.fragment))

        resp = requests.get(cleaned_url, headers={""Host"": url_parts.netloc, ""User-Agent"": ""Mozilla/5.0""})
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        print(f""Error fetching datastore from {presigned_url}: {e}"")
        return {}",Fetch the JSON datastore from the presigned URL.,"Fetch and return JSON data from a cleaned presigned URL, handling errors.",4.0,5.0,5.0,0.6164383561643836,0.4166666666666667,0.625,0.6418895721435547,0.5623945,0.6398908,0.7061746,0.916290521621704,0.4166666666666667,0.6078355416988374,5.0,5.0,5.0
384,384,384,385,cleanup_memory,"def cleanup_memory(self, force: bool = False) -> None:
        
        # Run Python garbage collection
        gc.collect()
        
        # Empty CUDA cache if available
        if self.cuda_available:
            torch.cuda.empty_cache()
            
        # Log memory status after cleanup
        if force:
            info = self.get_memory_info()
            logger.info(
                f""Memory after cleanup: RAM: {info['ram_used_gb']:.2f}GB / {info['ram_total_gb']:.2f}GB, ""
                f""VRAM: {info.get('vram_used_gb', 0):.2f}GB / {info.get('vram_total_gb', 0):.2f}GB""
            )",Free up memory by garbage collection and emptying CUDA cache.,Optimize system memory by clearing caches and logging status if forced,5.0,5.0,4.0,0.7428571428571429,0.3636363636363636,0.3,0.5790725350379944,0.45578375,0.56668717,0.7056979,0.8616779446601868,0.4545454545454545,0.5228353966224123,4.0,4.0,4.0
385,385,385,386,add_prompt,"def add_prompt(
        self,
        prompt: Prompt,
    ) -> Prompt:
        

        # Check for duplicates
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing

        self._prompts[prompt.name] = prompt
        return prompt",Add a prompt to the manager.,"Add unique prompt to collection, warn if duplicate exists",5.0,5.0,3.0,0.4035087719298245,0.3333333333333333,0.5,0.5435367822647095,0.4475317,0.39872414,0.6600569,0.7962614297866821,0.4444444444444444,0.5860873368326345,5.0,5.0,4.0
386,386,386,387,save_model_settings,"def save_model_settings(model, state):
    
    if model == 'None':
        yield (""Not saving the settings because no model is selected in the menu."")
        return

    user_config = shared.load_user_config()
    model_regex = model + '$'  # For exact matches
    if model_regex not in user_config:
        user_config[model_regex] = {}

    for k in ui.list_model_elements():
        if k == 'loader' or k in loaders.loaders_and_params[state['loader']]:
            user_config[model_regex][k] = state[k]

    shared.user_config = user_config

    output = yaml.dump(user_config, sort_keys=False)
    p = Path(f'{shared.args.model_dir}/config-user.yaml')
    with open(p, 'w') as f:
        f.write(output)

    yield (f""Settings for `{model}` saved to `{p}`."")",Save the settings for this model to models/config-user.yaml,Save model configuration to a YAML file if a model is selected.,4.0,5.0,5.0,0.7301587301587301,0.4166666666666667,0.3636363636363636,0.6340999603271484,0.3656456,0.8501743,0.76798695,0.8301814794540405,0.75,0.6028342854511437,4.0,5.0,5.0
387,387,387,388,default_usage,"def default_usage(self):
        
        return {
            ""completion_tokens"": 100,
            ""prompt_tokens"": 150,
            ""total_tokens"": 250,
        }",Returns a default usage object for testing.,Defines a method to return default token usage statistics as a dictionary.,4.0,5.0,5.0,0.527027027027027,0.3333333333333333,0.4285714285714285,0.5623735785484314,0.4542203,0.5927584,0.7927093,0.881583571434021,0.25,0.5311945727809128,5.0,5.0,5.0
388,388,388,389,generate_audio,"def generate_audio(self, text: str, voice: str, model: str, voice2: str = None) -> bytes:
        
        self.validate_parameters(text, voice, model)
        
        try:
            response = openai.audio.speech.create(
                model=model,
                voice=voice,
                input=text
            )
            return response.content
        except Exception as e:
            raise RuntimeError(f""Failed to generate audio: {str(e)}"") from e",Generate audio using OpenAI API.,Generate audio from text using specified voice and model.,5.0,5.0,5.0,0.4561403508771929,0.3333333333333333,0.6,0.5867081880569458,0.51946265,0.4895507,0.77851325,0.9012960195541382,0.6666666666666666,0.6615081219859217,4.0,5.0,5.0
389,389,389,390,get_target_module,"def get_target_module(obj: object) -> Optional[str]:
    
    if not hasattr(obj, ""__module__""):
        return None

    fqn = f""{obj.__module__}.{obj.__name__}""
    return _PDOC_MODULE_EXPORT_MAPPINGS.get(fqn)",Get the target module where an object should be documented.,Determine the module name of an object using its fully qualified name.,2.9,2.8,4.5,0.6571428571428571,0.3333333333333333,0.4,0.6441521644592285,0.4164015,0.7083922,0.7513337,0.8826481103897095,0.1666666666666666,0.4580401519740924,3.0,4.0,4.0
390,390,390,391,validate_graph_fields,"def validate_graph_fields(self) -> ""GraphPromptOverrides"":
        
        allowed_fields = {""entity_extraction"", ""entity_resolution""}
        for field in self.model_fields:
            if field not in allowed_fields and getattr(self, field, None) is not None:
                raise ValueError(f""Field '{field}' is not allowed in graph prompt overrides"")
        return self",Ensure only graph-related fields are present.,Ensure graph fields are valid by checking against allowed set,2.6,3.0,2.7,0.639344262295082,0.4,0.5714285714285714,0.605628252029419,0.36883122,0.6874396,0.58970046,0.9264317154884338,0.3,0.5440165771413585,5.0,5.0,4.0
391,391,391,392,_log_vectorization_start,"def _log_vectorization_start(
        self, processed_entities: List[BaseEntity], sync_context: SyncContext, entity_context: str
    ) -> None:
        
        embedding_model = sync_context.embedding_model
        entity_count = len(processed_entities)

        sync_context.logger.info(
            f""Computing vectors for {entity_count} entities using {embedding_model.model_name}""
        )",Log vectorization startup information.,Log vectorization process start for entities with model details,5.0,5.0,4.0,0.5555555555555556,0.2222222222222222,0.5,0.5558763742446899,0.55864996,0.78822136,0.42974126,0.921064555644989,0.2222222222222222,0.6563001333482397,5.0,5.0,4.0
392,392,392,393,_truncate_multimodal_text,"def _truncate_multimodal_text(self, contents: list[dict[str, Any]], n_tokens: int) -> list[dict[str, Any]]:
        
        tmp_contents = []
        for content in contents:
            if content[""type""] == ""text"":
                truncated_text = self._truncate_tokens(content[""text""], n_tokens)
                tmp_contents.append({""type"": ""text"", ""text"": truncated_text})
            else:
                tmp_contents.append(content)
        return tmp_contents","Truncates text content within a list of multimodal elements, preserving the overall structure.",Truncate text content in a multimodal list to a specified token limit.,5.0,5.0,4.0,0.6691830473640327,0.5,0.3846153846153846,0.6254178285598755,0.5599931,0.621616,0.83523905,0.92705237865448,0.3333333333333333,0.6141062813006923,5.0,5.0,5.0
393,393,393,394,format_validation_errors,"def format_validation_errors(errors: list) -> str:
    
    formatted_errors = []

    for error in errors:
        loc = error.get(""loc"", [])
        msg = error.get(""msg"", """")
        error_type = error.get(""type"", """")

        # Build field path
        field_path = "" -> "".join(str(part) for part in loc if part != ""body"")

        # Format the error message with type information
        if field_path:
            if error_type:
                formatted_error = f""Field '{field_path}' ({error_type}): {msg}""
            else:
                formatted_error = f""Field '{field_path}': {msg}""
        else:
            formatted_error = msg

        formatted_errors.append(formatted_error)

    return ""; "".join(formatted_errors)",Format validation errors into a more readable string.,Format a list of validation errors into a structured string message.,5.0,5.0,5.0,0.7647058823529411,0.5454545454545454,0.75,0.5424818396568298,0.60280716,0.7302001,0.82972,0.9860341548919678,0.6363636363636364,0.7006572617634816,5.0,5.0,5.0
394,394,394,395,check_hub_revision_exists,"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id, revision=training_args.hub_model_revision
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )",Checks if a given Hub revision exists.,Check if a specific model revision exists on the hub and validate its README presence.,5.0,5.0,5.0,0.4186046511627907,0.4,0.7142857142857143,0.6277941465377808,0.6280203,0.7298037,0.7177168,0.935046374797821,0.6666666666666666,0.586204548177967,5.0,5.0,4.0
395,395,395,396,save_partial_results,"def save_partial_results(self, agent, task):
        
        try:
            if task in self.tasks and agent in self.completions and task in self.completions[agent]:
                task_client = self.tasks[task]
                overall = task_client.calculate_overall(self.completions[agent][task])
                output_dir = self.get_output_dir(agent, task)
                os.makedirs(output_dir, exist_ok=True)
                with open(os.path.join(output_dir, ""overall.json""), ""w"") as f:
                    f.write(json.dumps(overall, indent=4, ensure_ascii=False))
                return overall
        except Exception as e:
            print(ColorMessage.yellow(f""Warning: Failed to save partial results for {agent}/{task}: {str(e)}""))
        return None",Calculate and save partial results for a specific agent-task pair,"Save and log task completion results for agents, handling exceptions.",4.0,5.0,3.0,0.7391304347826086,0.6,0.3636363636363636,0.5452163219451904,0.41435236,0.6328442,0.74059314,0.843643844127655,0.6,0.6806897743397323,3.0,4.0,4.0
396,396,396,397,do_Tm,"def do_Tm(
        self,
        a: PDFStackT,
        b: PDFStackT,
        c: PDFStackT,
        d: PDFStackT,
        e: PDFStackT,
        f: PDFStackT,
    ) -> None:
        
        values = (a, b, c, d, e, f)
        matrix = safe_matrix(*values)

        if matrix is None:
            log.warning(
                f""Could not set text matrix because not all values in {values!r} can be parsed as floats""
            )
        else:
            self.textstate.matrix = matrix
            self.textstate.linematrix = (0, 0)",Set text matrix and text line matrix,"Set text transformation matrix if values are valid floats, else log warning.",4.0,5.0,5.0,0.4473684210526316,0.25,0.4285714285714285,0.6185796856880188,0.44455513,0.44022,0.7281212,0.7757858633995056,0.5833333333333334,0.4595067539559841,5.0,5.0,5.0
397,397,397,398,get_tool_metadata,"def get_tool_metadata(tool_name_or_schema: str | ToolSchema) -> ToolMetadata:
    
    tool_name: str | None = (
        tool_name_or_schema
        if isinstance(tool_name_or_schema, str)
        else tool_name_or_schema.get(""name"")
    )

    metadata = _tool_metadata.get(tool_name)

    if metadata is None:
        raise ValueError(f""Tool {tool_name} not found in metadata."")

    return metadata",Get the metadata for a tool by its name.,"Retrieve tool metadata using name or schema, raising error if not found.",4.0,5.0,5.0,0.5,0.25,0.2222222222222222,0.615593433380127,0.32777444,0.49077946,0.71375036,0.9579521417617798,0.3333333333333333,0.6771474532003617,5.0,5.0,5.0
398,398,398,399,fetch_branches,"def fetch_branches(owner: str, repo: str):
        

        response = requests.get(url, headers=headers)

        if response.status_code == 404:
            if not token:
                print(f""Error 404: Repository not found or is private.\n""
                      f""If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable."")
            else:
                print(f""Error 404: Repository not found or insufficient permissions with the provided token.\n""
                      f""Please verify the repository exists and the token has access to this repository."")
            return []
            
        if response.status_code != 200:
            print(f""Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}"")
            return []

        return response.json()",Get brancshes of the repository,"Fetch GitHub repository branches, handling errors for access and existence.",5.0,5.0,5.0,0.4133333333333333,0.1,0.2,0.5876521468162537,0.15455349,0.40027264,0.55417466,0.969549834728241,0.6,0.5438590452625742,4.0,5.0,5.0
399,399,399,400,from_runnable_config,"def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        
        # Get raw values from environment or config
        raw_values: dict[str, Any] = {
            name: os.environ.get(name.upper(), configurable.get(name))
            for name in cls.model_fields.keys()
        }
        
        # Filter out None values
        values = {k: v for k, v in raw_values.items() if v is not None}
        
        return cls(**values)",Create a Configuration instance from a RunnableConfig.,Create a configuration object using environment variables and optional settings.,5.0,5.0,5.0,0.6125,0.3,0.4285714285714285,0.5752326846122742,0.39839268,0.5459637,0.70015764,0.8645334839820862,0.3,0.3984888612566548,4.0,5.0,5.0
400,400,400,401,get_model,"def get_model(cls, base_id, task_type):
        
        if base_id in cls.registry[""model_specific""]:
            return cls.registry[""model_specific""][base_id]
        elif task_type in cls.registry[""task_type""]:
            return cls.registry[""task_type""][task_type]
        else:
            raise ValueError(f""No model class found for model '{base_id}' or task type '{task_type}'"")","Get a model class, first by model name, then by task type.",Determine model class based on base ID or task type from registry,5.0,5.0,5.0,0.7692307692307693,0.3333333333333333,0.3333333333333333,0.5989616513252258,0.37112334,0.6567634,0.6658205,0.9303324222564696,0.5833333333333334,0.650330592414101,5.0,5.0,4.0
401,401,401,402,_format_sources,"def _format_sources(self, documents: List[Document]) -> str:
        
        sources = []
        for doc in documents:
            source_id = doc.metadata[""index""]
            sources.append(f""[{source_id}] {doc.page_content}"")
        return ""\n\n"".join(sources)",Format sources with numbers for citation.,Formats document sources into a structured string with indexed content.,5.0,5.0,5.0,0.5492957746478874,0.3,0.5,0.5561311841011047,0.18133962,0.5945735,0.6891358,0.9406166076660156,0.2,0.5843232651977794,5.0,5.0,5.0
402,402,402,403,_compile_dependencies,"def _compile_dependencies():
    
    if torch.distributed.get_rank() == 0:
        start_time = time.time()
        logger.info(""> Compiling dataset index builder..."")
        try:
            from core.datasets.utils import compile_helpers
            compile_helpers()
            logger.info(
                f"">>> Done with dataset index builder. Compilation time: {time.time() - start_time:.3f} seconds""
            )
        except Exception as e:
            logger.error(f""Failed to compile helpers: {e}"")
            raise",Compile dataset C++ code.,Compile dataset index builder on the main process and log the duration.,5.0,5.0,5.0,0.3098591549295774,0.1666666666666666,0.5,0.5998172760009766,0.3699377,0.47940147,0.63144,0.5792991518974304,0.3333333333333333,0.5734007511937533,5.0,5.0,5.0
403,403,403,404,crawl,"def crawl(self) -> List[Dict]:
        
        to_visit = [self.base_url]
        results = []
        
        while to_visit and len(self.visited) < self.max_pages:
            url = to_visit.pop(0)
            
            if url in self.visited:
                continue
                
            print(f""Crawling: {url}"")
            content = self.extract_page_content(url)
            
            if content:
                self.visited.add(url)
                results.append(content)
                
                # Add new URLs to visit
                new_urls = [url for url in content[""links""] 
                          if url not in self.visited 
                          and url not in to_visit]
                to_visit.extend(new_urls)
        
        return results",Crawl website starting from base_url,Web crawler iteratively collects page content and discovers new links.,5.0,5.0,5.0,0.4285714285714285,0.0,0.0,0.5424215197563171,0.12136601,0.56763405,0.51925,0.9402162432670592,0.3,0.5531425301920287,4.0,5.0,5.0
404,404,404,405,clean_response,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            return {}
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,Extracts and parses JSON data from a formatted string response.,5.0,5.0,2.0,0.7301587301587301,0.3,0.2222222222222222,0.6473636627197266,0.30014148,0.6875542,0.6923138,0.7162757515907288,0.1,0.3812312615540413,5.0,5.0,5.0
405,405,405,406,replace_message_at,"def replace_message_at(
        self, index, text_content, image_content=None, image_detail=""high""
    ):
        
        if index < len(self.messages):
            self.messages[index] = {
                ""role"": self.messages[index][""role""],
                ""content"": [{""type"": ""text"", ""text"": text_content}],
            }
            if image_content:
                base64_image = self.encode_image(image_content)
                self.messages[index][""content""].append(
                    {
                        ""type"": ""image_url"",
                        ""image_url"": {
                            ""url"": f""data:image/png;base64,{base64_image}"",
                            ""detail"": image_detail,
                        },
                    }
                )",Replace a message at a given index,Update message content with optional image in a message list,5.0,5.0,3.0,0.5166666666666667,0.2,0.2857142857142857,0.496770828962326,0.204464,0.49602365,0.716076,0.821196436882019,0.2,0.538636639527978,3.0,5.0,4.0
406,406,406,407,_make_output,"def _make_output(
        self,
        file_content: str,
        file_descriptor: str,
        init_line: int = 1,
        expand_tabs: bool = True,
    ) -> str:
        
        file_content = maybe_truncate(file_content)
        if expand_tabs:
            file_content = file_content.expandtabs()
        file_content = ""\n"".join(
            [f""{i + init_line:6}\t{line}"" for i, line in enumerate(file_content.split(""\n""))]
        )
        return (
            f""Here's the result of running `cat -n` on {file_descriptor}:\n"" + file_content + ""\n""
        )",Generate output for the CLI based on the content of a file.,Format file content with line numbers and optional tab expansion,5.0,5.0,5.0,0.6875,0.2,0.0833333333333333,0.4154972434043884,0.22934143,0.3492574,0.70261335,0.8180736899375916,0.1,0.4259105836799881,4.0,5.0,4.0
407,407,407,408,get_transcript_object,"def get_transcript_object(self) -> aai.Transcript:
        
        logger.info(""Starting video transcription"")
        transcript = self.transcriber.transcribe(self.video_path)
        if transcript.status == ""error"":
            logger.error(f""Transcription failed: {transcript.error}"")
            raise ValueError(f""Transcription failed: {transcript.error}"")
        if not transcript.words:
            logger.warning(""No words found in transcript"")
        logger.info(""Transcription completed successfully!"")

        return transcript",Get the transcript object from AssemblyAI,"Transcribe video, handle errors, and return transcript object.",5.0,5.0,5.0,0.5161290322580645,0.25,0.3333333333333333,0.6128183007240295,-0.0122954585,0.48761266,0.6046386,0.934136152267456,0.375,0.6253998942189445,5.0,5.0,5.0
408,408,408,409,update_status,"def update_status(self, agent_name: str, ticker: Optional[str] = None, status: str = """", analysis: Optional[str] = None):
        
        if agent_name not in self.agent_status:
            self.agent_status[agent_name] = {""status"": """", ""ticker"": None}

        if ticker:
            self.agent_status[agent_name][""ticker""] = ticker
        if status:
            self.agent_status[agent_name][""status""] = status
        if analysis:
            self.agent_status[agent_name][""analysis""] = analysis
        
        # Set the timestamp as UTC datetime
        timestamp = datetime.now(timezone.utc).isoformat()
        self.agent_status[agent_name][""timestamp""] = timestamp

        # Notify all registered handlers
        for handler in self.update_handlers:
            handler(agent_name, ticker, status, analysis, timestamp)

        self._refresh_display()",Update the status of an agent.,Update agent status and notify handlers with timestamped changes,5.0,5.0,5.0,0.453125,0.3333333333333333,0.3333333333333333,0.5105582475662231,0.39682427,0.7237394,0.69807726,0.9827880859375,0.3333333333333333,0.6486685584032497,5.0,5.0,5.0
409,409,409,410,reorder,"def reorder(self, indices: torch.Tensor) -> None:
        
        indices_np = indices.detach().numpy()
        self.batch = self.batch[indices]
        self.non_tensor_batch = {key: value[indices_np] for key, value in self.non_tensor_batch.items()}",Note that this operation is in-place,Reorder batch data using specified index mapping.,5.0,5.0,5.0,0.5510204081632653,0.0,0.0,0.5338035821914673,0.13604344,0.32785264,0.49287513,0.8713870644569397,0.0,0.562376468902349,4.0,5.0,5.0
410,410,410,411,register_mimetype_component_type,"def register_mimetype_component_type(mimetype: str, component_type: Optional[str] = None):
    
    if not re.match(r""^[^/]+/[^/]+$"", mimetype):
        logger.warning(f""[registry] Suspicious mimetype format: {mimetype}"")
    _mimetype_to_component_type[mimetype] = component_type or ""generic""",Register a component type string to handle a given mimetype.,"Validate and map mimetype to component type, defaulting to ""generic"".",5.0,5.0,5.0,0.7681159420289855,0.4,0.3,0.6697536706924438,0.4054378,0.61917657,0.62846684,0.8978786468505859,0.2,0.6529047753397101,5.0,5.0,5.0
411,411,411,412,is_ecr_image,"def is_ecr_image(image_uri: str) -> bool:
    
    import re

    try:
        else:
            parse_uri = urlparse(image_uri)

        hostname = parse_uri.netloc.lower()

        # Check for malformed hostnames (double dots, etc.)
        if "".."" in hostname or hostname.startswith(""."") or hostname.endswith("".""):
            return False

        # Ensure the hostname ends with amazonaws.com (proper domain validation)
        if not hostname.endswith("".amazonaws.com""):
            return False

        # Check for proper ECR hostname structure: account-id.dkr.ecr.region.amazonaws.com
        ecr_pattern = r""^\d{12}\.dkr\.ecr\.[a-z0-9-]+\.amazonaws\.com$""

        return bool(re.match(ecr_pattern, hostname))

    except Exception:
        return False",Determine if an image is from ECR.,Determine if a given URI is a valid Amazon ECR image URL.,5.0,5.0,5.0,0.5614035087719298,0.4166666666666667,0.5714285714285714,0.6260532736778259,0.7124051,0.5747627,0.6987583,0.873291015625,0.4166666666666667,0.7095489982388984,5.0,5.0,5.0
412,412,412,413,parse_transform,"def parse_transform(style: str) -> tuple[float, float] | None:
	
	try:
		parts = style.split('(')[1].split(')')[0].split(',')
		x_px_str = float(parts[0].strip().replace('px', ''))
		y_px_str = float(parts[1].strip().replace('px', ''))
		return x_px_str, y_px_str
	except Exception as e:
		logger.error(f'Error parsing transform style: {e}')
		return None, None",Extracts x and y pixel coordinates from a CSS transform string.,Extracts and converts pixel values from a CSS transform string into a coordinate tuple.,5.0,5.0,5.0,0.7011494252873564,0.6428571428571429,0.7272727272727273,0.6662793159484863,0.5899882,0.700794,0.8493639,0.6811222434043884,0.2142857142857142,0.4963703294298999,4.0,5.0,5.0
413,413,413,414,check_ollama_running,"def check_ollama_running(base_url):
    
    try:
        api_url = f""{base_url}/api/tags""
        response = requests.get(api_url, timeout=2)
        return response.status_code == 200
    except requests.RequestException:
        return False",Check if Ollama is running and accessible at the given URL.,Check if the Ollama service is active by querying its API endpoint.,5.0,5.0,5.0,0.746268656716418,0.4166666666666667,0.3636363636363636,0.6172070503234863,0.5333651,0.7585646,0.65215874,0.8610069751739502,0.0,0.5848053462321731,5.0,5.0,5.0
414,414,414,415,count,"def count(variable: Variable, sparql_executor):
    
    rtn_str = f""Observation: variable ##, which is a number""
    new_variable = Variable(""type.int"", f""(COUNT {variable.program})"")
    return new_variable, rtn_str",Count the number of a variable :param variable: a variable :return: the number of a variable,Create a new integer variable representing the count of a given variable's occurrences.,5.0,5.0,5.0,0.6945464060436618,0.5,0.375,0.6258265376091003,0.19003877,0.68947536,0.78558713,0.8844388127326965,0.2307692307692307,0.59523898211599,3.0,4.0,4.0
415,415,415,416,download_article,"def download_article(title: str) -> str:
    
    try:
        page = wikipedia.page(title)
        file_path = STORAGE_PATH / f""{title.replace(' ', '_')}.txt""
        file_path.write_text(page.content, encoding=""utf-8"")
        return f""Downloaded '{title}' to {file_path.name}""
    except wikipedia.exceptions.DisambiguationError as e:
        return f""Ambiguous title '{title}'. Possible options: {', '.join(e.options[:5])}...""
    except wikipedia.exceptions.PageError:
        return f""Article '{title}' not found.""",Download a Wikipedia article and store it as a text file.,"Download and save Wikipedia article content to a text file, handling errors.",4.0,5.0,5.0,0.75,0.5833333333333334,0.5454545454545454,0.5883274078369141,0.6363275,0.8002311,0.8362645,0.9173600673675536,0.1666666666666666,0.5514582904080907,5.0,5.0,5.0
416,416,416,417,is_unsubscribe_allowed,"def is_unsubscribe_allowed(
    mcp: FastMCP, sns_client: Any, kwargs: Dict[str, Any]
) -> Tuple[bool, str]:
    
    subscription_arn = kwargs.get('SubscriptionArn')

    if subscription_arn is None or subscription_arn == '':
        return False, 'SubscriptionArn is not passed to the tool'

    try:
        # Get subscription attributes to find the TopicArn
        attributes = sns_client.get_subscription_attributes(SubscriptionArn=subscription_arn)
        topic_arn = attributes.get('Attributes', {}).get('TopicArn')

        return is_mutative_action_allowed(mcp, sns_client, {'TopicArn': topic_arn})

    except Exception as e:
        return False, str(e)",Check if the SNS subscription being unsubscribed is from a tagged topic.,Determine if unsubscription is permitted based on subscription attributes and conditions.,5.0,5.0,4.0,0.6629213483146067,0.2727272727272727,0.1666666666666666,0.5698640942573547,0.31101638,0.66447616,0.68274987,0.9563683867454528,0.4545454545454545,0.5226331691895476,4.0,5.0,4.0
417,417,417,418,_cleanup_temp_files,"def _cleanup_temp_files(self, temp_dir: str) -> None:
        
        try:
            for file in os.listdir(temp_dir):
                os.remove(os.path.join(temp_dir, file))
            os.rmdir(temp_dir)
        except Exception as e:
            logger.error(f""Error cleaning up temporary files: {str(e)}"")",Clean up temporary files.,"Remove all files and directory in specified temporary folder, logging errors if any occur.",4.0,4.0,5.0,0.2666666666666666,0.1428571428571428,0.25,0.6880762577056885,0.5781713,0.6250868,0.38691014,0.9734935760498048,0.2857142857142857,0.6044793239167435,5.0,5.0,5.0
418,418,418,419,get_usage,"def get_usage(response: ChatCompletion) -> dict:
        
        return {
            ""prompt_tokens"": response.usage.prompt_tokens if response.usage is not None else 0,
            ""completion_tokens"": response.usage.completion_tokens if response.usage is not None else 0,
            ""total_tokens"": response.usage.total_tokens if response.usage is not None else 0,
            ""cost"": response.cost if hasattr(response, ""cost"") else 0.0,
            ""model"": response.model,
        }",Get the usage of tokens and their cost information.,Extracts token usage and cost details from a chat response object.,5.0,5.0,4.0,0.6818181818181818,0.3636363636363636,0.3333333333333333,0.5943182110786438,0.4987206,0.7982777,0.718947,0.8967706561088562,0.0909090909090909,0.6057009462841485,5.0,5.0,5.0
419,419,419,420,increment_and_enforce_llm_calls_limit,"def increment_and_enforce_llm_calls_limit(
      self, run_config: Optional[RunConfig]
  ):
    
    # We first increment the counter and then check the conditions.
    self._number_of_llm_calls += 1

    if (
        run_config
        and run_config.max_llm_calls > 0
        and self._number_of_llm_calls > run_config.max_llm_calls
    ):
      # We only enforce the limit if the limit is a positive number.
      raise LlmCallsLimitExceededError(
          ""Max number of llm calls limit of""
          f"" `{run_config.max_llm_calls}` exceeded""
      )",Increments _number_of_llm_calls and enforces the limit.,Increment call count and enforce maximum limit for LLM usage,5.0,5.0,5.0,0.7166666666666667,0.6,0.5555555555555556,0.5697252750396729,0.3603291,0.78658676,0.39894098,0.9415180683135986,0.6,0.6565087127589158,5.0,5.0,5.0
420,420,420,421,process_llm_request,"def process_llm_request(self, llm_request: LlmRequest) -> None:
    
    if llm_request.model and llm_request.model.startswith(""gemini-2""):
      llm_request.config = llm_request.config or types.GenerateContentConfig()
      llm_request.config.tools = llm_request.config.tools or []
      llm_request.config.tools.append(
          types.Tool(code_execution=types.ToolCodeExecution())
      )
      return
    raise ValueError(
        ""Gemini code execution tool is not supported for model""
        f"" {llm_request.model}""
    )",Pre-process the LLM request for Gemini 2.0+ models to use the code execution tool.,Configure code execution tools for specific AI model requests or raise an error.,4.0,4.0,3.0,0.768056555722312,0.4615384615384615,0.1875,0.6121405363082886,0.32055917,0.42039463,0.79299355,0.7063618302345276,0.6153846153846154,0.3926322552583849,5.0,5.0,4.0
421,421,421,422,_megatron_calc_global_rank,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    

    args = get_args()
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert (tp_size * dp_size * pp_size == torch.distributed.get_world_size()
           ), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    if args.switch_dp_and_pp_grouping:
        # TP-PP-DP grouping
        return (dp_rank * pp_size + pp_rank) * tp_size + tp_rank
    else:
        # TP-DP-PP grouping
        return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","given TP,DP,PP rank to get the global rank.","Calculate global rank based on tensor, data, and pipeline parallelism settings.",5.0,5.0,5.0,0.4177215189873418,0.1818181818181818,0.2,0.5758287310600281,0.2324719,0.40404078,0.6741652,0.9016333818435668,0.0,0.6002788065471878,5.0,5.0,5.0
422,422,422,423,infer_transport_type_from_url,"def infer_transport_type_from_url(
    url: str | AnyUrl,
) -> Literal[""streamable-http"", ""sse""]:
    
    url = str(url)
    if not url.startswith(""http""):
        raise ValueError(f""Invalid URL: {url}"")

    parsed_url = urlparse(url)
    path = parsed_url.path

    if ""/sse/"" in path or path.rstrip(""/"").endswith(""/sse""):
        return ""sse""
    else:
        return ""streamable-http""",Infer the appropriate transport type from the given URL.,Determine transport type from URL path for HTTP or SSE,5.0,5.0,5.0,0.7673433167584316,0.4,0.4444444444444444,0.5268434882164001,0.47056234,0.79048955,0.6637364,0.9585338234901428,0.4,0.616120380620207,5.0,5.0,4.0
423,423,423,424,load_local_model,"def load_local_model(local_path: str):
    
    try:
        st.session_state.messages = []
        nexa_model = NexaTextInference(
            model_path=""local_model"",
            local_path=local_path,
            **DEFAULT_PARAMS
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None",Load local model with default parameters.,Load a local machine learning model and update configuration options,4.0,5.0,3.0,0.5735294117647058,0.3,0.5,0.5016666054725647,0.5026561,0.4531295,0.73486876,0.8815123438835144,0.6,0.5494949940981969,4.0,5.0,5.0
424,424,424,425,get_status,"def get_status() -> dict[str, str | bool]:
    
    return {
        ""server_name"": server_name,
        ""debug_mode"": args.debug,
        ""original_name"": args.name,
    }",Get the current server configuration and status.,"Function returns server status with name, debug mode, and original name.",5.0,5.0,5.0,0.6111111111111112,0.2727272727272727,0.2857142857142857,0.6039390563964844,0.39664137,0.6619395,0.76002675,0.9539200663566588,0.0909090909090909,0.618208970470215,5.0,5.0,4.0
425,425,425,426,log_trainable_parameters,"def log_trainable_parameters(model: torch.nn.Module, logger: Optional[Logger] = None):
    
    trainable_params = 0
    all_param = 0
    for name, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            (logger or get_logger(__name__)).info(f""training with {name}"")
            trainable_params += param.numel()

    (logger or get_logger(__name__)).info(
        ""trainable params: %s || all params: %s || trainable%%: %s"",
        f""{trainable_params:,}"",
        f""{all_param:,}"",
        f""{trainable_params / all_param:.2%}"",
    )",Prints the number of trainable parameters in the model.,Log the count and percentage of trainable parameters in a neural network model.,5.0,5.0,5.0,0.620253164556962,0.4615384615384615,0.6666666666666666,0.6253101229667664,0.6808147,0.5226315,0.6424359,0.8555376529693604,0.3076923076923077,0.6447425947699382,5.0,5.0,5.0
426,426,426,427,_log_get_message_emoji,"def _log_get_message_emoji(message_type: str) -> str:
	
	emoji_map = {
		'HumanMessage': '町',
		'AIMessage': 'ｧ£',
		'ToolMessage': '畑',
	}
	return emoji_map.get(message_type, '式')",Get emoji for a message type - used only for logging display,Map message types to corresponding emoji symbols for logging purposes.,3.0,4.0,5.0,0.7285714285714285,0.5,0.3636363636363636,0.5607017278671265,0.30161262,0.837469,0.7435541,0.7781558036804199,0.0,0.6189831984185392,5.0,5.0,5.0
427,427,427,428,main,"def main():
    
    try:
        load_dotenv_files()

        # Register signal handler for graceful exit
        signal.signal(signal.SIGINT, handle_sigint)

        asyncio.run(run_agent_example())
    except Exception as e:
        print(f""Error running example: {e}"")
        traceback.print_exc()",Run the Anthropic agent example.,"Initialize environment, handle signals, and execute asynchronous agent with error handling.",4.0,4.0,2.0,0.3076923076923076,0.0909090909090909,0.2,0.6356247663497925,0.2811005,0.3822369,0.42239785,0.8491934537887573,0.0909090909090909,0.5451682307611352,5.0,5.0,5.0
428,428,428,429,_get_user_query,"def _get_user_query(self, task_send_params: TaskSendParams) -> str | None:
        
        if not task_send_params.message or not task_send_params.message.parts:
            logger.warning(f""No message parts found for task {task_send_params.id}"")
            return None
        for part in task_send_params.message.parts:
            # Ensure part is treated as a dictionary if it came from JSON
            part_dict = part if isinstance(part, dict) else part.model_dump()
            if part_dict.get(""type"") == ""text"" and ""text"" in part_dict:
                 return part_dict[""text""]
        logger.warning(f""No text part found in message for task {task_send_params.id}"")
        return None",Extracts the first text part from the user message.,Extracts text content from message parts in task parameters if available,5.0,3.0,3.0,0.6527777777777778,0.4545454545454545,0.4444444444444444,0.5997851490974426,0.429392,0.56825656,0.7520155,0.7784372568130493,0.6363636363636364,0.5706367776139537,5.0,5.0,3.0
429,429,429,430,calculate_cost,"def calculate_cost(input_tokens: int, output_tokens: int, model_id: str) -> float:
    
    if model_id in PRICES_PER_K_TOKENS:
        input_cost_per_k, output_cost_per_k = PRICES_PER_K_TOKENS[model_id]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        return input_cost + output_cost
    else:
        warnings.warn(
            f'Cannot get the costs for {model_id}. The cost will be 0. In your config_list, add field {{""price"" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.',
            UserWarning,
        )
        return 0",Calculate the cost of the completion using the Bedrock pricing.,Calculate total cost based on token usage and model pricing information.,5.0,5.0,3.0,0.7638888888888888,0.2727272727272727,0.3,0.5643025636672974,0.52445614,0.5835463,0.67026377,0.9458867907524108,0.2727272727272727,0.6495199314054544,5.0,5.0,5.0
430,430,430,431,get_step_footnote_content,"def get_step_footnote_content(step_log: ActionStep | PlanningStep, step_name: str) -> str:
    
    step_footnote = f""**{step_name}**""
    if step_log.token_usage is not None:
        step_footnote += f"" | Input tokens: {step_log.token_usage.input_tokens:,} | Output tokens: {step_log.token_usage.output_tokens:,}""
    step_footnote += f"" | Duration: {round(float(step_log.timing.duration), 2)}s"" if step_log.timing.duration else """"
    step_footnote_content = f
    return step_footnote_content",Get a footnote string for a step log with duration and token information,Generate a formatted footnote summarizing step details including tokens and duration.,1.4,4.5,1.5,0.7529411764705882,0.5454545454545454,0.3076923076923077,0.616517961025238,0.4265555,0.8625007,0.7766881,0.9457629323005676,0.2727272727272727,0.646837774246096,4.0,5.0,5.0
431,431,431,432,_cancel_interruptible_tasks,"def _cancel_interruptible_tasks(self) -> None:
		
		current_task = asyncio.current_task(self.loop)
		for task in asyncio.all_tasks(self.loop):
			if task != current_task and not task.done():
				task_name = task.get_name() if hasattr(task, 'get_name') else str(task)
				# Cancel tasks that match certain patterns
				if any(pattern in task_name for pattern in self.interruptible_task_patterns):
					logger.debug(f'Cancelling task: {task_name}')
					task.cancel()
					# Add exception handler to silence ""Task exception was never retrieved"" warnings
					task.add_done_callback(lambda t: t.exception() if t.cancelled() else None)

		# Also cancel the current task if it's interruptible
		if current_task and not current_task.done():
			task_name = current_task.get_name() if hasattr(current_task, 'get_name') else str(current_task)
			if any(pattern in task_name for pattern in self.interruptible_task_patterns):
				logger.debug(f'Cancelling current task: {task_name}')
				current_task.cancel()",Cancel current tasks that should be interruptible.,Cancel interruptible asynchronous tasks based on predefined name patterns.,4.0,5.0,4.0,0.6216216216216216,0.3333333333333333,0.2857142857142857,0.5465655326843262,0.42674476,0.7051151,0.7266782,0.9821007251739502,0.5555555555555556,0.5914805652750822,5.0,5.0,5.0
432,432,432,433,compare_hashes_gcs,"def compare_hashes_gcs(blob, local_file_path: str) -> bool:
    
    if os.path.exists(local_file_path):
        remote_md5_base64 = blob.md5_hash
        hash_md5 = hashlib.md5()
        with open(local_file_path, ""rb"") as f:
            for chunk in iter(lambda: f.read(8192), b""""):
                hash_md5.update(chunk)
        local_md5 = hash_md5.digest()
        remote_md5 = base64.b64decode(remote_md5_base64)
        if remote_md5 == local_md5:
            logger.info(f""File '{local_file_path}' already up-to-date. Skipping download."")
            return False
        else:
            logger.info(f""File '{local_file_path}' differs from GCS. Downloading."")
            return True
    else:
        logger.info(f""File '{local_file_path}' does not exist locally. Downloading."")
        return True",Compare MD5 hashes for GCS blobs.,Compares local and remote file hashes to determine if download is necessary.,5.0,5.0,5.0,0.3289473684210526,0.1666666666666666,0.3333333333333333,0.5507274866104126,0.3136336,0.45260754,0.618047,0.9256525039672852,0.1666666666666666,0.6242620766021966,5.0,5.0,5.0
433,433,433,434,setup_llm_logging,"def setup_llm_logging(log_dir: str) -> None:
    
    log_file = os.path.join(
        log_dir, f""llm_calls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log""
    )
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        ""%(message)s""
    )  # Only log the message since it contains the JSON
    file_handler.setFormatter(formatter)
    file_handler.addFilter(LLMCallFilter())
    for handler in logger_llm.handlers[:]:  # Remove any existing handlers
        logger_llm.removeHandler(handler)
    logger_llm.addHandler(file_handler)",Set up logging to capture only LLM calls to a directory.,Configure logging for language model calls to a timestamped file in a specified directory.,5.0,5.0,4.0,0.5555555555555556,0.3571428571428571,0.4545454545454545,0.6358115673065186,0.47707158,0.6320033,0.70939654,0.9483504891395568,0.2857142857142857,0.4957008861563678,5.0,5.0,5.0
434,434,434,435,_megatron_calc_global_rank,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0, cp_rank: int = 0, ep_rank: int = 0):
    

    # Get parallel sizes for each dimension
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    cp_size = mpu.get_context_parallel_world_size()
    # ep_size = mpu.get_expert_model_parallel_world_size()

    # Verify total GPU count matches (must be consistent with parallel_state.py)
    total_size = tp_size * dp_size * pp_size * cp_size
    assert total_size == torch.distributed.get_world_size(), f""{tp_size}x{dp_size}x{pp_size}x{cp_size} != {torch.distributed.get_world_size()}""

    # Core calculation logic (corresponds to RankGenerator order parameter)
    # Assumes default order is ""tp-cp-ep-dp-pp""
    return ((pp_rank * dp_size + dp_rank) * cp_size + cp_rank) * tp_size + tp_rank",Calculate global rank with support for CP/EP parallelism,Calculate global rank in a distributed system using parallel dimensions.,5.0,5.0,3.0,0.6111111111111112,0.4,0.4444444444444444,0.5405314564704895,0.31123713,0.525753,0.8187397,0.9711407423019408,0.3,0.5762262109566438,4.0,5.0,5.0
435,435,435,436,load_model_tokenizer,"def load_model_tokenizer(cls,model_name):
        
        model_dir = os.path.join(shared.args.model_dir,model_name)

        tokenizer = AutoTokenizer.from_pretrained(model_dir,
                                                  local_files_only=True)
        
        model = AutoModelForSequenceClassification.from_pretrained(
            model_dir,
            trust_remote_code=True,
            local_files_only=True
        )
        
        print(f""Model '{model_name}' loaded successfully "")
        return tokenizer, model",load model and tokenizer using configuration and local files.,Load tokenizer and model for sequence classification from local directory,5.0,5.0,3.0,0.7397260273972602,0.5,0.4444444444444444,0.5220933556556702,0.52922326,0.70489514,0.77432895,0.7529693841934204,0.2,0.5585879250430617,5.0,5.0,5.0
436,436,436,437,shutdown,"def shutdown(self):
        
        try:
            # Update the exporter's properties
            self._update_exporter_properties()
        except Exception as e:
            raise Exception(f""Error updating exporter properties: {e}"")

        try:
            # Forward the call to the underlying exporter
            return self._exporter.shutdown()
        except Exception as e:
            raise Exception(f""Error shutting down exporter: {e}"")",Shutdown the exporter by forwarding to the underlying exporter.,Handle exporter shutdown with error management and property updates.,4.0,5.0,2.0,0.7941176470588235,0.2222222222222222,0.1111111111111111,0.5484325885772705,0.2989749,0.4642275,0.72592455,0.957948923110962,0.1111111111111111,0.6468763030393595,5.0,5.0,5.0
437,437,437,438,__str__,"def __str__(self) -> str:
        
        info = """"
        info += ""Audio Extraction Task:\n""

        if self._auth_token:
            info += ""  auth_token: [redacted]\n""
        if self._grpc_endpoint:
            info += f""  grpc_endpoint: {self._grpc_endpoint}\n""
        if self._infer_protocol:
            info += f""  infer_protocol: {self._infer_protocol}\n""
        if self._function_id:
            info += ""  function_id: [redacted]\n""
        if self._use_ssl:
            info += f""  use_ssl: {self._use_ssl}\n""
        if self._ssl_cert:
            info += ""  ssl_cert: [redacted]\n""

        return info",Returns a string with the object's config and run time state,Generate a string summary of audio extraction task configuration,4.0,4.0,2.0,0.734375,0.2222222222222222,0.1666666666666666,0.4371577501296997,0.4196066,0.5607922,0.6636028,0.6664618253707886,0.4444444444444444,0.5614917628768739,5.0,5.0,5.0
438,438,438,439,mock_tool_context,"def mock_tool_context(self):
    
    mock_context = MagicMock(spec=ToolContext)
    mock_context.state = State({}, {})
    mock_context.get_auth_response.return_value = {}
    mock_context.request_credential.return_value = {}
    return mock_context",Fixture for a mock OperationParser.,Creates a mock tool context with predefined state and credential responses.,5.0,5.0,5.0,0.4,0.1818181818181818,0.4,0.6376016139984131,0.32548442,0.51128423,0.5380951,0.9679149985313416,0.0909090909090909,0.7424722968399813,5.0,5.0,5.0
439,439,439,440,trim_voice_tensor,"def trim_voice_tensor(tensor):
    
    if tensor.shape[0] != 511:
        raise ValueError(f""Expected tensor with first dimension 511, got {tensor.shape[0]}"")
    
    # Analyze variance contribution of each row
    variance = analyze_voice_content(tensor)
    
    # Determine which end has lower variance (less information)
    start_var = variance[:5].mean().item()
    end_var = variance[-5:].mean().item()
    
    # Remove from the end with lower variance
    if end_var < start_var:
        logger.info(""Trimming last row (lower variance at end)"")
        return tensor[:-1]
    else:
        logger.info(""Trimming first row (lower variance at start)"")
        return tensor[1:]",Trim a 511x1x256 tensor to 510x1x256 by removing the row with least impact.,Trim a voice data tensor by removing the row with the least variance.,5.0,5.0,5.0,0.730716411034808,0.6923076923076923,0.6923076923076923,0.6639723777770996,0.29510623,0.6627238,0.8720702,0.9740116000175476,0.6153846153846154,0.7044400461530286,4.0,4.0,4.0
440,440,440,441,clear_all,"def clear_all(cls, cache_dir: Path | None = None) -> None:
        
        cache_dir = cache_dir or default_cache_dir()
        if not cache_dir.exists():
            return

        file_types: list[Literal[""client_info"", ""tokens""]] = [""client_info"", ""tokens""]
        for file_type in file_types:
            for file in cache_dir.glob(f""*_{file_type}.json""):
                file.unlink(missing_ok=True)
        logger.info(""Cleared all OAuth client cache data."")",Clear all cached data for all servers.,Clear specified cache directory of OAuth client data files.,5.0,4.0,5.0,0.5423728813559322,0.3333333333333333,0.4285714285714285,0.6130730509757996,0.35585344,0.4158665,0.7480087,0.9794918298721312,0.4444444444444444,0.6229424516520451,4.0,4.0,4.0
441,441,441,442,run_simpleqa_benchmark_wrapper,"def run_simpleqa_benchmark_wrapper(args: Tuple) -> Dict[str, Any]:
    
    num_examples, output_dir, resume_from, search_config, evaluation_config = args

    logger.info(f""Starting SimpleQA benchmark with {num_examples} examples"")
    start_time = time.time()

    results = run_resumable_benchmark(
        dataset_type=""simpleqa"",
        num_examples=num_examples,
        output_dir=os.path.join(output_dir, ""simpleqa""),
        search_config=search_config,
        evaluation_config=evaluation_config,
        resume_from=resume_from,
    )

    duration = time.time() - start_time
    logger.info(f""SimpleQA benchmark completed in {duration:.1f} seconds"")

    return results",Wrapper for running SimpleQA benchmark in parallel.,Execute a resumable SimpleQA benchmark and log the results.,5.0,5.0,3.0,0.6440677966101694,0.2222222222222222,0.2857142857142857,0.5938104391098022,0.47138935,0.6816712,0.6331059,0.9208137392997742,0.4444444444444444,0.6315393345795468,4.0,5.0,5.0
442,442,442,443,_create_input_template,"def _create_input_template(self, input_data: EvaluationInput) -> str:
        
        scenarios_str = ""\n"".join(f""- {scenario}"" for scenario in input_data.scenarios)
        
        return f",Creates the input template for the LLM.,Generate a formatted string from evaluation input scenarios.,5.0,5.0,5.0,0.5333333333333333,0.125,0.1428571428571428,0.605570375919342,0.2865705,0.46292642,0.7096394,0.7500597238540649,0.125,0.6441423559912026,3.0,5.0,4.0
443,443,443,444,convert_to_regular_types,"def convert_to_regular_types(obj):
    
    from omegaconf import ListConfig, DictConfig
    if isinstance(obj, (ListConfig, DictConfig)):
        return {k: convert_to_regular_types(v) for k, v in obj.items()} if isinstance(obj, DictConfig) else list(obj)
    elif isinstance(obj, (list, tuple)):
        return [convert_to_regular_types(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: convert_to_regular_types(v) for k, v in obj.items()}
    return obj",Convert Hydra configs and other special types to regular Python types.,Converts nested OmegaConf objects to standard Python data types recursively.,5.0,5.0,4.0,0.7894736842105263,0.4,0.3636363636363636,0.5614383220672607,0.30757183,0.52040553,0.7311214,0.9176164865493774,0.1,0.5464996613493739,5.0,5.0,5.0
444,444,444,445,save_account_info,"def save_account_info(self, email, password, token, total_usage):
        
        try:
            with open(self.accounts_file, 'a', encoding='utf-8') as f:
                f.write(f""\n{'='*50}\n"")
                f.write(f""Email: {email}\n"")
                f.write(f""Password: {password}\n"")
                f.write(f""Token: {token}\n"")
                f.write(f""Usage Limit: {total_usage}\n"")
                f.write(f""{'='*50}\n"")
                
            print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('register.account_info_saved') if self.translator else 'Account information saved'}...{Style.RESET_ALL}"")
            return True
            
        except Exception as e:
            error_msg = self.translator.get('register.save_account_info_failed', error=str(e)) if self.translator else f'Failed to save account information: {str(e)}'
            print(f""{Fore.RED}{EMOJI['ERROR']} {error_msg}{Style.RESET_ALL}"")
            return False",Save account information to file,Function logs account details to a file and handles success or error messages.,4.0,3.0,4.0,0.3717948717948718,0.2307692307692307,0.6,0.5417040586471558,0.38460556,0.582694,0.6793867,0.7890086770057678,0.2307692307692307,0.424289191135302,4.0,4.0,4.0
445,445,445,446,load_pipelines,"def load_pipelines(self) -> Dict[str, Type[pipeline]]:
        
        animate_thinking(""Loading zero-shot pipeline..."", color=""status"")
        return {
            ""bart"": pipeline(""zero-shot-classification"", model=""facebook/bart-large-mnli"")
        }",Load the pipelines for the text classification used for routing.,Load a zero-shot classification pipeline using a BART model.,3.0,5.0,4.0,0.7795891541930149,0.4,0.3,0.6725555062294006,0.2901042,0.45153463,0.746842,0.9093215465545654,0.3333333333333333,0.4991761076560677,4.0,5.0,5.0
446,446,446,447,load_environment,"def load_environment():
    
    try:
        # Read and execute set_keys.sh
        with open('set_keys.sh', 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if line.startswith('export ') and '=' in line:
                # Parse export statements
                line = line.replace('export ', '')
                key, value = line.split('=', 1)
                # Remove quotes if present
                value = value.strip('""').strip(""'"")
                os.environ[key] = value
                print(f""Loaded: {key}"")
        
        print(""Environment variables loaded successfully"")
    except Exception as e:
        print(f""Error loading environment: {e}"")",Load environment variables from set_keys.sh,Load environment variables from a script file into the system environment.,5.0,5.0,4.0,0.5540540540540541,0.3636363636363636,0.5714285714285714,0.5967093706130981,0.45882082,0.70373654,0.7301114,0.9602571725845336,0.3636363636363636,0.5790105261122702,5.0,5.0,5.0
447,447,447,448,_run,"def _run(self, **kwargs) -> str:
        
        servers = self.server_manager.client.get_server_names()
        if not servers:
            return ""No MCP servers are currently defined.""

        result = ""Available MCP servers:\n""
        for i, server_name in enumerate(servers):
            active_marker = "" (ACTIVE)"" if server_name == self.server_manager.active_server else """"
            result += f""{i + 1}. {server_name}{active_marker}\n""

            tools: list = []
            try:
                # Check cache first
                if server_name in self.server_manager._server_tools:
                    tools = self.server_manager._server_tools[server_name]
                    tool_count = len(tools)
                    result += f""   {tool_count} tools available for this server\n""
            except Exception as e:
                logger.error(f""Unexpected error listing tools for server '{server_name}': {e}"")

        return result",List all available MCP servers along with their available tools.,"List available servers and their tools, highlighting the active server.",5.0,5.0,4.0,0.7183098591549296,0.5,0.5,0.5276488661766052,0.58016634,0.7708,0.841135,0.8758821487426758,0.5,0.5961540597746623,4.0,5.0,5.0
448,448,448,449,_convert_properties,"def _convert_properties(properties: Dict[str, Any]) -> str:
        
        props = []
        for k, v in properties.items():
            prop_name = GremlinStorage._to_value_map(k)
            props.append(f"".property({prop_name}, {GremlinStorage._to_value_map(v)})"")
        return """".join(props)",Create chained .property() commands from properties dict,Convert dictionary properties to Gremlin property string format.,5.0,5.0,5.0,0.71875,0.25,0.2857142857142857,0.5867921710014343,0.27023762,0.40920395,0.6611647,0.9803593754768372,0.0,0.6445122525225848,5.0,5.0,5.0
449,449,449,450,update_presigned_url,"def update_presigned_url(presigned_url, base_url):
    
    #To Do: If Proxy URL has domain name how do we handle such cases? Engineering Dependency.
    
    presigned_parts = urlparse(presigned_url)
    base_parts = urlparse(base_url)
    # Check if base_url contains localhost or an IP address
    if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
        new_netloc = base_parts.hostname  # Extract domain from base_url
        if base_parts.port:  # Add port if present in base_url
            new_netloc += f"":{base_parts.port}""
        updated_parts = presigned_parts._replace(netloc=new_netloc)
        return urlunparse(updated_parts)
    return presigned_url","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL.",Update presigned URL's domain if base URL is localhost or IP address.,4.0,4.0,2.0,0.5936356207025039,0.4615384615384615,0.2352941176470588,0.6871603727340698,0.22710094,0.63978744,0.7156831,0.9480124115943908,0.6666666666666666,0.6486546103181797,5.0,5.0,5.0
450,450,450,451,_get_current_mem_info,"def _get_current_mem_info(unit: str = ""GB"", precision: int = 2) -> Tuple[str]:
    
    assert unit in [""GB"", ""MB"", ""KB""]
    divisor = 1024**3 if unit == ""GB"" else 1024**2 if unit == ""MB"" else 1024
    mem_allocated = get_torch_device().memory_allocated()
    mem_reserved = get_torch_device().memory_reserved()
    # use get_torch_device().mem_get_info to profile device memory
    # since vllm's sleep mode works below pytorch
    mem_free, mem_total = get_torch_device().mem_get_info()
    mem_used = mem_total - mem_free
    mem_allocated = f""{mem_allocated / divisor:.{precision}f}""
    mem_reserved = f""{mem_reserved / divisor:.{precision}f}""
    mem_used = f""{mem_used / divisor:.{precision}f}""
    mem_total = f""{mem_total / divisor:.{precision}f}""
    return mem_allocated, mem_reserved, mem_used, mem_total",Get current memory usage.,Retrieve and format current device memory statistics in specified units.,5.0,4.0,4.0,0.3194444444444444,0.2,0.5,0.511608362197876,0.6122799,0.69274235,0.6384683,0.847820520401001,0.3,0.5439149485320631,5.0,5.0,5.0
451,451,451,452,summarize_text_content,"def summarize_text_content(text, text_inference):
    
    prompt = f

    response = text_inference.create_completion(prompt)
    summary = response['choices'][0]['text'].strip()
    return summary",Summarize the given text content.,Generate a summary of given text using an inference model.,5.0,5.0,5.0,0.4827586206896552,0.2,0.4,0.5649393796920776,0.39397678,0.6686983,0.5149941,0.9178079962730408,0.3,0.5705071237408441,3.0,5.0,5.0
452,452,452,453,get_aws_session,"def get_aws_session(region_name=None):
    
    profile_name = os.environ.get('AWS_PROFILE')
    region = region_name or get_region()

    if profile_name:
        logger.debug(f'Using AWS profile: {profile_name}')
        return boto3.Session(profile_name=profile_name, region_name=region)
    else:
        logger.debug('Using default AWS credential chain')
        return boto3.Session(region_name=region)",Create an AWS session using AWS Profile or default credentials.,Establish an AWS session using environment profile or default credentials.,5.0,5.0,5.0,0.7837837837837838,0.8,0.8,0.6424657702445984,0.8656856,0.90576226,0.8896985,0.8715358972549438,0.4,0.6581796158703198,5.0,5.0,5.0
453,453,453,454,handle_phue_error,"def handle_phue_error(
    light_or_group: str, operation: str, error: Exception
) -> dict[str, Any]:
    
    base_info = {""target"": light_or_group, ""operation"": operation, ""success"": False}
    if isinstance(error, KeyError):
        base_info[""error""] = f""Target '{light_or_group}' not found""
    elif isinstance(error, PhueException):
        base_info[""error""] = f""phue2 error during {operation}: {error}""
    else:
        base_info[""error""] = f""Unexpected error during {operation}: {error}""
    return base_info",Creates a standardized error response for phue2 operations.,"Handle errors in smart lighting operations, returning structured error details.",4.0,4.0,3.0,0.6329113924050633,0.2,0.25,0.5524095892906189,0.2660619,0.38318717,0.68684506,0.8758056163787842,0.4,0.4772389240796068,5.0,5.0,5.0
454,454,454,455,server_with_custom_route,"def server_with_custom_route(self):
        
        server = FastMCP()

        @server.custom_route(""/custom-route"", methods=[""GET""])
        async def custom_route(request: Request):
            return JSONResponse({""message"": ""custom route""})

        return server",Create a FastMCP server with a custom route.,Define a server with a custom GET route returning a JSON message.,5.0,5.0,3.0,0.5846153846153846,0.5,0.75,0.6440443992614746,0.59073347,0.60935295,0.70483685,0.9044787287712096,0.4166666666666667,0.57011731135094,5.0,5.0,5.0
455,455,455,456,to_dict,"def to_dict(self) -> Dict:
        
        task_properties = {}

        if self._api_key:
            task_properties[""api_key""] = self._api_key

        if self._endpoint_url:
            task_properties[""endpoint_url""] = self._endpoint_url

        if self._prompt:
            task_properties[""prompt""] = self._prompt

        if self._model_name:
            task_properties[""model_name""] = self._model_name

        return {""type"": ""caption"", ""task_properties"": task_properties}",Convert to a dict for submission to redis,Convert object attributes into a dictionary for a caption task configuration.,3.0,5.0,3.0,0.4805194805194805,0.2727272727272727,0.375,0.6027388572692871,0.2503806,0.4302522,0.73545724,0.9089292287826538,0.1818181818181818,0.5619297091289306,5.0,5.0,5.0
456,456,456,457,_exists,"def _exists(file_path: str):
    
    if file_path.startswith(""hdfs""):
        return _run_cmd(_hdfs_cmd(f""-test -e {file_path}"")) == 0
    return os.path.exists(file_path)",hdfs capable to check whether a file_path is exists,Check if a file exists locally or on HDFS based on its path.,5.0,5.0,5.0,0.6833333333333333,0.4615384615384615,0.4,0.6567065715789795,0.24236698,0.91920316,0.64330393,0.9875649213790894,0.1538461538461538,0.6174473452632835,5.0,5.0,5.0
457,457,457,458,invoke,"def invoke(self, input: Input, **kwargs) -> List[Output]:
        
        raise NotImplementedError(
            f""`invoke` is not currently supported for {self.__class__.__name__}.""
        )",Transform an input into an output sequence synchronously.,"Defines an unimplemented method to process input, raising an error if called.",3.0,4.0,2.0,0.6233766233766234,0.25,0.375,0.7007592916488647,0.23220767,0.26658973,0.6673968,0.810339093208313,0.1666666666666666,0.3211293953138963,5.0,5.0,4.0
458,458,458,459,create_sample_chunks,"def create_sample_chunks(num_chunks=2):
    
    chunks = []
    for i in range(num_chunks):
        img_b64 = create_sample_image()
        chunk = Chunk(content=img_b64, metadata={""filename"": f""test_image_{i}.png""})
        chunks.append(chunk)
    return chunks",Create sample chunks with base64-encoded images,Generate image chunks with metadata for a specified count.,4.0,5.0,3.0,0.6724137931034483,0.3333333333333333,0.2857142857142857,0.556749165058136,0.34534886,0.445126,0.8043417,0.91212797164917,0.3333333333333333,0.6127408815014095,4.0,5.0,4.0
459,459,459,460,prep,"def prep(self, shared):
        
        # List of images to process
        images = [""cat.jpg"", ""dog.jpg"", ""bird.jpg""]
        
        # List of filters to apply
        filters = [""grayscale"", ""blur"", ""sepia""]
        
        # Generate all combinations
        params = []
        for img in images:
            for f in filters:
                params.append({
                    ""input"": img,
                    ""filter"": f
                })
        
        return params",Generate parameters for each image-filter combination.,Generate combinations of images and filters for processing.,5.0,5.0,4.0,0.7627118644067796,0.625,0.4285714285714285,0.5835210084915161,0.55351496,0.7110988,0.6657171,0.9009287357330322,0.75,0.6662645973377638,5.0,5.0,5.0
460,460,460,461,language_token,"def language_token(self) -> int:
        
        if self.language is None:
            raise ValueError(f""This tokenizer does not have language token configured"")

        additional_tokens = dict(
            zip(
                self.tokenizer.additional_special_tokens,
                self.tokenizer.additional_special_tokens_ids,
            )
        )
        candidate = f""<|{self.language}|>""
        if candidate in additional_tokens:
            return additional_tokens[candidate]

        raise KeyError(f""Language {self.language} not found in tokenizer."")",Returns the token id corresponding to the value of the `language` field,Determine language token ID from tokenizer configuration or raise error if absent.,5.0,5.0,5.0,0.6829268292682927,0.25,0.1666666666666666,0.6040715575218201,0.17439461,0.49048555,0.70326304,0.9467247128486632,0.5,0.5687548141207567,5.0,5.0,4.0
461,461,461,462,_format_search_results_as_context,"def _format_search_results_as_context(self, search_results):
        
        context_snippets = []

        for i, result in enumerate(
            search_results[:10]
        ):  # Limit to prevent context overflow
            title = result.get(""title"", ""Untitled"")
            snippet = result.get(""snippet"", """")
            url = result.get(""link"", """")

            if snippet:
                context_snippets.append(
                    f""Source {i + 1}: {title}\nURL: {url}\nSnippet: {snippet}""
                )

        return ""\n\n"".join(context_snippets)",Format search results into context for question generation.,"Format top search results into a structured context summary with titles, URLs, and snippets.",5.0,5.0,5.0,0.5760869565217391,0.3571428571428571,0.625,0.6398715376853943,0.53909254,0.58626825,0.84631145,0.9104450345039368,0.2857142857142857,0.6471491355565325,5.0,5.0,5.0
462,462,462,463,tasks,"def tasks():
    
    return {
        ""split"": SplitTask(),
        ""extract_pdf"": ExtractTask(document_type=""pdf""),
    }",Fixture for common tasks.,Defines a function to return a dictionary of task objects for splitting and PDF extraction.,5.0,5.0,5.0,0.2527472527472527,0.1333333333333333,0.25,0.6336825489997864,0.19675352,0.3494112,0.34657806,0.7888904213905334,0.2,0.7121937253634824,5.0,5.0,5.0
463,463,463,464,create_html_redline,"def create_html_redline(text1: str, text2: str) -> str:
    
    d = dmp.diff_match_patch()
    diffs = d.diff_main(text2, text1)
    d.diff_cleanupSemantic(diffs)

    html_output = """"
    for op, text in diffs:
        if op == -1:  # Deletion
            html_output += (
                f'<del style=""background-color: #ffcccc;"">{text}</del>'
            )
        elif op == 1:  # Insertion
            html_output += (
                f'<ins style=""background-color: #ccffcc;"">{text}</ins>'
            )
        else:  # Unchanged
            html_output += text

    return html_output",Creates an HTML redline doc of differences between text1 and text2.,Generate HTML redline markup to highlight text differences.,5.0,5.0,5.0,0.6659982910244577,0.375,0.2727272727272727,0.5517820119857788,0.34597638,0.7177112,0.77611154,0.9765437245368958,0.125,0.6517420320144506,5.0,5.0,5.0
464,464,464,465,__init__,"def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):
        
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer

        self.use_remove_padding = self.config.get(""use_remove_padding"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_remove_padding={self.use_remove_padding}"")
        self.use_fused_kernels = self.config.get(""use_fused_kernels"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_fused_kernels={self.use_fused_kernels}"")

        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = (
            torch.compile(verl_F.entropy_from_logits, dynamic=True)
            if self.config.get(""use_torch_compile"", True)  #  use torch compile by default
            else verl_F.entropy_from_logits
        )
        self.device_name = get_device_name()","When optimizer is None, it is Reference Policy","Initialize actor module with configuration, optimizer, and optional performance settings.",5.0,5.0,5.0,0.449438202247191,0.1,0.125,0.5311424732208252,0.052955367,0.27558142,0.50334525,0.9302030801773072,0.0,0.5978663460235439,4.0,5.0,4.0
465,465,465,466,generate_and_save_audio,"def generate_and_save_audio(voice: str, output_path: str):
    
    print(f""\nGenerating audio for voice: {voice}"")
    start_time = time.time()
    
    # Generate audio using streaming response
    with client.audio.speech.with_streaming_response.create(
        model=""kokoro"",
        voice=voice,
        response_format=""wav"",
        input=text,
    ) as response:
        # Save the audio stream to file
        with open(output_path, ""wb"") as f:
            for chunk in response.iter_bytes():
                f.write(chunk)
    
    duration = time.time() - start_time
    print(f""Generated in {duration:.2f}s"")
    print(f""Saved to {output_path}"")
    return output_path",Generate audio using specified voice and save to WAV file.,Generate and save audio file using specified voice model and output path,5.0,5.0,4.0,0.75,0.6666666666666666,0.6,0.5607731342315674,0.6235782,0.75337636,0.8744526,0.9490658044815063,0.5,0.6975291754736419,5.0,5.0,5.0
466,466,466,467,greet_user,"def greet_user(
    name: str = Field(description=""The name of the person to greet""),
    title: str = Field(description=""Optional title like Mr/Ms/Dr"", default=""""),
    times: int = Field(description=""Number of times to repeat the greeting"", default=1),
) -> str:
    
    greeting = f""Hello {title + ' ' if title else ''}{name}!""
    return ""\n"".join([greeting] * times)",Greet a user with optional title and repetition,Generate a personalized greeting message repeated multiple times.,5.0,5.0,5.0,0.6153846153846154,0.25,0.125,0.6232736110687256,0.21161312,0.51406515,0.6306792,0.8968373537063599,0.375,0.4936182764726493,5.0,5.0,5.0
467,467,467,468,collate_fn,"def collate_fn(data_list: list[dict]) -> dict:
    
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)

    for data in data_list:
        for key, val in data.items():
            if isinstance(val, torch.Tensor):
                tensors[key].append(val)
            else:
                non_tensors[key].append(val)

    for key, val in tensors.items():
        tensors[key] = torch.stack(val, dim=0)

    for key, val in non_tensors.items():
        non_tensors[key] = np.array(val, dtype=object)

    return {**tensors, **non_tensors}",Collate a batch of data.,Aggregate and organize tensor and non-tensor data from a list of dictionaries.,4.0,5.0,5.0,0.2564102564102564,0.2307692307692307,0.4,0.6452904343605042,0.3904031,0.4248736,0.34295422,0.935539960861206,0.3333333333333333,0.6335662907074464,5.0,5.0,5.0
468,468,468,469,get_modules_by_category,"def get_modules_by_category():
    
    categories = {
        ""analytical"": [1, 3, 5, 10, 14, 17, 20, 23, 24, 25, 29],
        ""creative"": [2, 4, 11, 30, 34, 35, 37],
        ""systematic"": [9, 13, 16, 18, 22, 31, 33, 36, 38, 39],
        ""collaborative"": [7, 12, 15, 21],
        ""risk_oriented"": [6, 8, 14, 19],
        ""behavioral"": [27, 28],
        ""constraint_focused"": [26, 32]
    }
    
    return {
        category: [REASONING_MODULES[i-1] for i in indices]
        for category, indices in categories.items()
    }",Categorize modules by their primary focus.,Organize reasoning modules into predefined categories for structured retrieval.,5.0,5.0,5.0,0.4556962025316455,0.1111111111111111,0.1666666666666666,0.4997635185718536,0.35778478,0.5491961,0.56642497,0.7591021060943604,0.2222222222222222,0.578708976113814,4.0,5.0,5.0
469,469,469,470,format_category,"def format_category(
        self, category: RouterCategory, index: int | None = None
    ) -> str:
        

        index_str = f""{index}. "" if index is not None else "" ""
        category_str = """"

        if isinstance(category, ServerRouterCategory):
            category_str = self._format_server_category(category)
        elif isinstance(category, AgentRouterCategory):
            category_str = self._format_agent_category(category)
        else:
            category_str = self._format_function_category(category)

        return f""{index_str}{category_str}""",Format a category into a readable string.,Formats a category object into a string with optional index prefix.,5.0,5.0,4.0,0.5970149253731343,0.5454545454545454,0.8571428571428571,0.5568038821220398,0.6180859,0.66224456,0.67701393,0.9123247861862184,0.4545454545454545,0.624366377062339,5.0,5.0,5.0
470,470,470,471,tool_call_output_item,"def tool_call_output_item(
        cls, tool_call: ResponseFunctionToolCall, output: str
    ) -> FunctionCallOutput:
        
        return {
            ""call_id"": tool_call.call_id,
            ""output"": output,
            ""type"": ""function_call_output"",
        }",Creates a tool call output item from a tool call and its output.,"Create a dictionary with call ID, output, and type for function call results.",5.0,5.0,5.0,0.7402597402597403,0.4615384615384615,0.3846153846153846,0.6492296457290649,0.29240376,0.5655049,0.736857,0.7637850642204285,0.1538461538461538,0.5663954625170743,5.0,5.0,5.0
471,471,471,472,enter_text_into_element,"def enter_text_into_element(text_to_enter: str, element_id: str) -> str:
    
    print(
        f""📝 Entering text '{text_to_enter}' into element with ID: {element_id}""
    )  # Added print statement

    try:
        input_element = driver.find_element(By.ID, element_id)
        input_element.send_keys(text_to_enter)
        return (
            f""Entered text '{text_to_enter}' into element with ID: {element_id}""
        )
    except selenium.common.exceptions.NoSuchElementException:
        return ""Element with given ID not found.""
    except selenium.common.exceptions.ElementNotInteractableException:
        return ""Element not interactable, cannot click.""",Enters text into an element with the given ID.,"Automates text input into a web element by ID, handling exceptions.",5.0,5.0,5.0,0.6268656716417911,0.3636363636363636,0.4444444444444444,0.6180341839790344,0.44985887,0.46116066,0.74960166,0.7929954528808594,0.4545454545454545,0.5206744708231086,5.0,5.0,5.0
472,472,472,473,on_exit,"def on_exit(server):
    
    print(""="" * 80)
    print(""GUNICORN MASTER PROCESS: Shutting down"")
    print(f""Process ID: {os.getpid()}"")
    print(""="" * 80)

    # Release shared resources
    finalize_share_data()

    print(""="" * 80)
    print(""Gunicorn shutdown complete"")
    print(""="" * 80)",Executed when Gunicorn is shutting down.,Gracefully shutdown server and release resources on exit,5.0,5.0,5.0,0.5357142857142857,0.0,0.0,0.5220382213592529,0.11310337,0.28416246,0.5162066,0.9481536149978638,0.375,0.5940603719006334,4.0,5.0,4.0
473,473,473,474,add_prompt,"def add_prompt(self, prompt: Prompt, key: str | None = None) -> Prompt:
        
        key = key or prompt.name

        # Check for duplicates
        existing = self._prompts.get(key)
        if existing:
            if self.duplicate_behavior == ""warn"":
                logger.warning(f""Prompt already exists: {key}"")
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""replace"":
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""error"":
                raise ValueError(f""Prompt already exists: {key}"")
            elif self.duplicate_behavior == ""ignore"":
                return existing
        else:
            self._prompts[key] = prompt
        return prompt",Add a prompt to the manager.,"Add a prompt to a collection, handling duplicates based on specified behavior.",5.0,5.0,5.0,0.3461538461538461,0.3333333333333333,0.6666666666666666,0.5592259168624878,0.6476368,0.41039482,0.69532406,0.7788627743721008,0.4166666666666667,0.5823742257709059,5.0,5.0,5.0
474,474,474,475,database_setup_and_cleanup,"def database_setup_and_cleanup(db_session: Session) -> Generator[None, None, None]:
    

    inspector = cast(Inspector, inspect(db_session.bind))

    # Check if all tables defined in models are created in the db
    for table in Base.metadata.tables.values():
        if not inspector.has_table(table.name):
            pytest.exit(f""Table {table} does not exist in the database."")

    clear_database(db_session)
    yield  # This allows the test to run
    clear_database(db_session)",Setup and cleanup the database for each test case.,"Ensure database tables exist, clear data before and after tests",5.0,5.0,5.0,0.6507936507936508,0.3,0.2222222222222222,0.53055739402771,0.3689897,0.63776356,0.75367427,0.99480801820755,0.3,0.5796985168291735,5.0,5.0,4.0
475,475,475,476,extract_and_print_results,"def extract_and_print_results(results):
    
    try:
        text = "" "".join([output[0].decode(""utf-8"") for output in results.as_numpy(""output"")])
        logger.info(text)
        logger.info(json.loads(text))
    except Exception as e:
        logger.info(f""JSON extract failed: {e}"")

    output_data = results.as_numpy(""output"")
    logger.info(output_data)
    return output_data",Extract and print the results from the Triton server response.,"Extract, log, and return JSON results from output data.",5.0,5.0,2.0,0.6563678857364482,0.4444444444444444,0.4,0.591753363609314,0.32731742,0.55374813,0.66159296,0.914656698703766,0.3333333333333333,0.6370345441348649,1.0,4.0,4.0
476,476,476,477,_megatron_calc_global_rank,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    

    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert tp_size * dp_size * pp_size == torch.distributed.get_world_size(), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    # We only support TP-DP-PP grouping, for correctness when resharding
    return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","given TP,DP,PP rank to get the global rank.",Calculate global rank in distributed model using parallel dimensions.,5.0,5.0,5.0,0.4492753623188406,0.2222222222222222,0.2,0.5511873960494995,0.17854379,0.3585565,0.66863817,0.9518899321556092,0.1111111111111111,0.5382531578689853,5.0,5.0,4.0
477,477,477,478,decode_unicode_escapes_to_utf8,"def decode_unicode_escapes_to_utf8(text: str) -> str:
	

	if r'\u' not in text:
		# doesn't have any escape sequences that need to be decoded
		return text

	try:
		# Try to decode Unicode escape sequences
		return text.encode('latin1').decode('unicode_escape')
	except (UnicodeEncodeError, UnicodeDecodeError):
		# logger.debug(f""Failed to decode unicode escape sequences while generating gif text: {text}"")
		return text",Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text),"Convert Unicode escape sequences in a string to UTF-8 format, handling errors gracefully.",5.0,5.0,4.0,0.4478759650890833,0.5714285714285714,0.28,0.6150355339050293,0.10225888,0.6973895,0.79900515,0.9818286299705504,0.4615384615384615,0.4647497136324661,5.0,5.0,5.0
478,478,478,479,check_prime,"def check_prime(nums: list[int]) -> str:
  
  primes = set()
  for number in nums:
    number = int(number)
    if number <= 1:
      continue
    is_prime = True
    for i in range(2, int(number**0.5) + 1):
      if number % i == 0:
        is_prime = False
        break
    if is_prime:
      primes.add(number)
  return (
      ""No prime numbers found.""
      if not primes
      else f""{', '.join(str(num) for num in primes)} are prime numbers.""
  )",Check if a given list of numbers are prime.,Determine and return prime numbers from a list of integers.,5.0,5.0,5.0,0.6440677966101694,0.5,0.3333333333333333,0.5863240957260132,0.3741746,0.6877448,0.7414457,0.8201181888580322,0.5,0.5253990520851322,4.0,5.0,5.0
479,479,479,480,worker_task,"def worker_task(thread_id, args):
    
    for i in range(args.iterations):
        iteration = i + 1
        test_id = f""{thread_id:02d}_{iteration:02d}""
        text = generate_test_sentence(thread_id, iteration)
        success = request_tts(
            args.url, test_id, text, args.voice, args.output_dir, args.debug
        )

        if not success:
            log_message(
                f""Thread {thread_id}: Iteration {iteration} failed"",
                args.debug,
                is_error=True,
            )

        # Small delay between iterations to avoid overwhelming the API
        time.sleep(0.1)",Worker task for each thread,"Execute text-to-speech requests iteratively, logging failures for each thread.",5.0,5.0,5.0,0.3076923076923076,0.2727272727272727,0.6,0.6385168433189392,0.49586037,0.3436433,0.54476446,0.7047842741012573,0.1111111111111111,0.5842792748531712,5.0,5.0,5.0
480,480,480,481,_annotate_span_for_generation_message,"def _annotate_span_for_generation_message(
        self,
        span: trace.Span,
        message: MessageParamT | str | List[MessageParamT],
    ) -> None:
        
        if not self.context.tracing_enabled:
            return
        if isinstance(message, str):
            span.set_attribute(""message.content"", message)
        elif isinstance(message, list):
            for i, msg in enumerate(message):
                if isinstance(msg, str):
                    span.set_attribute(f""message.{i}.content"", msg)
                else:
                    span.set_attribute(f""message.{i}"", str(msg))
        else:
            span.set_attribute(""message"", str(message))",Annotate the span with the message content.,Annotate trace span with message content if tracing is enabled.,5.0,5.0,5.0,0.6507936507936508,0.5,0.7142857142857143,0.5712150931358337,0.7580119,0.7975789,0.68066126,0.7232996225357056,0.4,0.6728603122605866,5.0,5.0,5.0
481,481,481,482,load_local_model,"def load_local_model(local_path: str):
    
    try:
        nexa_model = NexaImageInference(
            model_path=""local_model"",
            local_path=local_path
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None",Load local model with default parameters.,Load a local image inference model and update configuration options,5.0,5.0,5.0,0.5522388059701493,0.3,0.5,0.5546079277992249,0.539627,0.52874255,0.7646276,0.9380999803543092,0.6,0.642837892005341,5.0,5.0,5.0
482,482,482,483,_build_messages,"def _build_messages(cls, prompt: str, schema: Dict[str, Any]) -> List[str]:
        
        return [
            f""Provide a valid JSON response matching this schema: {json.dumps(schema)}"",
            prompt
        ]",Construct the message sequence for JSON-schema enforcement.,Generate JSON schema prompt messages for validation.,5.0,5.0,5.0,0.6891564010955686,0.5714285714285714,0.25,0.5947755575180054,0.47973725,0.8374947,0.67731297,0.9065091609954834,0.4285714285714285,0.6119731102623571,4.0,4.0,4.0
483,483,483,484,__str__,"def __str__(self) -> str:
        
        base = f""{self.action.ljust(11)}. {self.target}""
        if self.details:
            base += f"" - {self.details}""
        if self.agent_name:
            base = f""[{self.agent_name}] {base}""
        return base",Format the progress event for display.,Formats and returns a descriptive string of an action with optional details and agent name.,5.0,5.0,5.0,0.4065934065934066,0.0666666666666666,0.1666666666666666,0.6599432229995728,0.24191634,0.24493179,0.52306986,0.7110824584960938,0.2,0.581748007509717,5.0,5.0,5.0
484,484,484,485,track_memory_usage,"def track_memory_usage(self) -> Optional[float]:
        
        try:
            memory_usage = psutil.Process().memory_info().rss
            return memory_usage / (1024 * 1024)  # Convert to MB
        except Exception as e:
            logger.warning(f""Failed to track memory usage: {str(e)}"")
            return None",Track memory usage in MB,"Monitor and log process memory usage in megabytes, handling exceptions.",5.0,5.0,5.0,0.2957746478873239,0.3,0.6,0.6665802597999573,0.6287716,0.3977675,0.7231163,0.8367055654525757,0.2,0.523428591712402,4.0,4.0,4.0
485,485,485,486,normalize_model_name,"def normalize_model_name(name, pp_rank, vpp_rank, transformer_config, layer_name=""layers""):
    
    from verl.utils.megatron_utils import get_transformer_layer_offset

    layer_offset = get_transformer_layer_offset(pp_rank, vpp_rank, transformer_config)

    if layer_name in name:  # belong to an intermediate layer
        split_name = name.split(""."")
        # find the num next to split_name
        for i, name in enumerate(split_name):
            if name == layer_name:
                break
        layer_num_idx = i + 1
        # check the name
        assert len(split_name) >= layer_num_idx + 1, f""split_name = {split_name}""
        assert split_name[layer_num_idx].isdigit(), f""split_name = {split_name}""
        # increment layer_num_idx by layer_offset
        split_name[layer_num_idx] = str(int(split_name[layer_num_idx]) + layer_offset)
        name = ""."".join(split_name)  # weight name in inference_tp_model
    return name",Transform the model name in each model_chunk in each pp stage into the name in inference engine,Adjusts model layer identifiers based on rank offsets for distributed processing.,4.0,5.0,4.0,0.5816205789148755,0.0909090909090909,0.0555555555555555,0.5002143383026123,0.10752513,0.50038767,0.67663705,0.4441979825496673,0.2727272727272727,0.4974132595601554,4.0,5.0,5.0
486,486,486,487,interpreter_feedback,"def interpreter_feedback(self, output:str) -> str:
        
        if self.execution_failure_check(output):
            feedback = f""[failure] Error in execution:\n{output}""
        else:
            feedback = ""[success] Execution success, code output:\n"" + output
        return feedback",Provide feedback based on the output of the code execution,Determine execution success or failure and generate feedback message,5.0,5.0,5.0,0.6323529411764706,0.2222222222222222,0.1,0.6280562877655029,0.33257166,0.696889,0.71698654,0.9113234877586364,0.3333333333333333,0.5446150066994291,5.0,5.0,4.0
487,487,487,488,__repr__,"def __repr__(self) -> str:
        
        output_str = f""sub question: {self.sub_question}""
        if self.chunks:
            output_str += f""\nretrieved chunks:\n{self.chunks}""
        if self.spo:
            output_str += f""\nretrieved spo:\n{self.spo}""
        if self.summary:
            output_str += f""\nsummary:\n{self.summary}""
        return output_str",Generate debug-friendly string representation,"Generate a string representation of an object detailing its sub-question, chunks, spo, and summary.",4.0,4.0,5.0,0.4545454545454545,0.2,0.6,0.693164050579071,0.47955757,0.6247465,0.7267207,0.8877848386764526,0.1428571428571428,0.6529898624023006,5.0,5.0,5.0
488,488,488,489,setup_data_dir,"def setup_data_dir():
    
    # Get the project root directory (3 levels up from this file)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "".."", ""..""))

    # Define the data directory path
    data_dir = os.path.join(project_root, ""data"")

    # Create the data directory if it doesn't exist
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f""Created data directory at: {data_dir}"")
    else:
        print(f""Data directory already exists at: {data_dir}"")

    # Return the path to the data directory
    return data_dir",Set up the data directory for the application.,Ensure data directory exists within project structure and return its path.,5.0,5.0,5.0,0.5540540540540541,0.1818181818181818,0.25,0.6048853993415833,0.39328277,0.5511168,0.71920514,0.7326886057853699,0.6363636363636364,0.6477105094372718,5.0,5.0,5.0
489,489,489,490,play_audio_data,"def play_audio_data(audio_data, sample_rate):
    
    try:
        print(f""Playing in-memory audio data (Sample rate: {sample_rate} Hz, Duration: {len(audio_data)/sample_rate:.2f}s)"")
        sd.play(audio_data, sample_rate)
        sd.wait()
        print(""Playback from memory finished."")
    except Exception as e:
        print(f""Error playing in-memory audio: {e}"")",Plays in-memory audio data (NumPy array).,Play in-memory audio data with specified sample rate and handle errors,4.0,5.0,5.0,0.4714285714285714,0.4166666666666667,0.7142857142857143,0.6024169325828552,0.48949546,0.6099318,0.7292693,0.9383010864257812,0.4545454545454545,0.6355186262436417,5.0,5.0,5.0
490,490,490,491,create_empty_state,"def create_empty_state(
    agent: BaseAgent, initialized_states: Optional[dict[str, Any]] = None
) -> dict[str, Any]:
  
  non_initialized_states = {}
  _create_empty_state(agent, non_initialized_states)
  for key in initialized_states or {}:
    if key in non_initialized_states:
      del non_initialized_states[key]
  return non_initialized_states",Creates empty str for non-initialized states.,Generate a state dictionary excluding initialized agent states,5.0,5.0,5.0,0.5806451612903226,0.25,0.2857142857142857,0.5303828120231628,0.28487015,0.5865917,0.4509288,0.7851181030273438,0.25,0.7122104132212391,4.0,5.0,4.0
491,491,491,492,_display_quality_summary,"def _display_quality_summary(self, metrics: Dict[str, Any]):
        
        overall_score = metrics.get(""overall_score"", 0)
        issues = metrics.get(""issues"", [])

        # Use formatter for consistent display
        quality_display = self.formatter.format_quality_score(overall_score, issues)
        self.console.print(f""[dim]{quality_display}[/dim]"")

        # Highlight specific test concerns
        if metrics.get(""premature_attempt""):
            self.console.print(
                ""  [yellow]⚠ Test detected premature answer attempt[/yellow]""
            )

        verbosity = metrics.get(""verbosity"", 0)
        if verbosity > 0.7:
            self.console.print(
                f""  [yellow]⚠ High verbosity detected ({verbosity:.0%})[/yellow]""
            )",Display quality metrics in test context,Display a quality summary using metrics with formatted output and warnings.,4.0,5.0,3.0,0.48,0.2727272727272727,0.5,0.5294813513755798,0.3436906,0.67336905,0.81609714,0.980022668838501,0.3636363636363636,0.6891559576710522,5.0,5.0,5.0
492,492,492,493,get_request_context,"def get_request_context(
    db_session: Annotated[Session, Depends(yield_db_session)],
    api_key_id: Annotated[UUID, Depends(validate_api_key)],
    agent: Annotated[Agent, Depends(validate_agent)],
    project: Annotated[Project, Depends(validate_project_quota)],
) -> RequestContext:
    
    logger.info(
        ""populating request context"",
        extra={""api_key_id"": api_key_id, ""project_id"": project.id, ""agent_id"": agent.id},
    )
    return RequestContext(
        db_session=db_session,
        api_key_id=api_key_id,
        project=project,
        agent=agent,
    )","Returns a RequestContext object containing the DB session, the validated API key ID, and the project ID.","Constructs a request context with validated database session, API key, agent, and project.",5.0,5.0,5.0,0.7703455710710388,0.5384615384615384,0.3529411764705882,0.6478052139282227,0.56233615,0.78073585,0.81072325,0.9608029127120972,0.3076923076923077,0.6890297207795808,5.0,5.0,5.0
493,493,493,494,__init__,"def __init__(self, endpoint: str) -> None:
        

        super().__init__()
        self._endpoint = endpoint
        self._read_stream_writers = {}
        logger.debug(f""SseServerTransport initialized with endpoint: {endpoint}"")","Creates a new SSE server transport, which will direct the client to POST messages to the relative or absolute URL given.",Initialize server transport with endpoint and logging setup,5.0,5.0,4.0,0.3073985649407255,0.25,0.0952380952380952,0.5565716028213501,0.10376822,0.52753484,0.6682961,0.8821049332618713,0.375,0.6236466249421,4.0,4.0,4.0
494,494,494,495,evaForSimilarity,"def evaForSimilarity(self, predictionlist: List[str], goldlist: List[str]):
        
        # data_samples = {
        #     'question': [],
        #     'answer': predictionlist,
        #     'ground_truth': goldlist
        # }
        # dataset = Dataset.from_dict(data_samples)
        # run_config = RunConfig(timeout=240, thread_timeout=240, max_workers=16)
        # embeddings = embedding_factory(self.embedding_factory, run_config)
        #
        # score = evaluate(dataset, metrics=[answer_similarity], embeddings = embeddings, run_config=run_config)
        # return np.average(score.to_pandas()[['answer_similarity']])
        return {""similarity"": 0.0}",evaluate the similarity between prediction and gold #TODO,"Evaluate prediction similarity to gold standard, returning a default score.",4.0,4.0,4.0,0.6266666666666667,0.4,0.375,0.5297715067863464,0.19049968,0.771468,0.7974322,0.9602792859077454,0.2,0.6198717559176623,1.0,5.0,4.0
495,495,495,496,get_markdown,"def get_markdown(research_id):
    
    conn = get_db_connection()
    conn.row_factory = lambda cursor, row: {
        column[0]: row[idx] for idx, column in enumerate(cursor.description)
    }
    cursor = conn.cursor()
    cursor.execute(""SELECT * FROM research_history WHERE id = ?"", (research_id,))
    result = cursor.fetchone()
    conn.close()

    if not result or not result.get(""report_path""):
        return jsonify({""status"": ""error"", ""message"": ""Report not found""}), 404

    try:
        with open(result[""report_path""], ""r"", encoding=""utf-8"") as f:
            content = f.read()
        return jsonify({""status"": ""success"", ""content"": content})
    except Exception as e:
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500",Get markdown export for a specific research,Retrieve and return a research report in Markdown format from a database by ID.,5.0,5.0,3.0,0.4810126582278481,0.2142857142857142,0.2857142857142857,0.59754878282547,0.2960213,0.45310813,0.71445525,0.928248405456543,0.5,0.7288231159722504,4.0,4.0,5.0
496,496,496,497,model_post_init,"def model_post_init(self, __context) -> None:
        
        super().model_post_init(__context)
        self.inference_url = settings.TEXT2VEC_INFERENCE_URL
        logger.info(f""Text2Vec model using inference URL: {self.inference_url}"")",Post initialization hook to set the inference URL from settings.,Initialize model with inference URL and log configuration details.,4.0,5.0,2.0,0.7727272727272727,0.3333333333333333,0.3,0.6380330324172974,0.33526066,0.6638052,0.74701583,0.8477120399475098,0.3333333333333333,0.587166632752638,4.0,5.0,5.0
497,497,497,498,connect,"def connect(self) -> bool:
        
        if self.sock:
            return True
            
        try:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.connect((self.host, self.port))
            logger.info(f""Connected to Blender at {self.host}:{self.port}"")
            return True
        except Exception as e:
            logger.error(f""Failed to connect to Blender: {str(e)}"")
            self.sock = None
            return False",Connect to the Blender addon socket server,"Establishes a socket connection to a specified host and port, logging success or failure.",5.0,5.0,3.0,0.4269662921348314,0.2142857142857142,0.2857142857142857,0.6308844685554504,0.1197212,0.5287867,0.68790305,0.9234522581100464,0.2142857142857142,0.4282387887921111,5.0,5.0,5.0
498,498,498,499,append_dims,"def append_dims(x, target_dims):
    
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f""input has {x.ndim} dims but target_dims is {target_dims}, which is less"")
    return x[(...,) + (None,) * dims_to_append]",Appends dimensions to the end of a tensor until it has target_dims dimensions.,Add extra dimensions to an array until it matches the target dimensions.,5.0,5.0,5.0,0.8050388628006578,0.5833333333333334,0.4285714285714285,0.6526254415512085,0.28264698,0.65969133,0.7154654,0.9472649097442628,0.0833333333333333,0.4445122047421228,5.0,5.0,5.0
499,499,499,500,api_delete_resource,"def api_delete_resource(research_id, resource_id):
    
    try:
        # Delete the resource
        success = delete_resource(resource_id)

        if success:
            return jsonify(
                {""status"": ""success"", ""message"": ""Resource deleted successfully""}
            )
        else:
            return jsonify({""status"": ""error"", ""message"": ""Resource not found""}), 404
    except Exception as e:
        logger.error(f""Error deleting resource: {str(e)}"")
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500",Delete a resource from a research project,"Handles resource deletion via API, returning success or error messages.",5.0,5.0,5.0,0.5070422535211268,0.2,0.1428571428571428,0.5611332058906555,0.10785127,0.46397746,0.60256577,0.9934871196746826,0.2,0.6145128080563698,4.0,5.0,5.0
