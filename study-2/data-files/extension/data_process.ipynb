{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "imput_file = '/home/user/projects/prompt-project/SIDE_p/scripts_and_data/data_files/extension/filtered_after500_method.jsonl'\n",
    "output_file = '/home/user/projects/prompt-project/SIDE_p/scripts_and_data/data_files/extension/extended_1000_method.jsonl'\n",
    "sample = []\n",
    "with open(imput_file, 'r', encoding='utf-8') as f:\n",
    "    # sample 1st 1000 instances\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 1000:\n",
    "            break\n",
    "        object = json.loads(line)\n",
    "        object['id'] = 501 + i  # set id to be 501, 502, ..., 1500\n",
    "        # make 'id' the very first key\n",
    "        keys = list(object.keys())\n",
    "        keys.insert(0, keys.pop(keys.index('id')))\n",
    "        object = {key: object[key] for key in keys}\n",
    "        sample.append(object)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for object in sample:\n",
    "        f.write(json.dumps(object) + '\\n')\n",
    "\n",
    "print(f'Sampled {len(sample)} instances and saved to {output_file}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "input_file = '/home/user/projects/prompt-project/SIDE_p/scripts_and_data/data_files/extension/filtered_after500_method.jsonl'\n",
    "output_file = '/home/user/projects/prompt-project/SIDE_p/scripts_and_data/data_files/extension/extended_2000_method.jsonl'\n",
    "\n",
    "sample = []\n",
    "start_line = 1000\n",
    "num_samples = 2000\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < start_line:\n",
    "            continue\n",
    "        if i >= start_line + num_samples:\n",
    "            break\n",
    "        obj = json.loads(line)\n",
    "        obj['id'] = 1501 + (i - start_line)  # id = 1501, 1502, ...\n",
    "        \n",
    "        # Move 'id' to first position\n",
    "        keys = list(obj.keys())\n",
    "        keys.insert(0, keys.pop(keys.index('id')))\n",
    "        obj = {key: obj[key] for key in keys}\n",
    "        sample.append(obj)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for obj in sample:\n",
    "        f.write(json.dumps(obj) + '\\n')\n",
    "\n",
    "print(f\" Sampled {len(sample)} instances (lines {start_line+1}–{start_line+num_samples}) to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#input_file = '/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50-beginner.csv'\n",
    "# read csv\n",
    "df = pd.read_csv('/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50-beginner.csv')\n",
    "\n",
    "# remove ??? ??? from the column codeComment in csv\n",
    "df['codeComment'] = df['codeComment'].str.strip('?').str.strip()\n",
    "# save to the same csv file\n",
    "df.to_csv('/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50-beginner.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv('/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/extended_2000_method_with_summaries_with_metrics_all.csv')\n",
    "df = df.drop(columns=['ROUGE-4-R'])\n",
    "# save to the same csv file\n",
    "df.to_csv('/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/extended_2000_method_with_summaries_with_metrics_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_1 = pd.read_csv('/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50-beginner.csv')\n",
    "df_2 = pd.read_csv('/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50_all.csv')\n",
    "df_2['codeComment-persona-beginner'] = df_1['codeComment']\n",
    "df_2.to_csv('/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "csv1_path = \"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/only_gpt_annotation-500.csv\"\n",
    "csv2_path = \"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_p1.csv\"\n",
    "csv3_path = \"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_p2.csv\"\n",
    "output_path = \"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_merged_prompts_1500.csv\"\n",
    "\n",
    "# Load all three CSVs\n",
    "df1 = pd.read_csv(csv1_path)\n",
    "df2 = pd.read_csv(csv2_path)\n",
    "df3 = pd.read_csv(csv3_path)\n",
    "\n",
    "# Convert 'id' to float with unique suffixes\n",
    "df1[\"id\"] = df1[\"id\"].astype(float) + 0.0\n",
    "df2[\"id\"] = df2[\"id\"].astype(float) + 0.1\n",
    "df3[\"id\"] = df3[\"id\"].astype(float) + 0.2\n",
    "\n",
    "# Concatenate all three\n",
    "merged_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Save to output\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Merged 3 CSVs into {output_path} with {len(merged_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# File paths\n",
    "csv_path = \"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_merged_prompts_1500.csv\"\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "# extract 1st 50 into a new CSV\n",
    "df_sample = df.head(50)\n",
    "# Save the sample to a new CSV\n",
    "output_path = \"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50-old.csv\"\n",
    "df_sample.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_file = '/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50_all.csv'\n",
    "# chnage column name 'codeComment' to 'codeComment-persona-expert'\n",
    "df = pd.read_csv(input_file)\n",
    "df.rename(columns={'codeComment': 'codeComment-persona-expert'}, inplace=True)\n",
    "# save to the same csv file\n",
    "df.to_csv(input_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_file = '/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/NEW-generation_from_3_models/CS-benchmark-Python-nohw.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "# remove the column 'Unnamed: 0'\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# save to the same csv file\n",
    "df.to_csv(input_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# File paths\n",
    "csv_path = \"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/NEW-generation_from_3_models/dsc33b_plus_human_annotation-500.csv\"\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "# delemte column model_content_adequacy, model_conciseness,model_fluency\n",
    "df.drop(columns=['codeComment'], inplace=True)\n",
    "#df.drop(columns=['model_conciseness'], inplace=True)\n",
    "#df.drop(columns=['model_fluency'], inplace=True)\n",
    "# Save the modified DataFrame back to the same CSV file\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "#input_file = \"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/benchmark-gpt-evaluation/gpt_generated_summaries_sample_50_all.csv\"\n",
    "df = pd.read_csv(\"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/NEW-generation_from_3_models/CS-benchmark-Python-nohw.csv\")\n",
    "\n",
    "# Extract the first line from the 'summary_postprocessed' column\n",
    "df[\"final_summary\"] = df[\"summary_postprocessed\"].astype(str).apply(lambda x: x.strip().split('\\n')[0])\n",
    "\n",
    "# Extract the first line from 'human_written_summary'\n",
    "df[\"final_human_written_summary\"] = df[\"human_written_summary\"].astype(str).apply(lambda x: x.strip().split('\\n')[0])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"/home/user/projects/prompt-project/SIDE_p/scripts_and_data/training_SIDE/evaluation_and_statistical_tests/NEW-generation_from_3_models/CS-benchmark-Python-with-final-summary.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
