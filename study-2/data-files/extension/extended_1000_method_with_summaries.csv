id,function_name,codeFunctions,originalComment,codeComment
501,add_task,"def add_task(task: TaskConfig, position: Optional[InsertionPoint] = None):
    
    framework = get_framework()
    module = get_framework_module(framework)

    if task.name in get_task_method_names():
        raise ValidationError(f""Task `{task.name}` already exists in {get_entrypoint_path(framework)}"")

    return module.add_task(task, position)",Add a task to the user's project.,"Add a new task to a framework, ensuring no duplicates exist."
502,slugify_did,"def slugify_did(cloud_server: str, did: str) -> str:
    
    return slugify(f'{cloud_server}_{did}', separator='_')",Slugify a device id.,Transforms a cloud server and DID into a URL-friendly slug format.
503,load_from_yaml,"def load_from_yaml(self, file_path: Union[str, Path]) -> None:
        
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f""YAML file '{file_path}' not found"")
        with file_path.open(""r"") as f:
            data = yaml.safe_load(f)
        try:
            self.workflow = WorkflowDefinition.model_validate(data)
            self._ensure_dependencies()
        except ValidationError as e:
            raise ValueError(f""Invalid workflow YAML: {e}"")",Load a workflow from a YAML file with validation.,Load and validate workflow configuration from a YAML file.
504,chess_transform,"def chess_transform(move_sequence: str) -> set:
    
    move_sequence = str_to_set(move_sequence)
    return {move_san.removesuffix(""!"").removesuffix(""#"") for move_san in move_sequence}",Transform a sequence of chess moves encoded in SAN into a set.,"Convert chess move sequence to a set, removing exclamation and hash suffixes."
505,_update_progress,"def _update_progress(self, stage: str, step: str, percentage: float, message: str):
        
        try:
            self.progress.progress.update_progress(
                stage,  # stage
                step,   # step
                Status.IN_PROGRESS,
                percentage
            )
            logger.info(f""Progress updated: {percentage}% - {message}"")
        except Exception as e:
            logger.error(f""Progress callback error: {str(e)}"")",Update progress for any stage and step,Update and log task progress with error handling.
506,post_cast,"def post_cast(self, message: str, image_url: Optional[str] = None) -> Tuple[Optional[str], Optional[str]]:
        
        try:
            endpoint = f""{self.base_url}/cast""

            data = {
                ""signer_uuid"": self.signer_uuid,
                ""text"": message,
            }

            if image_url:
                data[""embeds""] = [{""url"": image_url}]

            response = requests.post(endpoint, headers=self.headers, json=data)

            if response.status_code == 200:
                result = response.json()
                cast_hash = result.get(""cast"", {}).get(""hash"")
                username = result.get(""cast"", {}).get(""author"", {}).get(""username"")
                logger.info(f""Successfully posted cast with hash: {cast_hash}"")
                return cast_hash, username
            else:
                logger.error(f""Failed to post cast. Status: {response.status_code}, Response: {response.text}"")
                return None, None

        except Exception as e:
            logger.error(f""Error posting cast: {str(e)}"")
            return None, None","Post a cast to Farcaster, optionally with an image",Post a message with optional image and return cast hash and author username.
507,delete_empty_dirs_after_rename_or_delete,"def delete_empty_dirs_after_rename_or_delete(pdf_current_file_name: str, user_id: str):
    

    current_path = MEDIA_ROOT / pdf_current_file_name

    pdf_current_file_name_adjusted = pdf_current_file_name.replace(f'{user_id}/pdf/', '')

    for _ in pdf_current_file_name_adjusted.split('/'):
        current_parent_path = current_path.parent
        sub_paths = [sub_path for sub_path in current_parent_path.iterdir()]

        if not sub_paths:
            current_parent_path.rmdir()
            current_path = current_parent_path
        else:
            break",Delete empty directories in the users media/pdf directory that appear as a result of renaming or deleting pdfs.,Remove empty directories after renaming or deleting a PDF file.
508,temp_config_file,"def temp_config_file(tmp_path, temp_prompts_file):
    
    config_data = {
        ""prompts"": {
            ""inline_prompt"": ""This is an inline prompt"",
            ""another_inline"": ""Another inline prompt"",
        }
    }
    config_file = tmp_path / ""test_config.yaml""
    with open(config_file, ""w"") as f:
        yaml.safe_dump(config_data, f)
    return config_file",Create a temporary config file for testing.,Create temporary YAML config file with predefined prompts.
509,visit_Module,"def visit_Module(self, node):
        
        logger.debug(f""Visiting module with {len(node.body)} top-level statements"")
        for item in node.body:
            logger.debug(f""Processing top-level node: {type(item).__name__}"")
            if isinstance(item, ast.FunctionDef):
                self.visit_FunctionDef(item)
            elif isinstance(item, ast.AsyncFunctionDef):
                self.visit_AsyncFunctionDef(item)
            else:
                self.visit(item)",Log and explicitly process top-level statements in the module.,"Traverse and process top-level nodes in a module, handling functions specifically."
510,_display_final_summary,"def _display_final_summary(state: ConversationState):
    
    console.print(
        Panel.fit(
            f""[bold green]Conversation Summary[/bold green]\n\n""
            f""Total turns: {state.current_turn}\n""
            f""Messages exchanged: {len(state.messages)}\n""
            f""Requirements tracked: {len(state.requirements)}\n""
            f""Conversation ID: {state.conversation_id}"",
            border_style=""green"",
        )
    )",Display final conversation summary,Display a formatted summary of conversation details in the console
511,primary,"def primary(self):
        
        if self.match(TokenType.RULE):
            rule_tuple = self.previous().value
            attribute_name, rule_type = rule_tuple

            # Validate rule type
            if rule_type not in [""any"", ""all""] and not any(op in rule_type for op in ["">"", ""<"", "">="", ""<="", ""==""]):
                raise ValueError(f""Invalid rule type: {rule_type}. Supported types: 'any', 'all', or numeric comparison (e.g., 'avg>0.3')"")

            return RuleNode(attribute_name, rule_type)

        if self.match(TokenType.LPAREN):
            expr = self.expression()
            self.consume(TokenType.RPAREN, ""Expected ')' after expression."")
            return expr

        raise ValueError(f""Expected rule or '(' at position {self.current}"")",Parse a primary expression (rule or parenthesized expression).,"Parse and validate rules or expressions, returning a structured node or expression."
512,_get_inputs,"def _get_inputs(self, rng: Random, note_length: int, magazine_length: int, solvable: bool) -> tuple[str, str]:
        
        ransom_note = [rng.choice(list(self.letters)) for _ in range(note_length)]
        magazine = ransom_note.copy()
        if solvable:
            magazine.extend([rng.choice(list(self.letters)) for _ in range(magazine_length - note_length)])
        else:
            remove_letter = rng.choice(magazine)
            magazine.remove(remove_letter)
            magazine.extend(
                [rng.choice(list(self.letters - {remove_letter})) for _ in range(magazine_length - note_length + 1)]
            )
        rng.shuffle(ransom_note)
        rng.shuffle(magazine)
        return """".join(ransom_note), """".join(magazine)",Generate random ransom note and magazine,"Generate a random ransom note and magazine, ensuring solvability if specified."
513,_create_wallet,"def _create_wallet(self) -> bool:
        
        try:
            load_dotenv()
            rpc_url = os.getenv(""GOAT_RPC_PROVIDER_URL"")
            private_key = os.getenv(""GOAT_WALLET_PRIVATE_KEY"")

            if not rpc_url or not private_key:
                return False

            # Initialize Web3 and test connection
            w3 = Web3(Web3.HTTPProvider(rpc_url))
            if not w3.is_connected():
                logger.error(""Failed to connect to RPC provider"")
                return False

            # Test private key by creating account
            try:
                account = Account.from_key(private_key)
                w3.eth.default_account = account.address
                self._wallet_client = Web3EVMWalletClient(w3)
                # Register actions now that we have a wallet
                self._register_actions_with_wallet()
                return True
            except Exception as e:
                logger.error(f""Invalid private key: {str(e)}"")
                return False

        except Exception as e:
            logger.error(f""Failed to create wallet: {str(e)}"")
            return False",Create wallet from environment variables,Initialize blockchain wallet using environment variables and Web3 connection
514,is_configured,"def is_configured(self, verbose = False) -> bool:
        
        try:
            load_dotenv()
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                return False

            client = OpenAI(api_key=api_key)
            client.models.list()
            return True
            
        except Exception as e:
            if verbose:
                logger.debug(f""Configuration check failed: {e}"")
            return False",Check if OpenAI API key is configured and valid,Check if OpenAI API is configured by verifying API key presence and model access.
515,extract_last_content,"def extract_last_content(cls, message: TResponseOutputItem) -> str:
        
        if not isinstance(message, ResponseOutputMessage):
            return """"

        last_content = message.content[-1]
        if isinstance(last_content, ResponseOutputText):
            return last_content.text
        elif isinstance(last_content, ResponseOutputRefusal):
            return last_content.refusal
        else:
            raise ModelBehaviorError(f""Unexpected content type: {type(last_content)}"")",Extracts the last text content or refusal from a message.,Extracts and returns the last text or refusal from a message object.
516,messages_to_prompt,"def messages_to_prompt(self, messages: list[dict]):
        
        return ""\n"".join([f""{i['role']}: {i['content']}"" for i in messages])","[{""role"": ""user"", ""content"": msg}] to user:  etc.",Formats a list of message dictionaries into a role-content string prompt.
517,mock_nested_chat_target,"def mock_nested_chat_target(self) -> NestedChatTarget:
        
        nested_chat_config = {""chat_queue"": [""agent1"", ""agent2""], ""use_async"": True}
        return NestedChatTarget(nested_chat_config=nested_chat_config)",Create a mock NestedChatTarget for testing.,Create a nested chat target with asynchronous configuration.
518,list_agents,"def list_agents(self, input_list: List[str]) -> None:
        
        logger.info(""\nAvailable Agents:"")
        agents_dir = Path(""agents"")
        if not agents_dir.exists():
            logger.info(""No agents directory found."")
            return

        agents = list(agents_dir.glob(""*.json""))
        if not agents:
            logger.info(""No agents found. Use 'create-agent' to create a new agent."")
            return

        for agent_file in sorted(agents):
            if agent_file.stem == ""general"":
                continue
            logger.info(f""- {agent_file.stem}"")",Handle list agents command,"Log available agent files from a directory, excluding 'general'."
519,did_change,"def did_change(server: CodegenLanguageServer, params: types.DidChangeTextDocumentParams) -> None:
    
    logger.info(f""Document changed: {params.text_document.uri}"")
    # The document is automatically updated in the workspace by pygls
    # We can perform any additional processing here if needed
    path = get_path(params.text_document.uri)
    server.io.update_file(path, params.text_document.version)
    sync = DiffLite(change_type=ChangeType.Modified, path=path)
    server.codebase.ctx.apply_diffs([sync])",Handle document change notification.,Handle document change event by updating server file version and applying modifications.
520,mock_fetchall_sql_response,"def mock_fetchall_sql_response():
    
    return [
        {""Description"": ""Row5Description"", ""Name"": ""Row5Name""},
        {""Description"": ""Row6Description"", ""Name"": ""Row6Name""},
        {""Description"": ""Row5Description"", ""Name"": ""Row5Name""},
        {""Description"": ""Row6Description"", ""Name"": ""Row6Name""},
    ]",Mock response from select requests,Simulate database query returning list of dictionaries with name and description.
521,set_default_project,"def set_default_project(
    name: str = typer.Argument(..., help=""Name of the project to set as default""),
) -> None:
    
    try:
        project_url = config.project_url

        response = asyncio.run(call_put(client, f""{project_url}/project/projects/{name}/default""))
        result = ProjectStatusResponse.model_validate(response.json())

        console.print(f""[green]{result.message}[/green]"")
    except Exception as e:
        console.print(f""[red]Error setting default project: {str(e)}[/red]"")
        console.print(""[yellow]Note: Make sure the Basic Memory server is running.[/yellow]"")
        raise typer.Exit(1)

    # Always activate it for the current session
    os.environ[""BASIC_MEMORY_PROJECT""] = name

    # Reload configuration to apply the change
    from importlib import reload
    from basic_memory import config as config_module

    reload(config_module)

    console.print(""[green]Project activated for current session[/green]"")",Set the default project and activate it for the current session.,Set a specified project as default and activate it for the session
522,generate_solution,"def generate_solution(self, requirement: str, temperature: float) -> str:
        
        logger.info(""Starting solution generation phase with expert model..."")
        logger.info(f""Input requirement: {requirement[:100]}..."")
        
        chat_request = ChatRequest(
            message=requirement,
            system_prompt="""",  # Will be set by strategy
            temperature=temperature
        )
        
        logger.info(""Calling chat service with ExpertSolutionStrategy using expert model..."")
        response = chat_service.chat(
            request=chat_request,
            strategy_chain=[BasePromptStrategy, ExpertSolutionStrategy],
            stream=False,
            json_response=False,
            client=expert_llm_service.client  # Use expert model
        )
        
        solution = response.choices[0].message.content
        logger.info(f""Solution generation completed. Result: {solution[:100]}..."")
        return solution",Generate solution based on enhanced requirement,Generate a solution using an expert model based on input requirements and temperature settings.
523,mock_requests_post_success,"def mock_requests_post_success(monkeypatch: pytest.MonkeyPatch) -> None:
    
    class MockResponse:
        def __init__(self, status_code: int, json_data: List[Dict[str, Any]]):
            self.status_code = status_code
            self._json_data = json_data

        def json(self) -> List[Dict[str, Any]]:
            return self._json_data

    # Default dummy response for post, can be customized per test if needed
    dummy_chunks: List[Dict[str, Any]] = [
        {""text"": ""chunk1"", ""start_index"": 0, ""end_index"": 6, ""token_count"": 1},
        {""text"": ""chunk2"", ""start_index"": 7, ""end_index"": 13, ""token_count"": 1},
    ]
    def mock_post(*args: Any, **kwargs: Any) -> MockResponse:
        return MockResponse(200, dummy_chunks)

    monkeypatch.setattr(requests, ""post"", mock_post)",Mock requests.post to return a successful response with dummy chunk data.,Patch HTTP POST requests to return a predefined successful response for testing.
524,parse_jsonl_entries,"def parse_jsonl_entries(jsonl_files):
    
    all_entries = []
    pdf_sources = set()

    for jsonl_file in jsonl_files:
        print(f""Processing {jsonl_file.name}..."")

        with open(jsonl_file, ""r"", encoding=""utf-8"") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                entry = json.loads(line)
                text = entry.get(""text"", """")
                metadata = entry.get(""metadata"", {})
                source_file = metadata.get(""Source-File"", """")

                if source_file:
                    pdf_sources.add(source_file)

                all_entries.append({""text"": text, ""source_file"": source_file, ""metadata"": metadata, ""entry"": entry})

    print(f""Loaded {len(all_entries)} entries from JSONL files"")
    print(f""Found {len(pdf_sources)} unique PDF sources"")

    return all_entries, pdf_sources",Parse all JSONL files and extract entries with text and metadata.,Parse JSONL files to extract text entries and unique PDF sources
525,get_description,"def get_description(self) -> str:
        
        descriptions = {
            CounterMetricName.REQUEST: ""Total number of requests to the memobase server"",
            CounterMetricName.HEALTHCHECK: ""Total number of healthcheck requests to the memobase server"",
            CounterMetricName.LLM_INVOCATIONS: ""Total number of LLM invocations"",
            CounterMetricName.LLM_TOKENS_INPUT: ""Total number of input tokens"",
            CounterMetricName.LLM_TOKENS_OUTPUT: ""Total number of output tokens"",
            CounterMetricName.EMBEDDING_TOKENS: ""Total number of embedding tokens"",
        }
        return descriptions[self]",Get the description for this metric.,Map metric names to their descriptive strings for server statistics
526,default_branch,"def default_branch(self) -> str:
        
        headers = {
            ""Accept"": ""application/vnd.github.v3+json"",
        }
        if self.access_token:
            headers[""Authorization""] = f""token {self.access_token}""

        if response.status_code == 200:
            branch = response.json().get(""default_branch"", ""main"")
        else:
            # This happens sometimes when we exceed the Github rate limit. The best bet in this case is to assume the
            # most common naming for the default branch (""main"").
            logging.warn(f""Unable to fetch default branch for {self.repo_id}: {response.text}"")
            branch = ""main""
        return branch",Fetches the default branch of the repository from GitHub.,"Determine the default branch name from a GitHub repository, defaulting to ""main"" if unavailable."
527,reimburse,"def reimburse(purpose: str, amount: float) -> str:
  
  return {
      'status': 'ok',
  }",Reimburse the amount of money to the employee.,"Define a function to process reimbursement requests, returning a success status."
528,generate_item_hash,"def generate_item_hash(item):
    
    hash_input = f""{item['name']}_{item['size']}""
    return hashlib.md5(hash_input.encode('utf-8')).hexdigest()",Generate a unique hash for an item based on its name and size.,Create a unique hash for an item using its name and size.
529,_bind_workers_method_to_parent,"def _bind_workers_method_to_parent(cls, key, user_defined_cls):
    
    for method_name in dir(user_defined_cls):
        try:
            method = getattr(user_defined_cls, method_name)
            assert callable(method), f""{method_name} in {user_defined_cls} is not callable""
        except Exception as e:
            # if it is a property, it will fail because Class doesn't have instance property
            continue

        if hasattr(method, MAGIC_ATTR):

            def generate_function(name):

                def func(self, *args, **kwargs):
                    # dispatch to the actual worker
                    return getattr(self.worker_dict[key], name)(*args, **kwargs)

                return func

            func = generate_function(method_name)
            # pass MAGIC_ATTR for outer worker group
            setattr(func, MAGIC_ATTR, getattr(method, MAGIC_ATTR))
            try:
                method_name_with_prefix = key + '_' + method_name
                setattr(cls, method_name_with_prefix, func)
                # print(f'Binding {method_name_with_prefix}')
            except Exception as e:
                raise ValueError(f'Fail to set method_name {method_name}')",Binds the methods of each worker to the WorkerDict.,Bind user-defined class methods to a parent class with a key-based prefix if they have a specific attribute.
530,_generate_access_token,"def _generate_access_token(self, client_id: str, scopes: list[str]) -> str:
        
        payload = {
            ""iss"": self.issuer_url,
            ""sub"": client_id,
            ""aud"": ""basic-memory"",
            ""exp"": datetime.utcnow() + timedelta(hours=1),
            ""iat"": datetime.utcnow(),
            ""scopes"": scopes,
        }

        return jwt.encode(payload, self.secret_key, algorithm=""HS256"")",Generate a JWT access token.,Generate a JWT access token with client and scope details.
531,generate_left_right_of,"def generate_left_right_of(puzzle: Puzzle, solution: dict[Literal, int]) -> set[Clue]:
    

    clues: set[Clue] = set()
    for left, right in product(puzzle.houses, puzzle.houses):
        if left >= right:
            continue

        items_left = {item: loc for item, loc in solution.items() if loc == left}
        items_right = {item: loc for item, loc in solution.items() if loc == right}
        pairs: set[tuple[Literal, Literal]] = {(item1, item2) for item1, item2 in product(items_left, items_right)}
        # sorted, no hash randomization
        for pair in sorted(pairs):
            if puzzle.rng.randint(0, 1) == 0:
                clues.add(left_of(pair[0], pair[1], puzzle.houses))
            else:
                clues.add(right_of(pair[1], pair[0], puzzle.houses))

    return clues",Generate the `left_of` / `right_of` Clue instances,Generate spatial clues for a puzzle solution by comparing item positions.
532,add_model_output,"def add_model_output(self, output: AgentOutput) -> None:
		
		tool_calls = [
			{
				'name': 'AgentOutput',
				'args': output.model_dump(mode='json', exclude_unset=True),
				'id': '1',
				'type': 'tool_call',
			}
		]

		msg = AIMessage(
			content='',
			tool_calls=tool_calls,
		)
		self.add_message(msg, MessageMetadata(tokens=100))  # Estimate tokens for tool calls

		# Empty tool response
		tool_message = ToolMessage(content='', tool_call_id='1')
		self.add_message(tool_message, MessageMetadata(tokens=10))",Add model output as AI message,Add agent output to message queue with metadata and tool call details.
533,extract_first_complete_json,"def extract_first_complete_json(s: str):
    
    stack = []
    first_json_start = None

    for i, char in enumerate(s):
        if char == ""{"":
            stack.append(i)
            if first_json_start is None:
                first_json_start = i
        elif char == ""}"":
            if stack:
                start = stack.pop()
                if not stack:
                    first_json_str = s[first_json_start : i + 1]
                    try:
                        # Attempt to parse the JSON string
                        return json.loads(first_json_str.replace(""\n"", """"))
                    except json.JSONDecodeError as e:
                        LOG.error(
                            f""JSON decoding failed: {e}. Attempted string: {first_json_str[:50]}...""
                        )
                        return None
                    finally:
                        first_json_start = None
    LOG.warning(""No complete JSON object found in the input string."")
    return None",Extract the first complete JSON object from the string using a stack to track braces.,Extract and parse the first complete JSON object from a string.
534,get_remote_url_config,"def get_remote_url_config(url: str, authenticate_request: bool) -> dict[str, Any]:
    
    stream_url = urljoin(url, ""stream_messages"")
    creds, _ = google.auth.default()
    id_token = None
    if authenticate_request:
        auth_req = google.auth.transport.requests.Request()
        try:
            id_token = google.oauth2.id_token.fetch_id_token(auth_req, stream_url)
        except DefaultCredentialsError:
            creds.refresh(auth_req)
            id_token = creds.id_token
    return {
        ""url"": stream_url,
        ""authenticate_request"": authenticate_request,
        ""creds"": creds,
        ""id_token"": id_token,
    }",Get cached remote URL agent configuration.,Configure remote URL with optional authentication and token retrieval
535,_update_workflow_graph,"def _update_workflow_graph(self):
        
        for node in self.graph.nodes:
            if isinstance(node.agents[0], dict):
                node.agents[0] = self._update_agent_prompts(
                    node.agents[0], 
                    node.textgrad_agent.system_prompt.value, 
                    node.textgrad_agent.instruction.value
                )
            else:
                raise ValueError(f""Unsupported agent type {type(node.agents[0])}. Expected 'dict'."")",Updates the workflow graph with the latest prompts from the textgrad optimization.,Update workflow graph by modifying agent prompts if they are dictionaries.
536,_initialize_driver,"def _initialize_driver(self):
        
        try:
            chrome_options = Options()
            if self.headless:
                chrome_options.add_argument(""--headless"")
            
            # Add common options for stability
            chrome_options.add_argument(""--no-sandbox"")
            chrome_options.add_argument(""--disable-dev-shm-usage"")
            
            # Add custom options
            for option in self.custom_options:
                chrome_options.add_argument(option)

            # Initialize the driver
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(5)
            logger.info(""Successfully initialized Chrome WebDriver"")

        except Exception as e:
            logger.error(f""Failed to initialize WebDriver: {str(e)}"")
            raise",Initialize Chrome WebDriver with configured options.,Initialize a Chrome WebDriver with optional headless mode and custom settings.
537,consume_function,"def consume_function(skip_existing: bool):
    

    if not settings.CONSUME_DIR.exists():  # pragma: no cover
        settings.CONSUME_DIR.mkdir(exist_ok=True)

    user_consume_paths = [path for path in settings.CONSUME_DIR.iterdir() if path.is_dir()]

    for user_consume_path in user_consume_paths:
        user = User.objects.get(id=user_consume_path.name)
        user_consume_file_paths = [path for path in user_consume_path.iterdir() if path.is_file()]

        if skip_existing:
            pdf_info_list = service.get_pdf_info_list(user.profile)
        else:
            pdf_info_list = []

        for file_path in user_consume_file_paths:
            try:
                if passes_consume_condition(file_path, skip_existing, pdf_info_list):
                    pdf_name = service.create_unique_name_from_file(file_path, user.profile)

                    with file_path.open(mode=""rb"") as f:
                        pdf_file = File(f, name=file_path.name)

                        service.PdfProcessingServices.create_pdf(
                            name=pdf_name, owner=user.profile, pdf_file=pdf_file, tag_string=settings.CONSUME_TAG_STRING
                        )

            except Exception as e:  # pragma: no cover # nosec # noqa
                logger.info(f'Could not create pdf from ""{file_path.name}"" of user ""{user.id}""')
                logger.info(traceback.format_exc())

            file_path.unlink()",Create pdf instances for pdf files present in the consume folder.,"Process user directories to create PDFs, optionally skipping existing ones, and handle exceptions."
538,handle_list,"def handle_list(self, args: Optional[List[str]] = None) -> bool:  # pylint: disable=unused-argument # noqa: E501
        
        if not is_caiextensions_platform_available():
            console.print(""[red]Platform extensions are not available[/red]"")
            return False

        from caiextensions.platform.base import platform_manager  # pylint: disable=import-error,import-outside-toplevel,unused-import,line-too-long,no-name-in-module # noqa: E501
        platforms = platform_manager.list_platforms()

        console.print(Panel(
            ""\n"".join(f""[green]{p}[/green]"" for p in platforms),
            title=""Available Platforms"",
            border_style=""blue""
        ))
        return True",Handle /platform list command.,Check platform availability and display a list of available platforms.
539,get_pr_diff,"def get_pr_diff(self, owner: str, repo: str, pr_number: int) -> str:
        
        key = (owner, repo, pr_number)

        if getattr(self, ""_cached_diff_key"", None) == key and hasattr(self, ""_cached_diff_text""):
            assert self._cached_diff_text is not None
            return self._cached_diff_text

        url = f""{self.config.github.base_url}/repos/{owner}/{repo}/pulls/{pr_number}""
        headers = dict(self.github_session.headers)
        headers[""Accept""] = ""application/vnd.github.v3.diff""

        response = self.github_session.get(url, headers=headers)
        response.raise_for_status()

        self._cached_diff_key = key
        self._cached_diff_text = response.text
        if hasattr(self, ""_cached_parsed_diff""):
            delattr(self, ""_cached_parsed_diff"")

        return response.text",Get the full diff for the PR.,Fetch and cache GitHub pull request diff for efficient retrieval.
540,add_assistant_response,"def add_assistant_response(self, content: str, tool_calls: Optional[List[Dict]] = None):
        
        response_data = {
            'role': 'assistant',
            'content': content
        }
        if tool_calls:
            response_data['tool_calls'] = tool_calls
        
        self.completion_responses.append(response_data)",Add an expected assistant response.,Append assistant's reply and tool usage to responses list
541,extract_text_from_pdf,"def extract_text_from_pdf(file: str) -> str:
    
    text = """"
    with open(file, ""rb"") as f:
        reader = pypdf.PdfReader(f)
        if reader.is_encrypted:  # Check if the PDF is encrypted
            try:
                reader.decrypt("""")
            except pypdf.errors.FileNotDecryptedError as e:
                logger.warning(f""Could not decrypt PDF {file}, {e}"")
                return text  # Return empty text if PDF could not be decrypted

        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text += page.extract_text()

    if not text.strip():  # Debugging line to check if text is empty
        logger.warning(f""Could not decrypt PDF {file}"")

    return text",Extract text from PDF files,"Extracts and returns text from a PDF file, handling encryption if necessary."
542,_write_messages_to_file,"def _write_messages_to_file(self, f: Any, messages: list[BaseMessage]) -> None:
		
		for message in messages:
			f.write(f' {message.__class__.__name__} \n')

			if isinstance(message.content, list):
				for item in message.content:
					if isinstance(item, dict) and item.get('type') == 'text':
						f.write(item['text'].strip() + '\n')
			elif isinstance(message.content, str):
				try:
					content = json.loads(message.content)
					f.write(json.dumps(content, indent=2) + '\n')
				except json.JSONDecodeError:
					f.write(message.content.strip() + '\n')

			f.write('\n')",Write messages to conversation file,"Serialize and write structured message content to a file, handling text and JSON formats."
543,set_directory_permissions,"def set_directory_permissions(directory):
    
    if not is_admin():
        print(""WARNING: Cannot set permissions without admin rights"")
        return False
    
    try:
        # Use icacls to set permissions recursively
        subprocess.run(['icacls', directory, '/grant', 'Everyone:(OI)(CI)F', '/T'], 
                      check=True, capture_output=True)
        print(f""Set permissions on: {directory}"")
        return True
    except Exception as e:
        print(f""Error setting permissions: {e}"")
        return False",Set appropriate permissions on directory and children,"Set directory permissions for all users, requiring admin rights."
544,ensure_csv_exists,"def ensure_csv_exists(fold):
    
    csv_file = f'SAT_{fold}.csv'
    if not os.path.exists(csv_file):
        duckdb.sql(f)
    return csv_file",Ensure the CSV file exists by converting from Parquet if necessary.,Ensure a CSV file exists by checking and creating it if necessary.
545,inject_storage_user_unregistered,"def inject_storage_user_unregistered(self, microservices: list[str]):
        
        target_services = [""mongodb-rate"", ""mongodb-geo""]
        for service in target_services:
            if service in microservices:
                pods = self.kubectl.list_pods(self.namespace)
                target_mongo_pods = [
                    pod.metadata.name
                    for pod in pods.items
                    if service in pod.metadata.name
                ]
                print(f""Target MongoDB Pods: {target_mongo_pods}"")

                target_service_pods = [
                    pod.metadata.name
                    for pod in pods.items
                    if pod.metadata.name.startswith(self.mongo_service_pod_map[service])
                ]
                for pod in target_mongo_pods:
                    revoke_command = f""kubectl exec -it {pod} -n {self.namespace} -- /bin/bash /scripts/remove-admin-mongo.sh""
                    result = self.kubectl.exec_command(revoke_command)
                    print(f""Injection result for {service}: {result}"")

                self.delete_service_pods(target_service_pods)",Inject a fault to create an unregistered user in MongoDB.,Identify and revoke admin access from specific MongoDB pods in Kubernetes.
546,list_tables,"def list_tables(self, keyspace_name: str) -> List[TableInfo]:
        
        tables = []

        try:
            query = 'SELECT table_name FROM system_schema.tables WHERE keyspace_name = %s'
            rows = self.session.execute(query, [keyspace_name])

            for row in rows:
                name = row.table_name
                tables.append(TableInfo(name=name, keyspace=keyspace_name))

            return tables
        except Exception as e:
            logger.error(f'Error listing tables for keyspace {keyspace_name}: {str(e)}')
            raise RuntimeError(f'Failed to list tables for keyspace {keyspace_name}: {str(e)}')",List all tables in a keyspace.,Retrieve table information from a specified keyspace in a database schema
547,apply,"def apply(self, project_name: str) -> str:
        
        self.agent.serena_config.remove_project(project_name)
        return f""Successfully removed project '{project_name}' from configuration.""",Removes a project from the Serena configuration.,Remove a project from configuration and confirm success.
548,track_llm_usage,"def track_llm_usage(self, provider: LLMProvider, model: str, input_tokens: int, output_tokens: int):
        
        self.breakdown.llm_input_tokens += input_tokens
        self.breakdown.llm_output_tokens += output_tokens

        # Strip prefix from model name for pricing lookup
        stripped_model = self._strip_model_prefix(model)

        # Get pricing for this provider/model
        if provider in self.pricing and stripped_model in self.pricing[provider]:
            pricing = self.pricing[provider][stripped_model]
            input_cost = (input_tokens / 1_000_000) * pricing[""input_per_million""]
            output_cost = (output_tokens / 1_000_000) * pricing[""output_per_million""]

            self.breakdown.llm_cost_usd += input_cost + output_cost
        elif provider == LLMProvider.OLLAMA:
            # All Ollama models are free - use default $0.00 pricing
            if provider in self.pricing and ""_default"" in self.pricing[provider]:
                pricing = self.pricing[provider][""_default""]
                self.breakdown.llm_cost_usd += 0.00  # Always free
            else:
                self.breakdown.llm_cost_usd += 0.00  # Fallback - still free
        else:
            # Unknown model - use a reasonable estimate and warn
            print(f""⚠️  Unknown pricing for {provider.value}/{stripped_model}, using estimates"")
            print(""   Update pricing in ~/.kit/review-config.yaml or check current rates"")
            self.breakdown.llm_cost_usd += (input_tokens / 1_000_000) * 3.0
            self.breakdown.llm_cost_usd += (output_tokens / 1_000_000) * 15.0

        # Store the original model name with prefix for reference
        self.breakdown.model_used = model
        self._update_total()",Track LLM API usage and calculate costs.,"Calculate and update LLM usage costs based on token input, output, and provider pricing."
549,unix_pattern_to_parameter_names,"def unix_pattern_to_parameter_names(
    constraints: List[str], all_parameter_names: Sequence[str]
) -> Union[None, Set[str]]:
    
    parameter_names = []
    for param_name in constraints:
        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))
        assert (
            len(matching_parameters) > 0
        ), f""param_names {param_name} don't match any param in the given names.""
        parameter_names.append(matching_parameters)
    return set.union(*parameter_names)",Go through the list of parameter names and select those that match any of the provided constraints,Convert wildcard constraints to matching parameter names from a list.
550,get_fix_suggestion,"def get_fix_suggestion(issue: Dict[str, Any]) -> str:
    
    suggestions = {
        'B102': ""As an AI assistant, you should not use the exec() function. Instead, describe the code or suggest safer alternatives that don't involve direct code execution."",
        'B307': 'As an AI assistant, you should not use eval(). You can use ast.literal_eval() for parsing simple data structures.',
        'B602': 'As an AI assistant, you should not use subprocess calls. Instead, describe the system operation you want to perform or suggest higher-level library alternatives.',
        'B605': 'As an AI assistant, you should avoid shell commands. Instead, describe the desired operation or suggest library-based alternatives.',
        'B103': 'The pickle module is not secure. Use JSON or other secure serialization methods.',
        'B201': 'Flask app appears to be run with debug=True, which enables the Werkzeug debugger and should not be used in production.',
        'B301': 'Pickle and modules that wrap it can be unsafe when used to deserialize untrusted data.',
        'B324': 'Use of weak cryptographic key. Consider using stronger key lengths.',
        'B501': 'Request with verify=False disables SSL certificate verification and is not secure.',
        'B506': 'Use of yaml.load() can result in arbitrary code execution. Use yaml.safe_load() instead.',
        'DangerousFunctionDetection': 'This function allows arbitrary code execution and should be avoided. Consider safer alternatives.',
    }

    issue_type = issue.get('issue_type', '')
    default_msg = (
        'This is a security issue that should be addressed. As an AI assistant, '
        'you should avoid suggesting code that could pose security risks. Instead, '
        'describe the intended functionality or suggest safer alternatives.'
    )

    return suggestions.get(issue_type, default_msg)",Provide suggestions for fixing security issues.,Provide security-focused code suggestions based on identified issue types.
551,register,"def register(cls, name: str, local_path: str, module_path: str):
        

        def add_subclass_to_registry(subclass: Type[""BaseOp""]):
            subclass.name = name
            subclass._local_path = local_path
            subclass._module_path = module_path
            if name in cls._registry:
                raise ValueError(
                    f""Operator [{name}] conflict in {subclass._local_path} and {cls.by_name(name)._local_path}.""
                )
            cls._registry[name] = subclass
            if hasattr(subclass, ""bind_to""):
                subclass.__bases__[0].bind_schemas[subclass.bind_to] = name
            return subclass

        return add_subclass_to_registry",Register a class as subclass of BaseOp with name and local_path.,Register and validate subclasses in a central registry with conflict checks.
552,sample_df,"def sample_df():
    
    return pd.DataFrame([
        {
            ""instance_id"": ""iproute2_101"", ""repo"": ""iproute2"", ""version"": 1, ""problem:"": ""incorrect output""
        },
        {
            ""instance_id"": ""frr_101"", ""repo"": ""frr"", ""version"": 2, ""problem:"": ""NULL ptr access""
        },
        {
            ""instance_id"": ""vxlan_101"", ""repo"": ""vxlan"", ""version"": 1, ""problem:"": ""bridge driver inaccessibility""
        },
        {
            ""instance_id"": ""iproute2_99"", ""repo"": ""iproute2"", ""version"": 4, ""problem:"": ""memory leak""
        },
        {
            ""instance_id"": ""vxlan_102"", ""repo"": ""vxlan"", ""version"": 2, ""problem"": ""kernel panic""
        },
    ])",Fixture for a sample DataFrame.,Create a DataFrame with software issue details for various repositories.
553,_verify_jwt,"def _verify_jwt(self, token: str) -> dict[str, Any] | None:
        
        jwt_secret = self._get_jwt_secret()  # Get secret first
        # _diag_logger.info(f""GolfOAuthProvider: _verify_jwt attempting to use secret: {jwt_secret[:5]}..."")

        try:
            payload = jwt.decode(
                token,
                jwt_secret,
                algorithms=[""HS256""],
                options={""verify_signature"": True},
            )

            if payload.get(""exp"", 0) < time.time():
                exp_timestamp = payload.get(""exp"")
                current_timestamp = time.time()
                (
                    str(datetime.fromtimestamp(exp_timestamp))
                    if exp_timestamp is not None
                    else ""N/A""
                )
                str(datetime.fromtimestamp(current_timestamp))
                return None
            return payload
        except jwt.ExpiredSignatureError:
            return None
        except jwt.PyJWTError:
            return None
        except Exception:  # Catch any other unexpected error during decode
            return None",Verify a JWT token.,"Validate and decode JWT token, returning payload if valid and not expired"
554,_determine_file_extension,"def _determine_file_extension(self, content: str, chunk_metadata: Optional[str]) -> str:
        
        try:
            # Parse chunk metadata to check if it's an image
            if chunk_metadata:
                metadata = json.loads(chunk_metadata)
                is_image = metadata.get(""is_image"", False)

                if is_image:
                    # For images, auto-detect from base64 content
                    return detect_file_type(content)
                else:
                    # For text content, use .txt
                    return "".txt""
            else:
                # No metadata, try to auto-detect
                return detect_file_type(content)

        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f""Error parsing chunk metadata: {e}"")
            # Fallback to auto-detection
            return detect_file_type(content)",Determine appropriate file extension based on content and metadata.,Determine file extension based on metadata or content type detection
555,parse,"def parse(self, input: dict, response: Recipe) -> dict:
        
        return {
            ""title"": response.title,
            ""ingredients"": response.ingredients,
            ""instructions"": response.instructions,
            ""prep_time"": response.prep_time,
            ""cook_time"": response.cook_time,
            ""servings"": response.servings,
            ""cuisine"": input[""cuisine""],
        }",Parse the model response along with the input to the model into the desired output format..,Extracts and formats recipe details from a response object into a dictionary.
556,_validate_dependencies,"def _validate_dependencies(self):
        
        for atom in self.atoms.values():
            for dep in atom.dependencies:
                if dep not in self.atoms:
                    raise ValueError(f""Atom '{atom.name}' depends on missing atom '{dep}'"")

        visited = set()
        temp_visited = set()
        stack = []

        def has_cycle(atom_name: str) -> bool:
            if atom_name in temp_visited:
                logger.error(f""[DAG] Cycle detected -> {' -> '.join([*stack, atom_name])}"")
                return True
            if atom_name in visited:
                return False

            temp_visited.add(atom_name)
            stack.append(atom_name)

            for dep in self.atoms[atom_name].dependencies:
                if has_cycle(dep):
                    return True

            temp_visited.remove(atom_name)
            visited.add(atom_name)
            stack.pop()
            return False

        for atom_name in self.atoms:
            if has_cycle(atom_name):
                raise ValueError(""Cycle detected in DAG"")",Validate all atoms reference valid dependencies and no cycles exist.,Validate and detect cycles in a directed acyclic graph of dependencies
557,navigate_to_history_entry,"def navigate_to_history_entry(
    entry_id: int,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""entryId""] = entry_id
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Page.navigateToHistoryEntry"",
        ""params"": params,
    }
    json = yield cmd_dict",Navigates current page to the given history entry.,Generate a command to navigate to a specific browser history entry.
558,load_nodes,"def load_nodes():
    
    for file in current_dir.glob(""*.py""):
        if file.stem == ""__init__"":
            continue
            
        try:
            # Import module
            spec = importlib.util.spec_from_file_location(file.stem, file)
            if spec and spec.loader:
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                # Update mappings
                if hasattr(module, ""NODE_CLASS_MAPPINGS""):
                    NODE_CLASS_MAPPINGS.update(module.NODE_CLASS_MAPPINGS)
                if hasattr(module, ""NODE_DISPLAY_NAME_MAPPINGS""):
                    NODE_DISPLAY_NAME_MAPPINGS.update(module.NODE_DISPLAY_NAME_MAPPINGS)
                    
                # Initialize paths if available
                if hasattr(module, ""Paths"") and hasattr(module.Paths, ""LLM_DIR""):
                    os.makedirs(module.Paths.LLM_DIR, exist_ok=True)
                    
        except Exception as e:
            print(f""Error loading {file.name}: {str(e)}"")",Automatically discover and load node definitions,"Load and integrate Python modules, updating node mappings and initializing directories."
559,get_files,"def get_files(self):
        
        files = []
        for path in self.paths:
            path = Path(path)
            if path.is_dir():
                extensions = [""*.pt"", ""*.onnx"", ""*.yaml""]
                files.extend([file for ext in extensions for file in glob.glob(str(path / ext))])
            elif path.suffix in {"".pt"", "".yaml"", "".yml""}:  # add non-existing
                files.append(str(path))
            else:
                files.extend(glob.glob(str(path)))

        print(f""Profiling: {sorted(files)}"")
        return [Path(file) for file in sorted(files)]",Returns a list of paths for all relevant model files given by the user.,Retrieve and sort files with specific extensions from given paths.
560,get_token_by_ticker,"def get_token_by_ticker(agent, **kwargs):
    
    try:
        ticker = kwargs.get(""ticker"")
        if not ticker:
            logger.error(""No ticker provided"")
            return None
            
        token_address = agent.connection_manager.connections[""ethereum""].get_token_by_ticker(ticker)
        
        if token_address:
            logger.info(f""Found token address for {ticker}: {token_address}"")
        else:
            logger.info(f""No token found for ticker {ticker}"")
            
        return token_address

    except Exception as e:
        logger.error(f""Failed to get token by ticker: {str(e)}"")
        return None",Get token address by ticker symbol,Retrieve cryptocurrency token address using ticker symbol via Ethereum connection
561,get_all,"def get_all(self) -> list[Message]:
        
        try:
            query = self.SELECT_ALL_MESSAGES_QUERY.format(index_name=self.index_name)
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(query)
                rows = cursor.fetchall()
            return [Message(role=row[1], content=row[2], metadata=json.loads(row[3] or ""{}"")) for row in rows]

        except sqlite3.Error as e:
            raise SQLiteError(f""Error retrieving messages from database: {e}"") from e",Retrieves all messages from the SQLite database.,"Retrieve all messages from a database, handling errors gracefully."
562,_load_plugins,"def _load_plugins() -> Union[None, List[Any]]:
    
    global _plugins

    # Skip if we've already loaded plugins
    if _plugins is not None:
        return _plugins

    # Load plugins
    _plugins = []
    for entry_point in entry_points(group=""markitdown.plugin""):
        try:
            _plugins.append(entry_point.load())
        except Exception:
            tb = traceback.format_exc()
            warn(f""Plugin '{entry_point.name}' failed to load ... skipping:\n{tb}"")

    return _plugins","Lazy load plugins, exiting early if already loaded.","Load and return a list of plugins, handling exceptions gracefully."
563,_create_hub_entrypoint,"def _create_hub_entrypoint(model):
    

    def entrypoint(**kwargs):
        return _load(model, **kwargs)

    entrypoint.__doc__ = f
    return entrypoint",Creates an entry point for loading the specified CLIP model with adjustable parameters.,Create a function to load a model with dynamic parameters.
564,read_colabfold_inputs,"def read_colabfold_inputs(fname: Path) -> dict[str, list[Fasta]]:
    
    df = pd.read_csv(fname, delimiter="","")
    assert list(df.columns) == [""id"", ""sequence""]
    retval: dict[str, list[Fasta]] = {}
    for row in df.itertuples():
        sequences: list[str] = row.sequence.split("":"")  # type: ignore
        complex: list[Fasta] = [
            Fasta(header=f""protein|{get_chain_letter(i)}"", sequence=seq)
            for i, seq in enumerate(sequences, start=1)
        ]
        retval[row.id] = complex  # type: ignore
    return retval",Extracts sequences from colabfold input table.,Parse CSV to map protein IDs to sequence complexes as Fasta objects
565,_get_relevant_chunks,"def _get_relevant_chunks(self, repo, parameters: Dict[str, Any]) -> str:
        
        chunks = repo.chunk_file_by_symbols(parameters[""file_path""])
        relevance_query = parameters[""relevance_query""].lower()
        max_chunks = parameters.get(""max_chunks"", 3)

        # Score chunks based on relevance
        scored_chunks = []
        for i, chunk in enumerate(chunks):
            content_lower = chunk.content.lower()
            score = sum(content_lower.count(word) for word in relevance_query.split())
            # Boost for function/class definitions
            if any(
                f""def {word}"" in content_lower or f""class {word}"" in content_lower for word in relevance_query.split()
            ):
                score += 10
            scored_chunks.append((score, i, chunk))

        # Sort by relevance and take top chunks
        scored_chunks.sort(key=lambda x: x[0], reverse=True)
        relevant_chunks = scored_chunks[:max_chunks]

        result = f""Relevant chunks for '{relevance_query}' in {parameters['file_path']}:\n""
        for score, chunk_idx, chunk in relevant_chunks:
            if score > 0:
                result += f""\nChunk {chunk_idx + 1}:\n{chunk.content}\n---\n""

        if not any(score > 0 for score, _, _ in relevant_chunks):
            result += f""\nNo chunks found matching '{relevance_query}'""

        return result",Get relevant chunks from a file based on query.,Identify and rank relevant text chunks from a file based on a query.
566,extract_values_from_json,"def extract_values_from_json(json_string, allow_no_quotes=False):
    
    extracted_values = {}

    # Enhanced pattern to match both quoted and unquoted values, as well as nested objects
    regex_pattern = r'(?P<key>""?\w+""?)\s*:\s*(?P<value>{[^}]*}|"".*?""|[^,}]+)'

    for match in re.finditer(regex_pattern, json_string, re.DOTALL):
        key = match.group(""key"").strip('""')  # Strip quotes from key
        value = match.group(""value"").strip()

        # If the value is another nested JSON (starts with '{' and ends with '}'), recursively parse it
        if value.startswith(""{"") and value.endswith(""}""):
            extracted_values[key] = extract_values_from_json(value)
        else:
            # Parse the value into the appropriate type (int, float, bool, etc.)
            extracted_values[key] = parse_value(value)

    if not extracted_values:
        LOG.warning(""No values could be extracted from the string."")

    return extracted_values","Extract key values from a non-standard or malformed JSON string, handling nested objects.","Extracts key-value pairs from JSON strings, handling nested objects and optional quotes."
567,prompt,"def prompt(self) -> str:
        
        return AGENTS_PROMPT_TPL.format(**{
            'role': self.role,
            'goal': self.goal,
            'backstory': self.backstory,
        })","Concatenate the prompts for role, goal, and backstory.","Generates a formatted prompt using role, goal, and backstory attributes."
568,mock_auth_service,"def mock_auth_service():
    
    with patch.object(AuthService, 'sign_up', new_callable=AsyncMock) as mock_sign_up, \
         patch.object(AuthService, 'sign_in', new_callable=AsyncMock) as mock_sign_in, \
         patch.object(AuthService, 'sign_out', new_callable=AsyncMock) as mock_sign_out, \
         patch.object(AuthService, 'reset_password', new_callable=AsyncMock) as mock_reset_password, \
         patch.object(AuthService, 'refresh_token', new_callable=AsyncMock) as mock_refresh_token:
        
        yield {
            'sign_up': mock_sign_up,
            'sign_in': mock_sign_in,
            'sign_out': mock_sign_out,
            'reset_password': mock_reset_password,
            'refresh_token': mock_refresh_token
        }",Mock the AuthService class methods.,Mock authentication service methods for testing asynchronous behavior.
569,click_element,"def click_element(self, xpath: str) -> bool:
        
        try:
            element = self.wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))
            if not element.is_displayed():
                return False
            if not element.is_enabled():
                return False
            try:
                self.logger.error(f""Scrolling to element for click_element."")
                self.driver.execute_script(""arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});"", element)
                time.sleep(0.1)
                element.click()
                self.logger.info(f""Clicked element at {xpath}"")
                return True
            except ElementClickInterceptedException as e:
                self.logger.error(f""Error click_element: {str(e)}"")
                return False
        except TimeoutException:
            self.logger.warning(f""Timeout clicking element."")
            return False
        except Exception as e:
            self.logger.error(f""Unexpected error clicking element at {xpath}: {str(e)}"")
            return False",Click an element specified by XPath.,"Attempt to click a web element by XPath, handling visibility, enablement, and exceptions."
570,python_skill_func,"def python_skill_func(codebase: CodebaseType):
        
        import_replacements = {""List"": ""list"", ""Dict"": ""dict"", ""Set"": ""set"", ""Tuple"": ""tuple""}

        # Iterate over all imports in the codebase
        for imported in codebase.imports:
            # Check if the import is from the typing module and is a builtin type
            if imported.module == ""typing"" and imported.name in import_replacements:
                # Remove the type import
                imported.remove()
                # Iterate over all symbols that use this imported module
                for symbol in imported.symbol_usages:
                    # Find all exact matches (Editables) in the symbol with this imported module
                    for usage in symbol.find(imported.name, exact=True):
                        # Replace the usage with the builtin type
                        usage.edit(import_replacements[imported.name])",Replaces type annotations using typing module with builtin types.,Refactor codebase by replacing typing imports with built-in types for cleaner syntax.
571,_missing_,"def _missing_(cls, value: str) -> Optional[""LogFormat""]:
        
        try:
            # Convert to uppercase and look up directly
            return cls[value.upper()]
        except (KeyError, AttributeError):
            raise ValueError(
                f""'{value}' is not a valid LogFormat. ""
                f""Valid formats are: {', '.join(format.value for format in cls)}""
            )",Handle case-insensitive lookup of enum values.,Handle invalid log format by converting input to uppercase and checking against valid formats
572,scroll,"def scroll(element: MacElementNode, direction: str) -> bool:
	
	direction_map = {
		'left': 'AXScrollLeftByPage',
		'right': 'AXScrollRightByPage',
		'up': 'AXScrollUpByPage',
		'down': 'AXScrollDownByPage'
	}

	if direction not in direction_map:
		logger.error(f'❌ Invalid scroll direction: {direction}')
		return False

	action = direction_map[direction]
	if action in element.actions:
		return perform_action(element, action)
	else:
		logger.error(f'❌ Element does not support scrolling {direction}: {element}')
		return False",Scrolls an element in the specified direction.,Determine scroll action for an element based on direction input and execute if supported.
573,search_result,"def search_result():
    
    return SearchResult(
        title=""Test Search Result"",
        type=SearchItemType.ENTITY,
        permalink=""test/search-result"",
        score=0.95,
        content=""This is a test search result with some content."",
        file_path=""/path/to/test/search-result.md"",
        metadata={""created_at"": datetime.datetime(2023, 2, 1, 12, 0)},
    )",Create a sample SearchResult for testing.,Create a mock search result object with predefined attributes.
574,_filter_to_supported_schema,"def _filter_to_supported_schema(schema: dict[str, Any]) -> dict[str, Any]:
  
  supported_fields: set[str] = set(types.JSONSchema.model_fields.keys())
  schema_field_names: tuple[str] = (""items"",)  # 'additional_properties' to come
  list_schema_field_names: tuple[str] = (
      ""any_of"",  # 'one_of', 'all_of', 'not' to come
  )
  dict_schema_field_names: tuple[str] = (""properties"",)  # 'defs' to come
  for field_name, field_value in schema.items():
    if field_name in schema_field_names:
      schema[field_name] = _filter_to_supported_schema(field_value)
    elif field_name in list_schema_field_names:
      schema[field_name] = [
          _filter_to_supported_schema(value) for value in field_value
      ]
    elif field_name in dict_schema_field_names:
      schema[field_name] = {
          key: _filter_to_supported_schema(value)
          for key, value in field_value.items()
      }
  return {
      key: value for key, value in schema.items() if key in supported_fields
  }",Filters the schema to only include fields that are supported by JSONSchema.,Filter and recursively process schema fields to retain only supported keys.
575,get_latest_opset,"def get_latest_opset():
    
    if TORCH_1_13:
        # If the PyTorch>=1.13, dynamically compute the latest opset minus one using 'symbolic_opset'
        return max(int(k[14:]) for k in vars(torch.onnx) if ""symbolic_opset"" in k) - 1
    # Otherwise for PyTorch<=1.12 return the corresponding predefined opset
    version = torch.onnx.producer_version.rsplit(""."", 1)[0]  # i.e. '2.3'
    return {""1.12"": 15, ""1.11"": 14, ""1.10"": 13, ""1.9"": 12, ""1.8"": 12}.get(version, 12)","Return the second-most recent ONNX opset version supported by this version of PyTorch, adjusted for maturity.",Determine the latest ONNX opset version based on PyTorch version.
576,_assert_valid_name,"def _assert_valid_name(name):
        
        if not re.match(r""^[a-zA-Z0-9_-]+$"", name):
            raise ValueError(f""Invalid name: {name}. Only letters, numbers, '_' and '-' are allowed."")
        if len(name) > 64:
            raise ValueError(f""Invalid name: {name}. Name must be less than 64 characters."")
        return name","Ensure that configured names are valid, raises ValueError if not.",Validate identifier format and length constraints.
577,extract_rating_key,"def extract_rating_key(self, item, item_type):
        
        key = item.get('key') if item_type == 'movie' else item.get('grandparentKey') if item_type == 'episode' else None
        self.logger.debug(f""Extracted rating key: {key} for item type: {item_type}"")
        return key if key else None",Extract the appropriate key depending on the item type.,Determine and log the rating key based on item type.
578,__from_env,"def __from_env():
    
    #global ATTN
    global BACKEND
    global DEBUG
    
    # Get current settings from central config
    #ATTN = 
    BACKEND = get_attention_backend()
    DEBUG = get_debug_mode()
    
    print(f""[ATTENTION] Using backend: {BACKEND}"")",Read current backend configuration,Initialize backend and debug settings from configuration
579,validate_envs,"def validate_envs() -> None:
    
    required_envs = {
        ""LIVEKIT_URL"": ""LiveKit server URL"",
        ""LIVEKIT_API_KEY"": ""API Key for LiveKit"",
        ""LIVEKIT_API_SECRET"": ""API Secret for LiveKit"",
        ""DEEPGRAM_API_KEY"": ""API key for Deepgram (used for STT)"",
        ""ELEVEN_API_KEY"": ""API key for ElevenLabs (used for TTS)"",
    }
    for key, description in required_envs.items():
        if not os.environ.get(key):
            logger.warning(""Environment variable %s (%s) is not set."", key, description)",Check for the presence of all required environment variables.,Check and log missing required environment variables for LiveKit and other APIs.
580,match,"def match(cls, responses, targets) -> float:
        
        logging.debug(f""{responses=}, {targets=}"")
        if not isinstance(responses, (tuple | list)):
            responses = str_to_coords(responses, dim=2)
        if not isinstance(targets, (tuple | list)):
            targets = str_to_coords(targets, dim=2)

        return cls.compute_score(responses, targets)",Exact match between targets and responses.,Convert inputs to coordinates and compute similarity score.
581,exec,"def exec(self, inputs):
        
        query, retrieved_doc = inputs
        
        prompt = f
        
        answer = call_llm(prompt)
        return answer",Generate an answer using the LLM,Generate an answer using a language model based on input query and document.
582,_is_prometheus_running,"def _is_prometheus_running(self) -> bool:
        
        command = (
            f""kubectl get pods -n {self.namespace} -l app.kubernetes.io/name=prometheus""
        )
        try:
            result = KubeCtl().exec_command(command)
            if ""Running"" in result:
                return True
        except CalledProcessError:
            return False
        return False",Check if Prometheus is already running in the cluster.,Check if Prometheus is running in Kubernetes namespace using kubectl command.
583,review_pr,"def review_pr(
    pr_url: str,
    config: Optional[str] = typer.Option(None, ""--config"", ""-c"", help=""Path to config file""),
    dry_run: bool = typer.Option(True, ""--dry-run/--post"", help=""Run analysis but do not post comment""),
):
    
    try:
        from .config import ReviewConfig
        from .reviewer import PRReviewer

        # Load configuration
        if config:
            review_config = ReviewConfig.from_file(config)
        else:
            review_config = ReviewConfig.from_file()

        # Override post_as_comment if dry run
        if dry_run:
            review_config.post_as_comment = False

        # Run review
        reviewer = PRReviewer(review_config)
        result = reviewer.review_pr(pr_url)

        if dry_run:
            typer.echo(""\n"" + ""="" * 50)
            typer.echo(""REVIEW RESULT (DRY RUN)"")
            typer.echo(""="" * 50)
            typer.echo(result)
        else:
            typer.echo(""✅ Review posted to PR"")

    except Exception as e:
        typer.echo(f""❌ Error during review: {e}"", err=True)
        raise typer.Exit(1)",Review a GitHub PR using kit analysis for testing.,Automates pull request review with optional configuration and dry-run mode.
584,log,"def log(self, out_str, end='\n', log_time=False):
        
        # for classes, if __str__() is not defined, then it's the same as __repr__()
        if not isinstance(out_str, str):
            out_str = out_str.__repr__()
        if log_time:
            out_str = '{0:%Y-%m-%d %H:%M:%S} '.format(datetime.datetime.now()) + out_str
        with open(self.log_name, ""a"") as log_file:
            log_file.write(out_str + end)
        print(out_str, end=end, flush=True)",out_str: single object now,"Log message to file and console, optionally with timestamp."
585,add_task,"def add_task(
    name: str,
    description: Optional[str] = None,
    expected_output: Optional[str] = None,
    agent: Optional[str] = None,
    position: Optional[str] = None,
):
    
    conf.assert_project()
    _position = parse_insertion_point(position)

    repo.commit_user_changes()
    with repo.Transaction() as commit:
        commit.add_message(f""Added task {name}"")
        generation.add_task(
            name=name,
            description=description,
            expected_output=expected_output,
            agent=agent,
            position=_position,
        )",Add a task to the user's project.,Add a new task to the project repository with optional details and commit changes.
586,generate_text,"def generate_text(self, prompt: str, **kwargs) -> str:
        
        kwargs.pop(""llm_model"")
        try:
            response = self.client.generate_content(prompt, **kwargs)
        except Exception as e:
            # Handle the exception appropriately, e.g., log the error or raise a custom exception
            raise RuntimeError(f""Failed to generate text with Gemini API: {e}"") from e
        return response.text",Generate text using the Gemini API.,"Generate text using an API, handling exceptions for robust output."
587,create_migration,"def create_migration(description):
    
    db_path = get_db_path()
    migrations_dir = os.path.join(project_root, ""lpm_kernel"", ""database"", ""migrations"")
    
    manager = MigrationManager(db_path)
    filepath = manager.create_migration(description, migrations_dir)
    
    # logger.info(f""Created new migration at: {filepath}"")
    return filepath",Create a new migration file,Generate database migration file with specified description.
588,pre_send_hook,"def pre_send_hook(self, conversation: sm.Conversation) -> bool:
        
        self.llm_model = conversation.llm_model
        self.llm_provider = conversation.llm_provider

        last_message = conversation.get_last_message(role=""user"")
        if not last_message:
            return True

        # Handle special commands
        if result := self._handle_special_commands(conversation, last_message.text):
            return result

        self.logger.info(f""Processing user message: {last_message.text}"")

        # Process entities and markers
        self._process_user_message(last_message.text)

        # Add context
        self._add_context_to_conversation(conversation)

        return True",Process user message before sending to LLM,Prepares and processes user messages in a conversation before sending.
589,get_bootcamp_class,"def get_bootcamp_class(self, name: str) -> Type:
        
        if not self._discovered:
            self.discover_bootcamps()

        if name not in self._registry:
            available = self.list_available_bootcamps()
            raise ValueError(
                f""Unknown bootcamp: {name}. ""
                f""Available bootcamps: {', '.join(available[:10])}...""
                f"" ({len(available)} total)""
            )

        return self._registry[name]",Get a bootcamp class by name.,"Retrieve bootcamp class by name, ensuring discovery and validation."
590,_process_mermaid_diagrams,"def _process_mermaid_diagrams(self, text: str, output_dir: Path) -> str:
        
        def replace_mermaid(match):
            try:
                diagram = mermaid.generate_diagram(match.group(1))
                img_path = output_dir / f""diagram_{hash(match.group(1))}.pdf""
                
                # Save diagram as PDF
                with open(img_path, 'wb') as f:
                    f.write(diagram)
                
                return (
                    ""\\begin{figure}[H]\n""
                    ""\\centering\n""
                    f""\\includegraphics[width=0.8\\textwidth]{{{img_path}}}\n""
                    ""\\caption{Generated diagram}\n""
                    ""\\end{figure}\n""
                )
            except Exception as e:
                logger.error(f""Error processing Mermaid diagram: {e}"")
                return ""% Error processing diagram\n""
        
        return re.sub(
            r'```mermaid\n(.*?)\n```',
            replace_mermaid,
            text,
            flags=re.DOTALL
        )",Convert Mermaid diagrams to TikZ or image figures.,Convert Mermaid code blocks in text to PDF diagrams and embed them in LaTeX format.
591,list_available_files,"def list_available_files(repo_id, token=None):
    
    try:
        # For public repositories, token is not needed
        files = list_repo_files(repo_id=repo_id, token=token, repo_type=""dataset"")
        return files
    except Exception as e:
        logger.error(f""Error listing files from repository: {str(e)}"")
        logger.info(""If this is a public repository, you can ignore token-related errors."")
        return []",List all files in the repository.,"Retrieve dataset files from a repository, handling errors gracefully."
592,prepare_docs_markdown,"def prepare_docs_markdown(clone_repos=True):
    
    if SITE.exists():
        print(f""Removing existing {SITE}"")
        shutil.rmtree(SITE)

    # Get hub-sdk repo
    if clone_repos:
        local_dir = DOCS.parent / Path(repo).name
        if not local_dir.exists():
            os.system(f""git clone {repo} {local_dir}"")
        os.system(f""git -C {local_dir} pull"")  # update repo
        shutil.rmtree(DOCS / ""en/hub/sdk"", ignore_errors=True)  # delete if exists
        shutil.copytree(local_dir / ""docs"", DOCS / ""en/hub/sdk"")  # for docs
        shutil.rmtree(DOCS.parent / ""hub_sdk"", ignore_errors=True)  # delete if exists
        shutil.copytree(local_dir / ""hub_sdk"", DOCS.parent / ""hub_sdk"")  # for mkdocstrings
        print(f""Cloned/Updated {repo} in {local_dir}"")

    # Add frontmatter
    for file in tqdm((DOCS / ""en"").rglob(""*.md""), desc=""Adding frontmatter""):
        update_markdown_files(file)",Build docs using mkdocs.,Prepare and update documentation by cloning repositories and adding frontmatter to markdown files.
593,get_vectorizer_config,"def get_vectorizer_config(model: BaseEmbeddingModel) -> VectorizerConfig:
        
        if model.model_name == ""openai-text2vec"":
            return Configure.Vectorizer.text2vec_openai(
                api_key=model.api_key,
                dimensions=model.vector_dimensions,
                model_name=model.model_name,
            )
        elif model.model_name == ""local-text2vec-transformers"":
            return Configure.Vectorizer.text2vec_transformers(
                inference_url=model.inference_url,
            )

        raise ValueError(f""Unsupported model type: {model.model_name}"")",Convert model config to Weaviate vectorizer config.,Determine vectorizer configuration based on embedding model type.
594,validate_config,"def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        
        required_fields = [""api_url"", ""api_key"", ""room"", ""history_read_count"", ""sender_username"", ""sender_model""]
        missing_fields = [field for field in required_fields if not config.get(field)]
        if missing_fields:
            raise ValueError(f""Missing required configuration fields: {', '.join(missing_fields)}"")

        if not isinstance(config[""history_read_count""], int) or config[""history_read_count""] <= 0:
            raise ValueError(""history_read_count must be a positive integer"")

        return config",Validate Echochambers configuration from JSON,Validate configuration by checking required fields and ensuring correct data types.
595,get_artist_by_id,"def get_artist_by_id(api_url: str, api_key: str, api_timeout: int, artist_id: int) -> Optional[Dict[str, Any]]:
    
    return arr_request(api_url, api_key, api_timeout, f""artist/{artist_id}"")",Get artist details by ID from Lidarr.,Fetch artist details using API with specified parameters
596,save_cache,"def save_cache(self, enforce_constraints: bool = False):
        
        # Prune any buckets that have fewer samples than batch_size
        if enforce_constraints:
            self._enforce_min_bucket_size()
        if self.read_only:
            logger.debug(""Metadata backend is read-only, skipping cache save."")
            return
        # Convert any non-strings into strings as we save the index.
        aspect_ratio_bucket_indices_str = {
            key: [str(path) for path in value]
            for key, value in self.aspect_ratio_bucket_indices.items()
        }
        # Encode the cache as JSON.
        cache_data = {
            ""config"": StateTracker.get_data_backend_config(
                data_backend_id=self.data_backend.id
            ),
            ""aspect_ratio_bucket_indices"": aspect_ratio_bucket_indices_str,
        }
        logger.debug(f""save_cache has config to write: {cache_data['config']}"")
        cache_data_str = json.dumps(cache_data)
        # Use our DataBackend to write the cache file.
        self.data_backend.write(self.cache_file, cache_data_str)",Save cache data to file.,"Save cache data as JSON, enforcing constraints and handling read-only state."
597,save_api_key_to_env,"def save_api_key_to_env(self, provider: str, api_key: str) -> None:
        
        env_path = Path(__file__).parent.parent.parent.parent / '.env'
        
        # Create .env file if it doesn't exist
        if not env_path.exists():
            env_path.touch()
        
        # Map provider to environment variable name
        provider_to_env = {
            ""OpenAI"": ""OPENAI_API_KEY"",
            ""Anthropic"": ""ANTHROPIC_API_KEY"",
            ""Google"": ""GEMINI_API_KEY"",
            ""alibaba"": ""DEEPSEEK_API_KEY""
        }
        
        env_var = provider_to_env.get(provider)
        if env_var and api_key:
            set_key(str(env_path), env_var, api_key)",Save API key to .env file based on provider,Store API key in environment file based on provider mapping.
598,pred_to_json,"def pred_to_json(self, predn, filename):
        
        stem = Path(filename).stem
        image_id = int(stem) if stem.isnumeric() else stem
        box = ops.xyxy2xywh(predn[:, :4])  # xywh
        box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner
        for p, b in zip(predn.tolist(), box.tolist()):
            self.jdict.append(
                {
                    ""image_id"": image_id,
                    ""category_id"": self.class_map[int(p[5])],
                    ""bbox"": [round(x, 3) for x in b],
                    ""keypoints"": p[6:],
                    ""score"": round(p[4], 5),
                }
            )",Converts YOLO predictions to COCO JSON format.,Convert prediction data to JSON format for image annotations.
599,fetch_verified_codemods,"def fetch_verified_codemods(cli_api_key: str):
    
    VERIFIED_CODEMOD_DATA_DIR.mkdir(parents=True, exist_ok=True)
    repos_to_commits: dict[str, list[dict]] = {}

    # Fetch codemods in parallel
    with ProcessPoolExecutor() as executor:
        repos = filter_repos(REPO_ID_TO_URL)
        for result in executor.map(_fetch_and_store_codemod, repos.keys(), repos.values(), repeat(cli_api_key)):
            repo_name, commits_data = result
            if commits_data:
                repos_to_commits[repo_name] = commits_data

    # Store repo commits for cache validation
    repo_commits_file = VERIFIED_CODEMOD_DATA_DIR / ""repo_commits.json""
    print(f""Storing repo commits in {repo_commits_file!s}..."")
    with repo_commits_file.open(""w"") as f:
        f.write(json.dumps(repos_to_commits, indent=4))
        f.flush()",Fetch codemods for all repos in REPO_ID_TO_URL and save to JSON files.,Fetch and store verified code modifications using parallel processing and cache results.
600,filter_out_repeated_characters,"def filter_out_repeated_characters(text):
    
    # If any character appears more than 4 times in a row, filter it out
    import re
    if re.search(r'(.)\1{4,}', text):
        return False
    return True","Filter out strings with excessive character repetition, like 'aaaaaa' or 'hahahaha'",Checks if a string contains any character repeated over four times consecutively.
601,_execute_code,"def _execute_code(
        self,
        code: str,
        starting_globals: Dict[str, Any] = {},
        fn_name: str = ""prepare_jobs"",
        **kwargs,
    ) -> Tuple[Any, str]:
        
        # Add MCP tools to the execution globals
        exec_globals = {
            **starting_globals,
            ""mcp_tools"": self.mcp_tool_executor,
        }

        print(""About to execute code..."")
        # Compile and execute the code
        try:
            compile(code, ""<string>"", ""exec"")
            print(""Code compiled successfully"")
            exec(code, exec_globals)
            print(""Code executed successfully"")

            if fn_name not in exec_globals:
                raise ValueError(f""Function {fn_name} not found in the code block."")

            print(f""About to call {fn_name}..."")
            function = exec_globals[fn_name]
            output = function(**kwargs)
            print(""Function call completed"")

            return output, code

        except Exception as e:
            print(f""Error executing code: {e}"")
            raise",Execute code with MCP tools available,Execute and invoke a specified function from dynamically compiled code with custom globals.
602,attach_tool_from_function,"def attach_tool_from_function(self, func: Callable):
        

        name = func.__name__
        # Use the docstring for the description
        description = func.__doc__ or f""Tool based on function {name}""

        # Get type hints
        type_hints = func.__annotations__
        if not type_hints:
            raise ValueError(""Function must have type hints"")
        if ""return"" not in type_hints:
            raise ValueError(""Function must have a return type hint"")

        # Create parameter schema
        parameters = {}
        for param_name, param_type in type_hints.items():
            if param_name != ""return"":
                parameters[param_name] = {
                    ""type"": self._python_type_to_schema_type(param_type),
                    ""description"": f""Parameter {param_name} of type {param_type.__name__}"",
                    ""required"": True,
                }

        # Write a lambda around the code and persist it (for inline_agents, this will have to be different)
        lambda_file = agents_helper.create_lambda_file(func)
        tool = Tool.create(
            name, code_file=lambda_file, schema=parameters, description=description
        )
        return self.attach_tool(tool)",Attach the supplied code to this agent as a Tool,Convert a function into a tool with schema and attach it to a system.
603,process_soft_deletes,"def process_soft_deletes(codebase):
    
    soft_delete_models = {
        ""User"",
        ""Update"",
        ""Proposal"",
        ""Comment"",
        ""Project"",
        ""Team"",
        ""SavedSession"",
    }
    join_methods = {""join"", ""outerjoin"", ""innerjoin""}

    for file in codebase.files:
        for call in file.function_calls:
            if not should_process_join_call(call, soft_delete_models, join_methods):
                continue

            model_name = str(list(call.args)[0].value)
            print(f""Found join method for model {model_name} in file {file.filepath}"")
            add_deleted_at_check(file, call, model_name)

    codebase.commit()
    print(""commit"")
    print(codebase.get_diff())",Process soft delete conditions for join methods in the codebase.,Enhance codebase by integrating soft delete checks into join operations.
604,setup_agent,"def setup_agent():
    
    llm, embedder = setup_llm_and_embedder()

    try:

        weaviate_connection = WeaviateConnection()
        weaviate_backend = WeaviateMemoryBackend(
            connection=weaviate_connection,
            embedder=embedder,
            collection_name=WEAVIATE_INDEX_NAME,
            create_if_not_exist=True,
        )
        print(f""Weaviate memory backend initialized for index '{WEAVIATE_INDEX_NAME}'."")

    except Exception as e:
        print(f""FATAL: Failed to initialize Weaviate backend: {e}"", file=sys.stderr)
        print(""Please ensure Weaviate is running and accessible."", file=sys.stderr)
        raise

    memory = Memory(backend=weaviate_backend, message_limit=MEMORY_MESSAGE_LIMIT)

    AGENT_ROLE = ""Helpful assistant that remembers previous conversations using Weaviate.""
    agent = SimpleAgent(
        name=""ChatAgentWeaviate"",
        llm=llm,
        role=AGENT_ROLE,
        id=""agent-weaviate"",
        memory=memory,
    )
    return agent",Sets up the SimpleAgent with Weaviate memory.,Initialize a conversational agent with memory using Weaviate for persistent context storage.
605,_error_response,"def _error_response(
        self,
        message: str,
        status_code: int = 500,
    ) -> JSONResponse:
        

        return JSONResponse(status_code=status_code, content={""error"": message})",Create a simple error response.,Generate an error response with a custom message and status code.
606,_fetch_data_from_source,"def _fetch_data_from_source(self, table_name):
        
        db_path = ""~/npcsh_history.db""
        try:
            df = pd.read_sql(f""SELECT * FROM {table_name}"", engine)
            return df.to_json(orient=""records"")
        except Exception as e:
            print(f""Error fetching data from {table_name}: {e}"")
            return ""[]""",Fetch data from a database table,"Retrieve and convert database table data to JSON format, handling errors."
607,format_secret_id,"def format_secret_id(service, customer_id, service_type=""ai_provider""):
        
        if os.environ.get(""isDevelopmentMode"") == ""enabled"":
            return None

        prefix = """"
        if service_type == ""integration"":
            prefix = ""integration-""

        return f""{prefix}{service}-api-key-{customer_id}""",Generate a standardized secret ID.,"Generate API key string based on service, customer, and environment."
608,get_traces,"def get_traces(
        self,
        service_name: str,
        start_time: datetime,
        end_time: datetime,
        limit: int = None,
    ) -> list:
        
        # Calculate the lookback in milliseconds.
        lookback = int((datetime.now() - start_time).total_seconds())

        url = f""{self.base_url}/api/traces?service={service_name}&lookback={lookback}s""
        if limit is not None:
            url += f""&limit={limit}""

        try:
            response = requests.get(url)
            response.raise_for_status()
            return response.json().get(""data"", [])
        except requests.RequestException as e:
            print(f""Failed to get traces for {service_name}: {e}"")
            return []",Fetch traces for a specific service between start_time and end_time.,"Fetch service traces within a time range, handling request errors."
609,get_skill_mapping,"def get_skill_mapping() -> Dict[str, Dict[str, Set[str]]]:
    
    mapping = {}
    all_real_skills = get_all_real_skills()

    # Build mapping from configuration
    for skill_name, keywords in SKILL_KEYWORD_CONFIG.items():
        if skill_name in all_real_skills:
            skill_states = all_real_skills[skill_name]

            # Special case for twitter tweet - use only post_tweet state
            if skill_name == ""twitter"":
                for keyword in keywords:
                    if keyword == ""tweet"":
                        mapping[keyword] = {
                            skill_name: {""post_tweet""}
                            if ""post_tweet"" in skill_states
                            else skill_states
                        }
                    else:
                        mapping[keyword] = {skill_name: skill_states}
            else:
                # Standard mapping for all keywords
                for keyword in keywords:
                    mapping[keyword] = {skill_name: skill_states}

    # Add direct skill name mappings for any skills not in config
    for skill_name, skill_states in all_real_skills.items():
        if skill_name not in SKILL_KEYWORD_CONFIG and skill_name not in mapping:
            mapping[skill_name] = {skill_name: skill_states}

    return mapping",Generate skill mapping dynamically from actual skill implementations.,Generate a skill-to-keyword mapping with special handling for Twitter skills.
610,_convert_initial_actions,"def _convert_initial_actions(self, actions: List[Dict[str, Dict[str, Any]]]) -> List[ActionModel]:
		
		converted_actions = []
		action_model = self.ActionModel
		for action_dict in actions:
			# Each action_dict should have a single key-value pair
			action_name = next(iter(action_dict))
			params = action_dict[action_name]

			# Get the parameter model for this action from registry
			action_info = self.controller.registry.registry.actions[action_name]
			param_model = action_info.param_model

			# Create validated parameters using the appropriate param model
			validated_params = param_model(**params)

			# Create ActionModel instance with the validated parameters
			action_model = self.ActionModel(**{action_name: validated_params})
			converted_actions.append(action_model)

		return converted_actions",Convert dictionary-based actions to ActionModel instances,Convert action dictionaries into validated action model instances.
611,load_custom_config,"def load_custom_config(config_path):
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f""Config file not found: {config_path}"")
    
    spec = importlib.util.spec_from_file_location(""custom_config"", config_path)
    custom_config = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(custom_config)
    return custom_config",Load custom configuration from a Python file,Load and execute a custom configuration file from a specified path.
612,call_judge_with_retry,"def call_judge_with_retry(client, model_name, prompt, temperature=0, max_tokens=256, max_retries=5, base_delay=1):
    
    retries = 0
    while True:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return completion
        except RateLimitError as e:
            retries += 1
            if retries > max_retries:
                raise Exception(f""Max retries ({max_retries}) exceeded for prompt. Error: {e}"")
            delay = base_delay * (2 ** (retries - 1))
            print(f""Rate limit hit (Attempt {retries}/{max_retries}). Retrying in {delay} seconds..."")
            time.sleep(delay)
        except Exception as e:
            raise Exception(f""An unexpected error occurred during API call: {e}"")",Calls the OpenAI API's chat completion endpoint with exponential backoff.,Retry API call for chat completion with exponential backoff on rate limit errors
613,_fetch_stock_data,"def _fetch_stock_data(self, symbol: str, start: str, end: str, timeframe: str = ""1d"") -> pd.DataFrame:
        
        with st.spinner(f""Fetching {symbol} {timeframe} data...""):
            try:
                stock = yf.Ticker(symbol)
                hist = stock.history(start=start, end=end, interval=timeframe)

                if hist.empty:
                    st.warning(f""No historical data found for {symbol}"")
                    return pd.DataFrame()

                return hist.reset_index()
            except Exception as e:
                raise ValueError(f""Data fetch failed: {str(e)}"")",Fetch and validate stock data with timeframe support,Fetch historical stock data for a given symbol and timeframe using Yahoo Finance.
614,extract_code_examples,"def extract_code_examples(cls, html_section: Optional[str]) -> List[Dict[str, str]]:
        
        if not html_section:
            return []

        soup = BeautifulSoup(html_section, 'html.parser')
        code_blocks = soup.find_all('pre')

        examples = []
        for block in code_blocks:
            # Make sure we're working with a Tag
            if not isinstance(block, Tag):
                continue

            # Try to determine the language
            language = 'typescript'  # Default
            classes = block.attrs.get('class', [])

            # Make sure classes is a list of strings
            if not isinstance(classes, list):
                classes = [str(classes)]

            class_str = ' '.join(classes)

            if 'python' in class_str.lower():
                language = 'python'
            elif 'javascript' in class_str.lower():
                language = 'javascript'

            # Get the code content
            code = block.get_text()
            examples.append({'language': language, 'code': code})

        return examples",Extract code examples from an HTML section.,Extracts and categorizes code snippets from HTML by programming language.
615,heart_input_schema,"def heart_input_schema():
    
    return create_model(
        ""HeartDiseaseInput"",
        **{
            ""age"": int,
            ""gender"": int,
            ""cp"": int,
            ""trtbps"": int,
            ""chol"": int,
            ""fbs"": int,
            ""restecg"": int,
            ""thalachh"": int,
            ""exng"": int,
            ""oldpeak"": float,
            ""slp"": int,
            ""caa"": int,
            ""thall"": int,
        },
    )",Define the input schema for heart disease prediction.,Define a data model for heart disease prediction input parameters.
616,_parse_assistant_message,"def _parse_assistant_message(self, message: Any) -> Union[str, AgentAction]:
        
        logger.debug(f""Assistant message content: {getattr(message, 'content', '')}"")
        logger.debug(f""Tool calls present: {bool(getattr(message, 'tool_calls', None))}"")

        if getattr(message, ""tool_calls"", None):
            tool_call = message.tool_calls[0]
            logger.info(""Function call detected"")
            logger.debug(f""Function name: {tool_call.function.name}"")
            logger.debug(f""Function arguments: {tool_call.function.arguments}"")

            try:
                arguments = json.loads(tool_call.function.arguments)
            except json.JSONDecodeError as e:
                logger.error(f""Failed to parse function arguments: {e}"")
                arguments = {""error"": ""Invalid JSON parameters""}

            return AgentAction(thought=message.content or """", action=tool_call.function.name, action_input=arguments)

        logger.info(""Returning text response"")
        return message.content or """"",Parse assistant message structure.,Parse assistant messages to determine if they contain tool calls or return text responses.
617,download_file_from_s3,"def download_file_from_s3(bucket_name, key, local_file_path):
    
    s3_client.download_file(bucket_name, key, local_file_path)
    print(f""Downloaded {key} to {local_file_path}"")",Download a single file from S3.,Downloads a file from an S3 bucket to a local path and logs the action.
618,get_model_loader,"def get_model_loader(load_config: LoadConfig) -> BaseModelLoader:
    

    if isinstance(load_config.load_format, type):
        return load_config.load_format(load_config)

    if load_config.load_format == LoadFormat.AUTO:
        update_megatron_weight_loader()
        return MegatronLoader(load_config)

    # NOTE(sgm): change the weight_loader function in runtime
    if load_config.load_format == LoadFormat.MEGATRON:
        update_megatron_weight_loader()
        return MegatronLoader(load_config)

    if load_config.load_format == LoadFormat.HF:
        update_hf_weight_loader()
        return HFLoader(load_config)

    if load_config.load_format == LoadFormat.DTENSOR:
        update_dtensor_weight_loader()
        return DTensorLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_HF:
        update_hf_weight_loader()
        return DummyModelLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_MEGATRON:
        update_megatron_weight_loader()
        return DummyModelLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_DTENSOR:
        update_dtensor_weight_loader()
        return DummyModelLoader(load_config)

    raise ValueError(""load format not supported in verl: {}, only support {} and {}"".format(load_config.load_format, LoadFormat.MEGATRON, LoadFormat.HF))",Get a model loader based on the load format.,Determine and return the appropriate model loader based on the specified configuration format.
619,list_groups,"def list_groups() -> list[str]:
    
    if not (bridge := _get_bridge()):
        return [""Error: Bridge not connected""]
    try:
        # phue2 get_group() returns a dict {id: {details}} including name
        groups = bridge.get_group()
        return [group_details[""name""] for group_details in groups.values()]
    except (PhueException, Exception) as e:
        return [f""Error listing groups: {e}""]",Lists the names of all available Hue light groups.,Retrieve and return group names from a connected bridge or error message.
620,lora_info,"def lora_info(args):
    
    if ""lora"" not in args.model_type:
        return """"
    if args.lora_type.lower() == ""standard"":
        return f
    if args.lora_type.lower() == ""lycoris"":
        lycoris_config_file = args.lycoris_config
        # read the json file
        with open(lycoris_config_file, ""r"") as file:
            lycoris_config = json.load(file)
        return f",Return a string with the LORA information.,Determine LoRA configuration based on model type and LoRA variant.
621,parse,"def parse(self, input: dict, response: Topics) -> dict:
        
        return [{""topic"": t} for t in response.topics_list]",Parse the model response along with the input to the model into the desired output format..,Transforms a list of topics into a dictionary format.
622,get_user_id,"def get_user_id(username):
    
    try:
        # Remove @ if present
        username = username.lstrip(""@"")

        # Look up user
        user = client.get_user(username=username)

        if user.data:
            return str(user.data.id)
        return None

    except Exception as e:
        print(f""Error getting user ID for {username}: {str(e)}"")
        return None",Get Twitter user ID from username,"Retrieve user ID from username, handling exceptions and formatting."
623,set_dimension,"def set_dimension(name: str, value: Any) -> None:
        
        logger = logging.getLogger(""cua.computer.telemetry"")
        logger.debug(f""Setting dimension {name}={value}"")",Set a dimension that will be attached to all events.,Log a debug message for setting a telemetry dimension with a specified name and value.
624,print_user_message,"def print_user_message(self, message: str) -> None:
        
        try:
            # Use Text object to prevent markup issues
            message_text = Text(message or ""[No Message]"")
            print(Panel(message_text, style=""bold yellow"", title=""You""))
            self.tool_calls.clear()
            if not self.verbose_mode:
                self.live_display = None
        except Exception as exc:
            log.error(f""Error printing user message: {exc}"")
            # Fallback to plain text
            print(""\n[yellow]You:[/yellow]"")
            print(message or ""[No Message]"")",Display user message with error handling.,"Display user message with styled panel, handle errors gracefully"
625,process_interactive_command,"def process_interactive_command(
    command: str,
    agent,
    console: Console
) -> bool:
    
    if command == ""/clear"":
        logger.info(""Clearing agent memory..."")
        console.print(""[yellow]Clearing agent memory...[/yellow]"")
        agent.clear_memory()
        console.print(""[green]Memory cleared successfully![/green]"")
        return True
    else:
        console.print(f""[red]Unknown command: {command}[/red]"")
        return True",Process interactive commands and return whether to continue.,Handle interactive commands to clear agent memory or report unknown commands.
626,_run_agent_loop,"def _run_agent_loop(self):
        
        try:
            log_once = False
            while not self._stop_event.is_set():
                if self.cli.agent:
                    try:
                        if not log_once:
                            logger.info(""Loop logic not implemented"")
                            log_once = True

                    except Exception as e:
                        logger.error(f""Error in agent action: {e}"")
                        if self._stop_event.wait(timeout=30):
                            break
        except Exception as e:
            logger.error(f""Error in agent loop thread: {e}"")
        finally:
            self.agent_running = False
            logger.info(""Agent loop stopped"")",Run agent loop in a separate thread,"Continuously execute agent actions until stopped, logging errors and status updates."
627,print_cicd_summary,"def print_cicd_summary(
    config: ProjectConfig, github_username: str, repo_url: str, cloud_build_url: str
) -> None:
    
    console.print(""\n🎉 CI/CD Infrastructure Setup Complete!"", style=""bold green"")
    console.print(""===================================="")
    console.print(""\n📊 Resource Summary:"")
    console.print(f""   • Development Project: {config.dev_project_id}"")
    console.print(f""   • Staging Project: {config.staging_project_id}"")
    console.print(f""   • Production Project: {config.prod_project_id}"")
    console.print(f""   • CICD Project: {config.cicd_project_id}"")
    console.print(f""   • Repository: {config.repository_name}"")
    console.print(f""   • Region: {config.region}"")

    console.print(""\n🔗 Important Links:"")
    console.print(f""   • GitHub Repository: {repo_url}"")
    console.print(f""   • Cloud Build Console: {cloud_build_url}"")

    console.print(""\n📝 Next Steps:"", style=""bold blue"")
    console.print(""1. Push your code to the repository"")
    console.print(""2. Create and merge a pull request to trigger CI/CD pipelines"")
    console.print(""3. Monitor builds in the Cloud Build console"")
    console.print(
        ""4. After successful staging deployment, approve production deployment in Cloud Build""
    )
    console.print(
        ""\n🌟 Enjoy building your new Agent! Happy coding! 🚀"", style=""bold green""
    )",Print a summary of the CI/CD setup.,Display CI/CD setup summary with project details and next steps.
628,_process_resources,"def _process_resources(self) -> None:
        
        for component in self.components[ComponentType.RESOURCE]:
            if not component.uri_template:
                console.print(
                    f""[yellow]Warning: Resource {component.name} has no URI template[/yellow]""
                )
                continue

            resource_schema = {
                ""uri"": component.uri_template,
                ""name"": component.name,
                ""description"": component.docstring or """",
                ""entry_function"": component.entry_function,
            }

            # Add the resource to the manifest
            self.manifest[""resources""].append(resource_schema)",Process all resource components and add them to the manifest.,Iterate components to validate and append resource metadata to manifest
629,verify_and_fix_categories,"def verify_and_fix_categories(self):
        
        self.bagels_cur.execute(
            ,
            (self.default_category_id,),
        )

        null_categories = self.bagels_cur.execute().fetchone()[0]

        if null_categories > 0:
            raise Exception(
                f""Found {null_categories} records with NULL categories after fix""
            )",Verify all records have valid categories and fix any issues,"Ensure database records have valid categories, raising an error if any remain null."
630,get_node_id,"def get_node_id(node_type: str, name: str, currency_id: int) -> str:
        
        return f""{node_type}_{name}_{currency_id}"".lower().replace("" "", ""_"")",Generate unique node ID including currency information.,"Generate a unique identifier by combining node type, name, and currency ID."
631,_format_quantifier_statement,"def _format_quantifier_statement(self, quantifier: Quantifier, subject: Term, predicate: Term) -> str:
        
        if quantifier == Quantifier.SOME_NOT:
            return f""Some {subject.plural} are not {predicate.plural}""
        else:
            return f""{quantifier.value} {subject.plural} are {predicate.plural}""",Format a quantified statement in natural language,"Format logical statements based on quantifier, subject, and predicate terms."
632,pred_to_json,"def pred_to_json(self, predn, filename):
        
        stem = Path(filename).stem
        image_id = int(stem) if stem.isnumeric() else stem
        rbox = torch.cat([predn[:, :4], predn[:, -1:]], dim=-1)
        poly = ops.xywhr2xyxyxyxy(rbox).view(-1, 8)
        for i, (r, b) in enumerate(zip(rbox.tolist(), poly.tolist())):
            self.jdict.append(
                {
                    ""image_id"": image_id,
                    ""category_id"": self.class_map[int(predn[i, 5].item())],
                    ""score"": round(predn[i, 4].item(), 5),
                    ""rbox"": [round(x, 3) for x in r],
                    ""poly"": [round(x, 3) for x in b],
                }
            )",Serialize YOLO predictions to COCO json format.,Convert prediction data to JSON format with image and category details.
633,print_results,"def print_results(self, results: List[EmbeddingMetrics]) -> None:
        
        print(""\n=== Embedding Model Benchmark Results ===\n"")

        # Sort by recall@5 (accuracy)
        sorted_results = sorted(results, key=lambda x: x.recall_at_5, reverse=True)

        print(
            f""{'Model':<25} {'Recall@5':<10} {'MRR':<8} {'Index Time':<12} {'Query Time':<12} {'Cost/1k':<10}""
        )
        print(""-"" * 85)

        for result in sorted_results:
            print(
                f""{result.model_name:<25} ""
                f""{result.recall_at_5:<10.3f} ""
                f""{result.mrr:<8.3f} ""
                f""{result.indexing_time:<12.3f} ""
                f""{result.avg_inference_time:<12.4f} ""
                f""${result.cost_per_1k_tokens:<9.4f}""
            )

        print()

        # Show recommendations
        selector = EmbeddingModelSelector(results)

        print(""=== Recommendations ==="")
        print(f""Best for accuracy: {selector.select_model(prioritize_accuracy=True)}"")
        print(f""Best for speed: {selector.select_model(prioritize_speed=True)}"")
        print(f""Best for cost: {selector.select_model(prioritize_cost=True)}"")
        print(f""Best free option: {selector.select_model(max_cost_per_1k=0.0)}"")
        print()",Print benchmark results in a readable format.,"Display and recommend embedding models based on benchmark metrics like accuracy, speed, and cost."
634,install_ollama,"def install_ollama():
    
    if platform.system() not in [""Darwin"", ""Linux""]:
        print(""❌ This script only supports macOS and Linux"")
        return False

    print(""🦙 Ollama not found. Installing Ollama..."")
    response = input(""   Install Ollama now? (y/n): "").lower().strip()

    if response != ""y"":
        print(""❌ Ollama installation cancelled"")
        return False

    print(""📥 Downloading and installing Ollama..."")

    if success:
        print(""✅ Ollama installed successfully!"")
        return True
    else:
        print(f""❌ Ollama installation failed: {stderr}"")
        return False",Install Ollama with user permission.,Install Ollama on macOS/Linux with user confirmation and error handling.
635,sample_for_query,"def sample_for_query(qid, ranking, npositives, depth_positive, depth_negative, cutoff_negative):
    
    assert npositives <= depth_positive < cutoff_negative < depth_negative

    positives, negatives, triples = [], [], []

    for pid, rank, *_ in ranking:
        assert rank >= 1, f""ranks should start at 1 \t\t got rank = {rank}""

        if rank > depth_negative:
            break

        if rank <= depth_positive:
            positives.append(pid)
        elif rank > cutoff_negative:
            negatives.append(pid)

    num_sampled = 100

    for neg in sample_negatives(negatives, num_sampled):
        positives_ = random.sample(positives, npositives)
        positives_ = positives_[0] if npositives == 1 else positives_
        triples.append((qid, positives_, neg))

    return triples",Requires that the ranks are sorted per qid.,Generate query-based positive-negative sample triples from ranked data.
636,mock_cosmos_client,"def mock_cosmos_client():
    
    mock_client = AsyncMock()
    mock_container = AsyncMock()
    mock_client.create_container_if_not_exists.return_value = mock_container

    # Mocking context methods
    mock_context = AsyncMock()
    mock_context.store_message = AsyncMock()
    mock_context.retrieve_messages = AsyncMock(
        return_value=async_iterable([{""id"": ""test_id"", ""content"": ""test_content""}])
    )

    return mock_client, mock_container, mock_context",Fixture for mocking Cosmos DB client and container.,Simulates a Cosmos client with mocked container and context methods for testing.
637,validate_instantiation,"def validate_instantiation(component: Dict[str, Any]) -> Optional[ValidationError]:
        
        try:
            model = ComponentModel(**component)
            # Attempt to load the component
            module_path, class_name = model.provider.rsplit(""."", maxsplit=1)
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)
            component_class.load_component(model)
            return None
        except Exception as e:
            return ValidationError(
                field=""instantiation"",
                error=f""Failed to instantiate component: {str(e)}"",
                suggestion=""Check that the component can be properly instantiated with the given config"",
            )",Validate that the component can be instantiated,Validate component instantiation by attempting to load and handle errors with suggestions.
638,_add_eos_if_not_present,"def _add_eos_if_not_present(self, token_ids: List[int]) -> List[int]:
        
        if len(token_ids) > 0 and token_ids[-1] == self.eos_token_id:
            warnings.warn(
                f""This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated""
                "" eos tokens being added.""
            )
            return token_ids
        else:
            return token_ids + [self.eos_token_id]",Do not add eos again if user already added it.,Ensure end-of-sequence token is appended to token list if absent.
639,convert,"def convert(
        self, value: Any, param: click.Parameter | None, ctx: click.Context | None
    ) -> GitHash | None:
        
        if value is None:
            return None

        if not (8 <= len(value) <= 40):
            self.fail(f'Git hash must be between 8 and 40 characters, got {len(value)}')

        if not re.match(r'^[0-9a-fA-F]+$', value):
            self.fail('Git hash must contain only hex digits (0-9, a-f)')

        try:
            # Verify hash exists in repo
            command = ['git', 'rev-parse', '--verify', value]
            # The `value` is verified to match a GitHash above
            subprocess.run(command, check=True, shell=False, capture_output=True)  # nosec B603
        except subprocess.CalledProcessError:
            self.fail(f'Git hash {value} not found in repository')

        return GitHash(value.lower())",Convert the value to a GitHash.,Validate and verify a Git hash string within a specified repository context.
640,add_node,"def add_node(node_id: str, display_name: str) -> None:
        
        nodes[node_id] = {
            ""id"": node_id,
            ""name"": display_name,
            ""priority"": get_node_priority(node_id),
        }","Add node with ID, display name and priority.","Add a node with ID, name, and priority to the nodes dictionary."
641,_make_chat_request,"def _make_chat_request(self, messages, temperature=0.0, max_tokens=None):
        
        payload = {
            ""model"": self.model_name, 
            ""messages"": messages, 
            ""temperature"": temperature
        }
        if max_tokens:
            payload[""max_tokens""] = max_tokens
        
        try:
            resp = requests.post(f""{self.base_url}/engines/llama.cpp/v1/chat/completions"", json=payload, timeout=30)
            if resp.ok:
                return resp.json()
            else:
                resp.raise_for_status()
        except requests.exceptions.ConnectionError:
            raise RuntimeError(f""Cannot connect to Docker Model Runner at {self.base_url}. Is Docker Desktop running with Model Runner enabled?"")",Make a chat request to the Docker Model Runner API,Send chat request to model API with configurable parameters and handle connection errors.
642,create_user,"def create_user(user: UserInput, flag: bool) -> dict:
            
            return {""id"": 1, **user.model_dump()}",Create a new user.,Create a user dictionary with an ID and user input data.
643,do_translate,"def do_translate(self, text) -> str:
        
        translation_options = {
            ""source_lang"": self.lang_mapping(self.lang_in),
            ""target_lang"": self.lang_mapping(self.lang_out),
            ""domains"": self.envs[""ALI_DOMAINS""],
        }
        response = self.client.chat.completions.create(
            model=self.model,
            **self.options,
            messages=[{""role"": ""user"", ""content"": text}],
            extra_body={""translation_options"": translation_options},
        )
        return response.choices[0].message.content.strip()",Qwen-MT Model reqeust to send translation_options to the server.,Translate text using specified language and domain settings via a chat model.
644,build_keyspace_details_context,"def build_keyspace_details_context(keyspace_details: Dict[str, Any]) -> str:
    
    context = {
        'cassandra_knowledge': build_cassandra_knowledge(),
        'amazon_keyspaces_knowledge': build_amazon_keyspaces_knowledge(),
    }

    # Add keyspace-specific guidance
    keyspace_guidance = {
        'replication_strategy': 'Replication strategy determines how data is distributed across nodes. '
        'Amazon Keyspaces manages replication automatically for high availability.',
        'durable_writes': 'Durable writes ensure data is written to the commit log before acknowledging the write. '
        'This provides durability in case of node failures.',
    }
    context['keyspace_guidance'] = keyspace_guidance

    return dict_to_markdown(context)",Provide LLM context for keyspace details.,Generate a markdown summary of keyspace details with guidance and knowledge context.
645,add_observer,"def add_observer(self, observer: Callable, event_types: List[str]) -> 'Agent':
        
        try:
            self._observers.append((observer, event_types))
            self.react_agent.add_observer(observer, event_types)
            return self
        except Exception as e:
            logger.error(f""Failed to add observer: {e}"")
            raise",Add an observer to both the Agent and its internal react_agent.,"Attach observer to agent for specified event types, handling errors."
646,click,"def click(element: MacElementNode, action: str) -> bool:
	
	if not element._element:
		logger.error(f'❌ Cannot click: Element reference is missing for {element}')
		return False

	# Check if element is enabled
	if not element.enabled:
		logger.error(f'❌ Cannot click: Element is disabled: {element}')
		return False

	# Verify element has the press/click action
	actions = ['AXPress', 'AXClick', 'AXOpen', ""AXConfirm"", ""AXShowMenu""]
	if action not in actions:
		logger.error(f'❌ Cannot click: Element does not support {action} action: {element}')
		return False

	return perform_action(element, action)",Simulates a click on a Mac UI element.,Validate and execute a click action on a UI element if conditions are met.
647,_split_chapters,"def _split_chapters(self, content: str) -> List[Dict[str, str]]:
        
        chapters = []
        current_chapter = []
        current_title = ""Untitled Chapter""

        for line in content.split('\n'):
            if line.strip() == '---':
                if current_chapter:
                    chapters.append({
                        'title': current_title,
                        'content': '\n'.join(current_chapter)
                    })
                    current_chapter = []
                    current_title = ""Untitled Chapter""
            else:
                if line.startswith('# '):
                    current_title = line[2:].strip()
                current_chapter.append(line)

        if current_chapter:
            chapters.append({
                'title': current_title,
                'content': '\n'.join(current_chapter)
            })

        return chapters",Split markdown content into chapters.,Split text into chapters by titles and separators
648,load_state_dict,"def load_state_dict(self, state):
        
        if ""last_epoch"" in state:
            self.last_epoch = state[""last_epoch""]
            print(""Load last_epoch"")

        for k, v in self.__dict__.items():
            if hasattr(v, ""load_state_dict"") and k in state:
                v = dist_utils.de_parallel(v)
                v.load_state_dict(state[k])
                print(f""Load {k}.state_dict"")

            if hasattr(v, ""load_state_dict"") and k not in state:
                if k == ""ema"":
                    model = getattr(self, ""model"", None)
                    if model is not None:
                        ema = dist_utils.de_parallel(v)
                        model_state_dict = remove_module_prefix(model.state_dict())
                        ema.load_state_dict({""module"": model_state_dict})
                        print(f""Load {k}.state_dict from model.state_dict"")
                else:
                    print(f""Not load {k}.state_dict"")","Load state dict, train/eval","Load model components' states from a given state dictionary, handling special cases."
649,_log_history_lines,"def _log_history_lines(self) -> str:
		
		try:
			total_input_tokens = 0
			message_lines = []
			terminal_width = shutil.get_terminal_size((80, 20)).columns

			for i, m in enumerate(self.state.history.messages):
				try:
					total_input_tokens += m.metadata.tokens
					is_last_message = i == len(self.state.history.messages) - 1

					# Extract content for logging
					content = _log_extract_message_content(m.message, is_last_message, m.metadata)

					# Format the message line(s)
					lines = _log_format_message_line(m, content, is_last_message, terminal_width)
					message_lines.extend(lines)
				except Exception as e:
					logger.warning(f'Failed to format message {i} for logging: {e}')
					# Add a fallback line for this message
					message_lines.append('❓[   ?]: [Error formatting this message]')

			# Build final log message
			return (
				f'📜 LLM Message history ({len(self.state.history.messages)} messages, {total_input_tokens} tokens):\n'
				+ '\n'.join(message_lines)
			)
		except Exception as e:
			logger.warning(f'Failed to generate history log: {e}')
			# Return a minimal fallback message
			return f'📜 LLM Message history (error generating log: {e})'",Generate a formatted log string of message history for debugging / printing to terminal,Generate a formatted log of message history with token count and error handling.
650,_maybe_extract_session_id_from_response,"def _maybe_extract_session_id_from_response(
        self,
        response: httpx.Response,
    ) -> None:
        
        new_session_id = response.headers.get(MCP_SESSION_ID)
        if new_session_id:
            self.session_id = new_session_id
            logger.info(f""Received session ID: {self.session_id}"")",Extract and store session ID from response headers.,Extracts and logs session ID from HTTP response headers if available.
651,query_spg_type,"def query_spg_type(self, spg_type_name: str) -> BaseSpgType:
        
        rest_model = self._rest_client.schema_query_spg_type_get(spg_type_name)
        type_class = BaseSpgType.by_type_enum(f""{rest_model.spg_type_enum}"")

        if rest_model.spg_type_enum == SpgTypeEnum.Concept:
            return type_class(
                name=spg_type_name,
                hypernym_predicate=rest_model.concept_layer_config.hypernym_predicate,
                rest_model=rest_model,
            )
        else:
            return type_class(name=spg_type_name, rest_model=rest_model)",Query SPG type by name.,Retrieve and instantiate a specific SPG type based on its name and classification
652,_log_chat_progress,"def _log_chat_progress(
        self, chat_turn: Optional[int] = None, model: Optional[str] = None
    ) -> None:
        
        # Determine action type based on verb
        if hasattr(self, ""verb"") and self.verb:
            # Use verb directly regardless of type
            act = self.verb
        else:
            act = ProgressAction.CHATTING

        data = {
            ""progress_action"": act,
            ""model"": model,
            ""agent_name"": self.name,
            ""chat_turn"": chat_turn if chat_turn is not None else None,
        }
        self.logger.debug(""Chat in progress"", data=data)",Log a chat progress event,"Log chat progress with action type, model, and agent details."
653,_handle_part_finish,"def _handle_part_finish(self, **kwargs):
        
        if kwargs[""type""] == ""error"":
            logger.info(f""progress_monitor handle_part_finish: {kwargs['error']}"")
            self.finish_callback(type=""error"", error=kwargs[""error""])
            return
        if ""translate_result"" in kwargs:
            part_index = kwargs.get(""part_index"")
            if part_index is not None:
                self.part_results[part_index] = kwargs[""translate_result""]",Handle completion of a part translation,"Handle completion of a task part, logging errors and storing results."
654,store_task_info,"def store_task_info(task_id, task_info):
    
    try:
        redis_client.set(f""task:{task_id}:info"", json.dumps(task_info))
        redis_client.expire(f""task:{task_id}:info"", 60 * 60 * 24 * 7)  # 7 days
    except Exception as e:
        logger.error(f""Error storing task info: {e}"")",Store task information in Redis,Store task details in Redis with a 7-day expiration and log errors.
655,_save_images,"def _save_images(self, images):
        
        output_dir = os.path.dirname(self.params[""output_path""])
        os.makedirs(output_dir, exist_ok=True)

        for i, image in enumerate(images):
            file_name = f""image_{i+1}_{int(time.time())}.png""
            file_path = os.path.join(output_dir, file_name)
            image.save(file_path)
            print(f""\nImage {i+1} saved to: {os.path.abspath(file_path)}"")",Save the generated images to the specified output path.,Save images to a specified directory with unique filenames.
656,delete_collection,"def delete_collection(self) -> bool:
        
        try:
            if self.exists:
                self.client.delete_collection(self.collection_name)
                self.collection = None
                self.exists = False
                return True
            else:
                print(f""No collection {self.collection_name} to delete."")
                return False
        except Exception as e:
            print(f""Error deleting collection {self.collection_name}:"")
            traceback.print_exc()
            return False",Delete the current collection from ChromaDB.,"Safely delete a collection if it exists, handling errors."
657,iris_input_schema,"def iris_input_schema():
    
    return create_model(
        ""IrisInput"",
        **{
            ""sepal_length"": float,
            ""sepal_width"": float,
            ""petal_length"": float,
            ""petal_width"": float,
        },
    )",Define the input schema for iris classification.,Define a data model schema for Iris flower input attributes.
658,setup_routes,"def setup_routes(self, app):
        

        @app.get(""/"", include_in_schema=False)
        async def root():
            
            return JSONResponse(
                content={""message"": ""Welcome to NyaProxy!""},
                status_code=200,
            )

        # Info endpoint
        @app.get(""/info"")
        async def info():
            
            apis = {}
            if self.config:
                for name, config in self.config.get_apis().items():
                    apis[name] = {
                        ""name"": config.get(""name"", name),
                        ""endpoint"": config.get(""endpoint"", """"),
                        ""aliases"": config.get(""aliases"", []),
                    }

            return {""status"": ""running"", ""version"": __version__, ""apis"": apis}",Set up FastAPI routes,"Defines HTTP routes for a web app, including a welcome and info endpoint."
659,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DOM.disable"",
    }
    json = yield cmd_dict",Disables DOM agent for the given page.,Disables the DOM feature by sending a command dictionary.
660,_handle_tool_result,"def _handle_tool_result(self, result, tool_id):
        
        if result.error:
            logger.error(f""Tool {tool_id} error: {result.error}"")
        else:
            logger.info(f""Tool {tool_id} output: {result.output}"")",Handle tool execution results.,"Log tool execution results, distinguishing errors from successful outputs."
661,get_mouse_position,"def get_mouse_position(self) -> Coordinate:
        
        script = 
        self.driver.execute_script(script)

        # Small mouse movement to trigger event
        actions = ActionChains(self.driver)
        actions.move_by_offset(3, 3).perform()
        actions.move_by_offset(-3, -3).perform()

        # Give time for event to register
        time.sleep(0.1)

        x, y = self.driver.execute_script(
            ""return [window.last_mouse_x, window.last_mouse_y]""
        )

        if isinstance(x, (int, float)) and isinstance(y, (int, float)):
            return Coordinate(x=int(x), y=int(y))

        return Coordinate(x=0, y=0)",Get current mouse cursor position.,Retrieve and return the current mouse position on a webpage.
662,search_things_to_note_in_prompt,"def search_things_to_note_in_prompt(sentence):
    
    with open(_4_1_TERMINOLOGY, 'r', encoding='utf-8') as file:
        things_to_note = json.load(file)
    things_to_note_list = [term['src'] for term in things_to_note['terms'] if term['src'].lower() in sentence.lower()]
    if things_to_note_list:
        prompt = '\n'.join(
            f'{i+1}. ""{term[""src""]}"": ""{term[""tgt""]}"",'
            f' meaning: {term[""note""]}'
            for i, term in enumerate(things_to_note['terms'])
            if term['src'] in things_to_note_list
        )
        return prompt
    else:
        return None",Search for terms to note in the given sentence,Extracts and formats relevant terminology notes from a sentence using a predefined list.
663,get_new_agent_method,"def get_new_agent_method(self, agent: AgentConfig) -> str:
        
        assert agent.provider in PROVIDERS.keys()  # this gets validated in `add_agent`
        llm_class_name = PROVIDERS[agent.provider].class_name
        return f",Get the content of a new agent method.,Retrieve agent method name based on provider configuration
664,_sample_exploration_parent,"def _sample_exploration_parent(self) -> Program:
        
        current_island_programs = self.islands[self.current_island]

        if not current_island_programs:
            # If current island is empty, initialize with best program or random program
            if self.best_program_id and self.best_program_id in self.programs:
                # Clone best program to current island
                best_program = self.programs[self.best_program_id]
                self.islands[self.current_island].add(self.best_program_id)
                best_program.metadata[""island""] = self.current_island
                logger.debug(f""Initialized empty island {self.current_island} with best program"")
                return best_program
            else:
                # Use any available program
                return next(iter(self.programs.values()))

        # Sample from current island
        parent_id = random.choice(list(current_island_programs))
        return self.programs[parent_id]",Sample a parent for exploration (from current island),Selects or initializes a program for exploration based on island status
665,to_env,"def to_env(self) -> dict[str, Any]:
        
        return {
            ""SECURITY"": json.dumps(self.dump()),
        }",Convert the security parameters to a dictionary.,Convert object data to a JSON string for security settings.
666,workspace_text_document_content,"def workspace_text_document_content(server: CodegenLanguageServer, params: types.TextDocumentContentParams) -> types.TextDocumentContentResult:
    
    logger.debug(f""Workspace text document content: {params.uri}"")
    path = get_path(params.uri)
    if not server.io.file_exists(path):
        logger.warning(f""File does not exist: {path}"")
        return types.TextDocumentContentResult(
            text="""",
        )
    content = server.io.read_text(path)
    return types.TextDocumentContentResult(
        text=content,
    )",Handle workspace text document content notification.,Fetches and returns the content of a text document from a server if it exists.
667,extract_links,"def extract_links(self, search_result: str) -> List[str]:
        
        matches = re.findall(pattern, search_result)
        trailing_punct = "".,!?;:)""
        cleaned_links = [link.rstrip(trailing_punct) for link in matches]
        self.logger.info(f""Extracted links: {cleaned_links}"")
        return self.clean_links(cleaned_links)",Extract all links from a sentence.,"Extract and clean URLs from search results, logging the output."
668,_get_example_value,"def _get_example_value(self, param_type: str) -> Any:
        
        type_examples = {
            ""string"": ""example_value"",
            ""integer"": 42,
            ""number"": 3.14,
            ""boolean"": True,
            ""array"": [],
            ""object"": {},
        }
        return type_examples.get(param_type, ""example_value"")",Generate a sensible example value based on parameter type.,Retrieve default example based on parameter type.
669,_get_component_summary,"def _get_component_summary(self, analysis: ProjectAnalysis) -> str:
        
        summary = []
        if analysis and analysis.frontend:
            summary.append('- Frontend')
            if isinstance(analysis.frontend, dict) and analysis.frontend.get('framework'):
                summary[-1] += f' ({analysis.frontend[""framework""]})'

        if analysis and analysis.backend:
            summary.append('- Backend')
            if isinstance(analysis.backend, dict):
                if analysis.backend.get('framework'):
                    summary[-1] += f' ({analysis.backend[""framework""]})'
                if analysis.backend.get('database'):
                    db_value = analysis.backend['database']
                    if isinstance(db_value, dict) and db_value.get('type'):
                        summary.append(f'  - Database: {db_value.get(""type"")}')
                    else:
                        # Handle case where database is a string instead of dict
                        summary.append(f'  - Database: {db_value}')

        if analysis and analysis.apis:
            summary.append('- API Layer')
            if isinstance(analysis.apis, dict):
                api_type = analysis.apis.get('type', 'Unknown')
                summary[-1] += f' ({api_type})'

        return '\n'.join(summary)",Get a text summary of components for placeholder text.,"Generate a summary of project components including frontend, backend, and API details."
670,_load_faiss_index,"def _load_faiss_index(self):
        
        if not os.path.exists(self.index_path):
            raise FileNotFoundError(f""FAISS index file not found at {self.index_path}"")
        print(f""Loading FAISS index from {self.index_path}..."")
        return faiss.read_index(self.index_path)",Load the FAISS HNSW index from disk.,"Load FAISS index from file path, raising error if not found."
671,run_brain_tumor_agent,"def run_brain_tumor_agent(state: AgentState) -> AgentState:
        

        print(f""Selected agent: BRAIN_TUMOR_AGENT"")

        response = AIMessage(content=""This would be handled by the brain tumor agent, analyzing the MRI image."")

        return {
            **state,
            ""output"": response,
            ""needs_human_validation"": True,  # Medical diagnosis always needs validation
            ""agent_name"": ""BRAIN_TUMOR_AGENT""
        }",Handle brain MRI image analysis.,The code defines a function for an AI agent to analyze MRI images for brain tumors.
672,cancel_artist_download,"def cancel_artist_download():
    
    return Response(
        json.dumps({""error"": ""Artist download cancellation is not supported.""}),
        status=400,
        mimetype=""application/json"",
    )",Cancelling an artist download is not supported since the endpoint only enqueues album tasks.,Returns a JSON error response for unsupported artist download cancellation.
673,enforce,"def enforce(
        self,
        obj: Any,
        partial: bool = False,
        strip_extras: bool = False,
        strip_opt_none_and_empty_strings: bool = False,
        files_as_strings: bool = False,
    ) -> None:
        

        if partial:
            if self._optional_json_schema is None:
                self._optional_json_schema = make_optional(self.json_schema)
            schema = self._optional_json_schema
        else:
            schema = self.json_schema
        if files_as_strings:
            schema = self._add_files_as_strings(schema)

        navigators: list[JsonSchema.Navigator] = []
        if strip_opt_none_and_empty_strings:
            navigators.append(remove_optional_nulls_and_empty_strings)
        if strip_extras:
            navigators.append(remove_extra_keys)

        if navigators:
            JsonSchema(schema).navigate(obj, navigators=navigators)

        try:
            validate(obj, schema)
        except SchemaValidationError as e:
            kp = ""."".join([str(p) for p in e.path])
            raise JSONSchemaValidationError(f""at [{kp}], {e.message}"")",Enforce validates that an object matches the schema.,Validate and optionally modify an object against a JSON schema with error handling.
674,discover_bootcamps,"def discover_bootcamps(self) -> None:
        
        if self._discovered:
            return

        try:
            # Import the internbootcamp.bootcamp module
            bootcamp_module = importlib.import_module(""internbootcamp.bootcamp"")

            # Get all attributes from the module
            for name in dir(bootcamp_module):
                if name.endswith(""bootcamp"") and not name.startswith(""_""):
                    try:
                        obj = getattr(bootcamp_module, name)
                        # Check if it's a class and has the required methods
                        if (
                            inspect.isclass(obj)
                            and hasattr(obj, ""case_generator"")
                            and hasattr(obj, ""prompt_func"")
                            and hasattr(obj, ""verify_score"")
                        ):
                            self._registry[name] = obj
                            logger.debug(f""Registered bootcamp: {name}"")
                    except Exception as e:
                        logger.warning(f""Failed to register {name}: {e}"")

            self._discovered = True
            logger.info(f""Discovered {len(self._registry)} bootcamp tasks"")

        except ImportError as e:
            logger.error(f""Failed to import internbootcamp.bootcamp: {e}"")
            raise",Dynamically discover all available bootcamp classes from InternBootcamp.,Automatically discover and register valid bootcamp classes from a specified module.
675,check_format,"def check_format(action: str, templates: Any) -> bool:
    
    if ""None"" in action:
        return False
    
    # Skip validation for basic actions that don't have placeholders
    basic_actions = [""look"", ""inventory""]
    if action in basic_actions:
        return True
        
    # Check if the action follows any of our templates
    for template in templates:
        # Skip ""None"" and basic actions we already checked
        if template == ""None"" or template in basic_actions:
            continue
            
        # Convert template to regex pattern
        # Replace <something> with regex that matches any word(s)
        pattern = template.replace(""<receptacle>"", ""([\\w\\s]+)"") \
                        .replace(""<object>"", ""([\\w\\s]+)"") \
                        .replace(""<something>"", ""([\\w\\s]+)"")
        pattern = f""^{pattern}$""  # Match the entire string
        
        if re.match(pattern, action):
            return True
            
    return False",Validate that the action matches one of our action templates.,Validate action string against predefined templates using regex patterns.
676,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        result = {
            ""id"": self.id,
        }

        if self.type:
            result[""type""] = self.type.to_simplified_dict()

        if self.inward_issue:
            result[""inward_issue""] = self.inward_issue.to_simplified_dict()

        if self.outward_issue:
            result[""outward_issue""] = self.outward_issue.to_simplified_dict()

        return result",Convert to simplified dictionary for API response.,Convert object attributes to a simplified dictionary format.
677,setup_distributed_training,"def setup_distributed_training():
    
    try:
        # Initialize process group for distributed training
        local_rank = int(os.environ.get(""LOCAL_RANK"", ""0""))
        world_size = int(os.environ.get(""WORLD_SIZE"", ""1""))
        
        if world_size > 1:
            # Multi-GPU setup
            torch.cuda.set_device(local_rank)
            if not torch.distributed.is_initialized():
                torch.distributed.init_process_group(backend=""nccl"")
            logger.info(f""Distributed training initialized with world size: {world_size}, local rank: {local_rank}"")
        else:
            # Single GPU setup
            logger.info(f""Running on a single GPU (device {local_rank})"")
            torch.cuda.set_device(local_rank)
        
        return local_rank
    except Exception as e:
        logger.error(f""Failed to setup distributed training: {e}"")
        raise",Setup distributed training environment.,Initialize and configure distributed training for multi-GPU environments.
678,windows_exception_handler,"def windows_exception_handler(func):
    
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except PermissionError as e:
            logger = logging.getLogger(""WindowsStartup"")
            logger.error(f""Windows permission error in {func.__name__}: {e}"")
            logger.error(traceback.format_exc())
            
            # Try to provide helpful error message for common Windows permission issues
            if ""access is denied"" in str(e).lower():
                logger.error(""This appears to be a Windows permission issue. Try running as Administrator or check folder permissions."")
            
            return {""error"": f""Windows permission error: {str(e)}""}, 500
        except Exception as e:
            logger = logging.getLogger(""WindowsStartup"")
            logger.error(f""Error in {func.__name__}: {e}"")
            logger.error(traceback.format_exc())
            return {""error"": str(e)}, 500
    
    return wrapper",Decorator to catch and log Windows-specific exceptions,Decorator handles Windows-specific permission errors and logs exceptions for wrapped functions.
679,resolve_reference,"def resolve_reference(
        self, ref: Union[Reference_3_1_0, Reference_3_0]
    ) -> Union[Parameter_3_1_0, Parameter_3_0, Schema_3_1_0, Schema_3_0]:
        
        parts = ref.ref.split(""/"")
        if parts[0] != ""#"":
            raise ValueError(""Currently only local references are supported."")
        try:
            return getattr(self.spec.components, parts[-2])[parts[-1]]
        except KeyError:
            raise ValueError(f""Reference not found: {ref.ref}"")",Resolve a reference to a Parameter or Schema within the spec components.,"Resolve local references to components within a specification, raising errors if not found."
680,reset_model,"def reset_model(model_id=""""):
    
    r = requests.post(f""{HUB_API_ROOT}/model-reset"", json={""modelId"": model_id}, headers={""x-api-key"": Auth().api_key})
    if r.status_code == 200:
        LOGGER.info(f""{PREFIX}Model reset successfully"")
        return
    LOGGER.warning(f""{PREFIX}Model reset failure {r.status_code} {r.reason}"")",Reset a trained model to an untrained state.,Reset a model via API and log the outcome
681,get_hash_from_filename,"def get_hash_from_filename(filename):
    
    # Use just the filename without path
    name = os.path.basename(filename)
    # Create hash from filename and size for uniqueness
    file_size = os.path.getsize(filename)
    hash_input = f""{name}_{file_size}""
    return hashlib.md5(hash_input.encode('utf-8')).hexdigest()",Generate a hash from just the filename (not the full path).,Generate a unique hash based on a file's name and size.
682,load_config,"def load_config(self):
        
        try:
            # Try importing the configuration file
            config = importlib.import_module('filter_config')
            
            # Add additional patterns from config
            if hasattr(config, 'ADDITIONAL_FILTER_PATTERNS'):
                self.patterns.extend(config.ADDITIONAL_FILTER_PATTERNS)
                logger.info(f""Added {len(config.ADDITIONAL_FILTER_PATTERNS)} patterns from config"")
            
            # Set minimum character length
            if hasattr(config, 'MIN_CHARACTER_LENGTH'):
                self.min_character_length = config.MIN_CHARACTER_LENGTH
                logger.info(f""Set minimum character length to {self.min_character_length}"")
            
            # Set minimum real words
            if hasattr(config, 'MIN_REAL_WORDS'):
                self.min_real_words = config.MIN_REAL_WORDS
                logger.info(f""Set minimum real words to {self.min_real_words}"")
            
            # Add custom filter functions
            if hasattr(config, 'CUSTOM_FILTERS'):
                self.custom_filters.extend(config.CUSTOM_FILTERS)
                logger.info(f""Added {len(config.CUSTOM_FILTERS)} custom filter functions"")
            
            # Add stopwords
            if hasattr(config, 'STOPWORDS'):
                self.stopwords = config.STOPWORDS
                logger.info(f""Loaded stopwords for {len(config.STOPWORDS)} languages"")
                
            logger.info(""Successfully loaded filter configuration"")
        except ImportError:
            logger.warning(""No filter_config.py found, using default settings"")
        except Exception as e:
            logger.error(f""Error loading filter configuration: {e}"")",Load filter configuration from filter_config.py,"Load and apply filter configuration settings from an external module, with error handling."
683,_generate,"def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        

        processed = []
        prev_role = None

        for msg in messages:
            current_role = ""user"" if msg.type == ""human"" else ""assistant""

            if prev_role == current_role:
                if processed:
                    processed[-1].content += f""\n\n{msg.content}""
            else:
                processed.append(msg)
                prev_role = current_role

        return super()._generate(
            processed, stop=stop, run_manager=run_manager, **kwargs
        )",Override _generate to ensure message alternation in accordance to Deepseek API.,Consolidate consecutive messages by role before generating chat results.
684,mock_tool,"def mock_tool():
    
    return Tool(
        name=""test_tool"",
        description=""A test tool"",
        inputSchema={""type"": ""object"", ""properties"": {""query"": {""type"": ""string""}}},
    )",Creates a mock MCP tool for testing,Create a mock tool object with predefined attributes and input schema.
685,update_struc_data,"def update_struc_data(self):
        
        self.struc_data = {} # {comm1: [vol1, cut1, community_node_SE, leaf_nodes_SE], comm2:[vol2, cut2, community_node_SE, leaf_nodes_SE]，... }
        for vname in self.division.keys():
            comm = self.division[vname]
            volume = self.get_volume(comm)
            cut = self.get_cut(comm)
            if volume == 0:
                vSE = 0
            else:
                vSE = - (cut / self.vol) * math.log2(volume / self.vol)
            vnodeSE = 0
            for node in comm:
                d = self.graph.degree(node, weight = 'weight')
                if d != 0:
                    vnodeSE -= (d / self.vol) * math.log2(d / volume)
            self.struc_data[vname] = [volume, cut, vSE, vnodeSE]","calculate the volume, cut, communitiy mode SE, and leaf nodes SE of each cummunity, then store them into self.struc_data",Calculate and store structural metrics for graph communities.
686,resume_training,"def resume_training(self, ckpt):
        
        if ckpt is None or not self.resume:
            return
        best_fitness = 0.0
        start_epoch = ckpt.get(""epoch"", -1) + 1
        if ckpt.get(""optimizer"", None) is not None:
            self.optimizer.load_state_dict(ckpt[""optimizer""])  # optimizer
            best_fitness = ckpt[""best_fitness""]
        if self.ema and ckpt.get(""ema""):
            self.ema.ema.load_state_dict(ckpt[""ema""].float().state_dict())  # EMA
            self.ema.updates = ckpt[""updates""]
        assert start_epoch > 0, (
            f""{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\n""
            f""Start a new training without resuming, i.e. 'yolo train model={self.args.model}'""
        )
        LOGGER.info(f""Resuming training {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs"")
        if self.epochs < start_epoch:
            LOGGER.info(
                f""{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs.""
            )
            self.epochs += ckpt[""epoch""]  # finetune additional epochs
        self.best_fitness = best_fitness
        self.start_epoch = start_epoch
        if start_epoch > (self.epochs - self.args.close_mosaic):
            self._close_dataloader_mosaic()",Resume YOLO training from given epoch and best fitness.,"Resume model training from checkpoint, updating optimizer and EMA states."
687,_parse_hunk,"def _parse_hunk(self, lines: List[str]) -> None:
        
        if not lines or not lines[0].startswith(""@@ ""):
            raise PatchError(""Invalid hunk format"", {""First line"": lines[0] if lines else ""No lines""})

        header = self._parse_hunk_header(lines[0])
        patch_lines: List[PatchLine] = []
        orig_line = header.orig_start
        new_line = header.new_start

        for line in lines[1:]:
            line_type = LineType.from_line(line)
            if line_type:
                content = line[1:]
                patch_line = PatchLine(line_type, content)
                if line_type in (LineType.CONTEXT, LineType.DELETION):
                    patch_line.original_line_number = orig_line
                    orig_line += 1
                if line_type in (LineType.CONTEXT, LineType.ADDITION):
                    patch_line.new_line_number = new_line
                    new_line += 1
                patch_lines.append(patch_line)

        self.hunks.append(Hunk(header, patch_lines))",Parse a single hunk from its lines.,"Parse and validate patch hunk lines, updating line numbers accordingly."
688,get_checkpoint_path,"def get_checkpoint_path(self, filename: str) -> str:
        
        ckpts_folder = os.path.join(self.model_dir, ""ckpts"")
        # Add .safetensors extension if not present
        if not filename.endswith('.safetensors'):
            filename = f""{filename}.safetensors""
        full_path = os.path.join(ckpts_folder, filename)
        if not os.path.exists(full_path):
            raise FileNotFoundError(f""Checkpoint file not found: {full_path}"")
        return full_path",Returns the full path to a checkpoint file.,Constructs and validates the file path for a model checkpoint with a specific extension.
689,_fetch_data,"def _fetch_data(self, ticker: str, start_date: datetime, end_date: datetime, interval: str) -> pd.DataFrame:
        
        try:
            # Construct the URL
            params = {
                'period': self.INTERVAL_MAPPING.get(interval, self.INTERVAL_MAPPING['1d']),
                'window': int((end_date - start_date).total_seconds())
            }
            url = f""{self.BASE_URL}/{ticker}/historical""
            
            # Make the request
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            # Parse the response and convert to DataFrame
            data = response.json()
            df = pd.DataFrame(data['prices'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
            df.set_index('timestamp', inplace=True)
            
            return df
            
        except requests.RequestException as e:
            logger.error(f""Error fetching data from Google Finance: {e}"")
            raise",Fetch financial data from Google Finance.,"Fetch historical financial data for a given stock ticker and time range, returning a DataFrame."
690,get_platform_id,"def get_platform_id(cls) -> PlatformId:
        
        system = platform.system()
        machine = platform.machine()
        bitness = platform.architecture()[0]
        if system == ""Windows"" and machine == """":
            machine = cls._determine_windows_machine_type()
        system_map = {""Windows"": ""win"", ""Darwin"": ""osx"", ""Linux"": ""linux""}
        machine_map = {""AMD64"": ""x64"", ""x86_64"": ""x64"", ""i386"": ""x86"", ""i686"": ""x86"", ""aarch64"": ""arm64"", ""arm64"": ""arm64""}
        if system in system_map and machine in machine_map:
            platform_id = system_map[system] + ""-"" + machine_map[machine]
            if system == ""Linux"" and bitness == ""64bit"":
                libc = platform.libc_ver()[0]
                if libc != 'glibc':
                    platform_id += ""-"" + libc
            return PlatformId(platform_id)
        else:
            raise MultilspyException(f""Unknown platform: {system=}, {machine=}, {bitness=}"")",Returns the platform id for the current system,"Determine platform identifier based on system, machine, and architecture details."
691,run_item_operations,"def run_item_operations(service: ItemService, config: dict[str, Any]) -> None:
    
    logger.info(""Running item operations"")

    # Get configuration
    item_count = config.get(""item_count"", 5)

    # Create items
    items = create_sample_items(service, item_count)
    logger.info(f""Created {len(items)} items"")

    # Demonstrate some operations
    total_price = 0.0
    for item in items:
        price_display = item.get_display_price()
        logger.info(f""Item: {item.name}, Price: {price_display}"")
        total_price += item.price

    logger.info(f""Total price of all items: ${total_price:.2f}"")",Run operations related to items.,Execute item operations and log creation and pricing details.
692,clear,"def clear(self) -> None:
        
        try:
            self.vector_store.delete_documents(delete_all=True)
        except Exception as e:
            raise PineconeError(f""Error clearing Pinecone index: {e}"") from e",Clears the Pinecone index.,"Clear all documents from a vector store, handling potential errors."
693,convert_histogram_to_iso8601,"def convert_histogram_to_iso8601(cls, v):
        
        return {epoch_ms_to_utc_iso(int(timestamp)): count for timestamp, count in v.items()}","If value passed is an int of Unix Epoch, convert to an ISO timestamp string.",Convert a histogram's timestamps to ISO 8601 format using a utility function.
694,exec,"def exec(self, answer):
        
        print(f""    🔍 Supervisor checking answer quality..."")
        
        # Check for obvious markers of the nonsense answers
        nonsense_markers = [
            ""coffee break"", 
            ""purple unicorns"", 
            ""made up"", 
            ""42"", 
            ""Who knows?""
        ]
        
        # Check if the answer contains any nonsense markers
        is_nonsense = any(marker in answer for marker in nonsense_markers)
        
        if is_nonsense:
            return {""valid"": False, ""reason"": ""Answer appears to be nonsensical or unhelpful""}
        else:
            return {""valid"": True, ""reason"": ""Answer appears to be legitimate""}",Check if the answer is valid or nonsensical.,Validate answer quality by detecting predefined nonsense markers.
695,tool_def_to_mistral,"def tool_def_to_mistral(tool_definitions: list[dict[str, Any]]) -> list[dict[str, Any]]:
    
    mistral_tools = []

    for autogen_tool in tool_definitions:
        mistral_tool = {
            ""type"": ""function"",
            ""function"": Function(
                name=autogen_tool[""function""][""name""],
                description=autogen_tool[""function""][""description""],
                parameters=autogen_tool[""function""][""parameters""],
            ),
        }

        mistral_tools.append(mistral_tool)

    return mistral_tools",Converts AG2 tool definition to a mistral tool format,Transforms tool definitions into Mistral-compatible function objects.
696,try_to_remove,"def try_to_remove(puzzle: Puzzle, clues: set[Clue], n: int, must_have=set()) -> set[Clue]:
    

    def weight(clue: Clue) -> float:
        # relative probabilities of each type of clue being selected for removal
        weights: dict[Type[Clue], float] = {
            not_at: 0.75,
            found_at: 0.75,
            same_house: 0.75,
            beside: 1.2,
            left_of: 1.2,
            right_of: 1.2,
            one_between: 1.5,
            two_between: 1.5,
        }

        return weights.get(type(clue), 1)

    # sorted, no hash randomization
    weights = [weight(clue) for clue in sorted(clues)]
    candidates: set[Clue] = set(puzzle.rng.choices(sorted(clues), weights, k=n))
    candidates = candidates - must_have
    clues = clues.difference(candidates)
    if has_unique_solution(puzzle, clues):
        # print(f""Removed {len(candidates)} clues."")
        return clues

    # we needed at least one of those, add them all back
    clues = clues | candidates
    return clues","Attempt to remove n clues from a set of candidate clues; if we are able to, return the new, smaller set of clues.",Attempt to remove clues from a puzzle while ensuring a unique solution remains.
697,analyze_model_fields,"def analyze_model_fields(method) -> dict:
    
    print(f""\n🔍 Analyzing model fields for method: {method.name}"")
    schema = {}

    # Look for model definitions in doc decorators
    for decorator in method.decorators:
        if "".doc"" in decorator.source:
            try:
                if ""model="" in decorator.source:
                    model_def = decorator.source.split(""model="")[1]
                    if ""fields."" in model_def:
                        # Parse the fields
                        fields_str = model_def.split(""{"")[1].split(""}"")[0]
                        for field in fields_str.split("",""):
                            if "":"" in field:
                                name, field_type = field.split("":"", 1)
                                name = name.strip()
                                if ""fields.String"" in field_type:
                                    schema[name] = {""type"": ""string""}
                                elif ""fields.Boolean"" in field_type:
                                    schema[name] = {""type"": ""boolean""}
                                elif ""fields.Integer"" in field_type:
                                    schema[name] = {""type"": ""integer""}
                                elif ""fields.Nested"" in field_type:
                                    schema[name] = {""type"": ""object""}
                                else:
                                    schema[name] = {""type"": ""any""}
            except Exception as e:
                print(f""   ⚠️ Couldn't parse model fields: {str(e)}"")

    return schema",Analyze model fields from ns_conf.model definitions.,Extracts and categorizes model field types from method decorators into a schema dictionary.
698,mock_context,"def mock_context():
    
    return DocumentationContext(
        project_name='test-project',
        working_dir='/path/to/repo',
        repomix_path='/path/to/repo/generated-docs',
        status='initialized',
        current_step='analysis',
        analysis_result=ProjectAnalysis(
            project_type='Web Application',
            features=['Feature 1', 'Feature 2'],
            file_structure={'root': ['/path/to/repo']},
            dependencies={'react': '^18.2.0'},
            primary_languages=['JavaScript', 'TypeScript'],
            frontend={'framework': 'React'},
            backend={'framework': 'Express', 'database': {'type': 'MongoDB'}},
            apis=None,
        ),
    )",Create a mock DocumentationContext for testing.,Create a mock documentation context for a web application project analysis.
699,switch_verbose,"def switch_verbose(verbose_mode: bool, log_level: str = ""info"") -> None:
    
    if log_level == ""debug"":
        configure_logger(""DEBUG"")
    else:
        configure_logger(log_level)
    
    verbose_mode = verbose_mode if verbose_mode is not None else False
    set_litellm_verbose(verbose_mode)
    logger.debug(f""litellm verbose mode set to: {verbose_mode}"")",Switch verbose mode and configure logger and litellm verbosity.,Toggle logging verbosity and configure logger based on mode and level.
700,process_data,"def process_data(df, fold, total_num):
    
    conversations = []
    
    for index, example in tqdm(df.iterrows(), total=total_num, desc=""Processing indices""):
        if index >= total_num:
            break
        
        images_list = extract_images(example['image_bytes'])
        if len(images_list) > 1:
            continue  # Skip multiple image cases

        images = save_images(images_list, fold, index)
        image_token = ""<image>"" if images else """"
        
        question = example['question']
        answer_choices = list(map(str, example['answers'].strip('[]').split(', ')))
        random.shuffle(answer_choices)
        correct_answer = example['correct_answer']
        
        answer = "", "".join(answer_choices[:-1]) + "" or "" + answer_choices[-1]
        prompt = f""{question} Choose between the following options: {answer}""
        messages = [
            {""role"": ""user"", ""content"": f""{image_token} Answer in natural language. {prompt}""},
            {""role"": ""assistant"", ""content"": correct_answer}
        ]
        
        conversation = {""messages"": messages, ""images"": images}

        conversations.append(conversation)
    
    with open(f'SAT_{fold}_{total_num}.json', 'w') as f:
        json.dump(conversations, f, indent=4)",Process dataset and generate conversation JSON files.,Iterate dataset to generate and save structured Q&A conversations with images.
701,get_available_actions,"def get_available_actions(self):
        
        # Determine if a search bar is available
        try:
            search_bar = self.browser.find_element_by_id('search_input')
        except Exception:
            has_search_bar = False
        else:
            has_search_bar = True

        # Collect buttons, links, and options as clickables
        buttons = self.browser.find_elements_by_class_name('btn')
        product_links = self.browser.find_elements_by_class_name('product-link')
        buying_options = self.browser.find_elements_by_css_selector(""input[type='radio']"")

        self.text_to_clickable = {
            f'{b.text}': b
            for b in buttons + product_links
        }
        for opt in buying_options:
            opt_value = opt.get_attribute('value')
            self.text_to_clickable[f'{opt_value}'] = opt
        return dict(
            has_search_bar=has_search_bar,
            clickables=list(self.text_to_clickable.keys()),
        )",Returns list of available actions at the current step,Identify available UI elements and return their interactable states.
702,install_packages,"def install_packages(self, *packages: str):
        
        subprocess.run(
            [""uv"", ""pip"", ""install"", *packages],
            check=True,
            env={**os.environ, ""VIRTUAL_ENV"": str(self.venv_dir)},
        )",Install packages into the virtual environment using uv pip.,Install specified packages in a virtual environment using pip.
703,extract_text_from_pdf,"def extract_text_from_pdf(pdf_path):
    
    try:
        if not os.path.exists(pdf_path):
            st.error(f""PDF file not found: {pdf_path}"")
            return None

        pdf_content = """"
        try:
            # Open the PDF file
            with fitz.open(pdf_path) as doc:
                # Get the number of pages
                num_pages = len(doc)
                st.info(f""Processing PDF with {num_pages} pages..."")

                # Iterate through each page
                for page_num in range(num_pages):
                    # Get the page
                    page = doc[page_num]
                    # Extract text from the page
                    pdf_content += page.get_text()
                    # Add a separator between pages
                    if page_num < num_pages - 1:
                        pdf_content += ""\n\n""
        except Exception as e:
            st.error(f""Error extracting text from PDF: {str(e)}"")
            return None

        return pdf_content

    except Exception as e:
        st.error(f""Error processing PDF: {str(e)}"")
        return None",Extract text content from a PDF file.,"Extracts and returns text content from a specified PDF file, handling errors gracefully."
704,filename_to_url,"def filename_to_url(filename, cache_dir=None):
    
    if cache_dir is None:
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):
        cache_dir = str(cache_dir)

    cache_path = os.path.join(cache_dir, filename)
    if not os.path.exists(cache_path):
        raise EnvironmentError(""file {} not found"".format(cache_path))

    meta_path = cache_path + "".json""
    if not os.path.exists(meta_path):
        raise EnvironmentError(""file {} not found"".format(meta_path))

    with open(meta_path, encoding=""utf-8"") as meta_file:
        metadata = json.load(meta_file)
    url = metadata[""url""]
    etag = metadata[""etag""]

    return url, etag",Return the url and etag (which may be ``None``) stored for `filename`.,Convert a filename to its corresponding URL and metadata by checking cache and loading JSON.
705,_load_yaml,"def _load_yaml(cls, yaml_path: str) -> List[dict]:
        
        try:
            with open(yaml_path, ""r"", encoding=""utf-8"") as file:
                content = file.read()

            processed_content = cls._preprocess_yaml(content)

            try:
                data = yaml.safe_load(processed_content)
                if not isinstance(data, list):
                    raise YAMLParseError(""YAML root must be a list"")
                return data

            except yaml.YAMLError as e:
                error_msg = str(e)
                if ""found character"" in error_msg and ""that cannot start any token"" in error_msg:
                    raise YAMLParseError(
                        ""Invalid character found in YAML file. Ensure no tabs are used ""
                        ""(use spaces for indentation) and no special characters are present.""
                    )
                raise YAMLParseError(f""YAML parsing error: {error_msg}"")

        except (OSError, IOError) as e:
            logger.error(f""Failed to read signatures file: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error loading YAML: {e}"")
            raise",Load and parse the YAML file containing signature patterns.,"Load and validate YAML file content, ensuring it is a list and handling errors."
706,mock_trajectory,"def mock_trajectory():
    
    return TrajectoryModel(
        id=1,
        created_at=datetime.datetime(2025, 1, 1, 0, 0, 0),
        updated_at=datetime.datetime(2025, 1, 1, 0, 0, 0),
        human_input_id=1,
        tool_name=""test_tool"",
        tool_parameters={""param"": ""value""},
        tool_result={""result"": ""success""},
        step_data={""step"": ""data""},
        record_type=""tool_execution"",
        current_cost=0.01,
        input_tokens=10,
        output_tokens=20,
        is_error=False,
        session_id=1
    )",Return a mock trajectory for testing.,Create a mock trajectory model with predefined attributes for testing purposes.
707,create_transport,"def create_transport(
    settings: LoggerSettings, event_filter: EventFilter | None = None
) -> EventTransport:
    
    if settings.type == ""none"":
        return NoOpTransport(event_filter=event_filter)
    elif settings.type == ""console"":
        return ConsoleTransport(event_filter=event_filter)
    elif settings.type == ""file"":
        if not settings.path:
            raise ValueError(""File path required for file transport"")
        return FileTransport(
            filepath=settings.path,
            event_filter=event_filter,
        )
    elif settings.type == ""http"":
        if not settings.http_endpoint:
            raise ValueError(""HTTP endpoint required for HTTP transport"")
        return HTTPTransport(
            endpoint=settings.http_endpoint,
            headers=settings.http_headers,
            batch_size=settings.batch_size,
            timeout=settings.http_timeout,
            event_filter=event_filter,
        )
    else:
        raise ValueError(f""Unsupported transport type: {settings.type}"")",Create event transport based on settings.,Configure and instantiate event transport based on logger settings and type
708,update_script_template,"def update_script_template(self):
        
        template_path = self.alembic_dir / ""script.py.mako""
        try:
            with open(template_path, ""r"") as f:
                content = f.read()

            # Add sqlmodel import to imports section
            import_section = ""from alembic import op\nimport sqlalchemy as sa""
            new_imports = ""from alembic import op\nimport sqlalchemy as sa\nimport sqlmodel""

            content = content.replace(import_section, new_imports)

            with open(template_path, ""w"") as f:
                f.write(content)

            return True

        except Exception as e:
            logger.error(f""Failed to update script template: {e}"")
            return False",Update the Alembic script template to include SQLModel.,Enhance script template by adding sqlmodel import to existing Alembic and SQLAlchemy imports.
709,print_results,"def print_results(title: str, consensus: Dict[str, str], 
                 consensus_proportion: Dict[str, float], 
                 entropy: Dict[str, float]) -> None:
    
    print(f""\n=== {title} ==="")
    print(f""{'Cluster':<10} {'Consensus':<30} {'Proportion':<10} {'Entropy':<10}"")
    print(""-"" * 60)
    
    for cluster in sorted(consensus.keys()):
        print(f""{cluster:<10} {consensus[cluster]:<30} {consensus_proportion[cluster]:.2f}      {entropy[cluster]:.2f}"")",Print the results in a formatted way.,"Display formatted cluster analysis results with consensus, proportion, and entropy."
710,clear_subtree,"def clear_subtree(self, name_root: str):
        
        with self._lock:
            name_root = os.path.normpath(name_root)
            count = ray.get(self._kv_store.delete_prefix.remote(name_root))

            # Clean up local tracking for deleted keys
            keys_to_remove = []
            for key in self._entries.keys():
                normalized_key = os.path.normpath(key)
                if normalized_key == name_root or normalized_key.startswith(
                    name_root.rstrip(""/"") + ""/""
                ):
                    keys_to_remove.append(key)

            for key in keys_to_remove:
                del self._entries[key]

            logger.debug(f""Deleted {count} entries under {name_root}"")",Delete all keys with the given prefix.,Safely delete and log entries under a specified directory path in a key-value store.
711,normalize_dict,"def normalize_dict(self, data: Union[Dict, List, Any]):
        
        if not data:
            return {}

        if isinstance(data, dict):
            new_data = dict()
            for key in sorted(data.keys(), key=lambda k: (len(k), k)):
                value = self.normalize_dict(data[key])
                if value:
                    if not isinstance(value, list):
                        value = [value]
                    new_data[key] = value

        elif isinstance(data, list):
            if all(isinstance(item, dict) for item in data):
                new_data = []
                for item in data:
                    item = self.normalize_dict(item)
                    if item:
                        new_data.append(item)
            else:
                new_data = [str(item).strip() for item in data if type(item) in {str, int, float} and str(item).strip()]
        else:
            new_data = [str(data).strip()]

        return new_data","Sort by value, while iterate over element if data is list",Normalize nested data structures into a consistent dictionary format.
712,_check_execution_env,"def _check_execution_env(self) -> Dict[str, str]:
        
        env_info = {
            ""type"": ""system"",
            ""details"": ""Unknown environment""
        }
        
        # Check if in docker environment - first check environment variable
        if os.environ.get(""IN_DOCKER_ENV"") == ""1"":
            env_info[""type""] = ""docker""
            env_info[""details""] = ""docker-env-variable""
            return env_info
        
        # Regular system environment
        try:
            import platform
            system_info = platform.platform()
            env_info[""details""] = system_info
        except Exception:
            pass
            
        return env_info","Get current execution environment information, supporting docker or regular system environment","Determine execution environment type and details, checking for Docker or system info."
713,init_metrics,"def init_metrics(self, model):
        
        val = self.data.get(self.args.split, """")  # validation path
        val = val[0] if isinstance(val, list) else val
        
        self.is_coco = (
            isinstance(val, str)
            and ""coco"" in val
            and (val.endswith(f""{os.sep}val2017.txt"") or val.endswith(f""{os.sep}test-dev2017.txt""))
        )  # is COCO
        self.is_lvis = isinstance(val, str) and ""lvis"" in val and not self.is_coco  # is LVIS
        self.class_map = converter.coco80_to_coco91_class() if self.is_coco else list(range(len(model.names)))
        self.args.save_json |= self.args.val and (self.is_coco or self.is_lvis)  # run final val
        self.names = model.names
        self.nc = len(model.names)
        self.metrics.names = self.names
        self.metrics.plot = self.args.plots
        self.confusion_matrix = ConfusionMatrix(nc=self.nc, conf=self.args.conf)
        self.seen = 0
        self.jdict = []
        self.stats = dict(tp=[], conf=[], pred_cls=[], target_cls=[], target_img=[])",Initialize evaluation metrics for YOLO.,Initialize evaluation metrics for model validation and dataset type detection.
714,_set_browser_use_version_and_source,"def _set_browser_use_version_and_source(self, source_override: str | None = None) -> None:
		
		# Use the helper function for version detection
		version = get_browser_use_version()

		# Determine source
		try:
			package_root = Path(__file__).parent.parent.parent
			repo_files = ['.git', 'README.md', 'docs', 'examples']
			if all(Path(package_root / file).exists() for file in repo_files):
				source = 'git'
			else:
				source = 'pip'
		except Exception as e:
			self.logger.debug(f'Error determining source: {e}')
			source = 'unknown'

		if source_override is not None:
			source = source_override
		self.logger.debug(f'Version: {version}, Source: {source}')
		self.version = version
		self.source = source",Get the version from pyproject.toml and determine the source of the browser-use package,"Determine browser version and source, with optional source override"
715,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        result = {
            ""id"": self.id,
            ""name"": self.name,
        }

        if self.to_status:
            result[""to_status""] = self.to_status.to_simplified_dict()

        return result",Convert to simplified dictionary for API response.,Convert object attributes to a simplified dictionary representation.
716,get_pdf_page_refs,"def get_pdf_page_refs(dataset_jsonl):
    
    pdf_page_tests = defaultdict(list)

    with open(dataset_jsonl, ""r"") as f:
        for line in f:
            if not line.strip():
                continue

            try:
                test = json.loads(line)
                pdf_name = test.get(""pdf"")
                page_num = test.get(""page"")
                test_id = test.get(""id"")

                if pdf_name and page_num and test_id:
                    pdf_page_tests[(pdf_name, page_num)].append(test_id)
            except json.JSONDecodeError:
                print(f""Warning: Could not parse line: {line}"")
                continue

    return pdf_page_tests",Parse dataset.jsonl to extract all PDF page references.,Extracts and groups test IDs by PDF name and page from a JSONL dataset.
717,process_jsonl_file_present,"def process_jsonl_file_present(input_file, output_file):
    
    with open(input_file, ""r"") as infile, open(output_file, ""w"") as outfile:
        for line in infile:
            if line.strip():  # Skip empty lines
                data = json.loads(line)
                image = data[""image""]
                original_text = data[""text""]
                num_cases = random.randint(1, 3)

                for _ in range(num_cases):
                    processed_num = random.randint(5, 10)
                    processed_id = f""{image}_processed{processed_num:02d}""
                    max_diffs = random.randint(1, 2)
                    text_segment = extract_random_segment(original_text)

                    new_case = {
                        ""pdf"": f""{image}.pdf"",
                        ""page"": 1,
                        ""id"": processed_id,
                        ""type"": ""present"",
                        ""max_diffs"": max_diffs,
                        ""text"": text_segment,
                        ""case_sensitive"": True,
                        ""first_n"": None,
                        ""last_n"": None,
                    }
                    outfile.write(json.dumps(new_case) + ""\n"")",Process a JSONL file and create multiple random cases for each PDF.,Generate randomized text cases from JSONL input for output file storage.
718,enhance_requirement,"def enhance_requirement(self, request: AdvancedChatRequest) -> str:
        
        logger.info(""Starting requirement enhancement phase..."")
        
        # Convert AdvancedChatRequest to ChatRequest
        chat_request = ChatRequest(
            message=request.requirement,
            system_prompt="""",  # Will be set by strategy
            enable_l0_retrieval=request.enable_l0_retrieval,
            enable_l1_retrieval=request.enable_l1_retrieval,
            temperature=request.temperature
        )
        logger.info(f""Created chat request with message: {chat_request.message[:100]}..."")
        
        # Use chat service with RequirementEnhancementStrategy
        logger.info(""Calling chat service with RequirementEnhancementStrategy..."")
        response = chat_service.chat(
            request=chat_request,
            strategy_chain=[BasePromptStrategy, RequirementEnhancementStrategy],
            stream=False,
            json_response=False
        )
        
        enhanced_requirement = response.choices[0].message.content
        logger.info(f""Requirement enhancement completed. Result: {enhanced_requirement[:100]}..."")
        return enhanced_requirement",Enhance the requirement with knowledge context,Enhance chat request requirements using a strategic chat service process.
719,canonical_model,"def canonical_model(self) -> BaseLlm:
    
    if isinstance(self.model, BaseLlm):
      return self.model
    elif self.model:  # model is non-empty str
      return LLMRegistry.new_llm(self.model)
    else:  # find model from ancestors.
      ancestor_agent = self.parent_agent
      while ancestor_agent is not None:
        if isinstance(ancestor_agent, LlmAgent):
          return ancestor_agent.canonical_model
        ancestor_agent = ancestor_agent.parent_agent
      raise ValueError(f'No model found for {self.name}.')",The resolved self.model field as BaseLlm.,Determine the appropriate language model instance from current or ancestor agents.
720,torch_distributed_zero_first,"def torch_distributed_zero_first(local_rank: int):
    
    initialized = dist.is_available() and dist.is_initialized()

    if initialized and local_rank not in {-1, 0}:
        dist.barrier(device_ids=[local_rank])
    yield
    if initialized and local_rank == 0:
        dist.barrier(device_ids=[local_rank])",Ensures all processes in distributed training wait for the local master (rank 0) to complete a task first.,"Synchronize distributed processes, prioritizing rank zero execution."
721,handle_get_char,"def handle_get_char(args: argparse.Namespace) -> int:
    
    try: result = get_char_at(args.string, args.index); print(f""Character at index {args.index}: {result}""); return 0
    except IndexError: logger.error(""Index out of range""); print(""Error: Index out of range""); return 1
    except Exception as e: logger.error(f""Error get_char: {e}""); print(f""Error: {e}""); return 1",Handle the get_char command.,Handle character retrieval from string with error logging and reporting
722,_reinsert_empty_vectors,"def _reinsert_empty_vectors(
        self, embeddings: List[List[float]], empty_indices: set, total_length: int
    ) -> List[List[float]]:
        
        result = []
        embedding_idx = 0

        for i in range(total_length):
            if i in empty_indices:
                result.append([0.0] * self.vector_dimensions)
            else:
                result.append(embeddings[embedding_idx])
                embedding_idx += 1

        logger.info(f""📦 OPENAI_FINAL Final result: {len(result)} vectors"")
        return result",Reinsert empty vectors at their original positions.,Reinsert zero vectors at specified indices within a list of embeddings.
723,default_class_names,"def default_class_names(data=None):
    
    if data:
        try:
            return yaml_load(check_yaml(data))[""names""]
        except Exception:
            pass
    return {i: f""class{i}"" for i in range(999)}",Applies default class names to an input YAML file or returns numerical class names.,Generate class names from YAML data or default to numbered classes.
724,_resolve_type,"def _resolve_type(self, raw_value: str, module) -> Any:
        
        try:
            # Try to load from plugin module first
            return getattr(module, raw_value)
        except AttributeError:
            try:
                # Try as fully qualified import
                module_path, class_name = raw_value.rsplit(""."", 1)
                type_module = importlib.import_module(module_path)
                return getattr(type_module, class_name)
            except (ValueError, ImportError, AttributeError) as e:
                raise GoatConfigurationError(
                    f""Could not resolve type '{raw_value}'""
                ) from e","Resolve a type from a string, either from plugin module or fully qualified path",Resolve a type from a string by checking a module or importing it dynamically.
725,ensure_max_steps,"def ensure_max_steps(self):
        
        if self.task.max_steps is None:
            max_steps = self._calc_max_steps()
            self._config[self.task_key][""max_steps""] = max_steps
            if self._default_config[self.task_key][""max_steps""] is None:
                self._default_config[self.task_key][""max_steps""] = max_steps
            self._generate_configs()
            log_info(
                f""'max_steps' not set explicitly, using {max_steps=} (calculated from ""
                + f""max_epochs={self.task.max_epochs}, batch_size={self.task.batch_size}, ""
                + f""devices={self.engine.devices})""
            )",Ensures that `self.task.max_steps` is calculated and set correctly.,Automatically calculate and set maximum steps if not explicitly defined in task configuration.
726,_save_one_file,"def _save_one_file(file):
        
        plt.savefig(file, dpi=200)
        plt.close()
        LOGGER.info(f""Saved {file}"")",Save one matplotlib plot to 'file'.,Save a plot to a file and log the action.
727,_split_kwargs,"def _split_kwargs(cls, kwargs):
        
        check_peft_kwargs = False

        if is_peft_available():
            from peft import prepare_model_for_kbit_training

            check_peft_kwargs = True

        supported_kwargs = {}
        unsupported_kwargs = {}
        peft_kwargs = {}

        for key, value in kwargs.items():
            if key in cls.supported_args:
                supported_kwargs[key] = value
            else:
                unsupported_kwargs[key] = value

            if check_peft_kwargs:
                if key in prepare_model_for_kbit_training.__code__.co_varnames:
                    peft_kwargs[key] = value
                    if key in unsupported_kwargs:
                        unsupported_kwargs.pop(key)

        return supported_kwargs, unsupported_kwargs, peft_kwargs",Separate the kwargs from the arguments that we support inside `supported_args` and the ones that we don't.,"Categorizes keyword arguments into supported, unsupported, and PEFT-specific groups."
728,register_agentic_system,"def register_agentic_system(self) -> None:
        
        try:
            # Update the agents registry store with the new agent metadata
            self.register_agent(
                store_name=self.agents_registry_store_name,
                store_key=self.agents_registry_key,
                agent_name=self.name,
                agent_metadata=self._agent_metadata,
            )
        except Exception as e:
            logger.error(f""Failed to register metadata for agent {self.name}: {e}"")
            raise e",Registers the agent's metadata in the Dapr state store under 'agents_metadata'.,"Registers agent metadata in a registry, handling exceptions with logging."
729,setup_telemetry,"def setup_telemetry(self) -> None:
        
        resource = Resource(
            attributes={
                SERVICE_NAME: self._service_name,
                DEPLOYMENT_ENVIRONMENT: self._deployment_environment,
            }
        )
        reader = PrometheusMetricReader()
        provider = MeterProvider(resource=resource, metric_readers=[reader])
        metrics.set_meter_provider(provider)

        # Start Prometheus HTTP server, skip if port is already in use
        try:
            start_http_server(self._prometheus_port)
        except OSError as e:
            if e.errno == 48:  # Address already in use
                LOG.warning(
                    f""Prometheus HTTP server already running on port {self._prometheus_port}""
                )
            else:
                raise e

        # Initialize meter
        self._meter = metrics.get_meter(self._service_name)",Initialize OpenTelemetry with Prometheus exporter.,Initialize telemetry with Prometheus metrics and handle server port conflicts.
730,train,"def train(self, model, train_iterator, optimizer, loss_func, log_interval=40):
        
        n_batches = len(train_iterator)
        epoch_loss = 0
        for i, batch in enumerate(train_iterator):
            optimizer.zero_grad()
            text, text_lengths = batch['text']
            predictions = model(text, text_lengths)
            loss, num_triplets = loss_func(predictions, batch['label'])
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            if i % log_interval == 0:
                print(
                    f'\tBatch: [{i}/{n_batches} ({100. * (i + 1) / n_batches:.0f}%)]\tLoss: {epoch_loss / (i + 1):.4f}\tNum_triplets: {num_triplets}')
        return epoch_loss / n_batches",Train the BiLSTM model.,"The code trains a model using batches, updating weights, and logging loss periodically."
731,_get_unique_class_name,"def _get_unique_class_name(self, candidate_name: str) -> str:
        
        if not MODULE_REGISTRY.has_module(candidate_name):
            return candidate_name 
        
        i = 1 
        while True:
            unique_name = f""{candidate_name}V{i}""
            if not MODULE_REGISTRY.has_module(unique_name):
                break
            i += 1 
        return unique_name",Get a unique class name by checking if it already exists in the registry.,Generate a unique class name by appending a version number if needed.
732,move_to_pose,"def move_to_pose(
    obs, obs_saver, robot_ik, robot, scenario, ee_pos_target, ee_quat_target, steps=10, open_gripper=False
):
    
    curr_robot_q = obs.robots[robot.name].joint_pos

    seed_config = curr_robot_q[:, :curobo_n_dof].unsqueeze(1).tile([1, robot_ik._num_seeds, 1])

    result = robot_ik.solve_batch(Pose(ee_pos_target, ee_quat_target), seed_config=seed_config)

    q = torch.zeros((scenario.num_envs, robot.num_joints), device=""cuda:0"")
    ik_succ = result.success.squeeze(1)
    q[ik_succ, :curobo_n_dof] = result.solution[ik_succ, 0].clone()
    q[:, -ee_n_dof:] = 0.04 if open_gripper else 0.0
    actions = [
        {robot.name: {""dof_pos_target"": dict(zip(robot.actuators.keys(), q[i_env].tolist()))}}
        for i_env in range(scenario.num_envs)
    ]
    for i in range(steps):
        obs, reward, success, time_out, extras = env.step(actions)
        obs_saver.add(obs)
    return obs",Move the robot to the target pose.,Calculate and execute robot arm movements to reach a target pose over multiple steps.
733,create_graph,"def create_graph(selected_agents: list[str]) -> StateGraph:
    
    graph = StateGraph(AgentState)
    graph.add_node(""start_node"", start)

    # Filter out any agents that are not in analyst.py
    selected_agents = [agent for agent in selected_agents if agent in ANALYST_CONFIG]

    # Get analyst nodes from the configuration
    analyst_nodes = {key: (f""{key}_agent"", config[""agent_func""]) for key, config in ANALYST_CONFIG.items()}

    # Add selected analyst nodes
    for agent_name in selected_agents:
        node_name, node_func = analyst_nodes[agent_name]
        graph.add_node(node_name, node_func)
        graph.add_edge(""start_node"", node_name)

    # Always add risk and portfolio management (for now)
    graph.add_node(""risk_management_agent"", risk_management_agent)
    graph.add_node(""portfolio_manager"", portfolio_management_agent)

    # Connect selected agents to risk management
    for agent_name in selected_agents:
        node_name = analyst_nodes[agent_name][0]
        graph.add_edge(node_name, ""risk_management_agent"")

    # Connect the risk management agent to the portfolio management agent
    graph.add_edge(""risk_management_agent"", ""portfolio_manager"")

    # Connect the portfolio management agent to the end node
    graph.add_edge(""portfolio_manager"", END)

    # Set the entry point to the start node
    graph.set_entry_point(""start_node"")
    return graph",Create the workflow with selected agents.,"Constructs a state graph with selected agents, linking them to risk and portfolio management."
734,start_server,"def start_server(script: str | None = None, port: int = 8501):
    
    app = create_app(script)

    config = uvicorn.Config(app, host=""0.0.0.0"", port=port, loop=""asyncio"")
    server = uvicorn.Server(config)

    # Handle shutdown signals
    async def handle_shutdown(signum=None, frame=None):
        
        logger.info(""Shutting down server..."")
        await app.state.service.shutdown()

    # Handle shutdown signals
    def sync_handle_shutdown(signum, frame):
        
        loop = asyncio.get_event_loop()
        loop.create_task(handle_shutdown(signum, frame))  # noqa: RUF006

    signal.signal(signal.SIGINT, sync_handle_shutdown)
    signal.signal(signal.SIGTERM, sync_handle_shutdown)

    try:
        import asyncio

        asyncio.run(server.serve())
    except KeyboardInterrupt:
        asyncio.run(handle_shutdown())
    except Exception as e:
        logger.error(f""Unexpected error: {e}"")
        raise",Start the FastAPI server,Initialize and run an asynchronous server with graceful shutdown handling.
735,separator,"def separator(component_id: str | None = None, **kwargs) -> ComponentReturn:
    
    component = {""type"": ""separator"", ""id"": component_id}

    logger.debug(f""[separator] ID={component_id}"")
    return ComponentReturn(component, component)",Create a separator component that forces a new row.,Create a separator component with optional ID and log its creation.
736,_save_results,"def _save_results(file_path: str, data: Any) -> None:
        
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        temp_filename = f""{file_path}.tmp""
        with open(temp_filename, ""w"", encoding=""utf-8"") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        os.replace(temp_filename, file_path)",Safe-write a JSON file via temp file + replace.,"Safely write JSON data to a file, ensuring directory creation."
737,_get_base_output_directory,"def _get_base_output_directory(self) -> Optional[str]:
        
        base_dir = os.getenv('NLWEB_OUTPUT_DIR')
        if base_dir and not os.path.exists(base_dir):
            try:
                os.makedirs(base_dir, exist_ok=True)
                print(f""Created output directory: {base_dir}"")
            except Exception as e:
                print(f""Warning: Failed to create output directory {base_dir}: {e}"")
                return None
        return base_dir",Get the base directory for all output files from the environment variable.,Determine and ensure the existence of a base output directory.
738,reset_page_scale_factor,"def reset_page_scale_factor() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Emulation.resetPageScaleFactor"",
    }
    json = yield cmd_dict",Requests that page scale factor is reset to initial values.,Reset the page's zoom level using an emulation command generator.
739,_get_container_workspace_path,"def _get_container_workspace_path() -> str:
    
    workspace_name = os.getenv(""CAI_WORKSPACE"") 
    if workspace_name:
        if not all(c.isalnum() or c in ['_', '-'] for c in workspace_name):
            print(color(f""Invalid CAI_WORKSPACE name '{workspace_name}' for container. ""
                        f""Using '/workspace'."", fg=""yellow""))
            return ""/""
        # Standard path inside CAI containers
        return f""/workspace/workspaces/{workspace_name}""
    else:
        return ""/""",Determines the target workspace path inside the container.,Determine valid workspace path based on environment variable for container use
740,assert_message_history_contains,"def assert_message_history_contains(self, role: str, content_substring: str):
        
        for msg in message_history:
            if (msg.get('role') == role and 
                content_substring in str(msg.get('content', ''))):
                return True
        raise AssertionError(
            f""Message history does not contain {role} message with content '{content_substring}'""
        )",Assert that message history contains a message with the given role and content.,Verify message history includes specific role and content substring.
741,_convert_floats_to_decimals,"def _convert_floats_to_decimals(obj: Any) -> Any:
    
    if isinstance(obj, float):
        if obj != obj or obj == float(""inf"") or obj == float(""-inf""):
            raise ValueError(f""Cannot convert non-finite float {obj} to Decimal for DynamoDB"")
        try:
            return Decimal(str(obj))
        except InvalidOperation:
            raise ValueError(f""Could not convert float {obj} to Decimal"")
    elif isinstance(obj, dict):
        return {k: _convert_floats_to_decimals(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_convert_floats_to_decimals(elem) for elem in obj]
    else:
        return obj",Recursively walk through obj and convert any finite float to a Decimal.,"Convert floats to Decimals for DynamoDB, handling non-finite values and nested structures."
742,external_model_cost,"def external_model_cost(model, completion_window=""*"", provider=""default""):
    
    if provider not in _DEFAULT_COST_MAP[""external""][""providers""]:
        return {""input_cost_per_token"": None, ""output_cost_per_token"": None}
    provider_cost = _DEFAULT_COST_MAP[""external""][""providers""][provider][""cost""]
    if model not in provider_cost:
        raise KeyError(f""Model {model} is not supported by {provider}."")

    if completion_window not in provider_cost[model][""input_cost_per_million""]:
        raise KeyError(f""Completion window {completion_window} is not supported for {model} by {provider}."")

    cost = provider_cost[model][""input_cost_per_million""][completion_window]
    return {""input_cost_per_token"": cost / 1e6, ""output_cost_per_token"": cost / 1e6}",Get the cost of the model from the external providers registered.,Determine token cost for a model based on provider and completion window.
743,_generate_story,"def _generate_story(self, family: set[Person]) -> str:
        
        story_parts = []

        # Find married couples
        couples = set()
        for person in family:
            if person.spouse and (person.spouse, person) not in couples:
                couples.add((person, person.spouse))

        # Describe marriages and children for each couple
        described_children = set()  # Track which children have been described
        for person1, person2 in couples:
            story_parts.append(f""{person1.name} is married to {person2.name}."")

            # Only describe children once per couple
            children = [c for c in person1.children if c not in described_children]
            if children:
                children_names = [c.name for c in children]
                described_children.update(children)  # Mark these children as described

                if len(children_names) == 1:
                    story_parts.append(f""They have a child called {children_names[0]}."")
                else:
                    *first, last = children_names
                    children_str = "", "".join(first) + f"" and {last}""
                    story_parts.append(f""They have children called {children_str}."")

        return "" "".join(story_parts)",Generate a story describing the family relationships,Generate a narrative detailing family relationships and offspring from a set of individuals.
744,_configure_window,"def _configure_window(self):
        
        self.title(""ClipCascade"")
        self.geometry(
            ""600x300+{}+{}"".format(
            )
        )
        self.attributes(""-topmost"", True)
        self.resizable(False, False)
        self.focus_force()",Configure the dialog window properties.,"Configure a non-resizable, always-on-top window titled ""ClipCascade""."
745,check_following,"def check_following(self, value):
        
        try:
            lang = langdetect.detect(value)
        except:
            print(""Failed to detect language, got value:"", value)
            lang = 'en'
        if lang == ""th"":
            # print(f""shervin4. lang is {lang}"")
            # print(value)
            num_words = len(word_tokenize_thai(value))
            # print(f""num words: {num_words}"")
        elif lang in [""zh"", ""zh-cn"", ""zh-tw"", ""ja""]:
            # print(f""shervin5. lang is {lang}"")
            # print(value)
            num_words = count_words_chinese_japanese(value)
            # print(f""num words: {num_words}"")
        else:
            # print(f""shervin6. lang is {lang}"")
            # print(value)
            num_words = count_words(value)
            # print(f""num words: {num_words}"")

        if self._comparison_relation == _COMPARISON_RELATION[0]:
            return num_words < self._num_words
        elif self._comparison_relation == _COMPARISON_RELATION[1]:
            return num_words >= self._num_words",Checks if the response contains the expected number of words.,Detect language and compare word count against a threshold
746,load_2024_dataset,"def load_2024_dataset() -> list[dict]:
    
    dataset_original = load_dataset(""AI-MO/aimo-validation-aime"")
    # Filter out problems that are not from 2024
    dataset = dataset_original[""train""].filter(lambda example: ""2024"" in example[""url""])
    logging.debug(f""Filtered dataset size: {len(dataset)}."")
    assert len(dataset) == 30, f""Expected 30 problems after filtering by 2024, but found {len(dataset)}""
    return dataset",Load the dataset of problems.,Filter and return 2024-specific dataset entries from a larger dataset.
747,_setup_routes,"def _setup_routes(self):
        

        @self.router.post(f""/{self.provider_route_name}/chat/completions"")
        @self.router.post(f""/{self.provider_route_name}/completions"")
        @self.router.post(f""/{self.provider_route_name}/v1/chat/completions"")
        @DetectClient()
        async def create_completion(
            request: Request,
            authorization: str = Header(..., description=""Bearer token""),
        ):
            if not authorization.startswith(""Bearer ""):
                raise HTTPException(status_code=401, detail=""Invalid authorization header"")

            api_key = authorization.split("" "")[1]
            body = await request.body()
            req = ChatCompletionRequest.model_validate_json(body)
            is_fim_request = FIMAnalyzer.is_fim_request(request.url.path, req)

            if not req.stream:
                logger.warn(""We got a non-streaming request, forcing to a streaming one"")
                req.stream = True

            return await self.process_request(
                req,
                api_key,
                self.base_url,
                is_fim_request,
                request.state.detected_client,
            )",Sets up the /chat/completions route for the provider as expected by the,Define asynchronous API endpoints for processing chat completion requests with authorization validation.
748,get_account_balance,"def get_account_balance(account_id: str, **kwargs) -> str:
            

            logger.info(f""Looking up account balance for {account_id}"")
            context[""current_task""] = """"
            context[""task_result""] = ""Account has a balance of $1000""

            return f""Account {account_id} has a balance of $1000""",Useful for looking up an account balance.,Retrieve and log the balance for a specified account ID.
749,_handle_done,"def _handle_done(webui_manager: WebuiManager, history: AgentHistoryList):
    
    logger.info(
        f""Agent task finished. Duration: {history.total_duration_seconds():.2f}s, Tokens: {history.total_input_tokens()}""
    )
    final_summary = ""**Task Completed**\n""
    final_summary += f""- Duration: {history.total_duration_seconds():.2f} seconds\n""
    final_summary += f""- Total Input Tokens: {history.total_input_tokens()}\n""  # Or total tokens if available

    final_result = history.final_result()
    if final_result:
        final_summary += f""- Final Result: {final_result}\n""

    errors = history.errors()
    if errors and any(errors):
        final_summary += f""- **Errors:**\n```\n{errors}\n```\n""
    else:
        final_summary += ""- Status: Success\n""

    webui_manager.bu_chat_history.append(
        {""role"": ""assistant"", ""content"": final_summary}
    )",Callback when the agent finishes the task (success or failure).,Log and summarize agent task completion details for UI display.
750,generate_metrics_report,"def generate_metrics_report(stats: Dict, output_dir: str, annotator_names: List[str], skip_entities: Set[str] = set()):
    
    with open(os.path.join(output_dir, 'metrics.txt'), 'w') as f:
        f.write(""=== Overall Metrics by Annotator ===\n\n"")
        
        # Add information about skipped entities
        if skip_entities:
            f.write(f""Note: The following entity types were excluded from evaluation:\n"")
            f.write(f""{', '.join(sorted(skip_entities))}\n\n"")
            f.write(""="" * 50 + ""\n\n"")
        
        for annotator in annotator_names:
            if annotator in stats['metrics_per_annotator']:
                metrics = stats['metrics_per_annotator'][annotator]
                f.write(f""\n{annotator.upper()}:\n"")
                f.write(f""  Total Entities in Ground Truth: {metrics['total_entities']}\n"")
                f.write(f""  True Positives: {metrics['true_positives']}\n"")
                f.write(f""  False Positives: {metrics['false_positives']}\n"")
                f.write(f""  False Negatives: {metrics['false_negatives']}\n"")
                f.write(f""  Precision: {metrics['precision']:.4f}\n"")
                f.write(f""  Recall: {metrics['recall']:.4f}\n"")
                f.write(f""  F1 Score: {metrics['f1_score']:.4f}\n"")
                f.write(f""  F2 Score: {metrics['f2_score']:.4f}\n"")",Generate overall metrics report,"Generate a metrics report for annotators, excluding specified entities, and save to a file."
751,_new_video,"def _new_video(self, path):
        
        self.frame = 0
        self.cap = cv2.VideoCapture(path)
        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))
        if not self.cap.isOpened():
            raise FileNotFoundError(f""Failed to open video {path}"")
        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)",Creates a new video capture object for the given path and initializes video-related attributes.,Initialize video capture and validate file accessibility.
752,_benchmark_image,"def _benchmark_image(self, shortname, resolution):
        
        if not self.benchmark_exists():
            return None
        base_model_benchmark = self._benchmark_path(""base_model"")
        benchmark_image = None
        _test_filename = f""{shortname}_{resolution[0]}x{resolution[1]}.png""
        for _benchmark_image in os.listdir(base_model_benchmark):
            _basename = os.path.basename(_benchmark_image)
            if _basename == _test_filename:
                benchmark_image = Image.open(
                    os.path.join(base_model_benchmark, _benchmark_image)
                )
                break

        return benchmark_image",We will retrieve the benchmark image for the shortname.,Retrieve a benchmark image file based on name and resolution if it exists
753,_make_renaming_matrices,"def _make_renaming_matrices():
    
    # As the atom naming is ambiguous for 7 of the 20 amino acids, provide
    # alternative groundtruth coordinates where the naming is swapped
    restype_3 = [restype_1to3[res] for res in restypes]
    restype_3 += [""UNK""]
    # Matrices for renaming ambiguous atoms.
    all_matrices = {res: np.eye(14, dtype=np.float32) for res in restype_3}
    for resname, swap in residue_atom_renaming_swaps.items():
        correspondences = np.arange(14)
        for source_atom_swap, target_atom_swap in swap.items():
            source_index = restype_name_to_atom14_names[resname].index(source_atom_swap)
            target_index = restype_name_to_atom14_names[resname].index(target_atom_swap)
            correspondences[source_index] = target_index
            correspondences[target_index] = source_index
            renaming_matrix = np.zeros((14, 14), dtype=np.float32)
            for index, correspondence in enumerate(correspondences):
                renaming_matrix[index, correspondence] = 1.0
        all_matrices[resname] = renaming_matrix.astype(np.float32)
    renaming_matrices = np.stack([all_matrices[restype] for restype in restype_3])
    return renaming_matrices",Matrices to map atoms to symmetry partners in ambiguous case.,Generate matrices to handle ambiguous atom naming in amino acids.
754,save_schedules,"def save_schedules():
    
    try:
        ensure_config_dir()
        
        # Get schedule data from request
        schedules = request.json
        
        if not schedules or not isinstance(schedules, dict):
            return jsonify({""error"": ""Invalid schedule data format""}), 400
        
        # Save to file
        with open(SCHEDULE_FILE, 'w') as f:
            json.dump(schedules, f, indent=2)
        
        scheduler_logger.info(f""Saved schedules to {SCHEDULE_FILE}"")
        
        # Add timestamp to response
        response_data = {
            ""success"": True,
            ""message"": ""Schedules saved successfully"",
            ""timestamp"": datetime.now().isoformat(),
            ""file"": SCHEDULE_FILE
        }
        
        # Add CORS headers
        response = Response(json.dumps(response_data))
        response.headers['Content-Type'] = 'application/json'
        response.headers['Access-Control-Allow-Origin'] = '*'
        return response
    
    except Exception as e:
        error_msg = f""Error saving schedules: {str(e)}""
        scheduler_logger.error(error_msg)
        return jsonify({""error"": error_msg}), 500",Save schedules to the JSON file,"Handle and save schedule data from a request, ensuring proper format and logging."
755,upload_file,"def upload_file(server_url: str, file_path: str):
    
    try:
        with open(file_path, ""rb"") as file:
            files = {""file"": (file_path, file, ""application/octet-stream"")}
            data = {""purpose"": ""unknown""}

            with httpx.Client() as client:
                response = client.post(server_url, files=files, data=data)

                if response.status_code == 200:
                    print(""File uploaded successfully:"", response.json())
                else:
                    print(""Failed to upload file:"", response.text)
    except Exception as e:
        print(f""Error: {e}"")",Uploads a file to the production stack.,Uploads a file to a server and handles success or error responses.
756,_sanitize_results_dict,"def _sanitize_results_dict(self) -> Tuple[Dict[str, str], Dict[str, Any]]:
        
        _results = copy.deepcopy(self.results.get(""results"", dict()))

        # Remove None from the metric string name
        tmp_results = copy.deepcopy(_results)
        for task_name in self.task_names:
            task_result = tmp_results.get(task_name, dict())
            for metric_name, metric_value in task_result.items():
                _metric_name, removed = remove_none_pattern(metric_name)
                if removed:
                    _results[task_name][_metric_name] = metric_value
                    _results[task_name].pop(metric_name)

        # remove string valued keys from the results dict
        wandb_summary = {}
        for task in self.task_names:
            task_result = _results.get(task, dict())
            for metric_name, metric_value in task_result.items():
                if isinstance(metric_value, str):
                    wandb_summary[f""{task}/{metric_name}""] = metric_value

        for summary_metric, summary_value in wandb_summary.items():
            _task, _summary_metric = summary_metric.split(""/"")
            _results[_task].pop(_summary_metric)

        tmp_results = copy.deepcopy(_results)
        for task_name, task_results in tmp_results.items():
            for metric_name, metric_value in task_results.items():
                _results[f""{task_name}/{metric_name}""] = metric_value
                _results[task_name].pop(metric_name)
        for task in self.task_names:
            _results.pop(task)

        return wandb_summary, _results",Sanitize the results dictionary.,"Cleans and restructures task results, removing invalid entries and summarizing metrics."
757,bring_to_front,"def bring_to_front() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Page.bringToFront"",
    }
    json = yield cmd_dict",Brings page to front (activates tab).,Generates a command to bring a webpage to the front in a browser context.
758,clear_geolocation_override,"def clear_geolocation_override() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Emulation.clearGeolocationOverride"",
    }
    json = yield cmd_dict",Clears the overridden Geolocation Position and Error.,Disables geolocation simulation by sending a command to clear overrides.
759,describe_keyspace,"def describe_keyspace(self, keyspace_name: str) -> Dict[str, Any]:
        
        try:
            query = 'SELECT * FROM system_schema.keyspaces WHERE keyspace_name = %s'
            row = self.session.execute(query, [keyspace_name]).one()

            if not row:
                raise RuntimeError(f'Keyspace not found: {keyspace_name}')

            keyspace_details = {
                'name': row.keyspace_name,
                'replication': row.replication,
                'durable_writes': row.durable_writes,
            }

            # Add tables
            keyspace_details['tables'] = self.list_tables(keyspace_name)

            # Add Keyspaces-specific context if applicable
            if self.is_keyspaces:
                self._add_keyspaces_context(keyspace_details)

            return keyspace_details
        except Exception as e:
            logger.error(f'Error describing keyspace {keyspace_name}: {str(e)}')
            raise RuntimeError(f'Failed to describe keyspace {keyspace_name}: {str(e)}')",Get detailed information about a keyspace.,Retrieve and return detailed metadata for a specified database keyspace.
760,mock_response_side_effect,"def mock_response_side_effect(*args: Any, **kwargs: Any) -> requests.Response:
            
            mock_response = requests.Response()
            mock_response.status_code = 200
            
            # Get the batch size from the request
            batch_input = kwargs.get('json', {}).get('input', [])
            batch_size = len(batch_input)
            
            response_data = {
                ""object"": ""list"",
                ""data"": [
                    {""object"": ""embedding"", ""index"": i, ""embedding"": [0.1 + i * 0.1] * 1024}
                    for i in range(batch_size)
                ],
                ""model"": ""jina-embeddings-v3"",
                ""usage"": {""total_tokens"": batch_size * 10, ""prompt_tokens"": batch_size * 10}
            }
            mock_response._content = json.dumps(response_data).encode()
            return mock_response",Create response based on batch size.,Simulate API response generating embeddings for input batch requests.
761,pop_pending_reply,"def pop_pending_reply(self):
        
        data = self.read_data()

        # Find first pending reply that's not being processed
        for reply in data[""pending_replies""]:
            tweet_id = reply[""tweet_id""]
            if tweet_id not in data.get(""processing_replies"", {}):
                logger.debug(f""Found unprocessed reply {tweet_id}, marking as processing"")
                data[""processing_replies""][tweet_id] = {""data"": reply, ""started_at"": datetime.now().isoformat()}
                self.write_data(data)

                return {""message_id"": tweet_id, ""data"": json.dumps(reply)}

        logger.debug(""No pending replies found"")
        return None",Get next pending reply and mark as processing,Identify and process the first unprocessed pending reply from stored data.
762,get_platform,"def get_platform():
    
    if sys.platform.startswith(""linux""):
        return ""linux_x86_64""
    elif sys.platform == ""darwin"":
        mac_version = ""."".join(platform.mac_ver()[0].split(""."")[:2])
        return f""macosx_{mac_version}_x86_64""
    elif sys.platform == ""win32"":
        return ""win_amd64""
    else:
        raise ValueError(""Unsupported platform: {}"".format(sys.platform))",Returns the platform name as used in wheel filenames.,Determine system platform and return corresponding architecture identifier.
763,release_object,"def release_object(
    object_id: RemoteObjectId,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""objectId""] = object_id.to_json()
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Runtime.releaseObject"",
        ""params"": params,
    }
    json = yield cmd_dict",Releases remote object with given id.,Generate a command to release a remote object by its ID.
764,create_query_pipeline,"def create_query_pipeline(self) -> Pipeline:
        
        try:
            pipeline = Pipeline()

            # Create and add components
            embedder = self.component_factory.create_embedder()
            retriever = self.component_factory.create_retriever()
            llm_generator = self.component_factory.create_chat_generator()

            pipeline.add_component(""embedder"", embedder)
            pipeline.add_component(""retriever"", retriever)
            pipeline.add_component(
                ""prompt_builder"", ChatPromptBuilder(template=self.template)
            )
            pipeline.add_component(""llm"", llm_generator)

            # Connect components
            pipeline.connect(""embedder.embedding"", ""retriever.query_embedding"")
            pipeline.connect(""retriever"", ""prompt_builder.documents"")
            pipeline.connect(""prompt_builder.prompt"", ""llm.messages"")

            self.query_pipeline = pipeline
            return pipeline

        except Exception as e:
            self.logger.error(f""Pipeline creation failed: {str(e)}"", exc_info=True)
            raise",Initialize and configure the query pipeline components.,"Constructs a query processing pipeline with embedding, retrieval, and generation components."
765,cleanup,"def cleanup(self):
        
        self.kubectl.delete_namespace(self.namespace)
        time.sleep(10)
        pvs = self.kubectl.exec_command(
            ""kubectl get pv --no-headers | grep 'test-hotel-reservation' | awk '{print $1}'""
        ).splitlines()

        for pv in pvs:
            # Check if the PV is in a 'Terminating' state and remove the finalizers if necessary
            self._remove_pv_finalizers(pv)
            delete_command = f""kubectl delete pv {pv}""
            delete_result = self.kubectl.exec_command(delete_command)
            print(f""Deleted PersistentVolume {pv}: {delete_result.strip()}"")
        time.sleep(5)",Delete the entire namespace for the hotel reservation application.,Clean up Kubernetes namespace and delete associated persistent volumes.
766,retrieve_insight_memory,"def retrieve_insight_memory(self):
        
        if not self.retrieve_insight_top_k:
            return

        filter_dict = {
            ""user_name"": self.user_name,
            ""target_name"": self.target_name,
            ""store_status"": StoreStatusEnum.VALID.value,
            ""memory_type"": MemoryTypeEnum.INSIGHT.value,
        }
        nodes: List[MemoryNode] = self.memory_store.retrieve_memories(top_k=self.retrieve_insight_top_k,
                                                                      filter_dict=filter_dict)
        self.memory_manager.set_memories(INSIGHT_NODES, nodes)",Retrieves top-K insight memories based on the query and stores them in the memory handler.,Retrieve and store top insight memories based on user and target criteria.
767,on_train_end,"def on_train_end(trainer):
    
    _log_plots(trainer.validator.plots, step=trainer.epoch + 1)
    _log_plots(trainer.plots, step=trainer.epoch + 1)
    art = wb.Artifact(type=""model"", name=f""run_{wb.run.id}_model"")
    if trainer.best.exists():
        art.add_file(trainer.best)
        wb.run.log_artifact(art, aliases=[""best""])
    # Check if we actually have plots to save
    if trainer.args.plots and hasattr(trainer.validator.metrics, ""curves_results""):
        for curve_name, curve_values in zip(trainer.validator.metrics.curves, trainer.validator.metrics.curves_results):
            x, y, x_title, y_title = curve_values
            _plot_curve(
                x,
                y,
                names=list(trainer.validator.metrics.names.values()),
                id=f""curves/{curve_name}"",
                title=curve_name,
                x_title=x_title,
                y_title=y_title,
            )
    wb.run.finish()",Save the best model as an artifact at end of training.,"Log training results, save best model, and plot validation metrics at training end."
768,get_database_config,"def get_database_config() -> Dict[str, Union[str, int]]:
    
    # Database connection parameters
    # You can modify these or use environment variables
    return {
        ""host"": os.getenv(""POSTGRES_HOST"", ""localhost""),
        ""port"": int(os.getenv(""POSTGRES_PORT"", ""5432"")),
        ""database"": os.getenv(""POSTGRES_DB"", ""chonkie_demo""),
        ""user"": os.getenv(""POSTGRES_USER"", ""postgres""),
        ""password"": os.getenv(""POSTGRES_PASSWORD"", ""postgres""),
    }",Get database configuration from environment variables or defaults.,Function retrieves database connection settings with environment variable support.
769,mock_os_path,"def mock_os_path():
    
    with (
        patch('os.path.exists') as mock_exists,
        patch('os.path.isdir') as mock_isdir,
        patch('os.path.isabs') as mock_isabs,
    ):
        # Default return values
        mock_exists.return_value = True
        mock_isdir.return_value = True
        mock_isabs.return_value = True

        yield {'exists': mock_exists, 'isdir': mock_isdir, 'isabs': mock_isabs}",Create a mock os.path module.,Simulate file path checks by mocking os.path methods for testing.
770,normalize_annotations,"def normalize_annotations(df: pd.DataFrame, annotator_config: Dict[str, Dict[str, str]], skip_entities: Set[str] = set()) -> pd.DataFrame:
    
    # First normalize ground truth
    ground_truth_entities = df['target_text'].apply(lambda x: find_entities_with_positions(x, skip_entities))
    df['span_labels'] = df.apply(lambda row: generate_span_labels(row['target_text'], ground_truth_entities[row.name]), axis=1)
    df['mbert_bio_labels'] = df.apply(lambda row: generate_bio_labels(row['target_text'], ground_truth_entities[row.name]), axis=1)
    
    # Then normalize each annotator's data
    for annotator, config in annotator_config.items():
        target_col = config['target_text']
        if target_col not in df.columns:
            print(f""Warning: Column {target_col} not found for annotator {annotator}"")
            continue
            
        # Fill NaN values with empty string to avoid errors
        df[target_col] = df[target_col].fillna("""")
            
        # Generate entities and labels
        annotator_entities = df[target_col].apply(lambda x: find_entities_with_positions(x, skip_entities))
        df[f'span_labels_{annotator}'] = df.apply(
            lambda row: generate_span_labels(row[target_col], annotator_entities[row.name]), 
            axis=1
        )
        df[f'mbert_bio_labels_{annotator}'] = df.apply(
            lambda row: generate_bio_labels(row[target_col], annotator_entities[row.name]),
            axis=1
        )
    
    return df",Normalize annotations for ground truth and all annotators.,Normalize text annotations and generate labels for multiple annotators in a DataFrame.
771,get_capabilities_description,"def get_capabilities_description(self) -> str:
        
        if not self.servers:
            return ""No MCP servers available.""
            
        description_parts = []
        
        for server_name, server in self.servers.items():
            if not server.connected:
                description_parts.append(f""## {server_name}\nServer connection failed or not established.\n"")
                continue
                
            server_description = f""## {server_name}\n""
            
            if server.config.description:
                server_description += f""{server.config.description}\n\n""
                
            if server.tools:
                server_description += ""### Tools\n""
                for tool in server.tools:
                    server_description += f""- {server_name}.{tool.name}: {tool.description or 'No description'}\n""
                server_description += ""\n""
                
            if server.resources:
                server_description += ""### Resources\n""
                for resource in server.resources:
                    server_description += f""- {resource.uri}: {resource.name or 'No name'} - {resource.description or 'No description'}\n""
                server_description += ""\n""
                
            if server.prompts:
                server_description += ""### Prompts\n""
                for prompt in server.prompts:
                    server_description += f""- {prompt.name}: {prompt.description or 'No description'}\n""
                server_description += ""\n""
                
            description_parts.append(server_description)
            
        return ""\n"".join(description_parts)",Get a description of all capabilities,"Generate a detailed description of server capabilities, tools, resources, and prompts."
772,get_search_queries_prompt,"def get_search_queries_prompt() -> str:
    
    current_date = datetime.now(timezone.utc).strftime(""%B %d, %Y"")

    return f",Generates the prompt for refining search queries.,Generate a prompt for search queries using the current date.
773,communication_data,"def communication_data() -> Dict[str, Any]:
    
    return {
        ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
        ""body"": ""My EC2 instance i-1234567890abcdef0 is not starting."",
        ""submittedBy"": ""user@example.com"",
        ""timeCreated"": ""2023-01-01T12:00:00Z"",
        ""attachmentSet"": None,
    }",Return a dictionary with sample communication data.,"Returns a dictionary with support case details including ID, message, and metadata."
774,prepare_prompt,"def prepare_prompt(self, base_prompt: str) -> Tuple[str, Optional[str]]:
        
        prompt_body = base_prompt

        # 1.  {memory_context} placeholder
        if ""{memory_context}"" in prompt_body:
            prompt_body = prompt_body.replace(
                ""{memory_context}"",
                self.get_comprehensive_memory_string()
            )

        # 2.  Dialogue prefix
        additional_prefix = None
        dialogue_line = self.get_mapped_dialogue_event_for_prompt()
        if dialogue_line:
            additional_prefix = f""Previous Dialogue Context: {dialogue_line}\\n\\n""

        return prompt_body, additional_prefix","Return (prompt_body, additional_prefix) ready for the LLM.",Enhance prompt with memory context and dialogue prefix for contextual output
775,create_config_table,"def create_config_table(
    mode: str,
    model_name: str,
    vision_model_name: str | None,
    max_iterations: int,
    compact_every_n_iteration: int | None,
    max_tokens_working_memory: int | None,
) -> str:
    
    return (
        f""• Mode              [cyan]{mode}[/cyan]\n""
        f""• Language Model    [cyan]{model_name}[/cyan]\n""
        f""• Vision Model      {vision_model_name or '[dim]Not Configured[/dim]'}\n""
        f""• Max Iterations    {max_iterations}\n""
        f""• Memory Compaction {compact_every_n_iteration or '[dim]Default[/dim]'}\n""
        f""• Max Memory Tokens {max_tokens_working_memory or '[dim]Default[/dim]'}""
    )",Create a formatted string representation of the configuration table.,Generate a formatted configuration summary string for model settings.
776,_fmt_candidate,"def _fmt_candidate(doc: str, meta: Dict[str, Any]) -> str:
    
    meta_line = f""url={meta.get('url')} | method={meta.get('method', '').upper()} | name={meta.get('name')}""
    return f""{doc.strip()}\n{meta_line}""","Return a single nice, log-friendly candidate string.",Format document with metadata into a structured string output.
777,create_event,"def create_event(summary, description, start_time, end_time, timezone=TIMEZONE):
    
    service = get_calendar_service()
    
    event = {
        'summary': summary,
        'description': description,
        'start': {
            'dateTime': start_time.isoformat(),
            'timeZone': timezone,
        },
        'end': {
            'dateTime': end_time.isoformat(),
            'timeZone': timezone,
        },
    }

    event = service.events().insert(calendarId=CALENDAR_ID, body=event).execute()
    return event",Creates a new event in Google Calendar.,Create a calendar event with specified details and time zone.
778,load_checkpoint_with_inflation,"def load_checkpoint_with_inflation(model, ckpt_path):
    
    if ckpt_path.endswith("".pt"") or ckpt_path.endswith("".pth""):
        state_dict = find_model(ckpt_path)
        with torch.no_grad():
            for key in state_dict:
                if key in model:
                    # central inflation
                    if state_dict[key].size() == model[key][:, :, 0, :, :].size():
                        # temporal dimension
                        val = torch.zeros_like(model[key])
                        val[:, :, centre, :, :] = state_dict[key]
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        print(f""Missing keys: {missing_keys}"")
        print(f""Unexpected keys: {unexpected_keys}"")
    else:
        load_checkpoint(model, ckpt_path)","pre-train using image, then inflate to 3D videos",Load and adapt a model checkpoint with temporal dimension inflation if necessary.
779,register_interactive_handlers,"def register_interactive_handlers(app):
    

    @app.action(""login_button"")
    async def handle_login_button(ack, body, logger):
        
        await ack()
        logger.info(f""Login button clicked: {body.get('user', {}).get('id')}"")

    @app.action(""connect_mcp_server_button"")
    async def handle_connect_mcp_server_button(ack, body, logger):
        
        await ack()
        logger.info(f""Connect to MCP server button clicked: {body.get('user', {}).get('id')}"")",Register all interactive component handlers with the Slack app.,Register event handlers for login and server connection button actions in the app.
780,clear_placeholders,"def clear_placeholders(self, text: str) -> str:
        
        # Step 1: Find all unique {xx} placeholders (single braces only)
        matches = set(regex.findall(r""(?<!\{)\{([^\{\},\s]+)\}(?!\})"", text))

        for field in matches:
            # Pattern: only single-brace {field}, not {{field}} or {{{field}}}
            pattern = r""(?<!\{)\{"" + regex.escape(field) + r""\}(?!\})""

            def replacer(match):
                start, end = match.start(), match.end()
                before = text[start - 1] if start > 0 else """"
                after = text[end] if end < len(text) else """"

                replacement = field
                if before != ""`"":
                    replacement = ""`"" + replacement
                if after != ""`"":
                    replacement = replacement + ""`""

                return replacement

            text = regex.sub(pattern, replacer, text)

        return text","Find all {xx} placeholders in the text, and replace them with `xx`, adding backticks only if not already present.",Replace single-brace placeholders in text with backtick-enclosed versions.
781,load_parquet_database,"def load_parquet_database(self):
        
        if self.data_backend.exists(self.parquet_path):
            try:
                bytes_string = self.data_backend.read(self.parquet_path)
                import io

                pq = io.BytesIO(bytes_string)
            except Exception as e:
                raise e
            if self.is_json_lines or self.is_json_file:
                self.parquet_database = pd.read_json(pq, lines=self.is_json_lines)
            else:
                self.parquet_database = pd.read_parquet(pq, engine=""pyarrow"")
            self.parquet_database.set_index(
                self.parquet_config.get(""filename_column""), inplace=True
            )
        else:
            raise FileNotFoundError(
                f""Parquet could not be loaded from {self.parquet_path}: database file does not exist (path={self.parquet_path}).""
            )",Load the parquet database from file.,"Load and parse a Parquet database file, handling JSON formats if specified."
782,get_labels,"def get_labels(self):
        
        cache_path = Path(self.json_file).with_suffix('.cache')
        labels = np.load(str(cache_path), allow_pickle=True)
        self.verify_labels(labels)
        self.im_files = [str(label[""im_file""]) for label in labels]
        if LOCAL_RANK in {-1, 0}:
            LOGGER.info(f""Load {self.json_file} from cache file {cache_path}"") 
        return labels","Loads annotations from a JSON file, filters, and normalizes bounding boxes for each image.",Load and verify cached labels from a JSON file for processing
783,post_pr_comment,"def post_pr_comment(self, owner: str, repo: str, pr_number: int, comment: str) -> Dict[str, Any]:
        
        url = f""{self.config.github.base_url}/repos/{owner}/{repo}/issues/{pr_number}/comments""

        data = {""body"": comment}
        response = self.github_session.post(url, json=data)
        response.raise_for_status()

        return response.json()",Post a comment on the PR.,Post a comment on a GitHub pull request using API interaction.
784,update_watch_config,"def update_watch_config():
    
    try:
        new_watch_config = request.get_json()
        if not isinstance(new_watch_config, dict):
            return jsonify({""error"": ""Invalid watch config format""}), 400

        success, error_msg = save_watch_config_http(new_watch_config)
        if success:
            return jsonify({""message"": ""Watch configuration updated successfully""}), 200
        else:
            return jsonify(
                {""error"": ""Failed to update watch configuration"", ""details"": error_msg}
            ), 500
    except json.JSONDecodeError:
        return jsonify({""error"": ""Invalid JSON data for watch config""}), 400
    except Exception as e:
        logger.error(f""Error in POST/PUT /config/watch: {e}"", exc_info=True)
        return jsonify(
            {""error"": ""Failed to update watch configuration"", ""details"": str(e)}
        ), 500",Handles POST/PUT requests to update the watch configuration.,Handle watch configuration update with JSON validation and error logging.
785,mock_et_parse,"def mock_et_parse(self):
        
        with patch(""meta_agent.validation.ET.parse"") as mock_parse:
            # Configure the mock to return a coverage result
            mock_root = MagicMock()
            mock_root.attrib = {""line-rate"": ""0.95""}  # 95% coverage
            mock_tree = MagicMock()
            mock_tree.getroot.return_value = mock_root
            mock_parse.return_value = mock_tree
            yield mock_parse",Fixture for mocking ET.parse.,Mock XML parsing to simulate 95% coverage for testing purposes.
786,format_error,"def format_error(error: Exception, include_trace: bool = False) -> str:
		
		message = ''
		if isinstance(error, ValidationError):
			return f'{AgentError.VALIDATION_ERROR}\nDetails: {str(error)}'
		if isinstance(error, RateLimitError):
			return AgentError.RATE_LIMIT_ERROR
		if include_trace:
			return f'{str(error)}\nStacktrace:\n{traceback.format_exc()}'
		return f'{str(error)}'",Format error message based on error type and optionally include trace,Formats error messages with optional stack trace inclusion.
787,_load_config,"def _load_config(self) -> Dict[str, Any]:
        
        try:
            with open(self.config_path, 'r') as file:
                return yaml.safe_load(file)
        except FileNotFoundError:
            print(f""Warning: Logging config file not found at {self.config_path}"")
            return self._get_default_config()
        except Exception as e:
            print(f""Error loading logging config: {e}"")
            return self._get_default_config()",Load configuration from YAML file,Load configuration from file or use defaults on failure
788,get_widths,"def get_widths(seq: Iterable[object]) -> dict[str | int, float]:
    
    widths: dict[int, float] = {}
    r: list[float] = []
    for v in seq:
        v = resolve1(v)
        if isinstance(v, list):
            if r:
                char1 = r[-1]
                for i, w in enumerate(v):
                    widths[cast(int, char1) + i] = w
                r = []
        elif isinstance(v, (int, float)):  # == utils.isnumber(v)
            r.append(v)
            if len(r) == 3:
                (char1, char2, w) = r
                if isinstance(char1, int) and isinstance(char2, int):
                    for i in range(cast(int, char1), cast(int, char2) + 1):
                        widths[i] = w
                else:
                    log.warning(
                        f""Skipping invalid font width specification for {char1} to {char2} because either of them is not an int""
                    )
                r = []
        else:
            log.warning(
                f""Skipping invalid font width specification for {v} because it is not a number or a list""
            )
    return cast(dict[str | int, float], widths)",Build a mapping of character widths for horizontal writing.,"Calculate character widths from a sequence, handling lists and numeric ranges."
789,has_unique_solution,"def has_unique_solution(puzzle: Puzzle, clues: Iterable[Clue]) -> bool:
    

    with puzzle.with_clues(clues):
        # print(f""Testing puzzle with {len(puzzle.clues)} clues"")
        solutions = itersolve(puzzle.as_cnf())
        _first_solution = next(solutions)

        # test if second solution exists or not; if it doesn't, uniquely solvable
        if next(solutions, None) is None:
            return True
        else:
            return False",Test if a puzzle has a unique solution under a given set of clues.,Determine if a puzzle has a unique solution using given clues.
790,load_config,"def load_config(self) -> BasicMemoryConfig:
        
        if self.config_file.exists():
            try:
                data = json.loads(self.config_file.read_text(encoding=""utf-8""))
                return BasicMemoryConfig(**data)
            except Exception as e:  # pragma: no cover
                logger.error(f""Failed to load config: {e}"")
                config = BasicMemoryConfig()
                self.save_config(config)
                return config
        else:
            config = BasicMemoryConfig()
            self.save_config(config)
            return config",Load configuration from file or create default.,"Load or initialize configuration from a file, handling errors gracefully."
791,cuda_to_int,"def cuda_to_int(cuda_str: str) -> int:
    
    if cuda_str == ""cuda"":
        return 0
    device = torch.device(cuda_str)
    if device.type != ""cuda"":
        raise ValueError(f""Device type must be 'cuda', got: {device.type}"")
    return device.index","Convert the string with format ""cuda:X"" to integer X.","Convert CUDA device string to integer index, ensuring valid CUDA type"
792,handle_code,"def handle_code(self, element: lxml_html.HtmlElement) -> str:
        
        if element.getparent() is not None and element.getparent().tag == ""pre"":
            return element.text_content()

        content = element.text_content()
        return f""`{content}`""",Handle inline code elements,Formats HTML code elements as plain text or inline code.
793,open_app,"def open_app(self, app_name: str) -> tuple[bool, str]:
        
        try:
            # Use the 'open' command
            result = subprocess.run(
                ['open', '-a', app_name], # Command: open -a ""AppName""
                check=True,
                capture_output=True,
                text=True,
                timeout=15
            )
            return True, f""Successfully opened/activated {app_name}.""

        except subprocess.CalledProcessError as e:
            stderr_output = e.stderr.strip() if e.stderr else f""'open -a {app_name}' command failed with no stderr.""
            if ""Unable to find application"" in stderr_output:
                error_msg = f""Error: Application '{app_name}' could not be found by the 'open' command.""
            else:
                error_msg = f""Error: Failed to open {app_name} using 'open' command. Details: {stderr_output}""
            return False, error_msg

        except FileNotFoundError:
            return False, ""Error: The 'open' command was not found on this system.""

        except subprocess.TimeoutExpired:
            return False, f""Error: Timeout waiting for {app_name} to respond via 'open' command.""

        except Exception as e:
            return False, f""Error: An unexpected error ({type(e).__name__}) occurred trying to open {app_name}.""",Open a macOS application using the 'open' command.,Attempts to launch an application and returns success status with a message.
794,display_feedback,"def display_feedback(side_bar: SideBar) -> None:
    
    if st.session_state.run_id is not None:
        feedback = streamlit_feedback(
            feedback_type=""faces"",
            optional_text_label=""[Optional] Please provide an explanation"",
            key=f""feedback-{st.session_state.run_id}"",
        )
        if feedback is not None:
            client = Client(
                remote_agent_engine_id=side_bar.remote_agent_engine_id,
                agent_callable_path=side_bar.agent_callable_path,
                url=side_bar.url_input_field,
                authenticate_request=side_bar.should_authenticate_request,
            )
            client.log_feedback(
                feedback_dict=feedback,
                run_id=st.session_state.run_id,
            )",Display a feedback component and log the feedback if provided.,Display and log user feedback using a sidebar interface and client logging system.
795,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""WebAudio.enable"",
    }
    json = yield cmd_dict",Enables the WebAudio domain and starts sending context lifetime events.,Initiates WebAudio feature activation via command dictionary exchange
796,_search_keyword,"def _search_keyword(self, log_line, keyword, ignore_keyword_time=False):
        
        if isinstance(keyword, dict):
            if keyword.get(""regex"") is not None:
                regex_keyword = keyword[""regex""]
                if ignore_keyword_time or time.time() - self.time_per_keyword.get(regex_keyword) >= int(self.notification_cooldown):
                    match = re.search(regex_keyword, log_line, re.IGNORECASE)
                    if match:
                        self.time_per_keyword[regex_keyword] = time.time()
                        return ""Regex-Pattern"" if keyword.get(""hide_pattern_in_title"", """").strip().lower() == ""true"" else f""Regex: {regex_keyword}""
            elif keyword.get(""keyword"") is not None:
                keyword = str(keyword[""keyword""])
        if isinstance(keyword, str):
            if ignore_keyword_time or time.time() - self.time_per_keyword.get(keyword) >= int(self.notification_cooldown):
                if keyword.lower() in log_line.lower():
                    self.time_per_keyword[keyword] = time.time()
                    return keyword
        return None",searches for keywords and regex patterns in the log entry it is given-,Searches log lines for keywords or regex patterns with cooldown management
797,create_case_response_data,"def create_case_response_data() -> Dict[str, Any]:
    
    return {
        ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
        ""status"": ""success"",
        ""message"": ""Support case created successfully with ID: case-12345678910-2013-c4c1d2bf33c5cf47"",
    }",Return a dictionary with sample create case response data.,Generate a dictionary with support case creation details
798,register,"def register(self, tool: Tool) -> None:
        
        try:
            key = (tool.toolbox_name or ""default"", tool.name)
            if key in self.tools:
                loguru.logger.debug(f""Tool '{tool.name}' in toolbox '{tool.toolbox_name or 'default'}' already registered."")
                return
            self.tools[key] = tool
            loguru.logger.debug(f""Tool registered: {tool.name} in toolbox {tool.toolbox_name or 'default'}"")
        except Exception as e:
            loguru.logger.error(f""Failed to register tool {tool.name}: {e}"")
            raise","Register a tool, checking for conflicts within the same toolbox.","Register a tool in a toolbox, logging success or failure."
799,get_video_info,"def get_video_info(video_path, prompt_text):
    
    cap = cv2.VideoCapture(str(video_path))

    if not cap.isOpened():
        print(f""Error: Could not open video {video_path}"")
        return None

    # Get video properties
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = frame_count / fps if fps > 0 else 0

    cap.release()

    return {
        ""path"": video_path.name,
        ""resolution"": {
            ""width"": width,
            ""height"": height
        },
        ""fps"": fps,
        ""duration"": duration,
        ""cap"": [prompt_text]
    }",Extract video information using OpenCV and corresponding prompt text,"Extract video metadata including resolution, fps, and duration from a file path."
800,sub_stracks,"def sub_stracks(tlista, tlistb):
        
        track_ids_b = {t.track_id for t in tlistb}
        return [t for t in tlista if t.track_id not in track_ids_b]",Filters out the stracks present in the second list from the first list.,Filter tracks from list A not in list B by track ID
801,log_mcp_message,"def log_mcp_message(direction: str, method: str, params: Any = None, result: Any = None, error: Any = None):
    
    message_parts = [f""MCP {direction} - Method: {method}""]
    
    if params:
        try:
            params_str = json.dumps(params, indent=2)
            message_parts.append(f""Params: {params_str}"")
        except:
            message_parts.append(f""Params: {params}"")
    
    if result:
        try:
            result_str = json.dumps(result, indent=2)
            message_parts.append(f""Result: {result_str}"")
        except:
            message_parts.append(f""Result: {result}"")
    
    if error:
        message_parts.append(f""Error: {error}"")
    
    logger.debug(""\n"".join(message_parts))",Log MCP communication in detail,"Log and format MCP communication details including method, parameters, result, and errors."
802,_make_request,"def _make_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:
        
        url = f""{self.base_url}{endpoint}""
        
        try:
            response = requests.request(method, url, headers=self.headers, **kwargs)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f""API request failed: {e}"")
            raise RuntimeError(f""API request failed: {e}"")
        except ValueError as e:
            logger.error(f""Failed to parse JSON response: {e}"")
            raise ValueError(f""Invalid JSON response: {e}"")",Make HTTP request with error handling.,"Handles API requests and parses JSON responses, logging errors on failure."
803,get_extra_params,"def get_extra_params(self) -> Dict[str, Any]:
        
        standard_fields = {
            ""prompt"",
            ""model"",
            ""n"",
            ""quality"",
            ""response_format"",
            ""size"",
            ""style"",
            ""user"",
        }
        return {k: v for k, v in self.model_dump().items() if k not in standard_fields}",Get all extra parameters that aren't part of the standard OpenAI API.,Extracts non-standard parameters from a model's data dictionary.
804,process_web_results,"def process_web_results(self, query: str, chat_history: Optional[List[Dict[str, str]]] = None) -> str:
        
        # print(f""[WebSearchProcessor] Fetching web search results for: {query}"")
        web_search_query_prompt = self._build_prompt_for_web_search(query=query, chat_history=chat_history)
        # print(""Web Search Query Prompt:"", web_search_query_prompt)
        web_search_query = self.llm.invoke(web_search_query_prompt)
        # print(""Web Search Query:"", web_search_query)
        
        # Retrieve web search results
        web_results = self.web_search_agent.search(web_search_query.content)

        # print(f""[WebSearchProcessor] Fetched results: {web_results}"")
        
        # Construct prompt to LLM for processing the results
        llm_prompt = (
            ""You are an AI assistant specialized in medical information. Below are web search results ""
            ""retrieved for a user query. Summarize and generate a helpful, concise response. ""
            ""Use reliable sources only and ensure medical accuracy.\n\n""
            f""Query: {query}\n\nWeb Search Results:\n{web_results}\n\nResponse:""
        )
        
        # Invoke the LLM to process the results
        response = self.llm.invoke(llm_prompt)
        
        return response","Fetches web search results, processes them using LLM, and returns a user-friendly response.","Process web search results to generate a concise, medically accurate response."
805,get_model_loader,"def get_model_loader(load_config: LoadConfig) -> BaseModelLoader:
    

    if isinstance(load_config.load_format, type):
        return load_config.load_format(load_config)

    if load_config.load_format == LoadFormat.AUTO:
        update_megatron_weight_loader()
        return MegatronLoader(load_config)

    # NOTE(sgm): change the weight_loader function in runtime
    if load_config.load_format == LoadFormat.MEGATRON:
        update_megatron_weight_loader()
        return MegatronLoader(load_config)

    if load_config.load_format == LoadFormat.HF:
        update_hf_weight_loader()
        return HFLoader(load_config)

    if load_config.load_format == LoadFormat.DTENSOR:
        update_dtensor_weight_loader()
        return DTensorLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_HF:
        update_hf_weight_loader()
        return DummyModelLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_MEGATRON:
        update_megatron_weight_loader()
        return DummyModelLoader(load_config)

    if load_config.load_format == LoadFormat.DUMMY_DTENSOR:
        update_dtensor_weight_loader()
        return DummyModelLoader(load_config)

    raise ValueError('load format not supported in verl: {}, only support {} and {}'.format(
        load_config.load_format, LoadFormat.MEGATRON, LoadFormat.HF))",Get a model loader based on the load format.,Determine and return the appropriate model loader based on the specified configuration format.
806,sort_nested_dict,"def sort_nested_dict(d: dict) -> dict:
    
    return {key: sort_nested_dict(value) if isinstance(value, dict) else value for key, value in sorted(d.items())}",Sorts a nested dictionary recursively.,Recursively sort a dictionary and its nested dictionaries by keys.
807,_download,"def _download(url: str, root: str):
    
    os.makedirs(root, exist_ok=True)
    filename = os.path.basename(url)

    expected_sha256 = url.split(""/"")[-2]
    download_target = os.path.join(root, filename)

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f""{download_target} exists and is not a regular file"")

    if os.path.isfile(download_target):
        if hashlib.sha256(open(download_target, ""rb"").read()).hexdigest() == expected_sha256:
            return download_target
        else:
            warnings.warn(f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file"")

    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:
        with tqdm(
            total=int(source.info().get(""Content-Length"")), ncols=80, unit=""iB"", unit_scale=True, unit_divisor=1024
        ) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                output.write(buffer)
                loop.update(len(buffer))

    if hashlib.sha256(open(download_target, ""rb"").read()).hexdigest() != expected_sha256:
        raise RuntimeError(""Model has been downloaded but the SHA256 checksum does not not match"")

    return download_target","Downloads a file from the provided URL to the root directory, ensuring file integrity via SHA256 checksum validation.","Download file from URL, verify SHA256 checksum, and handle existing files"
808,pause,"def pause() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Debugger.pause"",
    }
    json = yield cmd_dict",Stops on the next JavaScript statement.,Generate a command to pause the debugger using a JSON dictionary.
809,on_plot,"def on_plot(self, name, data=None):
        
        self.plots[Path(name)] = {""data"": data, ""timestamp"": time.time()}",Registers plots (e.g. to be consumed in callbacks).,Store plot data and timestamp in a dictionary using a path key.
810,build_faiss_index_on_disk,"def build_faiss_index_on_disk(self):
        
        print(""Handling FAISS index..."")

        index_path = os.path.join(self.index_folder, ""faiss_index.bin"")
        embeddings_path = os.path.join(self.index_folder,  ""bge_embeddings.h5"")

        if os.path.exists(index_path):
            print(f""Loading existing FAISS index for '{self.index_type}' from {index_path}..."")
            self.index = faiss.read_index(index_path)
        else:
            print(f""Building FAISS index for '{self.index_type}' from embeddings at {embeddings_path}..."")
            with h5py.File(embeddings_path, ""r"") as f:
                embeddings = f[""embeddings""][:].astype(np.float32)

            embedding_dim = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(embedding_dim)

            chunk_size = 50000
            for start in tqdm(range(0, embeddings.shape[0], chunk_size), desc=""Adding embeddings to FAISS index""):
                end = min(start + chunk_size, embeddings.shape[0])
                chunk = embeddings[start:end]
                self.index.add(chunk)

            print(f""Saving FAISS index for '{self.index_type}' to {index_path}..."")
            faiss.write_index(self.index, index_path)

        print(f""FAISS index loaded with {self.index.ntotal} embeddings."")",Builds or loads a **FAISS index** for efficient **document retrieval**.,Manage FAISS index by loading or building from embeddings and saving to disk.
811,save_predicted_images,"def save_predicted_images(self, save_path="""", frame=0):
        
        im = self.plotted_img

        # Save videos and streams
        if self.dataset.mode in {""stream"", ""video""}:
            fps = self.dataset.fps if self.dataset.mode == ""video"" else 30
            frames_path = f'{save_path.split(""."", 1)[0]}_frames/'
            if save_path not in self.vid_writer:  # new video
                if self.args.save_frames:
                    Path(frames_path).mkdir(parents=True, exist_ok=True)
                suffix, fourcc = ("".mp4"", ""avc1"") if MACOS else ("".avi"", ""WMV2"") if WINDOWS else ("".avi"", ""MJPG"")
                self.vid_writer[save_path] = cv2.VideoWriter(
                    filename=str(Path(save_path).with_suffix(suffix)),
                    fourcc=cv2.VideoWriter_fourcc(*fourcc),
                    fps=fps,  # integer required, floats produce error in MP4 codec
                    frameSize=(im.shape[1], im.shape[0]),  # (width, height)
                )

            # Save video
            self.vid_writer[save_path].write(im)
            if self.args.save_frames:
                cv2.imwrite(f""{frames_path}{frame}.jpg"", im)

        # Save images
        else:
            cv2.imwrite(str(Path(save_path).with_suffix("".jpg"")), im)",Save video predictions as mp4 at specified path.,Save predicted images or videos based on dataset mode and user settings.
812,_request_id,"def _request_id(headers: list[str]) -> str:
        
        for header in headers:
            if header.startswith(""x-request-id""):
                print(f""Request ID found in headers: {header}"")
                return header.split("":"")[1].strip()
        print(""No request ID found in headers"")
        return """"",Extracts the request ID from the headers,Extracts and returns the request ID from HTTP headers if present
813,search_weather,"def search_weather(city: str) -> str:
    
    weather_data = {""london"": ""rainy"", ""paris"": ""sunny""}
    return weather_data.get(city.lower(), ""Unknown"")",Get weather information for a city.,Retrieve weather condition for a specified city from predefined data
814,read,"def read(self, s3_key):
        
        for i in range(self.read_retry_limit):
            try:
                response = self.client.get_object(
                    Bucket=self.bucket_name, Key=str(s3_key)
                )
                return response[""Body""].read()
            except self.client.exceptions.NoSuchKey:
                logger.debug(
                    f""File {s3_key} does not exist in S3 bucket ({self.bucket_name})""
                )
                return None
            except (NoCredentialsError, PartialCredentialsError) as e:
                raise e  # Raise credential errors to the caller
            except Exception as e:
                logger.error(f'Error reading S3 bucket key ""{s3_key}"": {e}')
                if i == self.read_retry_limit - 1:
                    # We have reached our maximum retry count.
                    raise e
                else:
                    # Sleep for a bit before retrying.
                    time.sleep(self.read_retry_interval)
            except:
                if i == self.read_retry_limit - 1:
                    # We have reached our maximum retry count.
                    raise
                else:
                    # Sleep for a bit before retrying.
                    time.sleep(self.read_retry_interval)",Retrieve and return the content of the file from S3.,Attempt to read an S3 object with retries and error handling
815,select,"def select(self, keys: List[str]):
        
        with self.disable_validation():
            keys = set(keys)
            return SequenceSample(
                keys=keys,
                dtypes={key: self.dtypes[key] for key in keys},
                trailing_shapes={key: self.trailing_shapes[key] for key in keys},
                ids=self.ids,
                seqlens={key: self.seqlens[key] for key in keys},
                data=(
                    None if self.data is None else {key: self.data[key] for key in keys}
                ),
                metadata=self.metadata,
            )",Select a subset of keys inside the SequenceSample.,Create a sequence sample with selected keys and associated attributes
816,from_otel,"def from_otel(cls, kind: Any | None) -> ""SpanKind"":
        
        if kind is None:
            return cls.INTERNAL

        mapping = {
            0: cls.INTERNAL,
            1: cls.SERVER,
            2: cls.CLIENT,
            3: cls.PRODUCER,
            4: cls.CONSUMER,
        }
        return mapping.get(kind.value, cls.INTERNAL)",Convert from OpenTelemetry SpanKind.,Map OpenTelemetry kind to SpanKind enumeration with default.
817,bracket,"def bracket(self):
        
        if self.tag == ""td"":
            result = '""tag"": %s, ""colspan"": %d, ""rowspan"": %d, ""text"": %s' % (self.tag, self.colspan, self.rowspan, self.content)
        else:
            result = '""tag"": %s' % self.tag
        for child in self.children:
            result += child.bracket()
        return ""{{{}}}"".format(result)",Show tree using brackets notation,Format HTML table cell attributes and recursively process child elements.
818,clear,"def clear(self) -> None:
        
        file_types: list[Literal[""client_info"", ""tokens""]] = [""client_info"", ""tokens""]
        for file_type in file_types:
            path = self._get_file_path(file_type)
            path.unlink(missing_ok=True)
        logger.info(f""Cleared OAuth cache for {self.get_base_url(self.server_url)}"")",Clear all cached data for this server.,Remove specified OAuth cache files and log the action.
819,_extract_multi_part_zip,"def _extract_multi_part_zip(self, folder):
        
        zip_parts = sorted([f for f in os.listdir(folder) if f.startswith(""wiki.zip."")], key=lambda x: int(x.split('.')[-1]))
        combined_zip_path = os.path.join(folder, ""combined.zip"")

        print(f""Combining {len(zip_parts)} parts into {combined_zip_path}..."")
        with open(combined_zip_path, ""wb"") as combined:
            for part in zip_parts:
                part_path = os.path.join(folder, part)
                print(f""Adding {part_path} to {combined_zip_path}..."")
                with open(part_path, ""rb"") as part_file:
                    combined.write(part_file.read())

        print(f""Extracting combined ZIP file: {combined_zip_path}..."")
        try:
            with zipfile.ZipFile(combined_zip_path, ""r"") as zip_ref:
                zip_ref.extractall(folder)
        except zipfile.BadZipFile:
            print(""Error: Combined file is not a valid ZIP. Please verify the downloaded parts."")
            raise
        finally:
            os.remove(combined_zip_path)",Extracts **multi-part ZIP files** by **combining** and **decompressing** them.,Combine and extract multi-part ZIP files from a specified directory.
820,create_auth_header,"def create_auth_header(
    entity_type: str = ""developer"", permissions: list = None, expired: bool = False
) -> Dict[str, str]:
    
    token = create_test_token(entity_type, permissions=permissions, expired=expired)
    return {""Authorization"": f""Bearer {token}""}",Create authorization header with test token,Generate authorization header with customizable token settings
821,_calculate_intelligence_score,"def _calculate_intelligence_score(
        self, model: ModelInfo, max_values: Dict[str, float]
    ) -> float:
        
        scores = []
        weights = []

        benchmark_dict: Dict[str, float] = model.metrics.intelligence.model_dump()
        use_weights = True
        for bench, score in benchmark_dict.items():
            key = f""max_{bench}""
            if score is not None and key in max_values:
                scores.append(score / max_values[key])
                if bench in self.benchmark_weights:
                    weights.append(self.benchmark_weights[bench])
                else:
                    # If a benchmark doesn't have a weight, don't use weights at all, we'll just average the scores
                    use_weights = False

        if not scores:
            return 0
        elif use_weights:
            return average(scores, weights=weights)
        else:
            return average(scores)",Return a normalized 0->1 intelligence score for a model based on its benchmark metrics.,Compute a weighted intelligence score for a model based on benchmark metrics.
822,to_set_of_tuples,"def to_set_of_tuples(self, list_of_lists: list[list[str]]) -> set[tuple[str]]:
        
        return {tuple(lst) for lst in list_of_lists}",Convert a list of lists to a set of tuples,Convert a list of string lists into a set of string tuples.
823,conn_params,"def conn_params(self) -> dict:
        
        return {
            ""api_base"": self.url,
            ""api_key"": self.api_key,
        }",Returns the parameters required for connection.,Returns a dictionary with API base URL and key for connection settings.
824,set_light_attributes,"def set_light_attributes(light_name: str, attributes: HueAttributes) -> dict[str, Any]:
    
    if not (bridge := _get_bridge()):
        return {""error"": ""Bridge not connected"", ""success"": False}

    # Basic validation (more specific validation could be added)
    if not isinstance(attributes, dict) or not attributes:
        return {
            ""error"": ""Attributes must be a non-empty dictionary"",
            ""success"": False,
            ""light"": light_name,
        }

    try:
        result = bridge.set_light(light_name, dict(attributes))
        return {
            ""light"": light_name,
            ""set_attributes"": attributes,
            ""success"": True,
            ""phue2_result"": result,
        }
    except (KeyError, PhueException, ValueError, Exception) as e:
        # ValueError might occur for invalid attribute values
        return handle_phue_error(light_name, ""set_light_attributes"", e)","Sets multiple attributes (e.g., hue, sat, bri, ct, xy, transitiontime) for a specific light.","Set light attributes on a bridge, handling errors and validation."
825,_graceful_shutdown_and_exit,"def _graceful_shutdown_and_exit(self):
        
        logging.info(""Self-monitor: Attempting graceful shutdown..."")
        
        # 1. Stop accepting new connections / mark as unhealthy for external checks
        self.is_healthy = False

        # 2. Stop the self-monitor thread from looping again
        self._stop_self_monitor.set()

        # 3. Close the HTTP health server
        if self.health_server:
            try:
                logging.info(""Self-monitor: Shutting down HTTP health check server..."")
                self.health_server.shutdown() # Graceful shutdown
                self.health_server.server_close() # Release port
                logging.info(""Self-monitor: HTTP health check server shut down."")
            except Exception as e:
                logging.error(f""Self-monitor: Error shutting down HTTP health_server: {e}"", exc_info=True)
        
        # 4. Disconnect the Redis collector client
        if self.collector_client:
            try:
                logging.info(""Self-monitor: Disconnecting TranscriptionCollectorClient..."")
                self.collector_client.disconnect()
                logging.info(""Self-monitor: TranscriptionCollectorClient disconnected."")
            except Exception as e:
                logging.error(f""Self-monitor: Error disconnecting collector_client: {e}"", exc_info=True)

        # 5. TODO: Add cleanup for active WebSocket client connections if possible.
        # This is complex as `server.serve_forever()` blocks the main thread.
        # Options: server.shutdown() if available, or rely on process exit for now.

        logging.critical(""Self-monitor: Graceful shutdown attempt complete. Exiting process with code 1."")
        sys.exit(1)",Attempts to gracefully shut down components and then exits the process.,Gracefully shuts down services and exits the process with logging.
826,validate_clip_request,"def validate_clip_request(
    camera: str, start_timestamp: str, end_timestamp: str, valid_cameras: List[str]
) -> Tuple[bool, str, Optional[tuple[datetime, datetime]]]:
    
    # Validate camera
    if not camera:
        return False, ""Bad request - Missing required camera parameter"", None

    if camera not in valid_cameras:
        return (
            False,
            f""Invalid camera location. Available cameras: {', '.join(valid_cameras)}"",
            None,
        )

    # Validate timestamps presence
    if not start_timestamp or not end_timestamp:
        return False, ""Bad request - Missing required timestamp parameters"", None

    # Validate timestamp format and convert to datetime
    try:
        start_time = datetime.fromisoformat(start_timestamp.replace(""Z"", ""+00:00""))
        end_time = datetime.fromisoformat(end_timestamp.replace(""Z"", ""+00:00""))

        return True, """", (start_time, end_time)

    except ValueError as e:
        return False, f""Invalid timestamp format: {str(e)}"", None",Validate clip creation request parameters.,"Validate camera and timestamps for clip request, returning status and details."
827,_get_agent_color,"def _get_agent_color(agent_name: str) -> str:
        
        agent_colors = {
            ""System"": ""bright_blue"",
            ""MLResearchScientist"": ""green"",
            ""MLEngineer"": ""yellow"",
            ""MLOperationsEngineer"": ""magenta"",
            ""Orchestrator"": ""cyan"",
            ""DatasetAnalyser"": ""red"",
            ""SchemaResolver"": ""orange"",
            ""DatasetSplitter"": ""purple"",
            # Default color
            ""default"": ""blue"",
        }

        # Match partial agent names (e.g. ""Engineer"" should match ""ML Engineer"")
        for role, color in agent_colors.items():
            if role in agent_name:
                return color

        return agent_colors[""default""]",Get the color for an agent based on its role.,Determine display color based on agent role name matching predefined categories.
828,_convert_object_of,"def _convert_object_of(self, value: Type, level: int) -> str:
        
        indent = ""    "" * level
        prev_indent = ""    "" * (level - 1)
        type_value = self._convert_proptype_to_typescript(value, None, level + 1)
        return f""{{\n{indent}[key: string]: {type_value};\n{prev_indent}}}""",Converts a dictionary of PropTypes to a TypeScript interface string.,Convert a Python type to a TypeScript object with indentation.
829,serialize_for_logging,"def serialize_for_logging(obj: Any) -> Any:
    
    if hasattr(obj, ""__dict__""):
        return str(obj)
    elif isinstance(obj, (datetime, bytes)):
        return str(obj)
    elif isinstance(obj, dict):
        return {k: serialize_for_logging(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [serialize_for_logging(item) for item in obj]
    return obj","Serialize objects for logging, handling non-JSON serializable types",Convert complex objects into loggable string representations recursively.
830,interactive_mode,"def interactive_mode(minion, context_chunks):
    
    print(""\nInteractive mode:"")
    print(""- Type 'exit' or 'quit' to end"")
    print(""- Type 'history' to see conversation history"")
    print(""- Type 'clear' to clear history"")
    
    q_num = 1
    while True:
        query = input(f""\n[Q{q_num}] > "")
        
        if query.lower() in [""exit"", ""quit""]:
            break
        elif query.lower() == ""history"":
            show_history(minion)
            continue
        elif query.lower() == ""clear"":
            minion.conversation_history.clear()
            print(""History cleared."")
            continue
        
        # Process query
        print(""\n[Processing...]"")
        result = minion(
            task=query,
            context=context_chunks,
            max_rounds=2
        )
        
        print(f""\n[A{q_num}]:\n{result['final_answer']}"")
        print(f""\nRemote tokens: {result['remote_usage'].total_tokens}"")
        print(f""Local tokens: {result['local_usage'].total_tokens}"")
        
        q_num += 1",Run in interactive mode.,"Interactive chatbot handles queries, history, and context processing."
831,_log_and_raise_error,"def _log_and_raise_error(self, error: Exception) -> None:
        
        logger.error(f""Error executing tool '{self.name}': {str(error)}"")
        raise ToolError(
            f""An error occurred during the execution of tool '{self.name}': {str(error)}""
        )",Log the error and raise a ToolError.,Logs and raises a custom error for tool execution failures.
832,create_action_model,"def create_action_model(self) -> Type[ActionModel]:
		
		fields = {
			name: (
				Optional[action.param_model],
				Field(default=None, description=action.description),
			)
			for name, action in self.registry.actions.items()
		}

		self.telemetry.capture(
			ControllerRegisteredFunctionsTelemetryEvent(
				registered_functions=[
					RegisteredFunction(name=name, params=action.param_model.model_json_schema())
					for name, action in self.registry.actions.items()
				]
			)
		)

		return create_model('ActionModel', __base__=ActionModel, **fields)",Creates a Pydantic model from registered actions,Generate a dynamic action model with telemetry for registered functions.
833,guac_handler,"def guac_handler( **kwargs):
    
    config_dir = safe_get(kwargs, 'config_dir', None)
    plots_dir = safe_get(kwargs, 'plots_dir', None)
    refresh_period = safe_get(kwargs, 'refresh_period', 100)
    lang = safe_get(kwargs, 'lang', None)
    messages = safe_get(kwargs, ""messages"", [])
    
    npc_file = '~/.npcsh/guac/npc_team/guac.npc'
    npc_team_dir = os.path.expanduser('~/.npcsh/guac/npc_team/')
    
    npc = NPC(file=npc_file, db_conn=db_conn)

    team = Team(npc_team_dir, db_conn=db_conn)

    
    enter_guac_mode(npc=npc, 
                    team=team, 
                    config_dir=config_dir, 
                    plots_dir=plots_dir,
                    npc_team_dir=npc_team_dir,
                    refresh_period=refresh_period, lang=lang)
    
    return {""output"": 'Exiting Guac Mode', ""messages"": safe_get(kwargs, ""messages"", [])}",Guac ignores input npc and npc_team dirs and manually sets them to be at ~/.npcsh/guac,Initialize and manage Guac mode with configuration and team settings.
834,get_agent_model_config,"def get_agent_model_config(state, agent_name):
    
    request = state.get(""metadata"", {}).get(""request"")

    if agent_name == 'portfolio_manager':
        # Get the model and provider from state metadata
        model_name = state.get(""metadata"", {}).get(""model_name"", ""gpt-4o"")
        model_provider = state.get(""metadata"", {}).get(""model_provider"", ""OPENAI"")
        return model_name, model_provider
    
    if request and hasattr(request, 'get_agent_model_config'):
        # Get agent-specific model configuration
        model_name, model_provider = request.get_agent_model_config(agent_name)
        return model_name, model_provider.value if hasattr(model_provider, 'value') else str(model_provider)
    
    # Fall back to global configuration
    model_name = state.get(""metadata"", {}).get(""model_name"", ""gpt-4o"")
    model_provider = state.get(""metadata"", {}).get(""model_provider"", ""OPENAI"")
    
    # Convert enum to string if necessary
    if hasattr(model_provider, 'value'):
        model_provider = model_provider.value
    
    return model_name, model_provider",Get model configuration for a specific agent from the state.,Retrieve model configuration based on agent type and state metadata
835,tabulate,"def tabulate(rows: List[List[Union[str, int]]], headers: List[str]) -> str:
    
    col_widths = [max(len(str(x)) for x in col) for col in zip(*rows, headers)]
    row_format = (""{{:{}}} "" * len(headers)).format(*col_widths)
    lines = []
    lines.append(row_format.format(*headers))
    lines.append(row_format.format(*[""-"" * w for w in col_widths]))
    for row in rows:
        lines.append(row_format.format(*row))
    return ""\n"".join(lines)",Inspired by: -  -,Formats and aligns tabular data with headers into a string representation.
836,_compile_graph,"def _compile_graph(self) -> StateGraph:
        
        workflow = StateGraph(DeepResearchState)

        # Add nodes
        workflow.add_node(""plan_research"", planning_node)
        workflow.add_node(""execute_research"", research_execution_node)
        workflow.add_node(""synthesize_report"", synthesis_node)
        workflow.add_node(
            ""end_run"", lambda state: logger.info(""--- Reached End Run Node ---"") or {}
        )  # Simple end node

        # Define edges
        workflow.set_entry_point(""plan_research"")

        workflow.add_edge(
            ""plan_research"", ""execute_research""
        )  # Always execute after planning

        # Conditional edge after execution
        workflow.add_conditional_edges(
            ""execute_research"",
            should_continue,
            {
                ""execute_research"": ""execute_research"",  # Loop back if more steps
                ""synthesize_report"": ""synthesize_report"",  # Move to synthesis if done
                ""end_run"": ""end_run"",  # End if stop requested or error
            },
        )

        workflow.add_edge(""synthesize_report"", ""end_run"")  # End after synthesis

        app = workflow.compile()
        return app",Compiles the Langgraph state machine.,Constructs a state graph for research workflow with nodes and conditional transitions.
837,_create_documents,"def _create_documents(
        self, search_results: Union[str, List[Dict]], nr_of_links: int = 0
    ) -> List[Document]:
        
        documents = []
        if isinstance(search_results, str):
            return documents

        for i, result in enumerate(search_results):
            if isinstance(result, dict):
                # Add index to the original search result dictionary
                result[""index""] = str(i + nr_of_links + 1)

                content = result.get(""full_content"", result.get(""snippet"", """"))
                documents.append(
                    Document(
                        page_content=content,
                        metadata={
                            ""source"": result.get(""link"", f""source_{i + 1}""),
                            ""title"": result.get(""title"", f""Source {i + 1}""),
                            ""index"": i + nr_of_links + 1,
                        },
                    )
                )
        return documents",Convert search results to LangChain documents format and add index to original search results.,Transform search results into indexed document objects with metadata.
838,do_Td,"def do_Td(self, tx: PDFStackT, ty: PDFStackT) -> None:
        
        tx_ = safe_float(tx)
        ty_ = safe_float(ty)
        if tx_ is not None and ty_ is not None:
            (a, b, c, d, e, f) = self.textstate.matrix
            e_new = tx_ * a + ty_ * c + e
            f_new = tx_ * b + ty_ * d + f
            self.textstate.matrix = (a, b, c, d, e_new, f_new)

        elif settings.STRICT:
            raise PDFValueError(f""Invalid offset ({tx!r}, {ty!r}) for Td"")

        self.textstate.linematrix = (0, 0)",Move to the start of the next line,Adjust text matrix and line position based on given offsets in PDF rendering.
839,validate_camera_request,"def validate_camera_request(
    camera: str, question: str, valid_cameras: List[str]
) -> Tuple[bool, str]:
    
    if not camera:
        return False, ""Missing required camera parameter""
    if camera not in valid_cameras:
        return (
            False,
            f""Invalid camera location. Available cameras: {', '.join(valid_cameras)}"",
        )
    if not question:
        return False, ""Missing required question parameter""
    return True, """"",Validate camera request parameters.,Validate camera and question inputs against a list of valid camera locations.
840,_resolve_timeout,"def _resolve_timeout(self, timeout: Optional[int]) -> int:
        
        try:
            return self.config.get(""timeout"", timeout) or int(os.getenv(""AGENT_TIMEOUT"", ""30""))
        except (ValueError, TypeError) as e:
            logger.error(f""Error resolving timeout: {e}. Using default."")
            return 30","Resolve the timeout from config, argument, or environment variable.","Determine effective timeout using configuration, environment, or default value."
841,_json_default,"def _json_default(obj):
        
        if isinstance(obj, Path):
            return str(obj)
        raise TypeError(f""Object of type {type(obj).__name__} is not JSON serializable"")",Handle JSON serialization of Path objects.,Custom JSON serializer converts Path objects to strings or raises TypeError.
842,get_credential_key,"def get_credential_key(
      self,
      auth_scheme: Optional[AuthScheme],
      auth_credential: Optional[AuthCredential],
  ) -> str:
    
    scheme_name = (
        f""{auth_scheme.type_.name}_{hash(auth_scheme.model_dump_json())}""
        if auth_scheme
        else """"
    )
    credential_name = (
        f""{auth_credential.auth_type.value}_{hash(auth_credential.model_dump_json())}""
        if auth_credential
        else """"
    )
    # no need to prepend temp: namespace, session state is a copy, changes to
    # it won't be persisted , only changes in event_action.state_delta will be
    # persisted. temp: namespace will be cleared after current run. but tool
    # want access token to be there stored across runs

    return f""{scheme_name}_{credential_name}_existing_exchanged_credential""",Generates a unique key for the given auth scheme and credential.,Generate a unique key for authentication credentials based on scheme and credential details.
843,_get_payload,"def _get_payload(self):
        
        payload = []
        for role, content in self.prompt.items():
            payload.append({""role"": role, ""content"": content})
        return payload",Prepare the payload for the LLM.,Constructs a list of role-content dictionaries from prompt data.
844,_setup_auth,"def _setup_auth(self, uri: str) -> None:
        
        parsed = urlparse(uri)
        if not parsed.netloc:
            raise ValueError(""Invalid URI format"")

        # Split host and auth parts
        auth, host = parsed.netloc.split(""@"")
        _, self._auth_token = auth.split("":"")

        # Set base URL

        # Basic token validation
        jwt.decode(self._auth_token, options={""verify_signature"": False})",Setup authentication from URI,Extract and validate authentication token from URI for setup
845,validate_api_key,"def validate_api_key(cls, key_name: str) -> str:
        
        key_value = getattr(cls, key_name, None) or os.getenv(key_name)
        if not key_value:
            raise ValueError(f""{key_name} environment variable is not set"")
        return key_value",Validate and return a specific API key when it's needed.,Check and retrieve API key from class attribute or environment variable.
846,generate,"def generate(self, smiles: str) -> ConformerData:
        
        mol = Chem.MolFromSmiles(smiles)
        assert mol is not None, f""Invalid smiles {smiles}""

        mol_with_hs = Chem.AddHs(mol)

        params = rdDistGeom.ETKDGv3()
        params.useSmallRingTorsions = True  # type: ignore
        params.randomSeed = 123  # type: ignore
        params.useChirality = True
        # below params were added after facing 'Value Error: Bad Conformer id'
        params.maxAttempts = 10_000
        params.useRandomCoords = True  # type: ignore

        rdDistGeom.EmbedMultipleConfs(mol_with_hs, numConfs=1, params=params)
        rdmolops.RemoveHs(mol_with_hs)

        element_counter: dict = defaultdict(int)
        for atom in mol_with_hs.GetAtoms():
            elem = atom.GetSymbol()
            element_counter[elem] += 1  # Start each counter at 1
            atom.SetProp(""name"", elem + str(element_counter[elem]))

        retval = self._load_ref_conformer_from_rdkit(mol_with_hs)
        retval.atom_names = [a.upper() for a in retval.atom_names]
        return retval",Generates a conformer for a ligand from its SMILES string.,Generate 3D molecular conformer from SMILES string with atom naming.
847,check_yolov5u_filename,"def check_yolov5u_filename(file: str, verbose: bool = True):
    
    if ""yolov3"" in file or ""yolov5"" in file:
        if ""u.yaml"" in file:
            file = file.replace(""u.yaml"", "".yaml"")  # i.e. yolov5nu.yaml -> yolov5n.yaml
        elif "".pt"" in file and ""u"" not in file:
            original_file = file
            file = re.sub(r""(.*yolov5([nsmlx]))\.pt"", ""\\1u.pt"", file)  # i.e. yolov5n.pt -> yolov5nu.pt
            file = re.sub(r""(.*yolov5([nsmlx])6)\.pt"", ""\\1u.pt"", file)  # i.e. yolov5n6.pt -> yolov5n6u.pt
            file = re.sub(r""(.*yolov3(|-tiny|-spp))\.pt"", ""\\1u.pt"", file)  # i.e. yolov3-spp.pt -> yolov3-sppu.pt
            if file != original_file and verbose:
                LOGGER.info(
                    f""PRO TIP 💡 Replace 'model={original_file}' with new 'model={file}'.\nYOLOv5 'u' models are ""
                )
    return file",Replace legacy YOLOv5 filenames with updated YOLOv5u filenames.,Adjusts YOLO model filenames to include 'u' suffix for compatibility.
848,list_tables,"def list_tables(self, keyspace_name: str) -> List[TableInfo]:
        
        logger.info(f'Listing tables for keyspace: {keyspace_name}')
        return self.cassandra_client.list_tables(keyspace_name)",List all tables in a keyspace.,Retrieve and log table information for a specified keyspace.
849,graph_class_methods,"def graph_class_methods(target_class: Class):
    
    G.add_node(target_class, color=COLOR_PALETTE[""StartClass""])

    for method in target_class.methods:
        method_name = f""{target_class.name}.{method.name}""
        G.add_node(method, name=method_name, color=COLOR_PALETTE[""StartMethod""])
        visited.add(method)
        G.add_edge(target_class, method)

    for method in target_class.methods:
        create_downstream_call_trace(method)",Creates a graph visualization of all methods in a class and their call relationships,Visualize class methods and their call relationships in a graph
850,get_default_hourly_caps,"def get_default_hourly_caps() -> Dict[str, Dict[str, int]]:
    
    return {
        ""sonarr"": {""api_hits"": 0},
        ""radarr"": {""api_hits"": 0},
        ""lidarr"": {""api_hits"": 0},
        ""readarr"": {""api_hits"": 0},
        ""whisparr"": {""api_hits"": 0},
        ""eros"": {""api_hits"": 0}
    }",Get the default hourly caps structure,Initialize default API hit limits for various services.
851,_softmax,"def _softmax(logits: np.ndarray, temperature: float) -> np.ndarray:
    
    try:
        if not np.all(np.isfinite(logits)):
            non_finites = set(logits[~np.isfinite(logits)])
            raise ValueError(f'`logits` contains non-finite value(s): {non_finites}')
        if not np.issubdtype(logits.dtype, np.floating):
            logits = np.array(logits, dtype=np.float32)

        result = scipy.special.softmax(logits / temperature, axis=-1)
        # Ensure that probabilities sum to 1 to prevent error in `np.random.choice`.
        index = np.argmax(result)
        result[index] = 1 - np.sum(result[0:index]) - np.sum(result[index + 1:])
        return result
    except TypeError as type_err:
        print(logits)
        raise type_err",Returns the tempered softmax of 1D finite `logits`.,Compute normalized probabilities from logits using temperature scaling.
852,get_agent_state,"def get_agent_state():
    
    global agent_instance
    state = {""paused"": False, ""stopped"": False}

    if agent_instance and hasattr(agent_instance, ""state""):
        state = {
            ""paused"": agent_instance.state.paused,
            ""stopped"": agent_instance.state.stopped,
        }

    # Send agent state update to frontend
    try:
        from .log_server import socketio

        socketio.emit(""agent_state"", {""state"": state})
    except Exception:
        pass

    return state",Get the agent state.,Retrieve and broadcast the current operational status of an agent instance.
853,get_dataset,"def get_dataset(self):
        
        try:
            if self.args.task == ""classify"":
                data = check_cls_dataset(self.args.data)
            elif self.args.data.split(""."")[-1] in {""yaml"", ""yml""} or self.args.task in {
                ""detect"",
                ""segment"",
                ""pose"",
                ""obb"",
            }:
                data = check_det_dataset(self.args.data)
                if ""yaml_file"" in data:
                    self.args.data = data[""yaml_file""]  # for validating 'yolo train data=url.zip' usage
        except Exception as e:
            raise RuntimeError(emojis(f""Dataset '{clean_url(self.args.data)}' error ❌ {e}"")) from e
        self.data = data
        return data[""train""], data.get(""val"") or data.get(""test"")","Get train, val path from data dict if it exists.","Determine dataset type and validate based on task requirements, handling errors gracefully."
854,match,"def match(cls, responses, targets) -> float:
        
        logging.debug(f""{responses=}, {targets=}"")
        targets = ast.literal_eval(targets)
        try:
            responses = ast.literal_eval(responses)
        except SyntaxError:
            return 0

        try:
            iou_scores = calculate_iou(
                [
                    responses,
                ],
                [
                    targets,
                ],
            )
            if not iou_scores:
                return 0
        except:
            return 0

        # Take the mean IoU score for now.
        return sum(iou_scores) / len(iou_scores)",Exact match between targets and responses.,Calculate mean Intersection over Union score between parsed response and target data.
855,parallel_weight_loader,"def parallel_weight_loader(self, param: torch.Tensor, loaded_weight: torch.Tensor) -> None:
    
    assert (param.size() == loaded_weight.size(
    )), ""the parameter size is not align with the loaded weight size, param size: {}, loaded_weight size: {}"".format(
        param.size(), loaded_weight.size())
    assert (param.data.dtype == loaded_weight.data.dtype
           ), ""if we want to shared weights, the data type should also be the same""

    param.data = loaded_weight.data",Parallel Linear weight loader.,Ensure parameter and weight compatibility before updating parameter data in parallel.
856,empty_communications_response_data,"def empty_communications_response_data() -> Dict[str, Any]:
    
    return {
        ""communications"": [],
        ""nextToken"": None,
    }",Return a dictionary with empty communications response data.,Initialize a dictionary for communication data with empty list and null token.
857,to_langchain,"def to_langchain(self) -> ChatOpenAI:
        
        kwargs = self.kwargs
        if self.json:
            kwargs[""response_format""] = {""type"": ""json_object""}

        return ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature or 0.5,
            max_tokens=self.max_tokens,
            model_kwargs=kwargs,
            streaming=self.streaming,
            api_key=SecretStr(os.environ.get(""XAI_API_KEY"", ""xai"")),
            top_p=self.top_p,
        )",Convert the language model to a LangChain chat model.,Convert configuration to a ChatOpenAI instance with specified parameters.
858,open_file_location,"def open_file_location():
    
    data = request.json
    file_path = data.get(""path"")

    if not file_path:
        return jsonify({""status"": ""error"", ""message"": ""Path is required""}), 400

    # Convert to absolute path if needed
    if not os.path.isabs(file_path):
        file_path = os.path.abspath(file_path)

    # Check if path exists
    if not os.path.exists(file_path):
        return jsonify({""status"": ""error"", ""message"": ""Path does not exist""}), 404

    try:
        if platform.system() == ""Windows"":
            # On Windows, open the folder and select the file
            if os.path.isfile(file_path):
                subprocess.run([""explorer"", ""/select,"", file_path], check=True)
            else:
                # If it's a directory, just open it
                subprocess.run([""explorer"", file_path], check=True)
        elif platform.system() == ""Darwin"":  # macOS
            subprocess.run([""open"", file_path], check=True)
        else:  # Linux and others
            subprocess.run([""xdg-open"", os.path.dirname(file_path)], check=True)

        return jsonify({""status"": ""success""})
    except Exception as e:
        logger.exception(""Error opening a file"")
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500",Open a file location in the system file explorer,"Open a specified file path in the system's file explorer, handling errors."
859,get_docs_by_ids,"def get_docs_by_ids(
        self,
        ids: Optional[list[ItemID]] = None,
        collection_name: str = None,
        include: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> list[Document]:
        
        if include is None:
            include = [TEXT_KEY, ""metadata"", ""id""]
        elif ""id"" not in include:
            include.append(""id"")

        collection = self.get_collection(collection_name)
        if ids is not None:
            docs = [collection.get(doc_id) for doc_id in ids]
        else:
            # Get all documents using couchbase query
            include_str = "", "".join(include)
            query = f""SELECT {include_str} FROM {self.bucket.name}.{self.scope.name}.{collection.name}""
            result = self.cluster.query(query)
            docs = []
            for row in result:
                docs.append(row)

        return [{k: v for k, v in doc.items() if k in include or k == ""id""} for doc in docs]",Retrieve documents from the collection of the vector database based on the ids.,Retrieve documents by IDs or query all from a specified collection with optional fields.
860,gather_feedback,"def gather_feedback(context: dict[str, Any], **kwargs):
    
    draft = context.get(""history"", [{}])[-1].get(""content"", ""No draft"")

    feedback = input(f""Email draft:\n"" f""{draft}\n"" f""Type in nothing to send email, provide feedback to refine it: \n"")

    result = f""Gathered feedback: {feedback}""

    feedback = feedback.strip().lower()
    if feedback == """":
        print(""####### Email was sent! #######"")
        result = ""Email was sent!""
        agent_input = None
    else:
        print(""####### Email was canceled! #######"")
        result = ""Email was canceled!""
        agent_input = f""Draft of canceled email: \n{draft}\n"" f""Feedback of user about this draft: \n{feedback}""

    return {""result"": result, ""agent_input"": agent_input}",Gather feedback about email draft.,Collects user feedback on an email draft to decide sending or refining.
861,get_eval_set_result,"def get_eval_set_result(
      self, app_name: str, eval_set_result_id: str
  ) -> EvalSetResult:
    
    # Load the eval set result file data.
    maybe_eval_result_file_path = (
        os.path.join(
            self._get_eval_history_dir(app_name),
            eval_set_result_id,
        )
        + _EVAL_SET_RESULT_FILE_EXTENSION
    )
    if not os.path.exists(maybe_eval_result_file_path):
      raise ValueError(
          f""Eval set result `{eval_set_result_id}` does not exist.""
      )
    with open(maybe_eval_result_file_path, ""r"") as file:
      eval_result_data = json.load(file)
    return EvalSetResult.model_validate_json(eval_result_data)",Returns an EvalSetResult identified by app_name and eval_set_result_id.,Retrieve and validate evaluation set results from a specified file path.
862,result_key,"def result_key(self) -> str:
        
        return {
            SearchType.WEB: ""organic_results"",
            SearchType.NEWS: ""news_results"",
            SearchType.IMAGES: ""image_results"",
            SearchType.VIDEOS: ""video_results"",
        }[self]",Returns the corresponding result key for the search type,Map search type to corresponding result category string.
863,generate_embedding,"def generate_embedding(
    openai_client: OpenAI, embedding_model: str, embedding_dimension: int, text: str
) -> list[float]:
    
    logger.debug(f""Generating embedding for text: {text}"")
    try:
        response = openai_client.embeddings.create(
            input=[text],
            model=embedding_model,
            dimensions=embedding_dimension,
        )
        embedding: list[float] = response.data[0].embedding
        return embedding
    except Exception:
        logger.error(""Error generating embedding"", exc_info=True)
        raise",Generate an embedding for the given text using OpenAI's model.,Generate a text embedding using OpenAI's API and return it as a float list.
864,fish_tts,"def fish_tts(text: str, save_as: str) -> bool:
    
    API_KEY = load_key(""fish_tts.api_key"")
    character = load_key(""fish_tts.character"")
    refer_id = load_key(""fish_tts.character_id_dict"")[character]
    
    payload = json.dumps({
        ""text"": text,
        ""reference_id"": refer_id,
        ""chunk_length"": 200,
        ""normalize"": True,
        ""format"": ""wav"",
        ""latency"": ""normal""
    })
    
    headers = {'Authorization': f'Bearer {API_KEY}', 'Content-Type': 'application/json'}
    
    response = requests.post(url, headers=headers, data=payload)
    response.raise_for_status()
    response_data = response.json()
    
    if ""url"" in response_data:
        audio_response = requests.get(response_data[""url""])
        audio_response.raise_for_status()
        
        with open(save_as, ""wb"") as f:
            f.write(audio_response.content)
        return True
    
    print(""Request failed:"", response_data)
    return False",302.ai Fish TTS conversion,Convert text to speech using an API and save as an audio file
865,_apply_parameters,"def _apply_parameters(self, merged_input: dict, params: dict, source: str, debug_info: list = None):
        
        if debug_info is None:
            debug_info = []
        for key, value in params.items():
            if key in merged_input and isinstance(value, dict) and isinstance(merged_input[key], dict):
                merged_nested = merged_input[key].copy()
                merged_input[key] = deep_merge(value, merged_nested)
                debug_info.append(f""  - From {source}: Merged nested {key}"")
            else:
                merged_input[key] = value
                debug_info.append(f""  - From {source}: Set {key}={value}"")",Apply parameters from the specified source to the merged input.,"Merge and update input dictionary with parameters, logging changes for debugging."
866,get_edit_form_dict,"def get_edit_form_dict():
        

        form_dict = {
            'description': SharedDescriptionForm,
            'name': SharedNameForm,
            'max_views': SharedMaxViewsForm,
            'password': SharedPasswordForm,
            'expiration_date': SharedExpirationDateForm,
            'deletion_date': SharedDeletionDateForm,
        }

        return form_dict",Get the forms of the fields that can be edited as a dict.,Create a dictionary mapping form types to shared form classes.
867,push_update,"def push_update(self, params: Dict[str, Any], content: str) -> None:
        
        update_task_id = self._origin_task_id or self._task_id
        if update_task_id:
            logger.info(f""Pushing update | Task: {update_task_id} | Content: {content}"")
            self.mesh_client.push_update(update_task_id, content)",Always push to origin_task_id if available,Pushes content updates to a task using a mesh client
868,close_db,"def close_db() -> None:
    
    db = db_var.get()
    if db is None:
        logger.warning(""No database connection to close"")
        return

    try:
        if not db.is_closed():
            db.close()
            logger.info(""Database connection closed successfully"")
        else:
            logger.debug(
                ""Database connection was already closed (normal during shutdown)""
            )
    except peewee.DatabaseError as e:
        logger.error(f""Database Error: Failed to close connection: {str(e)}"")
    except Exception as e:
        logger.error(f""Failed to close database connection: {str(e)}"")",Close the current database connection if it exists.,"Safely close database connection, logging outcomes and handling errors."
869,context_server,"def context_server(server_port: int) -> Generator[None, None, None]:
    
    proc = multiprocessing.Process(
        target=run_context_server, kwargs={""server_port"": server_port}, daemon=True
    )
    print(""starting context server process"")
    proc.start()

    # Wait for server to be running
    max_attempts = 20
    attempt = 0
    print(""waiting for context server to start"")
    while attempt < max_attempts:
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.connect((""127.0.0.1"", server_port))
                break
        except ConnectionRefusedError:
            time.sleep(0.1)
            attempt += 1
    else:
        raise RuntimeError(
            f""Context server failed to start after {max_attempts} attempts""
        )

    yield

    print(""killing context server"")
    proc.kill()
    proc.join(timeout=2)
    if proc.is_alive():
        print(""context server process failed to terminate"")",Fixture that provides a server with request context capture,"Launch and manage a context server process, ensuring it starts and stops correctly."
870,create_linkedin_content_workflow,"def create_linkedin_content_workflow() -> Workflow:
    
    # Create workflow starting with the first node
    workflow = (
        Workflow(""read_markdown_file"")
        .then(""analyze_content"")  # Analysis node with dynamically mapped model
        .then(""determine_viral_strategy"")  # Strategy node with dynamically mapped model
        .then(""generate_linkedin_post"")  # Generate post with dynamically mapped model
        .then(""clean_linkedin_post"")  # Clean post with dynamically mapped model
        .then(""format_linkedin_post"")  # Format post for LinkedIn platform
        .then(""save_linkedin_post"")  # Save to file
        .then(""copy_to_clipboard"")  # Copy to clipboard if requested
    )
    
    # Add input mappings for model parameters
    workflow.node_input_mappings = {
        ""analyze_content"": {""model"": ""analysis_model"", ""mock"": ""mock""},
        ""determine_viral_strategy"": {""model"": ""analysis_model""},
        ""generate_linkedin_post"": {""model"": ""writing_model"", ""intent"": ""intent""},
        ""clean_linkedin_post"": {""model"": ""cleaning_model""},
        ""format_linkedin_post"": {""model"": ""formatting_model""},
    }
    
    return workflow",Create a workflow to convert a markdown file to a viral LinkedIn post.,"Automates LinkedIn content creation through a sequential workflow of analysis, strategy, and formatting."
871,from_array4x4,"def from_array4x4(cls, array: jnp.ndarray) -> Self:
    
    if array.shape[-2:] != (4, 4):
      raise ValueError(f'array.shape({array.shape}) must be [..., 4, 4]')
    rotation = rotation_matrix.Rot3Array(
        *(array[..., 0, 0], array[..., 0, 1], array[..., 0, 2]),
        *(array[..., 1, 0], array[..., 1, 1], array[..., 1, 2]),
        *(array[..., 2, 0], array[..., 2, 1], array[..., 2, 2]),
    )
    translation = vector.Vec3Array(
        array[..., 0, 3], array[..., 1, 3], array[..., 2, 3]
    )
    return cls(rotation, translation)",Construct Rigid3Array from homogeneous 4x4 array.,Convert a 4x4 matrix into rotation and translation components.
872,uv_run_raaid,"def uv_run_raaid(repo_dir: Path, prompt: str) -> Optional[str]:
    
    cmd = [""uv"", ""run"", ""ra-aid"", ""--cowboy-mode"", ""-m"", prompt]
    # We are NOT capturing output, so it streams live:
    try:
        result = subprocess.run(
            cmd,
            cwd=repo_dir,
            text=True,
            check=False,  # We manually handle exit code
        )
        if result.returncode != 0:
            logging.error(""ra-aid returned non-zero exit code."")
            return None
    except subprocess.TimeoutExpired:
        logging.error(""ra-aid timed out"")
        return None
    except Exception as e:
        logging.error(f""ra-aid error: {e}"")
        return None

    # Collect patch
    patch = get_git_patch(repo_dir)
    return patch","Call 'uv run ra-aid' with the given prompt in the environment, streaming output directly to the console (capture_output=False).",Execute a command in a repository and return a generated patch if successful.
873,rollout,"def rollout(self,
                client: OpenAI,
                model: str,
                prompt: Union[str, List[Dict[str, Any]]],
                answer: str,
                sampling_args: Dict[str, Any] = {},
                **kwargs: Any) -> Tuple[Union[str, List[Dict[str, str]]], Dict[str, Any]]:
        
        completion = self.get_model_response(
            client=client,
            model=model,
            prompt=prompt,
            sampling_args=sampling_args,
            message_type=self.message_type
        )
        if self.message_type == 'chat': 
            return [{'role': 'assistant', 'content': completion}], {}
        return completion, {}",Returns completion (str or message list) and null state.,Generate AI model responses based on input prompts and return formatted results.
874,remove_task,"def remove_task(self, task_id: str) -> ControlMessageTask:
        
        if task_id in self._tasks:
            _task = self._tasks[task_id]

            del self._tasks[task_id]

            return _task
        else:
            raise RuntimeError(f""Attempted to remove non-existent task with id: {task_id}"")",Remove a task from the control message.,"Remove a task by ID from a task collection, raising an error if not found."
875,process_message_content,"def process_message_content(message: dict[str, Any]) -> Union[str, list[dict[str, Any]]]:
    
    content = message.get(""content"", """")

    # Handle empty content
    if content == """":
        return content

    # If content is already a string, return as is
    if isinstance(content, str):
        return content

    # Handle list content (mixed text and images)
    if isinstance(content, list):
        processed_content = []
        for item in content:
            if item[""type""] == ""text"":
                processed_content.append({""type"": ""text"", ""text"": item[""text""]})
            elif item[""type""] == ""image_url"":
                processed_content.append(process_image_content(item))
        return processed_content

    return content","Process message content, handling both string and list formats with images.",Process message content by returning text or processing mixed text and image lists.
876,find_verified_codemod_cases,"def find_verified_codemod_cases(metafunc):
    
    repos = {}
    config = SkillTestConfig.from_metafunc(metafunc)
    codemod_api = CodemodAPI(api_key=config.api_key)
    for repo_id, url in filter_repos(REPO_ID_TO_URL).items():
        if config.repo_id and repo_id != config.repo_id:
            continue

        codemods_data = RepoCodemodMetadata.from_json_file(VERIFIED_CODEMOD_DATA_DIR / f""{anonymize_id(repo_id)}.json"")
        codemods_data.filter(base_commit=config.base_commit, codemod_id=config.codemod_id)

        repo_name = codemods_data.repo_name
        programming_language = codemods_data.language

        for commit, codemods in codemods_data.codemods_by_base_commit.items():
            repo_dir_name = f""{repo_name}_{commit}""
            if repo_dir_name not in repos:
                repo = Repo(
                    commit=commit,
                    url=url,
                    language=programming_language,
                    size=Size.Large,
                    extra_repo=True,
                    name=repo_dir_name,
                )
                repos[repo_dir_name] = repo
            yield from generate_codemod_test_cases(repos[repo_dir_name], codemods, codemod_api)",Generate test cases for a list of codemods,Generate test cases for verified code modifications across repositories using configuration settings.
877,_load_config,"def _load_config(self):
        
        paths_to_try = [
            self.config_path,
            os.path.join(os.getcwd(), ""mcp.json""),
            os.path.join(os.getcwd(), "".mcp.json""),
            os.path.expanduser(""~/.mcp.json""),
        ]

        config_file = None
        for path in paths_to_try:
            if path and os.path.exists(path):
                config_file = path
                break

        if not config_file:
            return

        try:
            with open(config_file, ""r"") as f:
                config = json.load(f)

            if ""mcpServers"" in config:
                for server_name, server_config in config[""mcpServers""].items():
                    self.servers[server_name] = MCPServerConfig(
                        command=server_config[""command""],
                        args=server_config[""args""],
                        env=server_config.get(""env""),
                    )
        except Exception as e:
            raise ValueError(f""Failed to load MCP config from {config_file}: {str(e)}"")",Load MCP configuration from file,Load and parse server configuration from multiple potential file paths
878,kwargs_for_new_context,"def kwargs_for_new_context(self) -> BrowserNewContextArgs:
		
		return BrowserNewContextArgs(**self.model_dump(exclude={'args'}), args=self.get_args())",Return the kwargs for BrowserContext.new_context().,Create a new browser context with specific arguments excluding 'args'.
879,comma_separated_to_set,"def comma_separated_to_set(comma_separated: str) -> set[str]:
    
    items = comma_separated.split("","") if comma_separated else []
    non_empty_items = {item.strip() for item in items if item.strip()}
    return non_empty_items","Given a comma separated string, returns a set of the comma separated items.",Convert comma-separated string into a set of non-empty trimmed items.
880,structured_response,"def structured_response(
        self,
        prompt: str,
        response_model: Type[T],
        *,
        llm_model: str | None = None,
        **kwargs,
    ) -> T:
        
        messages = [
            {""role"": ""user"", ""content"": prompt},
        ]

        response = self.structured_client.chat.completions.create(
            messages=messages,
            model=llm_model or self.DEFAULT_MODEL,
            response_model=response_model,
            **{**self.DEFAULT_KWARGS, **kwargs},
        )
        return response_model.model_validate(response)",Get a structured response from the Ollama API.,Generate structured AI response using specified model and validation schema.
881,replay,"def replay():
    
    try:
        StockanalysisCrew().crew().replay(task_id=sys.argv[1])

    except Exception as e:
        raise Exception(f""An error occurred while replaying the crew: {e}"")",Replay the crew execution from a specific task.,Replays a stock analysis task using a specified task identifier.
882,_create_logger,"def _create_logger():
    

    handler = colorlog.StreamHandler()
    fmt = ""%(log_color)s%(levelname)-8s%(reset)s [%(filename)s:%(lineno)d] %(message)s""
    handler.setFormatter(
        colorlog.ColoredFormatter(
            fmt=fmt,
            log_colors={
                ""DEBUG"": ""blue"",
                ""INFO"": ""green"",
                ""WARNING"": ""yellow"",
                ""ERROR"": ""red"",
                ""CRITICAL"": ""red"",
            },
        )
    )
    # Get log level from LOG_LEVEL environment variable
    log_level = os.getenv(""LOG_LEVEL"", ""WARNING"").upper()
    logger = colorlog.getLogger(__package__)
    logger.setLevel(level=getattr(logging, log_level, logging.WARNING))
    # Setup logging to stdout
    logger.addHandler(handler)
    return logger",Create a logger with colorized output,Configure a color-coded logger with dynamic log level from environment variable
883,suggest_email,"def suggest_email(self, first_name, last_name):
        
        try:
            # Get the last used email domain
            domain = self.get_last_email_domain()
            if not domain:
                return None
            
            # Generate email prefix from first and last name (lowercase)
            email_prefix = f""{first_name.lower()}.{last_name.lower()}""
            
            # Combine prefix and domain
            suggested_email = f""{email_prefix}@{domain}""
            
            return suggested_email
        
        except Exception as e:
            error_msg = self.translator.get('account.suggest_email_failed', error=str(e)) if self.translator else f'Failed to suggest email: {str(e)}'
            print(f""{Fore.RED}{EMOJI['ERROR']} {error_msg}{Style.RESET_ALL}"")
            return None",Generate a suggested email based on first and last name with the last used domain,Generate a personalized email suggestion using name and domain data.
884,right_hand_velocity,"def right_hand_velocity(envstate, robot_name: str):
    
    # return envstate[f""{_METASIM_BODY_PREFIX}right_hand""][""right_hand_subtreelinvel""] # Only for mujoco
    return envstate[""robots""][robot_name][""body""][""right_elbow_link""][""vel""]",Returns the velocity of the right hand.,Retrieve right hand velocity from environment state for a specified robot.
885,to_device,"def to_device(data, device):
    
    if isinstance(data, torch.Tensor):
        return data.to(device)
    elif isinstance(data, dict):
        return {key: to_device(value, device) for key, value in data.items()}
    elif isinstance(data, list):
        return [to_device(item, device) for item in data]
    elif isinstance(data, tuple):
        return tuple(to_device(item, device) for item in data)
    elif isinstance(data, BatchEncoding):
        return data.to(device)
    else:
        return data","Recursively move tensors in a nested list, tuple, or dictionary to the specified device.",Recursively transfer data structures to specified computing device.
886,_save,"def _save(self) -> None:
        
        # Save strategies
        try:
            with open(self.db_path, 'w') as f:
                json.dump([s.to_dict() for s in self.strategies], f, indent=2)
            logger.info(f""Saved {len(self.strategies)} strategies to {self.db_path}"")
        except Exception as e:
            logger.error(f""Error saving strategies: {str(e)}"")
        
        # Save metrics
        try:
            with open(self.metrics_path, 'w') as f:
                json.dump(self.metrics, f, indent=2)
            logger.info(f""Saved metrics to {self.metrics_path}"")
        except Exception as e:
            logger.error(f""Error saving metrics: {str(e)}"")",Save strategies and metrics to disk.,Persist strategies and metrics to JSON files with error logging.
887,train,"def train():
    
    try:
        JobpostingCrew().crew().train(n_iterations=int(sys.argv[1]), filename=sys.argv[2], inputs=inputs)

    except Exception as e:
        raise Exception(f""An error occurred while training the crew: {e}"")",Train the crew for a given number of iterations.,"Initiates crew training with specified iterations and input file, handling errors."
888,save_removed_items,"def save_removed_items(app_name, removed_items):
    
    app_state_dir = ensure_state_directory(app_name)
    removed_file = os.path.join(app_state_dir, ""removed_items.json"")
    
    try:
        with open(removed_file, 'w') as f:
            json.dump(removed_items, f, indent=2)
    except IOError as e:
        swaparr_logger.error(f""Error saving removed items for {app_name}: {str(e)}"")
        SWAPARR_STATS['errors_encountered'] += 1",Save list of permanently removed items,Persist removed items to a JSON file within the application's state directory.
889,sample_postgres_specific_queries,"def sample_postgres_specific_queries() -> dict[str, str]:
    
    return {
        ""vacuum"": ""VACUUM users"",
        ""analyze"": ""ANALYZE users"",
        ""copy_to"": ""COPY users TO '/tmp/users.csv' WITH CSV"",
        ""copy_from"": ""COPY users FROM '/tmp/users.csv' WITH CSV"",
        ""explain"": ""EXPLAIN ANALYZE SELECT * FROM users"",
    }",Sample PostgreSQL-specific queries for testing.,Provides PostgreSQL commands for maintenance and data transfer.
890,sol_swap,"def sol_swap(agent, **kwargs):
    
    agent.logger.info(""\n🔄 INITIATING TOKEN SWAP"")
    try:
        result = agent.connection_manager.perform_action(
            connection_name=""solana"",
            action_name=""trade"",
            params=[
                kwargs.get('output_mint'),
                kwargs.get('input_amount'),
                kwargs.get('input_mint', None),
                kwargs.get('slippage_bps', 100)
            ]
        )
        agent.logger.info(""✅ Swap completed!"")
        return result
    except Exception as e:
        agent.logger.error(f""❌ Swap failed: {str(e)}"")
        return False",Swap tokens using Jupiter,"Initiates and executes a token swap on Solana, logging success or failure."
891,get_all_tasks,"def get_all_tasks():
    
    try:
        # Get all keys matching the task info pattern
        task_keys = redis_client.keys(""task:*:info"")

        # Extract task IDs from the keys
        task_ids = [key.decode(""utf-8"").split("":"")[1] for key in task_keys]

        # Get info for each task
        tasks = []
        for task_id in task_ids:
            task_info = get_task_info(task_id)
            last_status = get_last_task_status(task_id)

            if task_info and last_status:
                tasks.append(
                    {
                        ""task_id"": task_id,
                        ""type"": task_info.get(""type"", ""unknown""),
                        ""name"": task_info.get(""name"", ""Unknown""),
                        ""artist"": task_info.get(""artist"", """"),
                        ""download_type"": task_info.get(""download_type"", ""unknown""),
                        ""status"": last_status.get(""status"", ""unknown""),
                        ""timestamp"": last_status.get(""timestamp"", 0),
                    }
                )

        return tasks
    except Exception as e:
        logger.error(f""Error getting all tasks: {e}"")
        return []",Get all active task IDs,Retrieve and compile detailed information for all tasks from a Redis database.
892,unmask_pydantic,"def unmask_pydantic(document: MaskedDocument, data: TBaseModel) -> TBaseModel:
        
        # Step 1: first try to unmask the JSON string
        json_document = document.with_content(data.model_dump_json())
        try:
            unmasked = MarkdownPruningPipe.unmask(json_document)
            data = data.__class__.model_validate_json(unmasked)
        except Exception as e:
            # if that fails, try to unmask the markdown string
            logger.debug(f""Failed to unmask the JSON string: {e}"")

        # Step 2: look for string fields in the model that are exactly the same as the masked document
        def recursive_unmask(data: dict[str, Any]) -> dict[str, Any]:
            for key, value in data.items():
                if isinstance(value, str):
                    if value in document.links:
                        data[key] = document.links[value]
                    elif value in document.images:
                        data[key] = document.images[value]
                elif isinstance(value, dict):
                    data[key] = recursive_unmask(value)  # pyright: ignore[reportUnknownArgumentType]
                elif isinstance(value, list):
                    data[key] = [recursive_unmask(item) for item in value]  # pyright: ignore[reportUnknownArgumentType, reportUnknownVariableType]
            return data

        unmasked_data = recursive_unmask(data.model_dump())
        return data.__class__.model_validate(unmasked_data)",Unmask the links and images from the document using pydantic.,Unmask and validate Pydantic model data by processing JSON and markdown content.
893,_create_index,"def _create_index(self, document_paths: List[str]) -> Optional[VectorStoreIndex]:
        
        try:
            all_documents = self._load_documents(document_paths)

            if not all_documents:
                logger.warning(""No valid documents found"")
                return None

            total_chunks = 0
            for doc in all_documents:
                chunks = self.text_splitter.split_text(doc.text)
                total_chunks += len(chunks)
                logger.debug(f""Created {len(chunks)} chunks from document {doc.metadata.get('file_name', 'unknown')}"")
                for i, chunk in enumerate(chunks[:2]):  # Log only first 2 chunks as preview
                    logger.debug(f""Chunk {i+1} preview ({len(chunk)} chars): {chunk[:100]}..."")

            logger.info(f""Total chunks created: {total_chunks}"")
            logger.info(""Creating vector index..."")
            
            index = VectorStoreIndex.from_documents(
                all_documents,
                storage_context=self.storage_context,
                transformations=[self.text_splitter],
                show_progress=True
            )
            
            self.storage_context.persist(persist_dir=self.persist_dir)
            logger.info(f""Created and persisted index with {len(all_documents)} documents"")
            
            return index

        except Exception as e:
            logger.error(f""Error creating index: {str(e)}"")
            return None",Create a new index from documents.,"Create a vector index from documents, logging progress and handling errors."
894,_make_anthropic_request,"def _make_anthropic_request(image_handle: IO, media_type: str) -> anthropic.types.Message:
    
    client = anthropic.Anthropic()
    data = _encode_image(image_handle)
    return client.messages.create(
        model=MODEL,
        max_tokens=MAX_TOKENS,
        messages=[
            {
                ""role"": ""user"",
                ""content"": [
                    {  # type: ignore
                        ""type"": ""image"",
                        ""source"": {
                            ""type"": ""base64"",
                            ""media_type"": media_type,
                            ""data"": data,
                        },
                    },
                    {  # type: ignore
                        ""type"": ""text"",
                        ""text"": PROMPT,
                    },
                ],
            }
        ],
    )",Make a request to the Anthropic API using an image.,Send an encoded image and prompt to an AI model for message generation.
895,grade_agent,"def grade_agent(submissions: List[SubmissionInfo]):
        
        print(""📊 Grading the agent's performance..."")

        # Get current working directory
        original_cwd = Path.cwd()

        # Write the list of dicts to a JSONL file
        submissions_file = original_cwd / ""submissions.jsonl""
        with open(submissions_file, ""w"") as f:
            for submission in submissions:
                f.write(
                    json.dumps(
                        {""competition_id"": submission.competition_id, ""submission_path"": submission.submission_path}
                    )
                    + ""\n""
                )

        # Create grades directory if it doesn't exist
        grades_dir = original_cwd / ""grades""
        grades_dir.mkdir(exist_ok=True)

        CommandRunner.run(
            [""mlebench"", ""grade"", ""--submission"", str(submissions_file), ""--output-dir"", str(grades_dir)],
            ""Failed to grade the agent."",
            ""Agent graded successfully."",
        )
        print(f""🏆 Agent grading completed for {len(submissions)} tests."")

        return grades_dir",Grade the agent's performance based on the test results,"Automate grading of agent submissions, outputting results to a directory."
896,load_all_robots_data,"def load_all_robots_data():
    
    if not os.path.isfile(ROBOT_LIST_JSON):
        raise FileNotFoundError(f""Cannot find {ROBOT_LIST_JSON}"")
    with open(ROBOT_LIST_JSON, encoding=""utf-8"") as f:
        data = json.load(f)
    return data",Load the entire robots_init_list.json and return as a dict.,"Load and return robot data from a JSON file, raising an error if missing."
897,add_convergence_node,"def add_convergence_node(self, name: str) -> None:
        
        if name not in self.workflow.nodes:
            raise ValueError(f""Node '{name}' does not exist"")
        if name not in self.workflow.workflow.convergence_nodes:
            self.workflow.workflow.convergence_nodes.append(name)
            logger.debug(f""Added convergence node '{name}'"")",Add a convergence node to the workflow.,Ensure a node is added to the convergence list if it exists in the workflow
898,log_system_info,"def log_system_info():
    
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        gpu_info = []
        if torch.cuda.is_available():
            for gpu in GPUtil.getGPUs():
                gpu_info.append({
                    'id': gpu.id,
                    'name': gpu.name,
                    'load': f""{gpu.load*100}%"",
                    'memory_used': f""{gpu.memoryUsed}MB/{gpu.memoryTotal}MB"",
                    'temperature': f""{gpu.temperature}°C""
                })
        logger.info(f""System Info - CPU: {cpu_percent}%, RAM: {memory.percent}%, ""
                   f""Available RAM: {memory.available/1024/1024/1024:.1f}GB"")
        if gpu_info:
            logger.info(f""GPU Info: {gpu_info}"")
    except Exception as e:
        logger.warning(f""Failed to log system info: {str(e)}"")",Log system resource information,"Log and report CPU, RAM, and GPU usage statistics with error handling."
899,tokenize_row,"def tokenize_row(feature, is_encoder_decoder: bool, tokenizer: PreTrainedTokenizerBase) -> dict[str, Any]:
        
        if not is_encoder_decoder:
            batch = tokenizer(feature[""prompt""], add_special_tokens=False)
            # Add BOS token to head of prompt. Avoid adding if it's already there
            if tokenizer.bos_token_id is not None:
                prompt_len_input_ids = len(batch[""input_ids""])
                if prompt_len_input_ids == 0 or tokenizer.bos_token_id != batch[""input_ids""][0]:
                    batch[""input_ids""] = [tokenizer.bos_token_id] + batch[""input_ids""]
                    batch[""attention_mask""] = [1] + batch[""attention_mask""]
        else:
            batch = tokenizer(feature[""prompt""], add_special_tokens=True)
        batch = {f""prompt_{key}"": value for key, value in batch.items()}
        return batch",Tokenize a single row from a DPO specific dataset.,Tokenize input text with optional special tokens for encoder-decoder models
900,_clean_tss_markup,"def _clean_tss_markup(
        input_text: str, 
        additional_tags: List[str] = [""Person1"", ""Person2""]
    ) -> str:
        
        try:
            input_text = ContentCleanerMixin._clean_scratchpad(input_text)
            supported_tags = [""speak"", ""lang"", ""p"", ""phoneme"", ""s"", ""sub""]
            supported_tags.extend(additional_tags)

            pattern = r""</?(?!(?:"" + ""|"".join(supported_tags) + r"")\b)[^>]+>""
            cleaned_text = re.sub(pattern, """", input_text)
            cleaned_text = re.sub(r""\n\s*\n"", ""\n"", cleaned_text)
            cleaned_text = re.sub(r""\*"", """", cleaned_text)

            for tag in additional_tags:
                cleaned_text = re.sub(
                    f'<{tag}>(.*?)(?=<(?:{""|"".join(additional_tags)})>|$)',
                    f""<{tag}>\\1</{tag}>"",
                    cleaned_text,
                    flags=re.DOTALL,
                )
            


            return cleaned_text.strip()
            
        except Exception as e:
            logger.error(f""Error cleaning TSS markup: {str(e)}"")
            return input_text",Remove unsupported TSS markup tags while preserving supported ones.,Sanitizes text by removing unsupported tags and formatting specified ones.
901,left_hand_orientation,"def left_hand_orientation(envstate, robot_name: str):
    
    # return envstate[f""{_METASIM_SITE_PREFIX}left_hand""][""rot""] # Only for mujoco
    return envstate[""robots""][robot_name][""body""][""left_elbow_link""][""rot""]",Returns the orientation of the left hand.,Retrieve left elbow rotation from robot's environment state
902,_log_plots,"def _log_plots(plots, step):
    
    for name, params in plots.copy().items():  # shallow copy to prevent plots dict changing during iteration
        timestamp = params[""timestamp""]
        if _processed_plots.get(name) != timestamp:
            wb.run.log({name.stem: wb.Image(str(name))}, step=step)
            _processed_plots[name] = timestamp",Logs plots from the input dictionary if they haven't been logged already at the specified step.,Log unprocessed plot images with timestamps to a tracking system.
903,delete_collection,"def delete_collection(self):
        
        if not self.collection:
            raise ValueError(""Collection ID is required to delete a collection."")
        try:
            self.collection.delete()
            return {
                ""success"": True,
                ""message"": f""Collection {self.collection.id} deleted successfully"",
            }
        except Exception as e:
            raise Exception(
                f""Failed to delete collection {self.collection.id}: {str(e)}""
            )",Delete the current collection.,Delete a collection and handle potential errors during the process.
904,track_request,"def track_request(request_type: str) -> Callable[[AsyncCallable[P, R]], AsyncCallable[P, R]]:
    

    def decorator(func: AsyncCallable[P, R]) -> AsyncCallable[P, R]:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            diagnostics.track_request(request_type)
            if diagnostics.enabled:
                logger.debug(f""Request: {request_type}"")
            return await func(*args, **kwargs)

        return cast(AsyncCallable[P, R], wrapper)

    return decorator",Decorator to track request counts by type.,Decorator logs and tracks asynchronous requests based on request type.
905,register_actions,"def register_actions(self) -> None:
        
        self.actions = {
            ""generate-text"": Action(
                name=""generate-text"",
                parameters=[
                    ActionParameter(""prompt"", True, str, ""The input prompt for text generation""),
                    ActionParameter(""system_prompt"", True, str, ""System prompt to guide the model""),
                    ActionParameter(""model"", False, str, ""Model to use for generation""),
                    ActionParameter(""temperature"", False, float, ""A decimal number that determines the degree of randomness in the response."")
                ],
                description=""Generate text using Groq models""
            ),
            ""check-model"": Action(
                name=""check-model"",
                parameters=[
                    ActionParameter(""model"", True, str, ""Model name to check availability"")
                ],
                description=""Check if a specific model is available""
            ),
            ""list-models"": Action(
                name=""list-models"",
                parameters=[],
                description=""List all available Groq models""
            )
        }",Register available Groq actions,Define and register actions for text generation and model management.
906,_record,"def _record(self) -> None:
        
        stream = self.audio.open(format=self.format, channels=self.channels, rate=self.rate,
                                 input=True, frames_per_buffer=self.chunk)
        if self.verbose:
            print(Fore.GREEN + ""AudioRecorder: Started recording..."" + Fore.RESET)

        while not done:
            frames = []
            for _ in range(0, int(self.rate / self.chunk * self.record_seconds)):
                try:
                    data = stream.read(self.chunk, exception_on_overflow=False)
                    frames.append(data)
                except Exception as e:
                    print(Fore.RED + f""AudioRecorder: Failed to read stream - {e}"" + Fore.RESET)
            
            raw_data = b''.join(frames)
            audio_data = np.frombuffer(raw_data, dtype=np.int16)
            audio_queue.put((audio_data, self.rate))
            if self.verbose:
                print(Fore.GREEN + ""AudioRecorder: Added audio chunk to queue"" + Fore.RESET)

        stream.stop_stream()
        stream.close()
        self.audio.terminate()
        if self.verbose:
            print(Fore.GREEN + ""AudioRecorder: Stopped"" + Fore.RESET)",Record audio from the microphone and add it to the audio queue.,Capture and queue audio data in real-time with optional verbose logging.
907,is_mutative_action_allowed,"def is_mutative_action_allowed(
    mcp: FastMCP, sns_client: Any, kwargs: Dict[str, Any]
) -> Tuple[bool, str]:
    
    # Check for TopicArn (used by most operations)
    resource_arn = kwargs.get('TopicArn')

    if resource_arn is None or resource_arn == '':
        return False, 'TopicArn is not passed to the tool'

    try:
        tags = sns_client.list_tags_for_resource(ResourceArn=resource_arn)
        tag_dict = {tag.get('Key'): tag.get('Value') for tag in tags.get('Tags', [])}
        return validate_mcp_server_version_tag(tag_dict)
    except Exception as e:
        return False, str(e)",Check if the SNS resource being mutated is tagged with mcp_server_version.,Determine if a mutative action is permissible based on resource tags.
908,handle_http_exception,"def handle_http_exception(e):
    
    # start with the correct headers and status code from the error

    if e.code != 404:
        logger.exception(e)

    response = e.get_response()
    # replace the body with JSON
    response.data = json.dumps(
        {
            ""message"": e.description,
        }
    )
    response.content_type = ""application/json""
    return response",Return JSON instead of HTML for HTTP errors.,Handle HTTP exceptions by logging and returning JSON error responses.
909,_fetch_tp_shard_tensor_gate_up,"def _fetch_tp_shard_tensor_gate_up(tensor, gate_name, up_name) -> torch.Tensor:
        
        nonlocal state_dict
        nonlocal mp_group
        tp_rank = mpu.get_tensor_model_parallel_rank()
        tp_size = mpu.get_tensor_model_parallel_world_size()
        if gate_name in state_dict and up_name in state_dict:
            gate_weight = state_dict[gate_name]
            up_weight = state_dict[up_name]
            new_gate_up_weight = torch.empty(config.intermediate_size * 2, config.hidden_size, dtype=params_dtype, device=torch.cuda.current_device())
            for i in range(tp_size):
                gate_weight_tp = gate_weight[i * intermediate_size_tp : (i + 1) * intermediate_size_tp]
                up_weight_tp = up_weight[i * intermediate_size_tp : (i + 1) * intermediate_size_tp]
                new_gate_up_weight[intermediate_size_tp * 2 * i : intermediate_size_tp * 2 * (i + 1)].copy_(torch.cat([gate_weight_tp, up_weight_tp], dim=0))

            tensor_chunk = torch.chunk(new_gate_up_weight, tp_size, dim=0)
            if tensor is not None:
                tensor.data.copy_(tensor_chunk[tp_rank])
        else:
            print(f""tp_shard tensor:[{gate_name}, {up_name}] not in state_dict, skip loading"")",fetch gate_up tensor in tp shards,Load and distribute tensor shards across parallel processing units if available
910,transaction_context,"def transaction_context(name: str = ""default""):
    
    print(f""Starting transaction: {name}"")
    try:
        yield name
        print(f""Committing transaction: {name}"")
    except Exception as e:
        print(f""Rolling back transaction: {name}, error: {e}"")
        raise",Context manager for transaction-like operations.,"Manage transaction lifecycle with start, commit, and rollback operations."
911,_validate_tweet_text,"def _validate_tweet_text(self, text: str, context: str = ""Tweet"") -> None:
        
        if not text:
            error_msg = f""{context} text cannot be empty""
            logger.error(error_msg)
            raise ValueError(error_msg)
        if len(text) > 280:
            error_msg = f""{context} exceeds 280 character limit""
            logger.error(error_msg)
            raise ValueError(error_msg)
        logger.debug(f""Tweet text validation passed for {context.lower()}"")",Validate tweet text meets Twitter requirements,Validate tweet text for non-emptiness and character limit compliance
912,get_lm_model,"def get_lm_model(self) -> LMModel:
        
        device = next(self.parameters()).device
        lm = LMModel(self.quantizer.n_q, self.quantizer.bins, num_layers=5, dim=200,
                     past_context=int(3.5 * self.frame_rate)).to(device)
        checkpoints = {
            'encodec_24khz': 'encodec_lm_24khz-1608e3c0.th',
            'encodec_48khz': 'encodec_lm_48khz-7add9fc3.th',
        }
        try:
            checkpoint_name = checkpoints[self.name]
        except KeyError:
            raise RuntimeError(""No LM pre-trained for the current Encodec model."")
        url = _get_checkpoint_url(ROOT_URL, checkpoint_name)
        state = torch.hub.load_state_dict_from_url(
            url, map_location='cpu', check_hash=True)  # type: ignore
        lm.load_state_dict(state)
        lm.eval()
        return lm",Return the associated LM model to improve the compression rate.,Load and initialize a language model with pre-trained weights based on the Encodec model type.
913,clear_store,"def clear_store(self):
        
        try:
            # Get embedding dimension from existing index or create new
            if hasattr(self.store, ""index"") and self.store.index is not None:
                dim = self.store.index.d
            else:
                dim = len(self.embeddings.embed_query(""test""))

            # Reset FAISS index
            self.store.index = faiss.IndexFlatL2(dim)

            # Clear document store
            self.store.docstore._dict.clear()
            self.store.index_to_docstore_id.clear()

            # Save empty state
            self.save()
            logger.info(""Vector store cleared successfully"")
            return True

        except Exception as e:
            logger.error(f""Error clearing store: {e}"")
            raise e",Clear all documents and reset the FAISS index,"Clear and reset the vector and document store, handling exceptions."
914,is_configured,"def is_configured(self, verbose = False) -> bool:
        
        try:
            load_dotenv()
            api_key = os.getenv('XAI_API_KEY')
            if not api_key:
                return False

            client = self._get_client()
            client.models.list()
            return True
            
        except Exception as e:
            if verbose:
                logger.debug(f""Configuration check failed: {e}"")
            return False",Check if XAI API key is configured and valid,"Check if API configuration is valid and accessible, logging errors if verbose."
915,parse,"def parse(self, input: dict, response: PropertyList) -> dict:
        
        return [{""question"": input[""question""], ""property"": property} for property in response.properties_list]",Parse the model response into the desired output format.,Transform input question and properties into a list of dictionaries.
916,verify_repo_state,"def verify_repo_state(repo_path: Path, expected_content: dict[str, str | None]):
    
    for path, content in expected_content.items():
        file_path = repo_path / path
        if content is None:
            assert not file_path.exists(), f""File {path} should not exist""
        else:
            assert file_path.read_text() == content, f""File {path} has wrong content""",Verify file contents in repo,Verify repository files match expected content or absence
917,retrieve_not_reflected_memory,"def retrieve_not_reflected_memory(self):
        
        if not self.retrieve_not_reflected_top_k:
            return

        filter_dict = {
            ""user_name"": self.user_name,
            ""target_name"": self.target_name,
            ""store_status"": StoreStatusEnum.VALID.value,
            ""memory_type"": [MemoryTypeEnum.OBSERVATION.value, MemoryTypeEnum.OBS_CUSTOMIZED.value],
            ""obs_reflected"": 0,
        }
        nodes: List[MemoryNode] = self.memory_store.retrieve_memories(top_k=self.retrieve_not_reflected_top_k,
                                                                      filter_dict=filter_dict)
        self.memory_manager.set_memories(NOT_REFLECTED_NODES, nodes)",Retrieves top-K not reflected memories based on the query and stores them in the memory handler.,Retrieve and store unreflected memory nodes based on specific criteria.
918,_execute_shell_command,"def _execute_shell_command(self, command: str, sandbox: Sandbox | None = None) -> str:
        
        if not sandbox:
            raise ValueError(""Sandbox instance is required for command execution."")

        try:
            process = sandbox.commands.run(command, background=True)
        except Exception as e:
            raise ToolExecutionException(f""Error during shell command execution: {e}"", recoverable=True)

        output = process.wait()
        if output.exit_code != 0:
            raise ToolExecutionException(f""Error during shell command execution: {output.stderr}"", recoverable=True)
        return output.stdout",Executes a shell command in the specified sandbox.,"Execute shell command in sandbox, handle errors, and return output."
919,get_qrcode_file_path,"def get_qrcode_file_path(instance, _):
    

    file_name = f'{instance.id}.svg'
    file_path = '/'.join([str(instance.owner.user.id), 'qr', file_name])

    return str(file_path)",Get the file path for the qr code of a shared PDF.,Generate a file path for a user's QR code based on their ID.
920,get_app_icon_pixbuf,"def get_app_icon_pixbuf(icon_path, width, height):
    
    if not icon_path:
        return None
        icon_path = icon_path[7:]
    if not os.path.exists(icon_path):
        logger.warning(f""Icon path does not exist: {icon_path}"")
        return None
    try:
        pixbuf = GdkPixbuf.Pixbuf.new_from_file(icon_path)
        return pixbuf.scale_simple(width, height, GdkPixbuf.InterpType.BILINEAR)
    except Exception as e:
        logger.error(f""Failed to load or scale icon: {e}"")
        return None",Loads and scales a pixbuf from an app icon path.,"Load and scale an application icon from a file path, handling errors gracefully."
921,plex_status,"def plex_status():
    
    try:
        from src.primary.auth import get_user_data
        user_data = get_user_data()
        
        if not user_data:
            return jsonify({
                'success': False,
                'error': 'No user found'
            }), 404
        
        plex_linked = user_data.get('plex_linked', False)
        auth_type = user_data.get('auth_type', 'local')
        
        response_data = {
            'success': True,
            'plex_linked': plex_linked,
            'auth_type': auth_type
        }
        
        if plex_linked or auth_type == 'plex':
            response_data.update({
                'plex_username': user_data.get('plex_username'),
                'plex_email': user_data.get('plex_email'),
                'plex_linked_at': user_data.get('plex_linked_at')
            })
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f""Error getting Plex status: {e}"")
        return jsonify({
            'success': False,
            'error': 'Internal server error'
        }), 500",Get Plex authentication status for current user,"Check user authentication status and Plex account linkage, returning JSON response."
922,force_flush,"def force_flush(self):
        
        try:
            return self.exporter.force_flush()
        except Exception as e:
            self.logger.error(f""Error during trace force_flush: {str(e)}"")
            return False",Force flush the exporter.,"Attempt to flush data, log errors, and return success status"
923,extract_result_value,"def extract_result_value(result: str) -> str:
        
        try:
            root = etree.fromstring(result, parser=XMLResultHandler._parser)
            return root.findtext(""Value"") or """" if root is not None else """"
        except etree.XMLSyntaxError as e:
            logger.warning(f""XML extraction error: {e}"")
            return """"",Safely extract the value from the result XML.,"Extracts a specific value from XML string, handling errors gracefully."
924,display_tool_output,"def display_tool_output(
    tool_call_input: dict[str, Any], tool_call_output: dict[str, Any]
) -> None:
    
    tool_expander = st.expander(label=""Tool Calls:"", expanded=False)
    with tool_expander:
        msg = (
            f""\n\nEnding tool: `{tool_call_input}` with\n **args:**\n""
            f""```\n{json.dumps(tool_call_input, indent=2)}\n```\n""
            f""\n\n**output:**\n ""
            f""```\n{json.dumps(tool_call_output, indent=2)}\n```""
        )
        st.markdown(msg, unsafe_allow_html=True)",Display the input and output of a tool call in an expander.,Display formatted tool input and output in an expandable section using Streamlit.
925,process_file,"def process_file(path: str, parent: Path) -> Any:
        
        full_path = parent / Path(path).resolve()
        if not full_path.exists():
            raise FileNotFoundError(f""File {full_path} not found."")

        with open(full_path, ""r"") as file:
            items = json.load(file)
            if isinstance(items, list):
                return [PromptyHelper.normalize(value, parent) for value in items]
            elif isinstance(items, dict):
                return {
                    key: PromptyHelper.normalize(value, parent)
                    for key, value in items.items()
                }
            return items",Process file resolution and return its contents as JSON.,"Load and normalize JSON data from a file path, handling lists and dictionaries."
926,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""CSS.enable"",
    }
    json = yield cmd_dict",Enables the CSS agent for the given page.,Enable CSS debugging by sending a command dictionary to a generator.
927,format_board_pair,"def format_board_pair(
    index: int,
    pair: dict[str, list[list[int]]],
    formatting_options: BoardFormattingOptions,
) -> str:
    
    input_element = format_board(
        pair[""input""],
        formatting_options=formatting_options,
    )
    output_element = format_board(
        pair[""output""],
        formatting_options=formatting_options,
    )
    return f""Example {index}:\n\nInput:\n{input_element}\nOutput:\n{output_element}\n\n""",Format a board pair as a string,Format input-output board pairs into a structured string representation.
928,calc_group_id,"def calc_group_id(uid: str, home_id: str) -> str:
    
    return hashlib.sha1(
        f'{uid}central_service{home_id}'.encode('utf-8')).hexdigest()[:16]",Calculate the group ID based on a user ID and a home ID.,Generate a unique group identifier by hashing user and home IDs.
929,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Inspector.disable"",
    }
    json = yield cmd_dict",Disables inspector domain notifications.,Disables the Inspector tool by sending a command dictionary.
930,calc_maj_val,"def calc_maj_val(data: list[dict[str, Any]], vote_key: str, val_key: str) -> float:
    
    vote2vals = defaultdict(list)
    for d in data:
        vote2vals[d[vote_key]].append(d[val_key])

    vote2cnt = {k: len(v) for k, v in vote2vals.items()}
    maj_vote = max(vote2cnt, key=vote2cnt.get)

    maj_val = vote2vals[maj_vote][0]

    return maj_val",Calculate the majority voting metric,Determine the majority value based on vote frequency in data records.
931,log_metrics,"def log_metrics(self, raw_rewards: List[float], weighted_rewards: List[float]):
        
        if not self.wandb_logger or not raw_rewards:
            return

        metrics = {
            f""reward/{self.name}/mean_raw"": sum(raw_rewards) / len(raw_rewards),
            f""reward/{self.name}/mean_weighted"": sum(weighted_rewards)
            / len(weighted_rewards),
            f""reward/{self.name}/min"": min(raw_rewards),
            f""reward/{self.name}/max"": max(raw_rewards),
        }

        self.wandb_logger.log(metrics)",Log reward metrics to WandB,Log reward statistics using a Weights & Biases logger if available.
932,convert_fraction_word,"def convert_fraction_word(fraction_str: str) -> str:
        

        # Add fraction word mapping
        FRACTION_WORDS = {
            ""half"": ""1/2"",
            ""one-half"": ""1/2"",
            ""quarter"": ""1/4"",
        }
        return FRACTION_WORDS.get(fraction_str.lower(), fraction_str)",Convert word fractions to numeric form,Convert fraction words to numeric representation using a predefined mapping.
933,log_request,"def log_request(method: str, path: str, status_code: int, client: Any) -> None:
    
    logger = logging.getLogger(""proxy_pilot"")
    log_data = {
        ""timestamp"": datetime.now().isoformat(),
        ""type"": ""request"",
        ""method"": method,
        ""path"": path,
        ""status_code"": status_code,
        ""client"": serialize_for_logging(client),
    }
    logger.info(f""Request: {json.dumps(log_data, indent=2)}"")",Log HTTP request details,"Log HTTP request details including method, path, status, and client info."
934,_get_openai_response,"def _get_openai_response(
    openai_client,
    prompt,
    model, 
    temperature,
    max_tokens,
    ):
    
    try:
        response = openai_client.chat.completions.create(
            model=model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f""Error with OpenAI API: {str(e)}"")
        return None",Get response from OpenAI API,"Fetch AI-generated text using specified model and parameters, handling errors."
935,get_full_website_information,"def get_full_website_information(result_full, image_dir="""", fullpage_split_dict=None, save_slim_dir=None):
    
    if save_slim_dir is None:
        save_slim_dir = image_dir

    input_image_list = []
    inst = result_full[0]  # assert only 1 fullpage content

    template = f""Website Title: {inst['title']};\n Website Snippet: {inst['snippet']};\n""

    # add content
    template += f""Website Content: {inst['content']};\n""

    ## slim image to be tense
    if ""screenshot_fullpage_path"" in inst:
        split_list = inst[""screenshot_fullpage_path""].split(""/"")[-1].split(""."")
        save_name = ""."".join(split_list[:-1]) + f""_slim.{split_list[-1]}""
        save_path = os.path.join(save_slim_dir, save_name)

        slim_image_and_save(image_path=os.path.join(image_dir, inst[""screenshot_fullpage_path""]), save_path=save_path)
        save_slice_path = os.path.join(save_slim_dir, ""slices"")
        os.makedirs(save_slice_path, exist_ok=True)
    elif ""slimmed_website_fullpage_screenshot"" in inst:  # the screenshot is already slimmed
        save_path = inst[""slimmed_website_fullpage_screenshot""]
        save_slice_path = None  # do not save the slices
    else:
        raise ValueError(""seems that the inst variable does not contain relevant key"")

    # here, we split the fullpage to maximum 10 images, each with 512 height (the width depends on the website itself)
    screenshot_fullpage_split_list = crop_and_split(fullpage_path=save_path, fullpage_split_dict=fullpage_split_dict, save_slice_path=save_slice_path)
    template += f""Website Screenshot: {DEFAULT_IMAGE_TOKEN*len(screenshot_fullpage_split_list)};\n""
    input_image_list.extend(screenshot_fullpage_split_list)

    website_information = template

    return website_information, input_image_list","result_full: [{'title', 'snippet', 'content','screenshot_path'}]","Extracts and processes website data, including content and images, for structured output."
936,write_log_file,"def write_log_file(self, logs, filepath):
        
        content = """"
        for entry in logs:
            color_info = self.css_color_map.get(
                entry[""color""], (entry[""color""], ""#f5f5f5"")
            )
            content += f""<p style='color:{color_info[0]};background:{color_info[1]}'>{entry['text']}</p>\n""

        with open(filepath, ""w"") as f:
            f.write(self.log_file_template.replace(""{{content}}"", content))",Write the complete log file using the stored log entries,Generate an HTML log file with styled entries based on a color map.
937,dispatch_megatron_compute,"def dispatch_megatron_compute(worker_group, *args, **kwargs):
    
    from verl.single_controller.base.megatron.worker_group import MegatronWorkerGroup
    assert isinstance(worker_group,
                      MegatronWorkerGroup), f'worker_group must be MegatronWorkerGroup, Got {type(worker_group)}'

    all_args = []
    for arg in args:
        assert isinstance(arg, (Tuple, List)) and len(arg) == worker_group.dp_size
        transformed_args = []
        for i in range(worker_group.world_size):
            local_dp_rank = worker_group.get_megatron_rank_info(rank=i).dp_rank
            transformed_args.append(arg[local_dp_rank])
        all_args.append(transformed_args)
    all_args = tuple(all_args)

    all_kwargs = {}
    for k, v in kwargs.items():
        assert isinstance(v, (Tuple, List)) and len(v) == worker_group.dp_size
        transformed_v = []
        for i in range(worker_group.world_size):
            local_dp_rank = worker_group.get_megatron_rank_info(rank=i).dp_rank
            transformed_v.append(v[local_dp_rank])
        all_kwargs[k] = transformed_v
    return all_args, all_kwargs",User passes in dp data.,Distribute computation tasks across a Megatron worker group by transforming and aligning input arguments.
938,remove_project,"def remove_project(
    name: str = typer.Argument(..., help=""Name of the project to remove""),
) -> None:
    
    try:
        project_url = config.project_url

        response = asyncio.run(call_delete(client, f""{project_url}/project/projects/{name}""))
        result = ProjectStatusResponse.model_validate(response.json())

        console.print(f""[green]{result.message}[/green]"")
    except Exception as e:
        console.print(f""[red]Error removing project: {str(e)}[/red]"")
        console.print(""[yellow]Note: Make sure the Basic Memory server is running.[/yellow]"")
        raise typer.Exit(1)

    # Show this message regardless of method used
    console.print(""[yellow]Note: The project files have not been deleted from disk.[/yellow]"")",Remove a project from configuration.,"Remove a project by name, handling errors and displaying status messages."
939,create_checkpoint,"def create_checkpoint(self, label: str, directory: str = ""checkpoints"") -> bool:
        
        os.makedirs(directory, exist_ok=True)
        path = os.path.join(directory, f""{label}.json"")
        return self.save_state(path)",Save a checkpoint with the given label in the specified directory.,Creates a labeled checkpoint by saving the current state to a JSON file.
940,is_configured,"def is_configured(self, verbose: bool = False) -> bool:
        
        try:
            # First check if credentials exist and key is valid
            load_dotenv(override=True)
            private_key = os.getenv(""SOLANA_PRIVATE_KEY"")
            if not private_key:
                if verbose:
                    logger.debug(""Solana private key not found in environment"")
                return False

            # Validate the key format
            Keypair.from_base58_string(private_key)

            # We successfully validated the private key exists and is in correct format
            if verbose:
                logger.debug(""Solana configuration is valid"")
            return True

        except Exception as e:
            if verbose:
                error_msg = str(e)
                if isinstance(e, SolanaConfigurationError):
                    error_msg = f""Configuration error: {error_msg}""
                elif isinstance(e, SolanaConnectionError):
                    error_msg = f""API validation error: {error_msg}""
                logger.debug(f""Solana Configuration validation failed: {error_msg}"")
            return False",Check if Solana credentials are configured and valid,Check if Solana configuration is valid by verifying private key presence and format
941,policy,"def policy(cluster, tokenizer):
    
    vllm_config = deepcopy(basic_vllm_test_config)
    # Ensure async_engine is False for the standard policy fixture
    vllm_config[""vllm_cfg""][""async_engine""] = False
    vllm_config = configure_generation_config(vllm_config, tokenizer)
    p = VllmGeneration(cluster, vllm_config)
    yield p
    try:
        p.shutdown()
        import gc

        gc.collect()
        torch.cuda.empty_cache()
    except Exception as e:
        print(f""Error during policy cleanup: {e}"")",Initialize the vLLM policy (synchronous by default).,Configure and manage a VLLM generation policy with cleanup for a cluster environment.
942,chunk_embedding,"def chunk_embedding(self) -> bool:
        
        try:
            # Mark step as in progress
            self.progress.mark_step_status(ProcessStep.CHUNK_EMBEDDING, Status.IN_PROGRESS)
            documents = self.list_documents()
            for doc in documents:
                doc_id = doc.get(""id"")
                try:
                    # Directly call document service to generate chunk embeddings
                    processed_chunks = document_service.generate_document_chunk_embeddings(doc_id)
                    if not processed_chunks:
                        logger.warning(f""No chunks to process for document: {doc_id}"")
                        continue
                except Exception as e:
                    logger.error(
                        f""Generate chunk embeddings failed for doc_id: {doc_id}: {str(e)}""
                    )
                    self.progress.mark_step_status(ProcessStep.CHUNK_EMBEDDING, Status.FAILED)
                    return False
            # All documents' chunks processed successfully
            self.progress.mark_step_status(ProcessStep.CHUNK_EMBEDDING, Status.COMPLETED)
            return True
        except Exception as e:
            logger.error(f""Generate chunk embeddings failed: {str(e)}"")
            self.progress.mark_step_status(ProcessStep.CHUNK_EMBEDDING, Status.FAILED)
            return False",Process embeddings for all document chunks,"Process document chunks to generate embeddings, updating progress status accordingly."
943,safe_get_commit,"def safe_get_commit(self, commit: str) -> GitCommit | None:
        
        try:
            return self.git_cli.commit(commit)
        except Exception as e:
            logger.warning(f""Failed to get commit {commit}:\n\t{e}"")
            return None","Gets commit if it exists, else returns None","Retrieve a Git commit safely, logging failures and returning None on error."
944,get_service_json,"def get_service_json(self, service_name, namespace, deserialize=True):
        
        command = f""kubectl get service {service_name} -n {namespace} -o json""
        result = self.exec_command(command)

        return json.loads(result) if deserialize else result",Retrieve the JSON description of a specified service within a namespace.,"Retrieve Kubernetes service details as JSON, optionally deserializing"
945,ensure_face_icon,"def ensure_face_icon():
    
    face_icon_path = os.path.expanduser(""~/.face.icon"")
    default_icon_path = os.path.expanduser(f""~/.config/{APP_NAME_CAP}/assets/default.png"")
    if not os.path.exists(face_icon_path) and os.path.exists(default_icon_path):
        try:
            shutil.copy(default_icon_path, face_icon_path)
        except Exception as e:
            print(f""Error copying default face icon: {e}"")",Ensure the face icon exists.,Ensure user face icon exists by copying default if missing
946,_create_quick_visualizations,"def _create_quick_visualizations(self):
        
        if not PLOTTING_AVAILABLE or not self.study or len(self.study.trials) < 2:
            return

        # Create directory for visualizations
        viz_dir = os.path.join(self.output_dir, ""visualizations"")
        os.makedirs(viz_dir, exist_ok=True)

        # Create optimization history only (faster than full visualization)
        try:
            fig = plot_optimization_history(self.study)
            fig.write_image(
                os.path.join(
                    viz_dir, f""{self.study_name}_optimization_history_current.png""
                )
            )
        except Exception as e:
            logger.error(f""Error creating optimization history plot: {str(e)}"")",Create a smaller set of visualizations for intermediate progress.,Generate optimization history visualizations if conditions are met and save them to a directory.
947,to_base,"def to_base(self) -> SyncBase:
        
        return SyncBase(**self.model_dump(exclude={""run_immediately""}))",Convert to base schema.,"Converts current object to a SyncBase instance, excluding 'run_immediately'."
948,reinforce_identity,"def reinforce_identity(self)->bool:
        
        try:
            # Mark step as in progress
            self.progress.mark_step_status(ProcessStep.REINFORCE_IDENTITY, Status.IN_PROGRESS)
            logger.info(""Starting identity reinforcement..."")
            # Get or prepare L2 data
            self._prepare_l2_data()

            # Get training parameters
            training_params = TrainingParamsManager.get_latest_training_params()
            # Use data from l2_data dictionary
            l2_generator = L2Generator(
                data_path=os.path.join(os.getcwd(), ""resources""), is_cot=training_params.get(""is_cot"", False)
                )  
            l2_generator.gen_selfqa_data(
                    self.l2_data[""notes""],
                    self.l2_data[""basic_info""],
                    self.l2_data[""data_output_base_dir""],
                    self.l2_data[""topics_path""],
                    self.l2_data[""entitys_path""],
                    self.l2_data[""graph_path""],
                    self.l2_data[""config_path""]
                    )
            
            self.progress.mark_step_status(ProcessStep.REINFORCE_IDENTITY, Status.COMPLETED)
            logger.info(""Identity reinforcement completed successfully"")
            return True
            
        except Exception as e:
            logger.error(f""Reinforce identity failed: {str(e)}"")
            self.progress.mark_step_status(ProcessStep.REINFORCE_IDENTITY, Status.FAILED)
            return False",Reinforce identity using notes and related data,"Enhance identity by generating and processing L2 data, updating progress status accordingly."
949,issues_mixin,"def issues_mixin(self, jira_fetcher: JiraFetcher) -> IssuesMixin:
        
        mixin = jira_fetcher

        # Add mock methods that would be provided by other mixins
        mixin._get_account_id = MagicMock(return_value=""test-account-id"")
        mixin.get_available_transitions = MagicMock(
            return_value=[{""id"": ""10"", ""name"": ""In Progress""}]
        )
        mixin.transition_issue = MagicMock(
            return_value=JiraIssue(id=""123"", key=""TEST-123"", summary=""Test Issue"")
        )

        return mixin",Create an IssuesMixin instance with mocked dependencies.,Enhance JiraFetcher with mock methods for testing issue transitions.
950,stop_screencast,"def stop_screencast() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Page.stopScreencast"",
    }
    json = yield cmd_dict",Stops sending each frame in the ``screencastFrame``.,Stop a screencast by sending a command to the Page API.
951,process_image,"def process_image(image_path: str) -> Optional[str]:
    
    if not os.path.exists(image_path):
        print(f""Error: Image file not found at {image_path}"")
        return None
    
    try:
        # Just verify it's a valid image
        Image.open(image_path)
        return image_path
    except Exception as e:
        print(f""Error processing image {image_path}: {e}"")
        return None",Process image file and return path if valid,Validate and return image path if file exists and is valid
952,summarize_performance,"def summarize_performance(self):
        
        total = len(self.log)
        scores = [entry.get(""score"", 0.0) for entry in self.log if ""score"" in entry]
        avg_score = sum(scores) / len(scores) if scores else 0.0
        return {
            ""total_questions"": total,
            ""avg_teacher_score"": round(avg_score, 2),
            ""weak_areas"": self.weak_areas,
        }",Summarizes teacher performance based on all interactions.,"Calculate and return performance metrics including total questions, average score, and weak areas."
953,_validate_metric,"def _validate_metric(self, metric) -> None:
        
        if metric not in self._metrics:
            raise KeyError(f""Metric {metric} not initialized"")",Validate if the metric is initialized.,"Ensures specified metric exists in predefined metrics list, raising error if absent."
954,get_latest_run,"def get_latest_run(search_dir="".""):
    
    last_list = glob.glob(f""{search_dir}/**/last*.pt"", recursive=True)
    return max(last_list, key=os.path.getctime) if last_list else """"",Returns the path to the most recent 'last.pt' file in the specified directory for resuming training.,Find the most recently modified 'last*.pt' file in a directory.
955,_parse_atom_type,"def _parse_atom_type(self, doc: Document) -> DocumentConverterResult:
        
        root = doc.getElementsByTagName(""feed"")[0]
        title = self._get_data_by_tag_name(root, ""title"")
        subtitle = self._get_data_by_tag_name(root, ""subtitle"")
        entries = root.getElementsByTagName(""entry"")
        md_text = f""# {title}\n""
        if subtitle:
            md_text += f""{subtitle}\n""
        for entry in entries:
            entry_title = self._get_data_by_tag_name(entry, ""title"")
            entry_summary = self._get_data_by_tag_name(entry, ""summary"")
            entry_updated = self._get_data_by_tag_name(entry, ""updated"")
            entry_content = self._get_data_by_tag_name(entry, ""content"")

            if entry_title:
                md_text += f""\n## {entry_title}\n""
            if entry_updated:
                md_text += f""Updated on: {entry_updated}\n""
            if entry_summary:
                md_text += self._parse_content(entry_summary)
            if entry_content:
                md_text += self._parse_content(entry_content)

        return DocumentConverterResult(
            markdown=md_text,
            title=title,
        )",Parse the type of an Atom feed.,Convert Atom XML feed to Markdown format with title and entries.
956,kill_session,"def kill_session(self, session_name):
        
        try:
            # Kill session
            subprocess.run(
                [""tmux"", ""kill-session"", ""-t"", session_name],
                check=True
            )
            
            # Refresh the session list
            self.refresh_sessions()
            
            # Close the notch after killing session
            self.close_manager()
            
        except Exception as e:
            print(f""Error killing tmux session: {e}"")",Kill a tmux session,Terminate a tmux session and update session management state.
957,save_only_ema_weights,"def save_only_ema_weights(checkpoint_file):
    
    checkpoint = torch.load(checkpoint_file, map_location=""cpu"")

    weights = {}
    if ""ema"" in checkpoint:
        weights[""model""] = checkpoint[""ema""][""module""]
    else:
        raise ValueError(""The checkpoint does not contain 'ema'."")

    dir_name, base_name = os.path.split(checkpoint_file)
    name, ext = os.path.splitext(base_name)
    output_file = os.path.join(dir_name, f""{name}_converted{ext}"")

    torch.save(weights, output_file)
    print(f""EMA weights saved to {output_file}"")",Extract and save only the EMA weights.,Extract and save EMA model weights from a checkpoint file.
958,_handle_bucket_with_insufficient_images,"def _handle_bucket_with_insufficient_images(self, bucket):
        
        if (
            len(self.metadata_backend.aspect_ratio_bucket_indices[bucket])
            < self.batch_size
        ):
            self.debug_log(
                f""Bucket {bucket} has insufficient ({len(self.metadata_backend.aspect_ratio_bucket_indices[bucket])}) images.""
            )
            if bucket not in self.exhausted_buckets:
                self.debug_log(
                    f""Bucket {bucket} is now exhausted and sleepy, and we have to move it to the sleepy list before changing buckets.""
                )
                self.move_to_exhausted()
            self.debug_log(""Changing bucket to another random selection."")
            self.change_bucket()
            return True
        self.debug_log(
            f""Bucket {bucket} has sufficient ({len(self.metadata_backend.aspect_ratio_bucket_indices[bucket])}) images.""
        )
        return False",Handle buckets with insufficient images.,Check if image bucket is insufficient and switch if necessary.
959,filter_items,"def filter_items(self, items):
        
        filtered_items = []
        for item in items:
            # remove closeup videos
            if 'closeup' in item['gpt_response'][0] or \
                'close-up' in item['gpt_response'][0] or \
                    'close up' in item['gpt_response'][0] or \
                        'What you should do next' not in item['gpt_response'][0]:
                continue
            # item['gpt_response'][0] = item['gpt_response'][0].replace('blue', 'yellow')
            filtered_items.append(item)
        print(f""Filtered {len(items) - len(filtered_items)} items from {len(items)} items"")
        return filtered_items",filter out items that are not suitable for conversation construction,Filter out items with specific keywords from a list
960,_record_limit_reached,"def _record_limit_reached(
        self,
        limit_type: str,
        current_value: Union[int, float],
        limit_value: Union[int, float],
        exit_at_limit: bool,
    ) -> None:
        
        try:
            if not self.trajectory_repo or not self.session_totals[""session_id""]:
                return

            self.trajectory_repo.create(
                record_type=""limit_reached"",
                session_id=self.session_totals[""session_id""],
                step_data={
                    ""limit_type"": limit_type,
                    ""current_value"": current_value,
                    ""limit_value"": limit_value,
                    ""exit_at_limit"": exit_at_limit,
                },
            )
        except Exception as e:
            logger.error(f""Failed to record limit reached event: {e}"", exc_info=True)",Record a limit reached event in the trajectory.,Log limit breach event with session details and handle exceptions
961,get_fighter_image,"def get_fighter_image(self, fighter_name):
        
        try:
            # Convert name to slug format
            slug = fighter_name.lower().replace("" "", ""-"")

            image_path = os.path.join(self.config.image_folder, f""{slug}.jpg"")
            if not os.path.exists(image_path):
                return None

            # Convert image to base64
            with Image.open(image_path) as img:
                # Convert RGBA to RGB if necessary
                if img.mode == ""RGBA"":
                    img = img.convert(""RGB"")
                buf = io.BytesIO()
                img.save(buf, format=""JPEG"")
                image_bytes = buf.getvalue()
                return base64.b64encode(image_bytes).decode(""utf-8"")
        except Exception as e:
            print(f""Error getting image for {fighter_name}: {e}"")
            return None",Convert fighter name to image path and return base64 encoded image,Retrieve and encode fighter image as base64 from a specified directory
962,perform_action,"def perform_action(self, action_name: str, kwargs) -> Any:
        
        if action_name not in self.actions:
            raise KeyError(f""Unknown action: {action_name}"")

        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f""Invalid parameters: {', '.join(errors)}"")

        # Add config parameters if not provided
        if action_name == ""read-timeline"" and ""count"" not in kwargs:
            kwargs[""count""] = self.config[""timeline_read_count""]

        # Call the appropriate method based on action name
        method_name = action_name.replace('-', '_')
        method = getattr(self, method_name)
        return method(**kwargs)",Execute a Farcaster action with validation,Validate and execute specified action with parameters in a dynamic method call.
963,run_context_server,"def run_context_server(server_port: int) -> None:
    
    sse = SseServerTransport(""/messages/"")
    context_server = RequestContextServer()

    async def handle_sse(request: Request) -> Response:
        async with sse.connect_sse(
            request.scope, request.receive, request._send
        ) as streams:
            await context_server.run(
                streams[0], streams[1], context_server.create_initialization_options()
            )
        return Response()

    app = Starlette(
        routes=[
            Route(""/sse"", endpoint=handle_sse),
            Mount(""/messages/"", app=sse.handle_post_message),
        ]
    )

    server = uvicorn.Server(
        config=uvicorn.Config(
            app=app, host=""127.0.0.1"", port=server_port, log_level=""error""
        )
    )
    print(f""starting context server on {server_port}"")
    server.run()",Run a server that captures request context,Initialize and run a context server with SSE communication on a specified port.
964,load_config,"def load_config(config_path):
        
        print(f""🔍 Loading test configuration from {config_path}..."")
        if not os.path.exists(config_path):
            print(f""❌ Config file not found at: {config_path}"")
            sys.exit(1)
        try:
            with open(config_path, ""r"") as config_file:
                config = yaml.safe_load(config_file)
            print(""✅ Configuration loaded successfully."")
            return config
        except yaml.YAMLError as e:
            print(f""❌ Error parsing config file: {e}"")
            sys.exit(1)",Load configuration from YAML file,"Load and validate YAML configuration file, handling errors gracefully."
965,stop_sampling,"def stop_sampling() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Memory.stopSampling"",
    }
    json = yield cmd_dict",Stop collecting native memory profile.,Initiates a command to halt memory sampling and yields a JSON response.
966,openapi_30_schema,"def openapi_30_schema() -> dict[str, Any]:
    
    return {
        ""openapi"": ""3.0.0"",
        ""info"": {""title"": ""Simple API (OpenAPI 3.0)"", ""version"": ""1.0.0""},
        ""paths"": {
            ""/items"": {
                ""get"": {
                    ""summary"": ""List all items"",
                    ""operationId"": ""listItems"",
                    ""parameters"": [
                        {
                            ""name"": ""limit"",
                            ""in"": ""query"",
                            ""description"": ""How many items to return"",
                            ""required"": False,
                            ""schema"": {""type"": ""integer""},
                        }
                    ],
                    ""responses"": {""200"": {""description"": ""A list of items""}},
                }
            }
        },
    }",Fixture that returns a simple OpenAPI 3.0.0 schema.,Define an OpenAPI 3.0 schema for a simple API with item listing endpoint.
967,get_user_message,"def get_user_message(self) -> HumanMessage:
        
        step_info_str = f""Step {self.step_info.step_number + 1}/{self.step_info.max_steps}\n"" if self.step_info else """"
        
        state_description = f

        if self.result:
            for i, result in enumerate(self.result):
                if result.extracted_content:
                    state_description += f""\nACTION RESULT {i+1}: {result.extracted_content}""
                if result.error:
                    error = result.error[-self.max_error_length:]
                    state_description += f""\nACTION ERROR {i+1}: ...{error}""

        return HumanMessage(content=state_description)",Creates and returns a HumanMessage with formatted content.,Generate a human-readable message summarizing action results and errors.
968,perform_action,"def perform_action(self, action_name: str, kwargs: Dict[str, Any]) -> Any:
        
        if action_name not in self.actions:
            raise KeyError(f""Unknown action: {action_name}"")
        load_dotenv()
        if not self.is_configured(verbose=True):
            raise EthereumConnectionError(""Ethereum connection is not properly configured"")
        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f""Invalid parameters: {', '.join(errors)}"")
        method_name = action_name.replace('-', '_')
        method = getattr(self, method_name)
        return method(**kwargs)",Execute an Ethereum action with validation,Execute validated action method with dynamic parameters and error handling.
969,reconfigure_service_discovery,"def reconfigure_service_discovery(self, config: DynamicRouterConfig):
        
        if config.service_discovery == ""static"":
            reconfigure_service_discovery(
                ServiceDiscoveryType.STATIC,
                urls=parse_static_urls(config.static_backends),
                models=parse_comma_separated_args(config.static_models),
            )
        elif config.service_discovery == ""k8s"":
            reconfigure_service_discovery(
                ServiceDiscoveryType.K8S,
                namespace=config.k8s_namespace,
                port=config.k8s_port,
                label_selector=config.k8s_label_selector,
            )
        else:
            raise ValueError(
                f""Invalid service discovery type: {config.service_discovery}""
            )

        logger.info(f""DynamicConfigWatcher: Service discovery reconfiguration complete"")",Reconfigures the router with the given config.,Configure service discovery based on dynamic routing configuration settings
970,truncate_run,"def truncate_run(run, k):
    
    temp_d = {}
    for q_id in run:
        sorted_run = {k: v for k, v in sorted(run[q_id].items(), key=lambda item: item[1], reverse=True)}
        temp_d[q_id] = {k: sorted_run[k] for k in list(sorted_run.keys())[:k]}
    return temp_d",truncates run file to only contain top-k results for each query,Truncate and sort dictionary values by top k scores
971,_get_next_assistant_message,"def _get_next_assistant_message(self) -> PromptMessageMultipart:
        
        # Find next assistant message
        while self._current_index < len(self._messages):
            message = self._messages[self._current_index]
            self._current_index += 1
            if ""assistant"" != message.role:
                continue

            return message

        self._overage += 1
        return Prompt.assistant(
            f""MESSAGES EXHAUSTED (list size {len(self._messages)}) ({self._overage} overage)""
        )",Get the next assistant message from the loaded messages.,Retrieve next assistant message or notify if messages are exhausted.
972,set_user_message,"def set_user_message(state: DialogState) -> list[BaseMessage]:
    
    conversation = convert_messages_to_str(state['chatbot_messages'])
    text = f""You are provided with the conversation between the user and the chatbot.\n# Conversation:\n{conversation}""
    messages_list = [HumanMessage(content=text)]
    critique_feedback = state.get('critique_feedback', '')
    if not critique_feedback == '':
        text = f""{state['user_thoughts'][-1]}\nUser Response:\n{state['stop_signal']}""
        messages_list.append(AIMessage(content=text))
        text = 'Your response was inaccurate, you are provided with the feedback from the critique. ' \
               f'Please provide a new response (use the same format), you should also determine if the conversation should continue or stop.\nFeedback:\n{state[""critique_feedback""]}'
        messages_list.append(HumanMessage(content=text))
    return messages_list",Set the user message :param state: The current state :return: The AI message,Generate messages for user-chatbot interaction based on conversation state and feedback.
973,to_device,"def to_device(obj, device):
    
    if isinstance(obj, dict):
        for k, v in obj.items():
            if isinstance(v, dict):
                to_device(v, device)
            elif isinstance(v, torch.Tensor):
                obj[k] = obj[k].to(device)
    elif isinstance(obj, torch.Tensor):
        obj = obj.to(device)
    else:
        raise Exception(f""type {type(obj)} not supported"")
    return obj",Move tensor or dict of tensors to device,Recursively transfer tensors within structures to a specified device.
974,update_lock_expiration,"def update_lock_expiration(hours: int = None) -> bool:
    
    if hours is None:
        expiration_hours = get_advanced_setting(""stateful_management_hours"", DEFAULT_HOURS)
    else:
        expiration_hours = hours
    
    lock_info = get_lock_info()
    created_at = lock_info.get(""created_at"", int(time.time()))
    expires_at = created_at + (expiration_hours * 3600)
    
    lock_info[""expires_at""] = expires_at
    
    try:
        with open(LOCK_FILE, 'w') as f:
            json.dump(lock_info, f, indent=2)
        stateful_logger.info(f""Updated lock expiration to {datetime.datetime.fromtimestamp(expires_at)}"")
        return True
    except Exception as e:
        stateful_logger.error(f""Error updating lock expiration: {e}"")
        return False",Update the lock expiration based on the hours setting.,Update lock expiration time based on given or default hours and log the result.
975,value,"def value(self) -> int:
        
        return {
            ""POOR"": 0,
            ""FAIR"": 1,
            ""GOOD"": 2,
            ""EXCELLENT"": 3,
        }[self._value_]",Convert string enum values to integers for comparison.,Map string ratings to corresponding integer values.
976,calculate_groq_cost,"def calculate_groq_cost(input_tokens: int, output_tokens: int, model: str) -> float:
    
    total = 0.0

    if model in GROQ_PRICING_1K:
        input_cost_per_k, output_cost_per_k = GROQ_PRICING_1K[model]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        total = input_cost + output_cost
    else:
        warnings.warn(f""Cost calculation not available for model {model}"", UserWarning)

    return total",Calculate the cost of the completion using the Groq pricing.,Calculate total cost for processing tokens based on model-specific pricing.
977,_get_empty_config,"def _get_empty_config(self) -> Dict[str, Any]:
        
        return {""mcpServers"": {}, ""disabledServers"": {}}",Get empty config structure for Claude Desktop,Returns a default configuration with empty server mappings.
978,_basic_tool_from_spec,"def _basic_tool_from_spec(self, spec: Dict[str, Any]) -> GeneratedTool:
        
        name = spec.get(""name"", ""Generated"")
        description = spec.get(""description"", """")
        code = f

        return GeneratedTool(
            name=name,
            description=description,
            specification=spec.get(""specification"", spec),
            code=code,
        )",Create a very simple tool implementation directly from a spec.,Create a tool object from a specification dictionary with default values.
979,clear_device_metrics_override,"def clear_device_metrics_override() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Emulation.clearDeviceMetricsOverride"",
    }
    json = yield cmd_dict",Clears the overridden device metrics.,This function resets device metrics emulation settings via a command generator.
980,convert_training_run_to_dict,"def convert_training_run_to_dict(training_run):
        
        training_history = training_run.scan_history()
        d = defaultdict(dict)
        for row in training_history:
            if ""num_opt_steps"" not in row:
                continue
            row = filter_out_columns(row)
            opt_step = row[OPT_STEP_LOG]
            assert opt_step not in d, (
                f""The current code does not support having multiple entries for a single `opt_step` ({opt_step}).""
                "" Please double check what's happening, and if necessary, support this case (for instance by only""
                "" considering the entry with the last timestamp.)""
            )
            d[opt_step] = row
        return d",Get all the logged values from the training into a dictionary.,Convert training run history into a dictionary indexed by optimization steps.
981,sol_lend,"def sol_lend(agent, **kwargs):
    
    agent.logger.info(""\n🏦 INITIATING LENDING"")
    try:
        result = agent.connection_manager.perform_action(
            connection_name=""solana"",
            action_name=""lend-assets"",
            params=[kwargs.get('amount')]
        )
        agent.logger.info(""✅ Lending completed!"")
        return result
    except Exception as e:
        agent.logger.error(f""❌ Lending failed: {str(e)}"")
        return False",Lend assets using Lulo,"Initiates asset lending on Solana, logging success or failure."
982,file_modified,"def file_modified(file_path: str, project_root: str, cache_type: str = ""all"") -> None:
    
    norm_path = normalize_path(file_path)
    # Use raw f-string for regex pattern
    key_pattern_to_invalidate = rf"".*(?::|\||^){re.escape(norm_path)}(?:\||$).*"" # FIXED
    
    caches_to_scan = [cache_manager.get_cache(cache_name) for cache_name in list(cache_manager.caches.keys())] \
                     if cache_type == ""all"" else \
                     [cache_manager.get_cache(cache_type)]
    
    for cache_instance in caches_to_scan:
        if cache_instance: 
            cache_instance.invalidate(key_pattern_to_invalidate)
    logger.debug(f""Invalidated entries matching path '{norm_path}' (pattern '{key_pattern_to_invalidate}') in cache(s) type '{cache_type}'."")",Invalidate caches when a file is modified.,Invalidate cache entries based on file path changes within a project directory.
983,format_step_result_xml,"def format_step_result_xml(step_result: StepResult) -> str:
    
    from mcp_agent.llm.prompt_utils import format_fastagent_tag

    # Format each task result with XML
    task_results = []
    for task in step_result.task_results:
        task_results.append(format_task_result_xml(task))

    # Combine task results
    task_results_str = ""\n"".join(task_results)

    # Build step result with metadata and tasks
    step_content = (
        f""<fastagent:description>{step_result.step.description}</fastagent:description>\n""
        f""<fastagent:summary>{step_result.result}</fastagent:summary>\n""
        f""<fastagent:task-results>\n{task_results_str}\n</fastagent:task-results>\n""
    )

    return format_fastagent_tag(""step-result"", step_content)",Format a step result with XML tags for better semantic understanding,Convert step result data into structured XML format with metadata and task details.
984,save_config,"def save_config(config_data):
    
    try:
        MAIN_CONFIG_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)
        # Load current or default config
        existing_config = {}
        if MAIN_CONFIG_FILE_PATH.exists():
            with open(MAIN_CONFIG_FILE_PATH, ""r"") as f_read:
                existing_config = json.load(f_read)
        else:  # Should be rare if get_config_params was called
            existing_config = DEFAULT_MAIN_CONFIG.copy()

        # Update with new data
        for key, value in config_data.items():
            existing_config[key] = value

        # Ensure all default keys are still there
        for default_key, default_value in DEFAULT_MAIN_CONFIG.items():
            if default_key not in existing_config:
                existing_config[default_key] = default_value

        with open(MAIN_CONFIG_FILE_PATH, ""w"") as f:
            json.dump(existing_config, f, indent=4)
        logger.info(f""Main configuration saved to {MAIN_CONFIG_FILE_PATH}"")
        return True, None
    except Exception as e:
        logger.error(f""Error saving main configuration: {e}"", exc_info=True)
        return False, str(e)",Saves the main configuration data to main.json.,"Update and persist configuration data, ensuring defaults are maintained."
985,make_lambda_event,"def make_lambda_event(jsonrpc_payload):
    
    return {
        'resource': '/mcp',
        'path': '/mcp',
        'httpMethod': 'POST',
        'headers': {
            'content-type': 'application/json',
            'accept': 'application/json, text/event-stream',
        },
        'multiValueHeaders': {
            'content-type': ['application/json'],
            'accept': ['application/json, text/event-stream'],
        },
        'queryStringParameters': None,
        'multiValueQueryStringParameters': None,
        'pathParameters': None,
        'stageVariables': None,
        'requestContext': {
            'resourcePath': '/mcp',
            'httpMethod': 'POST',
            'path': '/Prod/mcp',
            'identity': {},
            'requestId': 'test-request-id',
        },
        'body': json.dumps(jsonrpc_payload)
        if isinstance(jsonrpc_payload, dict)
        else jsonrpc_payload,
        'isBase64Encoded': False,
    }",Create a realistic API Gateway proxy event for Lambda.,Create a structured HTTP POST event for AWS Lambda with JSON-RPC payload.
986,generate_stream_text,"def generate_stream_text(
        self,
        prompt: str,
        *,
        llm_model: str | None = None,
        image_url: str | None = None,
        **kwargs,
    ) -> Iterator[str]:
        
        messages = [
            {""role"": ""user"", ""content"": [{""type"": ""text"", ""text"": prompt}]},
        ]

        
        if image_url:
            messages[0][""content""].append(
                {""type"": ""image_url"", ""image_url"": {""url"": image_url}}
            )

        response = self.client.chat.completions.create(
            messages=messages,
            model=llm_model or self.DEFAULT_MODEL,
            stream=True,  # Enable streaming
            **{**self.DEFAULT_KWARGS, **kwargs},
        )

        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content",Generate streaming text using the OpenAI API.,Generate streaming text responses using a language model with optional image context.
987,_register_node,"def _register_node(self, name: str):
        
        if name not in Nodes.NODE_REGISTRY:
            raise ValueError(f""Node {name} not registered"")
        func, inputs, output = Nodes.NODE_REGISTRY[name]
        self.nodes[name] = func
        self.node_inputs[name] = inputs
        self.node_outputs[name] = output",Register a node without modifying the current node.,"Register node by name, ensuring it exists in the registry"
988,_save_current_query_traces,"def _save_current_query_traces(self, query_traces):
        
        self.query_count += 1
        timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
        filename = f""trace_query_{self.query_count}_{timestamp}.json""

        traces = self._add_traces_in_data(query_traces)

        # Write the tracer json files to a temporary directory
        temp_dir = tempfile.gettempdir()
        temp_file_path = f""{temp_dir}/{filename}""

        with open(temp_file_path, ""w"") as f:
            json.dump([traces], f, indent=2, cls=CustomEncoder)
        # print(f""Query traces saved to {temp_file_path}"")

        # Upload the traces
        self._create_dataset_schema_with_trace()
        presignedUrl = self._get_presigned_url()
        self._put_presigned_url(presignedUrl, temp_file_path)
        self._insert_traces(presignedUrl)",Save traces for the current query,Save and upload query traces with timestamped filenames.
989,_calculate_hash_key,"def _calculate_hash_key(self, message: str, provider: str) -> str:
        
        filepath = self._match_filepath(message, provider)
        if filepath is None:
            logger.warning(""No path found in messages. Creating hash key from message."")
            message_to_hash = f""{message}-{provider}""
        else:
            message_to_hash = f""{filepath}-{provider}""

        logger.debug(f""Message to hash: {message_to_hash}"")
        hashed_content = hashlib.sha256(message_to_hash.encode(""utf-8"")).hexdigest()
        logger.debug(f""Hashed content: {hashed_content}"")
        return hashed_content",Creates a hash key from the message and includes the provider,Generate a unique hash key from message and provider details for logging purposes.
990,get_auth_token,"def get_auth_token(self):
        
        try:
            auth_path = self.get_auth_token_path()
            self.log(f""Reading auth token from: {auth_path}"")

            if not auth_path.exists():
                self.log(""Auth token file does not exist"", True)
                return None

            with open(auth_path) as f:
                auth_data = json.load(f)

            if ""api_key"" in auth_data:
                return auth_data[""api_key""]
            else:
                self.log(""No API key found in auth data"", True)
                return None

        except Exception as e:
            self.log(f""Error reading auth token: {e!s}"", True)
            return None",Read the authentication token from the auth file.,"Retrieve API key from a specified authentication file, logging errors if encountered."
991,clear_store,"def clear_store(self, user_id: str = None) -> bool:
        
        session = None
        try:
            session = self._Session()
            
            # Build query based on whether user_id is provided
            query = session.query(ChunkEmbedding)
            if user_id:
                query = query.filter(ChunkEmbedding.user_id == user_id)
                
            query.delete(synchronize_session=False)
            session.commit()
            return True
        except Exception as e:
            logger.error(f""Failed to clear store: {e}"")
            if session:
                session.rollback()
            return False
        finally:
            if session:
                session.close()","Clear all documents from the store, optionally filtered by user_id.",Function deletes user-specific or all data from the database store and handles exceptions.
992,log_generations_to_tensorboard,"def log_generations_to_tensorboard(self, samples, step):
        
        # Initialize tensorboard writer if not exists
        if not hasattr(self, ""writer""):
            from torch.utils.tensorboard import SummaryWriter
            tensorboard_dir = os.environ.get(""TENSORBOARD_DIR"", ""tensorboard_log"")
            os.makedirs(tensorboard_dir, exist_ok=True)
            self.writer = SummaryWriter(log_dir=tensorboard_dir)
        
        # Format the samples data into readable text
        text_content = f""**Generation Results - Step {step}**\n\n""
        
        for i, sample in enumerate(samples):
            text_content += f""### Sample {i + 1}\n""
            
            # Assuming sample contains [input, output, score]
            if len(sample) >= 3:
                input_text, output_text, score = sample[0], sample[1], sample[2]
                
                text_content += f""**Input:** {input_text}\n\n""
                text_content += f""**Output:** {output_text}\n\n""
                text_content += f""**Score:** {score}\n\n""
            else:
                # Handle cases where sample format might be different
                text_content += f""**Data:** {sample}\n\n""
            
            text_content += ""---\n\n""
        
        # Log to tensorboard as text
        self.writer.add_text('val/generations', text_content, step)
        # Flush to ensure data is written
        self.writer.flush()",Log samples to tensorboard as text,Log model generation results to TensorBoard for visualization at each training step.
993,plot_pr_curve,"def plot_pr_curve(px, py, ap, save_dir=Path(""pr_curve.png""), names={}, on_plot=None):
    
    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)
    py = np.stack(py, axis=1)

    if 0 < len(names) < 21:  # display per-class legend if < 21 classes
        for i, y in enumerate(py.T):
            ax.plot(px, y, linewidth=1, label=f""{names[i]} {ap[i, 0]:.3f}"")  # plot(recall, precision)
    else:
        ax.plot(px, py, linewidth=1, color=""grey"")  # plot(recall, precision)

    ax.plot(px, py.mean(1), linewidth=3, color=""blue"", label=f""all classes {ap[:, 0].mean():.3f} mAP@0.5"")
    ax.set_xlabel(""Recall"")
    ax.set_ylabel(""Precision"")
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.legend(bbox_to_anchor=(1.04, 1), loc=""upper left"")
    ax.set_title(""Precision-Recall Curve"")
    fig.savefig(save_dir, dpi=250)
    plt.close(fig)
    if on_plot:
        on_plot(save_dir)",Plots a precision-recall curve.,Generate and save a precision-recall curve plot with optional class-specific legends.
994,execute,"def execute(
        self, input_data: AgentManagerInputSchema, config: RunnableConfig | None = None, **kwargs
    ) -> dict[str, Any]:
        
        log_data = dict(input_data).copy()

        if log_data.get(""images""):
            log_data[""images""] = [f""image_{i}"" for i in range(len(log_data[""images""]))]

        if log_data.get(""files""):
            log_data[""files""] = [f""file_{i}"" for i in range(len(log_data[""files""]))]

        logger.info(f""Agent {self.name} - {self.id}: started with input {log_data}"")
        self.reset_run_state()
        config = config or RunnableConfig()
        self.run_on_node_execute_run(config.callbacks, **kwargs)

        action = input_data.action

        self._prompt_variables.update(dict(input_data))

        kwargs = kwargs | {""parent_run_id"": kwargs.get(""run_id"")}
        kwargs.pop(""run_depends"", None)
        _result_llm = self._actions[action](config=config, **kwargs)
        result = {""action"": action, ""result"": _result_llm}

        execution_result = {
            ""content"": result,
            ""intermediate_steps"": self._intermediate_steps,
        }
        logger.info(f""Agent {self.name} - {self.id}: finished with RESULT:\n{str(result)[:200]}..."")

        return execution_result",Executes the manager agent with the given input data and action.,"Process input data, log actions, execute tasks, and return results with intermediate steps."
995,send_code,"def send_code(code: str):
    
    filename, optimizer_code = _write_optimizer_code_to_volume(code, optimizers_volume)

    # --- Sandbox Setup ---
    sandbox_app = App.lookup(SANDBOX_APP_NAME, create_if_missing=True)
    sandbox = Sandbox.create(app=sandbox_app, image=sandbox_image, timeout=60 * 60)

    # Write code to sandbox
    with sandbox.open(filename, ""w"") as f:
        f.write(optimizer_code)
    with sandbox.open(filename, ""rb"") as f:
        print(f.read())

    time.sleep(1)
    process = sandbox.exec(""python"", filename)

    stdout = process.stdout.read()
    stderr = process.stderr.read()

    print(stdout)
    print(""-"" * 32)
    print(stderr)

    return_obj = {
        ""stdout"": stdout,
        ""stderr"": stderr,
        ""code"": code,
        ""filename"": filename,
    }

    sandbox.terminate()

    return return_obj",Send and execute optimizer code in the sandbox environment.,Execute and capture output of code in a sandbox environment
996,apply_remove,"def apply_remove(target: dict, path_parts: List[str]) -> dict:
    
    target = target.copy()

    if not path_parts:
        raise ValueError(""Cannot remove root document"")

    current = target
    for i, part in enumerate(path_parts[:-1]):
        if part.isdigit() and isinstance(current, list):
            part = int(part)
        if part not in current:
            raise ValueError(f""Path not found: {'/'.join(path_parts[:i+1])}"")
        current = current[part]

    last_part = path_parts[-1]
    if last_part.isdigit() and isinstance(current, list):
        idx = int(last_part)
        if idx >= len(current):
            raise ValueError(f""Index out of bounds: {idx}"")
        del current[idx]
    elif last_part not in current:
        raise ValueError(f""Path not found: {'/'.join(path_parts)}"")
    else:
        del current[last_part]

    return target",Apply a 'remove' operation.,Remove specified element from nested dictionary or list by path
997,get_keywords,"def get_keywords() -> Dict[str, str]:
    
    # these strings will be replaced by git during git-archive.
    # setup.py/versioneer.py will grep for the variable names, so they must
    # each be defined on a line of their own. _version.py will just call
    # get_keywords().
    git_refnames = ""$Format:%d$""
    git_full = ""$Format:%H$""
    git_date = ""$Format:%ci$""
    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}
    return keywords",Get the keywords needed to look up the version information.,Extracts and returns Git metadata keywords for version control.
998,validate_json,"def validate_json(self, json_str: str) -> Any:
        
        validated = _json.validate_json(json_str, self._type_adapter, partial=False)
        if self._is_wrapped:
            if not isinstance(validated, dict):
                _error_tracing.attach_error_to_current_span(
                    SpanError(
                        message=""Invalid JSON"",
                        data={""details"": f""Expected a dict, got {type(validated)}""},
                    )
                )
                raise ModelBehaviorError(
                    f""Expected a dict, got {type(validated)} for JSON: {json_str}""
                )

            if _WRAPPER_DICT_KEY not in validated:
                _error_tracing.attach_error_to_current_span(
                    SpanError(
                        message=""Invalid JSON"",
                        data={""details"": f""Could not find key {_WRAPPER_DICT_KEY} in JSON""},
                    )
                )
                raise ModelBehaviorError(
                    f""Could not find key {_WRAPPER_DICT_KEY} in JSON: {json_str}""
                )
            return validated[_WRAPPER_DICT_KEY]
        return validated",Validate a JSON string against the output type.,"Validate and extract data from JSON, ensuring it meets expected structure and type"
999,apply,"def apply(self) -> str:
        
        list_memories_tool = self.agent.get_tool(ListMemoriesTool)
        memories = json.loads(list_memories_tool.apply())
        if len(memories) == 0:
            return (
                ""Onboarding not performed yet (no memories available). ""
                + ""You should perform onboarding by calling the `onboarding` tool before proceeding with the task.""
            )
        else:
            return f",Checks whether project onboarding was already performed.,Check if onboarding is done by retrieving and evaluating memory data.
1000,initialize,"def initialize(self):
        
        if not self.is_initialized:
            try:
                from huggingface_hub import hf_hub_download
                from csm_mlx import CSM, csm_1b, generate

                csm = CSM(csm_1b())  # csm_1b() is a configuration for the CSM model.
                weight = hf_hub_download(
                    repo_id=""senstella/csm-1b-mlx"", filename=""ckpt.safetensors""
                )
                csm.load_weights(weight)
                self.csm = csm
            except ImportError as e:
                st.error(f""Failed to import required modules for voice generation: {e}"")
                st.info(
                    ""Please install the required packages with: pip install csm-mlx""
                )
                return False
            except Exception as e:
                st.error(f""Failed to initialize voice generator: {e}"")
                return False
        return True",Initialize the voice generator model.,Initialize a voice generation model using Hugging Face and handle import errors.
1001,flatten_blocks,"def flatten_blocks(blocks: List[Dict[str, Any]]) -> str:
            
            lines = []
            for b in blocks:
                if b[""type""] == ""text"":
                    lines.append(b[""text""])
                elif b[""type""] == ""image_url"":
                    lines.append(""[IMAGE_URL]"")
                elif b[""type""] == ""input_audio"":
                    lines.append(""[AUDIO]"")
                else:
                    lines.append(f""[{b['type'].upper()}]"")
            return ""\n"".join(lines)",Return a textual representation of content blocks for approximate token counting / debugging.,Convert structured content blocks into a formatted string representation.
1002,get_trials_by_nctids,"def get_trials_by_nctids(self, nctid_list):
        
        err_msg = """"
        trials = None

        for i in range(self.retry):
            if i > 0:
                print(f""Retry {i} times"")
            try:
                nctid_list_str = "" OR "".join(list(set(nctid_list)))
                query = CTGOV_NCTID_BASE_URL + nctid_list_str + ""&pageSize=1000""
                response = requests.get(query)
                if response.status_code != 200:
                    raise ConnectionError(f""CTGOV connection error occurred - {response.text}"")

               # parse the response to format a list of trials to display
                trials = self._parse_response(response.text, query)

                break
            except:
                err_msg = traceback.format_exc()
                print(err_msg)

        return trials",Search nctids to get the summary of trials.,Fetch and parse clinical trials data by NCT IDs with retry logic
1003,_save_checkpoint,"def _save_checkpoint(self) -> None:
        
        checkpoint_data = {
            ""completed_datasets"": list(self.completed_datasets),
            ""last_updated"": datetime.now().isoformat(),
        }
        with open(self.checkpoint_path, ""w"") as f:
            json.dump(checkpoint_data, f, indent=2)",Save checkpoint to disk.,Persist progress and timestamp to a JSON file for recovery.
1004,log_trajectory,"def log_trajectory(request: EndReignRequest, reward: float):
    
    trajectory_data = {
        ""session_id"": request.session_id,
        ""trajectory"": [item.dict() for item in request.trajectory],
        ""final_metrics"": request.final_metrics,
        ""reign_length"": request.reign_length,
        ""cause_of_end"": request.cause_of_end,
        ""reward"": reward,
        ""weights"": category_weights,
    }

    try:
        # Load existing trajectories
        trajectories = []
        if os.path.exists(trajectories_path):
            with open(trajectories_path, ""r"") as f:
                trajectories = json.load(f)

        # Add new trajectory
        trajectories.append(trajectory_data)

        # Save back to file
        with open(trajectories_path, ""w"") as f:
            json.dump(trajectories, f, indent=2)

    except Exception as e:
        print(f""Error logging trajectory: {e}"")",Log the trajectory to a JSON file,Log and persist game session data with metrics and rewards to a JSON file.
1005,deepfloyd_pixels,"def deepfloyd_pixels(filepaths, data_backend_id: str):
    
    # Use a thread pool to fetch latents concurrently
    try:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            pixels = list(
                executor.map(
                    fetch_pixel_values, filepaths, [data_backend_id] * len(filepaths)
                )
            )
    except Exception as e:
        logger.error(f""(id={data_backend_id}) Error while computing pixels: {e}"")
        raise
    pixels = torch.stack(pixels)
    pixels = pixels.to(memory_format=torch.contiguous_format).float()

    return pixels",DeepFloyd doesn't use the VAE.,Concurrently fetch and process pixel data from file paths using a thread pool.
1006,has_boolean_operators,"def has_boolean_operators(self) -> bool:
        
        if not self.text:  # pragma: no cover
            return False

        # Check for common boolean operators with correct word boundaries
        # to avoid matching substrings like ""GRAND"" containing ""AND""
        boolean_patterns = ["" AND "", "" OR "", "" NOT "", ""("", "")""]
        text = f"" {self.text} ""  # Add spaces to ensure we match word boundaries
        return any(pattern in text for pattern in boolean_patterns)","Check if the text query contains boolean operators (AND, OR, NOT).",Determine if text contains specific boolean operators with word boundaries.
1007,dbg_set_breakpoint,"def dbg_set_breakpoint(
    address: Annotated[str, ""Set a breakpoint at the specified address""],
) -> str:
    
    ea = parse_address(address)
    if idaapi.add_bpt(ea, 0, idaapi.BPT_SOFT):
        return f""Breakpoint set at {hex(ea)}""
    breakpoints = list_breakpoints()
    for bpt in breakpoints:
        if bpt[""ea""] == hex(ea):
            return f""Breakpoint already exists at {hex(ea)}""
    return f""Failed to set breakpoint at address {hex(ea)}""",Set a breakpoint at the specified address,Set or verify a breakpoint at a given memory address.
1008,clear_database,"def clear_database(db_session: Session) -> None:
    
    for table in reversed(Base.metadata.sorted_tables):
        if table.name != ""alembic_version"" and db_session.query(table).count() > 0:
            logger.debug(f""Deleting all records from table {table.name}"")
            db_session.execute(table.delete())
    db_session.commit()",Clear all tables in the database except alembic_version.,Erase all records from database tables except versioning.
1009,update_stock,"def update_stock(self, quantity: int) -> None:
        
        self.stock_level = quantity
        self.update_timestamp(f""stock_update_{quantity}"")",Update stock level and timestamp,Update stock level and log the update with a timestamp.
1010,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Page.disable"",
    }
    json = yield cmd_dict",Disables page domain notifications.,Disables a page feature by sending a command and yielding a JSON response.
1011,_format_base_name,"def _format_base_name(self, base: int) -> str:
        
        if base == 2:
            return ""binary""
        elif base == 16:
            return ""hexadecimal""
        else:
            return f""base-{base}""",Get human-readable name for common bases,Convert numerical base to descriptive string representation.
1012,parse_xml_content,"def parse_xml_content(self, text: str, tag: str) -> str:
        
        match = re.search(f""<{tag}>(.*?)</{tag}>"", text, re.DOTALL)
        return match.group(1).strip() if match else """"",Extract content from XML-like tags.,Extracts and returns content between specified XML tags from a text string.
1013,_generate_intelligent_comment,"def _generate_intelligent_comment(
        self, pr_details: Dict[str, Any], files: list[Dict[str, Any]], analysis: str
    ) -> str:
        
        comment = f
        return comment",Generate an intelligent review comment using LLM analysis.,"Generate a comment based on pull request details, file changes, and analysis."
1014,mock_request,"def mock_request():
    

    async def get_json():
        return {""messages"": []}

    request = Mock(spec=Request)
    request.headers = Headers()
    request.json = get_json
    return request",Create a mock FastAPI request with configurable headers and body,Create a mock HTTP request object with empty JSON response.
1015,get_tools,"def get_tools(self) -> List[Tool]:
        
        try:
            loguru.logger.debug(f""Returning {len(self.tools)} tools: {list(self.tools.keys())}"")
            return list(self.tools.values())
        except Exception as e:
            loguru.logger.error(f""Error retrieving tools: {e}"")
            return []",Return all registered tools.,"Retrieve and log available tools, handling errors gracefully."
1016,clear_accepted_encodings_override,"def clear_accepted_encodings_override() -> typing.Generator[
    T_JSON_DICT, T_JSON_DICT, None
]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Network.clearAcceptedEncodingsOverride"",
    }
    json = yield cmd_dict",Clears accepted encodings set by setAcceptedEncodings,Generate a command to reset network encoding settings in a JSON format.
1017,create_nav_structure,"def create_nav_structure(paths: list[str], parent_groups: Optional[list[str]] = None) -> list[Any]:
    
    groups: dict[str, list[str]] = {}
    pages = []
    parent_groups = parent_groups or []

    for path in paths:
        parts = path.split(""/"")
        if len(parts) == 1:
            pages.append(add_prefix(path, parent_groups))
        else:
            group = parts[0]
            subpath = ""/"".join(parts[1:])
            groups.setdefault(group, []).append(subpath)

    # Sort directories and create their structures
    sorted_groups = [
        {
            ""group"": group,
            ""pages"": create_nav_structure(subpaths, parent_groups + [group]),
        }
        for group, subpaths in sorted(groups.items())
    ]

    # Sort pages
    overview_page = [page for page in pages if page.endswith(""overview"")]
    if overview_page:
        pages.remove(overview_page[0])

    sorted_pages = sorted(pages)
    if overview_page:
        sorted_pages.insert(0, overview_page[0])

    # Return directories first, then files
    return sorted_pages + sorted_groups",Convert list of file paths into nested navigation structure.,Organize file paths into a hierarchical navigation structure with sorted groups and pages.
1018,plot_latency_chart,"def plot_latency_chart(ax, models, values, colors):
    
    x_pos = np.arange(len(models))
    bar_width = 0.6
    bars_latency = ax.bar(x_pos, values, width=bar_width, color=colors)
    ax.set_title(""Full-Run Latency (Seconds)"", pad=15)
    ax.set_xticks(x_pos)
    ax.set_xticklabels(models, rotation=0, ha=""center"")
    ax.spines[""left""].set_visible(False)
    ax.spines[""top""].set_visible(False)
    ax.spines[""right""].set_visible(False)
    ax.set_yticks([])
    ax.grid(False)

    # Add latency value labels
    max_latency = max(values) if values else 0
    for bar in bars_latency:
        yval = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2.0,
            yval + max_latency * 0.02,  # Position label slightly above bar
            f""{yval:.1f}s"",
            ha=""center"",
            va=""bottom"",
            fontweight=""bold"",
        )
    return ax",Plots the latency bar chart onto the given Axes.,Render a bar chart visualizing model latency with customized aesthetics and labeled values.
1019,_index_folders,"def _index_folders(self, force_reindex: bool = False):
        
        indexed = []
        failed = []
        skipped = []

        # Keep track of invalid folders
        for folder in self.folder_paths:
            if folder not in self.valid_folder_paths:
                skipped.append(folder)
                continue

            success = self.embedding_manager.index_folder(folder, force_reindex)
            if success:
                indexed.append(folder)
            else:
                failed.append(folder)

        if indexed:
            logger.info(
                f""Successfully indexed {len(indexed)} folders: {', '.join(indexed)}""
            )

        if failed:
            logger.warning(
                f""Failed to index {len(failed)} folders: {', '.join(failed)}""
            )

        if skipped:
            logger.warning(
                f""Skipped {len(skipped)} invalid folders: {', '.join(skipped)}""
            )",Index all valid configured folders,"Indexes valid folders, logging successes, failures, and skips based on reindexing results."
1020,get_batch_input,"def get_batch_input(self, batch):
        
        # equal to collate_fn
        # the resonable video latents range is [-5,5], approximately.
        # videos
        videos = [self.encode_video(video) for video in batch[""video""]]
        videos = [video.sample() * self.vae.config.scaling_factor for video in videos]
        videos = torch.cat(videos, dim=0)
        videos = videos.to(memory_format=torch.contiguous_format)
        # prompt
        prompts = [item for item in batch[""caption""]]
        return {
            ""videos"": videos,
            ""prompts"": prompts,
        }",Prepare model batch inputs,Process video and caption data into a structured batch for model input.
1021,_apply_filter,"def _apply_filter(self, query):
        
        if not query:
            return self._data.copy()

        result = []
        for doc in self._data:
            match = True
            for field, criteria in query.items():
                if field.startswith('$'):
                    # Handle top-level operators like $and, $or
                    match = self._apply_logical_operator(doc, field, criteria)
                    if not match:
                        break
                elif isinstance(criteria, dict) and any(
                    k.startswith('$') for k in criteria.keys()
                ):
                    # Handle comparison operators like {field: {$gt: value}}
                    match = self._apply_comparison_operators(doc, field, criteria)
                    if not match:
                        break
                else:
                    # Simple equality match
                    if field not in doc or doc[field] != criteria:
                        match = False
                        break

            if match:
                result.append(doc)

        return result",Apply DocumentDB query filter to documents.,Filter dataset based on query with logical and comparison operators
1022,reset_config,"def reset_config(base_url):
    
    response = requests.get(f""{base_url}/config"")
    assert response.status_code == 200
    original_config = response.json()
    yield
    response = requests.post(f""{base_url}/config"", json=original_config)
    assert response.status_code == 200",A fixture to ensure the main config is reset after a test case.,Restore original configuration after temporary changes using HTTP requests.
1023,reformat_code_string,"def reformat_code_string(code: str, language: ProgrammingLanguage) -> str:
    
    if language == ProgrammingLanguage.PYTHON:
        output = autoflake.fix_code(
            code,
            remove_all_unused_imports=True,
        )
        output = isort.code(output, combine_as_imports=True)
        return black.format_str(output, mode=black.FileMode())
    elif language == ProgrammingLanguage.TYPESCRIPT:
        options = get_jsbeautifier_options()
        return jsbeautifier.beautify(code, options)
    else:
        msg = f""Unsupported programming language: {language}!""
        raise ValueError(msg)",Reformat the code string based on the programming language.,Reformats code strings based on specified programming language standards.
1024,load_extern_type,"def load_extern_type(file_path: Optional[str], type_name: Optional[str]):
    
    import importlib.util
    import os

    if not file_path:
        return None

    if not os.path.exists(file_path):
        raise FileNotFoundError(f""Custom type file '{file_path}' not found."")

    spec = importlib.util.spec_from_file_location(""custom_module"", file_path)
    module = importlib.util.module_from_spec(spec)
    try:
        spec.loader.exec_module(module)
    except Exception as e:
        raise RuntimeError(f""Error loading module from '{file_path}'"") from e

    if not hasattr(module, type_name):
        raise AttributeError(f""Custom type '{type_name}' not found in '{file_path}'."")

    return getattr(module, type_name)",Load a external data type based on the file path and type name,Load and return a specified type from an external Python file if it exists.
1025,get_observation,"def get_observation(self, states: Obs) -> dict[str, torch.Tensor]:
        
        # XXX: currently only one of ""joint_qpos"" or ""rgb"" is supported. If there's a mixture of
        # both, this will raise an error. (also joint_vel is currently not supported)
        obs_mode = """"
        if ""joint_qpos"" in self.obs_space.spaces.keys():
            obs_shape_from_space = self.obs_space[""joint_qpos""].shape
            obs_mode = ""joint_qpos""
        elif ""rgb"" in self.obs_space.spaces.keys():
            obs_shape_from_space = self.obs_space[""rgb""].shape
            obs_mode = ""rgb""
        else:
            raise ValueError(f""Observation space {self.obs_space} is not supported."")

        obs_tensor = torch.zeros((self.num_envs, *obs_shape_from_space), device=self.device, dtype=torch.float32)

        for i, state in enumerate(states):
            if obs_mode == ""joint_qpos"":
                obs_tensor[i, :] = torch.tensor(
                    list(state[""robots""][self._robot.name][""dof_pos""].values()), device=self.device
                )
            elif obs_mode == ""rgb"" and ""rgb"" in state:
                obs_tensor[i, :] = torch.tensor(
                    state[""cameras""][0][""rgb""], device=self.device
                )  # TODO: this need to be tested in the future

        return {""obs"": obs_tensor}",Process the observation from the handler/GymEnv state format into the dict expected by PPO.,Extracts and formats observation data into tensors based on mode and state input
1026,setup_agent,"def setup_agent(agent_role: str, streaming_enabled: bool, streaming_mode: str) -> ReflectionAgent:
    

    llm = setup_llm()
    memory = Memory(backend=InMemory())

    mode_mapping = {""Answer"": StreamingMode.FINAL, ""Steps"": StreamingMode.ALL}
    mode = mode_mapping.get(streaming_mode, StreamingMode.FINAL)
    streaming_config = StreamingConfig(enabled=streaming_enabled, mode=mode)

    agent = ReflectionAgent(
        name=""Agent"",
        llm=llm,
        role=agent_role,
        id=""agent"",
        memory=memory,
        streaming=streaming_config,
    )
    return agent",Initializes an AI agent with a specified role and streaming configuration.,Initialize a reflection agent with specified role and streaming settings.
1027,delete_knowledge_base,"def delete_knowledge_base(self, kb_name=""kb-metadatafiltering""):
        
        try:
            # First check if the KB exists
            exists = False
            try:
                kbs_available = self.bedrock_agent.list_knowledge_bases(maxResults=100)
                for kb in kbs_available.get(""knowledgeBaseSummaries"", []):
                    if kb[""name""] == kb_name:
                        exists = True
                        break
            except Exception:
                print(f""Could not list knowledge bases"")
                
            if not exists:
                print(f""Knowledge base {kb_name} not found, skipping deletion"")
                return True
                
            print(f""Deleting knowledge base: {kb_name}"")
            print(f""Deleting Bucket is set to {self.delete_bucket}"")
            self.kb_helper.delete_kb(
                kb_name=kb_name,
                delete_s3_bucket=self.delete_bucket,  # Keep the bucket as it might contain other data
                delete_iam_roles_and_policies=True,
                delete_aoss=True
            )
            return True
        except Exception as e:
            print(f""Error deleting knowledge base: {str(e)}"")
            return False",Delete the Knowledge Base and its associated resources,"Attempt to delete a specified knowledge base if it exists, handling exceptions."
1028,get_configured_apps,"def get_configured_apps() -> List[str]:
    
    configured = []
    for app_name in KNOWN_APP_TYPES:
        settings = load_settings(app_name)
        
        # First check if there are valid instances configured (multi-instance mode)
        if ""instances"" in settings and isinstance(settings[""instances""], list) and settings[""instances""]:
            for instance in settings[""instances""]:
                if instance.get(""enabled"", True) and instance.get(""api_url"") and instance.get(""api_key""):
                    configured.append(app_name)
                    break  # One valid instance is enough to consider the app configured
            continue  # Skip the single-instance check if we already checked instances
                
        # Fallback to legacy single-instance config
        if settings.get(""api_url"") and settings.get(""api_key""):
            configured.append(app_name)
    
    settings_logger.info(f""Configured apps: {configured}"")
    return configured",Return a list of app names that have basic configuration (API URL and Key).,Identify and return a list of apps with valid configuration settings.
1029,scale_coordinates,"def scale_coordinates(self, source: ScalingSource, x: int, y: int):
        
        x, y = int(x), int(y)
        if not self._scaling_enabled:
            return x, y
        ratio = self.width / self.height
        target_dimension = None

        for target_name, dimension in MAX_SCALING_TARGETS.items():
            # allow some error in the aspect ratio - not ratios are exactly 16:9
            if abs(dimension[""width""] / dimension[""height""] - ratio) < 0.02:
                if dimension[""width""] < self.width:
                    target_dimension = dimension
                    self.target_dimension = target_dimension
                    # print(f""target_dimension: {target_dimension}"")
                break

        if target_dimension is None:
            # TODO: currently we force the target to be WXGA (16:10), when it cannot find a match
            target_dimension = MAX_SCALING_TARGETS[""WXGA""]
            self.target_dimension = MAX_SCALING_TARGETS[""WXGA""]

        # should be less than 1
        x_scaling_factor = target_dimension[""width""] / self.width
        y_scaling_factor = target_dimension[""height""] / self.height
        if source == ScalingSource.API:
            if x > self.width or y > self.height:
                raise ToolError(f""Coordinates {x}, {y} are out of bounds"")
            # scale up
            return round(x / x_scaling_factor), round(y / y_scaling_factor)
        # scale down
        return round(x * x_scaling_factor), round(y * y_scaling_factor)",Scale coordinates to a target maximum resolution.,Adjust coordinates based on aspect ratio and scaling settings.
1030,extract_last_number,"def extract_last_number(self, text: str) -> float:
        
        matches = regex.findall(r""[-+]?\d+(?:,\d{3})*(?:\.\d+)?|\d+\.\d+"", str(text))
        if matches:
            last_number = matches[-1].replace("","", """").strip()
            try:
                last_number = float(last_number)
                return last_number
            except ValueError:
                return None
        return None",Extract the last number from a text.,Extracts and returns the last numeric value from a given text string.
1031,_load_cache_from_file,"def _load_cache_from_file(self) -> None:
        
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, ""r"", encoding=""utf-8"") as f:
                    cache_data = json.load(f)
                    self.servers_cache = cache_data.get(""servers"")

                    # Parse the last_refresh timestamp if it exists
                    last_refresh_str = cache_data.get(""last_refresh"")
                    if last_refresh_str:
                        self.last_refresh = datetime.fromisoformat(last_refresh_str)

                    logger.debug(f""Loaded servers cache from {self.cache_file}"")
            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f""Error parsing cache file: {self.cache_file}: {e}"")
                self.servers_cache = None
                self.last_refresh = None",Load servers cache from file if it exists,"Load and parse server cache data from a file, handling errors gracefully."
1032,_collection_loop,"def _collection_loop(self) -> None:
        
        while self.is_running:
            try:
                collection_time = time.time()
                relative_time = collection_time - self.start_time

                # Collect metrics with timing information
                metrics = self._collect_metrics()
                if metrics:
                    with self.lock:
                        self.metrics_buffer.append(
                            {
                                ""step"": int(
                                    relative_time
                                ),  # Store the relative time as step
                                ""metrics"": metrics,
                            }
                        )

                # Check if it's time to flush
                current_time = time.time()
                if current_time - self.last_flush_time >= self.flush_interval:
                    self.flush()
                    self.last_flush_time = current_time

                time.sleep(self.collection_interval)
            except Exception as e:
                print(
                    f""Error in GPU monitoring collection loop or stopped abruptly: {e}""
                )
                time.sleep(self.collection_interval)",Main collection loop that runs in a separate thread.,"Continuously collect and buffer metrics, flushing periodically while handling errors."
1033,handle_api_errors,"def handle_api_errors(f):
    
    @wraps(f)
    def decorated_function(*args, **kwargs):
        try:
            return f(*args, **kwargs)
        except ValueError as ve:
            return jsonify({'message': str(ve), 'type': 'error'}), 400
        except FileNotFoundError as fnfe:
            return jsonify({'message': str(fnfe), 'type': 'error'}), 404
        except Exception as e:
            return jsonify({'message': f'Unexpected error: {str(e)}', 'type': 'error'}), 500
    return decorated_function",A decorator to handle errors for API routes.,Decorator for handling API errors and returning JSON responses with status codes.
1034,_create_collection,"def _create_collection(self) -> None:
        
        self._client.create_collection(
            collection_name=self.index_name,
            vectors_config={""default"": {""size"": self.dimension, ""distance"": self.metric}},
        )",Create the collection in Qdrant.,"Create a collection with specified name, dimension, and distance metric."
1035,_emit_event,"def _emit_event(self, event: Event):
        
        loop = self._ensure_event_loop()
        try:
            is_running = loop.is_running()
        except NotImplementedError:
            # Handle Temporal workflow environment where is_running() is not implemented
            # Default to assuming the loop is not running
            is_running = False

        if is_running:
            # If we're in a thread with a running loop, schedule the coroutine
            asyncio.create_task(self.event_bus.emit(event))
        else:
            # If no loop is running, run it until the emit completes
            try:
                loop.run_until_complete(self.event_bus.emit(event))
            except NotImplementedError:
                # Handle Temporal workflow environment where run_until_complete() is not implemented
                # In Temporal, we can't block on async operations, so we'll need to avoid this
                # Simply log to stdout/stderr as a fallback
                import sys

                print(
                    f""[{event.type}] {event.namespace}: {event.message}"",
                    file=sys.stderr,
                )",Emit an event by running it in the event loop.,"Handle event emission with async support, adapting to Temporal workflow constraints."
1036,extract_file_content,"def extract_file_content(path: str) -> Tuple[str, DocumentTypeEnum]:
    
    document_type = get_or_infer_file_type(path)

    with open(path, ""rb"") as file:
        file_stream = BytesIO(file.read())

    try:
        if document_type in [
            DocumentTypeEnum.TXT,
            DocumentTypeEnum.MD,
            DocumentTypeEnum.HTML,
        ]:
            content = detect_encoding_and_read_text_file(file_stream)
        else:
            content = serialize_to_base64(file_stream)
    except Exception as e:
        logger.error(f""Error processing file {path}: {e}"")

        raise ValueError(f""Failed to extract content from {path}"") from e

    logger.debug(f""Content extracted from '{path}'"")
    return content, DocumentTypeEnum(document_type)","Extracts content from a file, supporting different formats.",Extracts and processes file content based on its inferred document type
1037,train,"def train():
    
    inputs = {
        ""topic"": ""AI LLMs""
    }
    try:
        HowardsagentCrew().crew().train(n_iterations=int(sys.argv[1]), filename=sys.argv[2], inputs=inputs)

    except Exception as e:
        raise Exception(f""An error occurred while training the crew: {e}"")",Train the crew for a given number of iterations.,Initiates AI model training with specified iterations and input parameters.
1038,get_system_info,"def get_system_info() -> dict[str, Any]:
    
    return {
        ""os"": platform.system(),
        ""python_version"": platform.python_version(),
        ""notte_version"": __version__,
    }",Get anonymous system information.,Retrieve system and software version details as a dictionary.
1039,validate_json,"def validate_json(self, json_str: str, partial: bool = False) -> Any:
        
        validated = _json.validate_json(json_str, self._type_adapter, partial)
        if self._is_wrapped:
            if not isinstance(validated, dict):
                _error_tracing.attach_error_to_current_span(
                    SpanError(
                        message=""Invalid JSON"",
                        data={""details"": f""Expected a dict, got {type(validated)}""},
                    )
                )
                raise ModelBehaviorError(
                    f""Expected a dict, got {type(validated)} for JSON: {json_str}""
                )

            if _WRAPPER_DICT_KEY not in validated:
                _error_tracing.attach_error_to_current_span(
                    SpanError(
                        message=""Invalid JSON"",
                        data={""details"": f""Could not find key {_WRAPPER_DICT_KEY} in JSON""},
                    )
                )
                raise ModelBehaviorError(
                    f""Could not find key {_WRAPPER_DICT_KEY} in JSON: {json_str}""
                )
            return validated[_WRAPPER_DICT_KEY]
        return validated",Validate a JSON string against the output type.,"Validate and process JSON input, handling errors for expected dictionary structure."
1040,register_tool,"def register_tool(self, tool: Tool) -> None:
        
        try:
            if tool.name in [t.name for t in self.default_tools]:
                raise ValueError(f""Tool '{tool.name}' is already registered"")
            self.default_tools.append(tool)
            self.plugin_manager.tools.register(tool)
        except Exception as e:
            logger.error(f""Failed to register tool {tool.name}: {e}"")
            raise",Register a new tool dynamically at runtime.,"Register a tool if not already registered, logging errors if unsuccessful."
1041,write_transcript_to_file,"def write_transcript_to_file(
    podcast_dialogue: List[Dict[str, Any]], output_path: str
) -> None:
    
    try:
        with open(output_path, ""w"", encoding=""utf-8"") as file:
            import json

            json.dump(podcast_dialogue, file, ensure_ascii=False, indent=4)
        logger.info(f""Podcast dialogue successfully written to {output_path}"")
    except Exception as e:
        logger.error(f""Error writing podcast dialogue to file: {e}"")
        raise",Write the final structured transcript to a file.,Serialize podcast dialogue to JSON file with error handling and logging.
1042,read_specific_json_files,"def read_specific_json_files(folder_path: str) -> List[Dict[str, Any]]:
    
    json_contents: List[Dict[str, Any]] = []

    # List files only in the current directory (no walk)
    files = os.listdir(folder_path)
    # Filter files
    matching_files = [f for f in files if 'mask' in f and f.endswith('.json')]
    print(f""Found {len(matching_files)} matching files: {matching_files}"")

    for file_name in matching_files:
        file_path = os.path.join(folder_path, file_name)
        with open(file_path) as file:
            data = json.load(file)
            json_contents.append(data)

    return json_contents",Read and parse JSON files containing mask search results.,Extract and return data from JSON files containing 'mask' in a specified directory.
1043,validate_embedding_args,"def validate_embedding_args(args):
    
    if args.llm_retriever:
        # When using an LLM to retrieve, we are not running the embedder.
        return True
    if args.embedding_provider == ""openai"":
        _validate_openai_embedding_args(args)
    elif args.embedding_provider == ""voyage"":
        _validate_voyage_embedding_args(args)
    elif args.embedding_provider == ""marqo"":
        _validate_marqo_embedding_args(args)
    elif args.embedding_provider == ""gemini"":
        _validate_gemini_embedding_args(args)
    else:
        raise ValueError(f""Unrecognized --embedding-provider={args.embedding_provider}"")",Validates the configuration of the batch embedder and sets defaults.,Validate embedding configuration based on provider and retriever settings.
1044,refresh,"def refresh(self) -> None:
        
        if self.id is None:
            return

        job_data = self._agents_api.get_agent_run_v1_organizations_org_id_agent_run_agent_run_id_get(
            agent_run_id=int(self.id), org_id=int(self.org_id), authorization=f""Bearer {self._api_client.configuration.access_token}""
        )

        # Convert API response to dict for attribute access
        job_dict = {}
        if hasattr(job_data, ""__dict__""):
            job_dict = job_data.__dict__
        elif isinstance(job_data, dict):
            job_dict = job_data

        self.status = job_dict.get(""status"")
        self.result = job_dict.get(""result"")",Refresh the job status from the API.,Update agent status and result by fetching data from API if ID exists.
1045,save_context_messages,"def save_context_messages(self):
        
        context = {
            ""reasoning"": [message.to_llm_msg() for message in self.reasoning_context],
        }
        self.db.add_or_update_context_msg(self.session_id, context)",Save the reasoning context messages to the database.,Store reasoning messages in database for a specific session.
1046,get_address,"def get_address(agent, **kwargs):
    
    try:
        return agent.connection_manager.connections[""ethereum""].get_address()
    except Exception as e:
        logger.error(f""Failed to get address: {str(e)}"")
        return None",Get configured Ethereum wallet address,"Retrieve Ethereum address via agent's connection manager, handling errors."
1047,denylist_filter,"def denylist_filter():
    
    return EvalFilterConfig(
        allowlist=None,
        denylist=EvalFilterEntryConfig(field={""repo"": [""vxlan""]})  # Remove rows where repo is ""vxlan""
    )",Fixture for a repo-based denylist filter.,"Configure a filter to exclude entries with ""vxlan"" in the repository field."
1048,get_time_steps,"def get_time_steps(self, skip_type, t_T, t_0, N, order, device):
        
        if skip_type == ""logSNR"":
            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))
            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))
            logSNR_steps = lambda_T + torch.linspace(
                torch.tensor(0.0).cpu().item(), (lambda_0 - lambda_T).cpu().item() ** (1.0 / order), N + 1
            ).pow(order).to(device)
            return self.noise_schedule.inverse_lambda(logSNR_steps)
        elif skip_type == ""time"":
            t = torch.linspace(t_T ** (1.0 / order), t_0 ** (1.0 / order), N + 1).pow(order).to(device)
            return t
        elif skip_type == ""karras"":
            sigma_min = max(0.002, self.sigma_min)
            sigma_max = min(80, self.sigma_max)
            sigma_steps = torch.linspace(sigma_max ** (1.0 / 7), sigma_min ** (1.0 / 7), N + 1).pow(7).to(device)
            t = self.noise_schedule.edm_inverse_sigma(sigma_steps)
            return t
        else:
            raise ValueError(f""Unsupported skip_type {skip_type}, need to be 'logSNR' or 'time' or 'karras'"")",Compute the intermediate time steps for sampling.,Calculate time steps based on skip type for noise scheduling in a device-specific context.
1049,_validate_upsert_ids,"def _validate_upsert_ids(self, collection_name: str, ids: list[str]) -> bool:
        
        retrieved_ids = [
            point.id for point in self.client.retrieve(collection_name, ids=ids, with_payload=False, with_vectors=False)
        ]

        if existing_ids := set(ids) & set(retrieved_ids):
            logger.log(f""Existing IDs: {existing_ids}."", level=logging.WARN)
            return False

        return True",Validate none of the IDs exist in the collection,"Check if IDs exist in collection, log warning if true, return validation result."
1050,update,"def update(self, *args, **kwargs):
        
        for k, v in kwargs.items():
            if k not in self.defaults:
                raise KeyError(f""No Ultralytics setting '{k}'. {self.help_msg}"")
            t = type(self.defaults[k])
            if not isinstance(v, t):
                raise TypeError(f""Ultralytics setting '{k}' must be of type '{t}', not '{type(v)}'. {self.help_msg}"")
        super().update(*args, **kwargs)","Updates settings, validating keys and types.",Validate and update configuration settings with type checking and error handling.
1051,do_l,"def do_l(self, x: PDFStackT, y: PDFStackT) -> None:
        
        x_f = safe_float(x)
        y_f = safe_float(y)
        if x_f is None or y_f is None:
            point = (""l"", x, y)
            log.warning(
                f""Cannot append straight line segment to path because not all values in {point!r} can be parsed as floats""
            )
        else:
            point = (""l"", x_f, y_f)
            self.curpath.append(point)",Append straight line segment to path,Append line segment to path if coordinates are valid floats
1052,task_repeat_pattern_full,"def task_repeat_pattern_full(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    # Generate initial pattern
    pattern_size = rng.randint(2, 5)
    pattern = [rng.randint(1, 9) for _ in range(pattern_size)]

    # Calculate total size needed for 2 repetitions
    double_size = pattern_size * 2
    if double_size >= size:
        return None

    # Create input with 2 repetitions
    question = gen_field(size)
    for i in range(pattern_size):
        question[i] = pattern[i]
        question[i + pattern_size] = pattern[i]

    # Create answer with maximum repetitions
    answer = gen_field(size)
    pos = 0
    while pos + pattern_size <= size:
        for i in range(pattern_size):
            answer[pos + i] = pattern[i]
        pos += pattern_size

    # Fill remaining space (if any) with pattern elements
    for i in range(pos, size):
        answer[i] = pattern[i - pos]

    return {""input"": question, ""output"": answer}",Generate a task where a pattern is repeated to fill the space.,Generate repeated pattern sequences for input-output mapping
1053,checkbox,"def checkbox(label: str, default: bool = False, size: float = 1.0, component_id: str | None = None, **kwargs) -> ComponentReturn:
    
    service = PreswaldService.get_instance()

    # Get current state or use default
    current_value = service.get_component_state(component_id)
    if current_value is None:
        current_value = default

    logger.debug(f""Creating checkbox component with id {component_id}, label: {label}"")
    component = {
        ""type"": ""checkbox"",
        ""id"": component_id,
        ""label"": label,
        ""value"": current_value,
        ""size"": size,
    }

    return ComponentReturn(current_value, component)",Create a checkbox component with consistent ID based on label.,"Create a checkbox component with customizable label, default state, and size."
1054,get_constructor,"def get_constructor(cls, cdp_type, val):
        
        if cdp_type == ""any"":
            return val
        else:
            cons = cls[cdp_type].value
            return f""{cons}({val})""",Return the code to construct a value for a given CDP type.,Selects and formats a constructor based on type and value.
1055,extract_metadata,"def extract_metadata(self, query: str) -> List[str]:
        
        
        urls = get_web_urls(query, num_urls=self.max_sources_per_round)
        if not urls:
            return []

        metadata = []

        # Parallelize the scraping of the urls
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(10, len(urls))) as executor:
            futures_to_url = {executor.submit(scrape_url, url): url for url in urls}

            for future in concurrent.futures.as_completed(futures_to_url):
                url = futures_to_url[future]
                try:
                    result = future.result()
                    if result and result.get(""markdown""):
                        metadata.append(result.get(""markdown""))
                except Exception as e:
                    metadata.append(f""Contents not found for {url}"")

        
        return urls,metadata",Extracts the metadata for a given query using the firecrawl API and SERPAPI utility functions,Extract metadata from web pages using parallel scraping and return results.
1056,get_project_path,"def get_project_path(self, project_name: Optional[str] = None) -> Path:  # pragma: no cover
        
        name = project_name or self.default_project

        if name not in self.projects:
            raise ValueError(f""Project '{name}' not found in configuration"")

        return Path(self.projects[name])",Get the path for a specific project or the default project.,Retrieve project directory path based on given or default project name.
1057,_load_template_metadata,"def _load_template_metadata(
        self, slug: str, version: str
    ) -> Optional[TemplateMetadata]:
        
        try:
            # Load metadata from the registry directory structure
            slug_sanitized = slug.replace("" "", ""_"").lower()
            version_sanitized = ""v"" + version.replace(""."", ""_"")
            metadata_path = (
                self.registry.templates_dir
                / slug_sanitized
                / version_sanitized
                / ""metadata.json""
            )

            if not metadata_path.exists():
                return None

            with open(metadata_path, ""r"", encoding=""utf-8"") as f:
                metadata_dict = json.load(f)

            return TemplateMetadata(**metadata_dict)
        except Exception as e:
            logger.error(f""Failed to load metadata for {slug} v{version}: {e}"")
            return None",Load template metadata from the registry.,"Load and parse template metadata from a structured directory, handling errors gracefully."
1058,_initialize_conversation,"def _initialize_conversation(self) -> None:
        
        system_prompt = generate_system_prompt(self.internal_tools)
        self.conversation_history = [{""role"": ""system"", ""content"": system_prompt}]",Initialize conversation with system prompt.,Initialize conversation history with a system-generated prompt using internal tools.
1059,_yield_n_from_exhausted_bucket,"def _yield_n_from_exhausted_bucket(self, n: int, bucket: str):
        
        available_images = self.metadata_backend.aspect_ratio_bucket_indices[bucket]
        if len(available_images) == 0:
            self.debug_log(f""Bucket {bucket} is empty."")
            return []
        samples = []
        while len(samples) < n:
            to_grab = min(n, len(available_images), (n - len(samples)))
            if to_grab == 0:
                break
            samples.extend(random.sample(available_images, k=to_grab))

        to_yield = self._validate_and_yield_images_from_samples(samples, bucket)
        return to_yield","when a bucket is exhausted, and we have to populate the remainder of the batch, we shall use this quick and dirty method to retrieve n samples from the exhausted bucket.",Select and validate images from a specified bucket until a target count is reached.
1060,match,"def match(cls, responses, eval_context) -> int:
        
        bounding_box_has_match = {bbox: False for bbox in eval_context[""bounding_boxes""]}
        bounding_boxes = [str_to_bboxes(bbox_str)[0] for bbox_str in eval_context[""bounding_boxes""]]
        assert bounding_boxes

        if not isinstance(responses, (tuple | list)):
            responses = parse_point_2d_from_xml(responses)
            if not responses:
                return 0, bounding_box_has_match
        elif len(responses) != 2:
            return 0, bounding_box_has_match

        x, y = responses
        for min_x, min_y, max_x, max_y in bounding_boxes:
            if min_x <= x <= max_x and min_y <= y <= max_y:
                bounding_box_has_match[str((min_x, min_y, max_x, max_y))] = True
                return 1, bounding_box_has_match
        return 0, bounding_box_has_match","Determine if the point is in the bounding box and return which bounding box was matched, if any.",Determine if a point lies within any bounding boxes and update match status.
1061,get_session,"def get_session(session_id):
    
    if not session_id:
        return {""message"": f""Please provide {session_id}.""}, 400

    session_handler = SessionHandler(
        db=load_db(os.getenv(""SERVER_DB_TYPE"", app.config[""DB_TYPE""]))
    )
    session = session_handler.get_session(session_id)
    if not session:
        return {""message"": ""Session not found.""}, 404

    if request.method == ""GET"":
        return session
    elif request.method == ""DELETE"":
        success, failed_components = session_handler.delete_session(session_id)
        if success:
            return {""message"": ""Session deleted successfully.""}, 200
        else:
            return {
                ""message"": f""Failed to delete the entry for following components: {', '.join(failed_components)}""
            }, 500",Get or delete the session details,Handle session retrieval and deletion based on HTTP request method.
1062,run_python_node_with_input,"def run_python_node_with_input():
    
    python_code_with_input = 

    python_node = Python(code=python_code_with_input, model_config=ConfigDict())
    input_data = {""name"": ""Alice"", ""age"": 30}
    config = None
    result = python_node.run(input_data, config)
    print(""Result with input:"")
    print(result)",Basic example of running Python node with input data.,Execute Python code with input data and print the result.
1063,generate_stream_text,"def generate_stream_text(
        self,
        prompt: str,
        *,
        llm_model: str | None = None,
        **kwargs,
    ) -> Iterator[str]:
        
        messages = [
            {""role"": ""user"", ""content"": prompt},
        ]

        response = self.client.chat.completions.create(
            messages=messages,
            model=llm_model or self.DEFAULT_MODEL,
            stream=True,
            **{**self.DEFAULT_KWARGS, **kwargs},
        )

        try:
            for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            raise RuntimeError(
                f""Failed to generate streaming text with Groq API: {e}""
            ) from e",Generate streaming text using the Groq API.,Generate streaming text responses using a language model with error handling.
1064,get_watched_artists,"def get_watched_artists():
    
    try:
        with _get_artists_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(""SELECT * FROM watched_artists WHERE is_active = 1"")
            artists = [dict(row) for row in cursor.fetchall()]
            return artists
    except sqlite3.Error as e:
        logger.error(
            f""Error retrieving watched artists from {ARTISTS_DB_PATH}: {e}"",
            exc_info=True,
        )
        return []",Retrieves all active artists from the watched_artists table in artists.db.,"Retrieve active watched artists from the database, handling errors."
1065,_initialize_processors,"def _initialize_processors(self):
        
        if self.lightrag is None:
            raise ValueError(
                ""LightRAG instance must be initialized before creating processors""
            )

        # Create different multimodal processors
        self.modal_processors = {
            ""image"": ImageModalProcessor(
                lightrag=self.lightrag,
                modal_caption_func=self.vision_model_func or self.llm_model_func,
            ),
            ""table"": TableModalProcessor(
                lightrag=self.lightrag, modal_caption_func=self.llm_model_func
            ),
            ""equation"": EquationModalProcessor(
                lightrag=self.lightrag, modal_caption_func=self.llm_model_func
            ),
            ""generic"": GenericModalProcessor(
                lightrag=self.lightrag, modal_caption_func=self.llm_model_func
            ),
        }

        self.logger.info(""Multimodal processors initialized"")
        self.logger.info(f""Available processors: {list(self.modal_processors.keys())}"")",Initialize multimodal processors with appropriate model functions,"Initialize multimodal processors for image, table, equation, and generic data types."
1066,collect_all_metrics,"def collect_all_metrics(self) -> Dict[str, Any]:
        
        perf_metrics = self.collect_performance_metrics()
        return {
            'timestamp': datetime.now().isoformat(),
            'memory': self.collect_memory_usage(),
            'performance': {
                'cpu_percent': perf_metrics.cpu_percent,
                'memory_percent': perf_metrics.memory_percent,
                'disk_usage_percent': perf_metrics.disk_usage_percent
            },
            'api': self.collect_api_stats(),
            'plugins': self.collect_plugin_usage()
        }",Collect all available metrics,"Collects and returns system performance, memory, API, and plugin metrics with a timestamp."
1067,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Animation.enable"",
    }
    json = yield cmd_dict",Enables animation domain notifications.,Enable animation feature by sending a command and awaiting a JSON response.
1068,_send_rewards_to_kinesis,"def _send_rewards_to_kinesis(self, leaders, round, stage):
        
        try:
            current_time = datetime.now(timezone.utc)
            rewards_data = []

            for leader in leaders:
                rewards_data.append(
                    RewardsMessageData(
                        peerId=leader[""id""],
                        peerName=leader[""nickname""],
                        amount=leader[""cumulativeScore""],
                        round=round,
                        stage=stage,
                        timestamp=current_time,
                    )
                )

            rewards_message = RewardsMessage(type=""rewards"", data=rewards_data)
            self.kinesis_client.put_rewards(rewards_message)

        except Exception as e:
            self.logger.error(f""!!! Failed to send rewards to Kinesis: {e}"")",Convert leaderboard data to RewardsMessage format and send to Kinesis,Send leader rewards data to Kinesis stream with error logging
1069,get_info,"def get_info(model: str) -> ModelInfo:
    
    # Check for exact match first
    if model in _MODEL_INFO:
        return _MODEL_INFO[model]
    raise KeyError(f""Model '{model}' not found in model info"")",Get the model information for a specific model.,Retrieve model information or raise error if not found
1070,cleanup_models,"def cleanup_models(cls):
        
        print(""HiDream: Cleaning up all cached models..."")
        keys_to_del = list(cls._model_cache.keys())
        for key in keys_to_del:
            print(f""  Removing '{key}'..."")
            try:
                pipe_to_del, _ = cls._model_cache.pop(key)
                # More aggressive cleanup - clear all major components
                if hasattr(pipe_to_del, 'transformer'):
                    pipe_to_del.transformer = None
                if hasattr(pipe_to_del, 'text_encoder_4'):
                    pipe_to_del.text_encoder_4 = None
                if hasattr(pipe_to_del, 'tokenizer_4'):
                    pipe_to_del.tokenizer_4 = None
                if hasattr(pipe_to_del, 'scheduler'):
                    pipe_to_del.scheduler = None
                del pipe_to_del
            except Exception as e:
                print(f""  Error cleaning up {key}: {e}"")
        # Multiple garbage collection passes
        for _ in range(3):
            gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            # Force synchronization
            torch.cuda.synchronize()
        print(""HiDream: Cache cleared"")
        return True",Clean up all cached models - can be called by external memory management,Function clears cached models and performs memory cleanup for efficient resource management.
1071,load_config,"def load_config(self) -> bool:
        
        try:
            if not self.config_path.exists():
                logger.warning(f""MCP config file not found at {self.config_path}"")
                return False
                
            with open(self.config_path, 'r') as f:
                config_data = f.read()
                logger.debug(f""Raw config data: {config_data}"")
                config = json.loads(config_data)
            
            # Set log level
            self.log_level = config.get(""log_level"", ""INFO"")
            log_level = getattr(logging, self.log_level.upper(), logging.INFO)
            logger.setLevel(log_level)
            
            # Load server configurations
            servers_config = config.get(""mcpServers"", {})
            for server_name, server_config in servers_config.items():
                self.servers[server_name] = ServerConfig.from_dict(server_config)
                logger.debug(f""Loaded server config for {server_name}: {server_config}"")
            
            logger.info(f""Loaded configuration with {len(self.servers)} servers"")
            return True
            
        except Exception as e:
            logger.error(f""Error loading MCP configuration: {e}"")
            logger.error(traceback.format_exc())
            return False",Load configuration from file,"Load and parse configuration file, set logging level, and initialize server settings."
1072,merge_qkv_for_head,"def merge_qkv_for_head(state_dict: dict, config: Siglip2Config) -> dict:
    
    # Make shallow copy
    state_dict = state_dict.copy()
    # Read and process q/k/v weights and biases
    qkv_weights, qkv_biases = [], []
    for name in [""query"", ""key"", ""value""]:
        prefix = f""params/img/MAPHead_0/MultiHeadDotProductAttention_0/{name}""
        weight = state_dict.pop(f""{prefix}/kernel"").reshape(-1, config.vision_config.hidden_size)
        bias = state_dict.pop(f""{prefix}/bias"").reshape(-1)
        qkv_weights.append(weight)
        qkv_biases.append(bias)

    # Combine into single tensors
    state_dict[""params/img/MAPHead_0/MultiHeadDotProductAttention_0/qkv/kernel""] = np.concatenate(qkv_weights, axis=1)
    state_dict[""params/img/MAPHead_0/MultiHeadDotProductAttention_0/qkv/bias""] = np.concatenate(qkv_biases, axis=0)
    return state_dict",Merge the q/k/v weights and biases for the attention head.,"Merge query, key, value weights and biases into unified attention parameters."
1073,_keepalive_thread_run,"def _keepalive_thread_run(self):
        
        while self._keepalive_running:
            time.sleep(self.KEEPALIVE_POLL_FREQUENCY)
            with self._lock:
                for name, entry in list(self._entries.items()):
                    if (
                        entry.keeper is not None
                        and entry.keepalive_ttl is not None
                        and entry.lease_id is not None
                        and entry.keeper.check()
                    ):
                        try:
                            # Refresh the lease
                            success = ray.get(
                                self._kv_store.refresh_lease.remote(
                                    name, entry.keepalive_ttl
                                )
                            )
                            if not success:
                                logger.warning(
                                    f""Failed to refresh lease for key: {name}""
                                )
                        except Exception as e:
                            logger.error(
                                f""Failed to refresh lease for key: K={name} V={entry.value}. Error: {e}""
                            )",Background thread to keep leases alive.,Periodically refreshes leases for active entries in a key-value store.
1074,do_k,"def do_k(self, c: PDFStackT, m: PDFStackT, y: PDFStackT, k: PDFStackT) -> None:
        
        cmyk = safe_cmyk(c, m, y, k)

        if cmyk is None:
            log.warning(
                f""Cannot set CMYK non-stroke color because not all values in {(c, m, y, k)!r} can be parsed as floats""
            )
        else:
            self.graphicstate.ncolor = cmyk
            self.ncs = self.csmap[""DeviceCMYK""]",Set CMYK color for nonstroking operations,"Set non-stroke color in CMYK if values are valid, else log warning."
1075,run_telegram,"def run_telegram(telegram_agent):
    
    try:
        logger.info(""Starting Telegram agent..."")
        telegram_agent.run()
    except Exception as e:
        logger.error(f""Telegram agent error: {str(e)}"")",Run the Telegram agent,Initialize and execute a Telegram agent with error logging.
1076,parse_aspect_ratio,"def parse_aspect_ratio(aspect_ratio_str):
        
        try:
            # Extract dimensions from the parenthesis
            dims_part = aspect_ratio_str.split(""("")[1].split("")"")[0]
            width, height = dims_part.split(""×"")
            return int(width), int(height)
        except Exception as e:
            print(f""Error parsing aspect ratio '{aspect_ratio_str}': {e}. Falling back to 1024x1024."")
            return 1024, 1024",Parse aspect ratio string to get width and height,"Extracts width and height from aspect ratio string, defaults on error."
1077,_format_search_results,"def _format_search_results(self, results: dict[str, Any]) -> str:
        
        formatted_results = []
        for result in results.get(""data"", []):
            formatted_results.extend(
                [
                    f""Source: {result.get('url')}"",
                    f""Title: {result.get('title')}"",
                    f""Description: {result.get('description')}"",
                    *(
                        [f""Content: {result.get('content')}""]
                        if self.include_full_content and result.get(""content"")
                        else []
                    ),
                    """",
                ]
            )
        return ""\n"".join(formatted_results).strip()",Format the search results into a readable string format.,Format search results into a structured string output with optional content inclusion.
1078,_does_collection_exist,"def _does_collection_exist(self) -> bool:
        
        try:
            collection_info = self.client.get_collections()
            collection_names = [collection.name for collection in collection_info.collections]
            return self.collection_name in collection_names
        except Exception as e:
            self.logger.error(f""Error checking for collection existence: {e}"")
            return False",Check if the collection already exists in Qdrant.,"Check if a specified collection exists in the database, logging errors if any occur."
1079,answer_questions,"def answer_questions(self, documents: List[Document], **kwargs) -> List[str]:
        
        answers = []

        for document in documents:
            # Extract question and contexts from the document
            question = document.question.question
            contexts = [context.text for context in document.contexts]

            # Construct the prompt
            prompt = f.join(contexts)
        
            # Generate the answer using the model
            answer = self.model.generate(prompt=prompt, **kwargs)
            
            # Append the answer to the list
            answers.append(answer)
        return answers",Answer a question using chain-of-thought reasoning.,Generate answers from documents using a model with contextual prompts.
1080,write_file,"def write_file(self, path: Path, file: str):
        
        try:
            path.write_text(file)
        except Exception as e:
            raise ToolError(f""Ran into {e} while trying to write to {path}"") from None",Write the content of a file to a given path; raise a ToolError if an error occurs.,Handles file writing and raises a custom error on failure.
1081,move_to_pose,"def move_to_pose(
    obs, obs_saver, robot_ik, robot, scenario, ee_pos_target, ee_quat_target, steps=10, open_gripper=False
):
    
    curr_robot_q = obs.robots[robot.name].joint_pos

    seed_config = curr_robot_q[:, :curobo_n_dof].unsqueeze(1).tile([1, robot_ik._num_seeds, 1])

    result = robot_ik.solve_batch(Pose(ee_pos_target, ee_quat_target), seed_config=seed_config)

    q = torch.zeros((scenario.num_envs, robot.num_joints), device=""cuda:0"")
    ik_succ = result.success.squeeze(1)
    q[ik_succ, :curobo_n_dof] = result.solution[ik_succ, 0].clone()
    q[:, -ee_n_dof:] = 0.04 if open_gripper else 0.0
    actions = [
        {""dof_pos_target"": dict(zip(robot.actuators.keys(), q[i_env].tolist()))} for i_env in range(scenario.num_envs)
    ]
    for i in range(steps):
        obs, reward, success, time_out, extras = env.step(actions)
        obs_saver.add(obs)
    return obs",Move the robot to the target pose.,Calculate and execute robot arm movements to reach a target pose over multiple steps.
1082,part_id_to_brick_id,"def part_id_to_brick_id(part_id: str) -> int:
    
    for brick_id, properties in brick_library.items():
        if properties['partID'] == part_id:
            return int(brick_id)
    raise ValueError(f'No brick ID for part ID: {part_id}')","Returns the brick ID of the given part ID, which is the ID of the brick used in the brick library.",Map part identifier to corresponding brick identifier using a library
1083,get_stripe_profile,"def get_stripe_profile(token: str) -> Optional[Dict]:
        
        headers = Config.BASE_HEADERS.copy()
        headers.update({""Authorization"": f""Bearer {token}""})
        try:
            proxies = UsageManager.get_proxy()
            response = requests.get(url, headers=headers, timeout=10, proxies=proxies)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.error(f""Get subscription info failed: {str(e)}"")
            return None",get user subscription info,Fetch Stripe user profile using token with error handling and proxy support.
1084,extract_elements_from_screenshot,"def extract_elements_from_screenshot(self, screenshot) -> Dict:
        

        # Convert screenshot to PIL image
        def send_image_to_ocr(screenshot) -> Dict:

            # url = os.environ.get(""OCR_SERVER_ADDRESS"", """")
            if url == """":
                raise Exception(""OCR SERVER ADDRESS NOT SET"")
            encoded_screenshot = base64.b64encode(screenshot).decode(""utf-8"")
            data = {""img_bytes"": encoded_screenshot}
            response = requests.post(url, json=data)

            if response.status_code == 200:
                return response.json()
            else:
                return {
                    ""error"": f""Request failed with status code {response.status_code}"",
                    ""results"": [],
                }

        return send_image_to_ocr(screenshot)[""results""]",Uses paddle-ocr to extract elements with text from the screenshot.,Extract text elements from a screenshot using OCR server communication.
1085,write,"def write(self, chunks: Union[Chunk, Sequence[Chunk]]) -> None:
        
        if isinstance(chunks, Chunk):
            chunks = [chunks]

        # Generate the ids and metadata
        ids = [self._generate_id(index, chunk) for (index, chunk) in enumerate(chunks)]
        metadata = [self._generate_metadata(chunk) for chunk in chunks]
        texts = [chunk.text for chunk in chunks]
        
        # Write the Chunks to the Chroma collection
        # Since this uses the `upsert` method, if the same index and same chunk text already exist, it will update the existing Chunk — which would only be the case if the Chunk has a different embedding
        self.collection.upsert(
            ids=ids,
            documents=texts,
            metadatas=metadata, # type: ignore
        )

        print(f""🦛 Chonkie wrote {len(chunks)} Chunks to the Chroma collection: {self.collection_name}"")",Write the Chunks to the Chroma collection.,Write and update data chunks in a Chroma collection with metadata.
1086,remove_app_connections,"def remove_app_connections(app_name: str):
    
    print(f""\n=== Removing All {app_name} Connections ==="")
    accounts = toolset.client.connected_accounts.get()
    removed = 0
    
    for account in accounts:
        if account.appUniqueId.lower() == app_name.lower():
            try:
                print(f""Removing connection (ID: {account.id})..."")
                toolset.client.connected_accounts.delete(account.id)
                removed += 1
                print(""✓ Successfully removed"")
            except Exception as e:
                print(f""✗ Error: {str(e)}"")
    
    print(f""\nRemoved {removed} connection(s) for {app_name}"")",Remove all connections for a specific app.,Remove all connections for a specified app by matching and deleting associated accounts.
1087,verify_chunk_indices,"def verify_chunk_indices(chunks: list[LateChunk], original_text: str) -> None:
    
    reconstructed_text = """"
    for i, chunk in enumerate(chunks):
        extracted_text = original_text[chunk.start_index : chunk.end_index]
        assert chunk.text == extracted_text, (
            f""Chunk {i} text mismatch:\n""
            f""Chunk text: '{chunk.text}'\n""
            f""Extracted text: '{extracted_text}'\n""
            f""Indices: [{chunk.start_index}:{chunk.end_index}]""
        )
        reconstructed_text += chunk.text

    # Allow minor discrepancies at the very end if needed, but usually should match
    assert reconstructed_text == original_text, (
        ""Reconstructed text does not match original""
    )",Verify that chunk indices correctly map to the original text.,Verify text chunk integrity against original content by comparing indices and text.
1088,_log_metric,"def _log_metric(self, metric: Metric, prefix: str = """", step: Optional[int] = None) -> None:
        
        if not mlflow.active_run() or not metric:
            return

        metric_name = self._safe_get(metric, [""name""])
        metric_value = self._safe_get(metric, [""value""])

        if not metric_name or metric_value is None:
            return

        # Clean metric name and convert value
        clean_name = re.sub(r""[^a-zA-Z0-9_]"", """", f""{prefix}{metric_name}"")

        try:
            # Try to log as numeric
            value = float(metric_value)
            if step is not None:
                mlflow.log_metric(clean_name, value, step=step)
            else:
                mlflow.log_metric(clean_name, value)
        except (ValueError, TypeError):
            # If not numeric, log as tag
            mlflow.set_tag(f""metric_{clean_name}"", str(metric_value))
            mlflow.set_tag(""non_numeric_metrics"", ""true"")",Safely log a Plexe Metric object to MLFlow.,"Log and tag metrics in MLflow, handling numeric and non-numeric values."
1089,_resolve_model,"def _resolve_model(self, model: Optional[str]) -> str:
        
        try:
            return self.config.get(""model"", model) or os.getenv(""AGENT_MODEL"", ""gemini/gemini-2.0-flash"")
        except Exception as e:
            logger.error(f""Error resolving model: {e}. Using default."")
            return ""gemini/gemini-2.0-flash""","Resolve the model from config, argument, or environment variable.",Determine model configuration with fallback to default setting
1090,get_instruction_args,"def get_instruction_args(self):
        
        return {
            ""num_paragraphs"": self._num_paragraphs,
            ""nth_paragraph"": self._nth_paragraph,
            ""first_word"": self._first_word,
        }",Returns the keyward args of `build_description`.,Extracts and returns paragraph-related attributes as a dictionary.
1091,get_graph,"def get_graph() -> list[graph.Edge]:
    
    log.debug(f""{NAME} does not support graph generation."")
    return []",Get the graph of the user's project.,Function returns an empty list indicating unsupported graph generation.
1092,calculate_rouge_scores,"def calculate_rouge_scores(prediction: str, reference: str) -> Dict[str, float]:
    
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, prediction)
    return {
        'rouge1_f': scores['rouge1'].fmeasure,
        'rouge2_f': scores['rouge2'].fmeasure,
        'rougeL_f': scores['rougeL'].fmeasure
    }",Calculate ROUGE scores for prediction against reference.,Calculate ROUGE scores for text prediction against a reference.
1093,create_model_column,"def create_model_column(voice_ids: Optional[list] = None) -> Tuple[gr.Column, dict]:
    
    if voice_ids is None:
        voice_ids = []

    with gr.Column(scale=1) as col:
        gr.Markdown(""### Model Settings"")

        # Status button starts in waiting state
        status_btn = gr.Button(
            ""⌛ TTS Service: Waiting for Service..."", variant=""secondary""
        )

        voice_input = gr.Dropdown(
            choices=voice_ids,
            label=""Voice(s)"",
            value=voice_ids[0] if voice_ids else None,
            interactive=True,
            multiselect=True,
        )
        format_input = gr.Dropdown(
            choices=config.AUDIO_FORMATS, label=""Audio Format"", value=""mp3""
        )
        speed_input = gr.Slider(
            minimum=0.5, maximum=2.0, value=1.0, step=0.1, label=""Speed""
        )

    components = {
        ""status_btn"": status_btn,
        ""voice"": voice_input,
        ""format"": format_input,
        ""speed"": speed_input,
    }

    return col, components",Create the model settings column.,"Create a UI column for TTS model settings with voice, format, and speed options."
1094,load_data,"def load_data(self):
        
        print(f""Loading {self.dataset} dataset (mock data)"")
        return {
            'texts': [],
            'labels': [],
            'metadata': {'name': self.dataset}
        }",Temporary implementation that returns empty dataset,Simulate loading a dataset by returning empty data structures with metadata.
1095,parse_project,"def parse_project(project_path: Path) -> dict[ComponentType, list[ParsedComponent]]:
    
    parser = AstParser(project_path)

    components: dict[ComponentType, list[ParsedComponent]] = {
        ComponentType.TOOL: [],
        ComponentType.RESOURCE: [],
        ComponentType.PROMPT: [],
    }

    # Parse each directory
    for comp_type, dir_name in [
        (ComponentType.TOOL, ""tools""),
        (ComponentType.RESOURCE, ""resources""),
        (ComponentType.PROMPT, ""prompts""),
    ]:
        dir_path = project_path / dir_name
        if dir_path.exists() and dir_path.is_dir():
            dir_components = parser.parse_directory(dir_path)
            components[comp_type].extend(
                [c for c in dir_components if c.type == comp_type]
            )

    # Check for ID collisions
    all_ids = []
    for comp_type, comps in components.items():
        for comp in comps:
            if comp.name in all_ids:
                raise ValueError(
                    f""ID collision detected: {comp.name} is used by multiple components""
                )
            all_ids.append(comp.name)

    return components",Parse a GolfMCP project to extract all components.,"Parse project directories into categorized components, checking for ID collisions."
1096,get_methods_for_path,"def get_methods_for_path(self, path: str) -> List[str]:
        
        path_item = self.spec.paths.get(path)
        if not path_item:
            raise ValueError(f""Path '{path}' not found in the specification."")

        results = []
        # List of possible HTTP methods that could be present in an OpenAPI path item
        possible_methods = [
            ""get"",
            ""put"",
            ""post"",
            ""delete"",
            ""options"",
            ""head"",
            ""patch"",
            ""trace"",
        ]
        for method in possible_methods:
            operation = getattr(path_item, method, None)
            if operation is not None:
                results.append(method)
        return results",Retrieve all HTTP methods available for a specified path.,Retrieve available HTTP methods for a specified API path from the specification.
1097,get_tables_query,"def get_tables_query(cls, schema_name: str) -> str:
        
        query = cls.load_sql(""get_tables"")
        return query.replace(""{schema_name}"", schema_name)",Get a query to list all tables in a schema.,Generates a SQL query to retrieve tables from a specified database schema.
1098,temp_image_prompt_file,"def temp_image_prompt_file(self, temp_image_file):
        
        with tempfile.NamedTemporaryFile(mode=""w+"", suffix="".txt"", delete=False) as tf:
            tf.write(f)
            tf_path = Path(tf.name)

        yield tf_path

        # Cleanup
        os.unlink(tf_path)",Create a prompt file that references an image,"Create a temporary text file, yield its path, then delete it."
1099,consolidate,"def consolidate(
        self, eval_results
    ) -> Tuple[str, Dict[str, float], Dict[str, float]]:
        
        iou_output = {}
        boundary_f_output = {}
        for obj_id, iou, boundary_f in eval_results:
            assert len(iou) == 1
            key = list(iou.keys())[0]
            iou_output[obj_id] = iou[key]
            boundary_f_output[obj_id] = boundary_f[key]
        return iou_output, boundary_f_output",Consolidate the results of all the objects from the video into one dictionary.,Aggregate evaluation results into separate dictionaries for IOU and boundary metrics.
1100,end_session,"def end_session(self):
        
        self.shared_key_supervisor = None
        self.supervisor_pub = None
        self.local_priv = None
        self.local_pub = None
        self.is_initialized = False

        self.logger.info(f""🔒 Secure session {self.session_id} terminated"")
        self.session_id = None",End the secure session and clear all sensitive data,Terminate secure session by resetting keys and logging closure
1101,wait_for_server,"def wait_for_server(self, timeout: int = 300):
        
        logger.info(""Waiting for vLLM server to be ready..."")
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                response = requests.get(self.url)
                if response.status_code == 200:
                    logger.info(
                        f""vLLM server started on {self.host}:{self.port} with PID: {self.server_process.pid if self.server_process else None}"",
                    )
                    return True
            except requests.RequestException:
                pass
            time.sleep(2)

        logger.error(""Error: vLLM server did not start in time."")
        self.stop_server()
        exit(1)",Wait until the vLLM server is ready.,"Waits for server readiness, logging success or stopping on timeout."
1102,repeat,"def repeat(self, session_id: str, agent_id: str) -> AgentResponse:
        
        # Step 1: get the agent status
        agent_status = self.agents.status(agent_id=agent_id)
        # Replay each step
        for step in agent_status.steps:
            try:
                action = ActionValidation.model_validate(step).action
            except Exception as e:
                raise ValueError(
                    f""Agent {agent_id} contains invalid action: {step}. Please record a new agent with the same task.""
                ) from e
            _ = self.sessions.page.step(session_id=session_id, action=action)
        return self.agents.status(agent_id=agent_id)",Repeat the agent_id action in sequence,Replay agent actions for a session and validate each step.
1103,handle_file_operations,"def handle_file_operations(event: dict, current_span):
    
    if ""files"" in event:
        with tracer.start_as_current_span(
            name=""file_processing"",
            attributes={
                SpanAttributes.OPENINFERENCE_SPAN_KIND: ""file_operation"",
                ""file.count"": len(event[""files""][""files""]),
                ""file.types"": json.dumps([f.get(""type"", ""unknown"") for f in event[""files""][""files""]])
            }
        ) as file_span:
            files_event = event[""files""]
            files_list = files_event[""files""]
            
            for idx, this_file in enumerate(files_list):
                file_span.set_attribute(f""file.{idx}.name"", this_file.get(""name"", """"))
                file_span.set_attribute(f""file.{idx}.type"", this_file.get(""type"", """"))
                file_span.set_attribute(f""file.{idx}.size"", this_file.get(""size"", 0))
                
                if ""metadata"" in this_file:
                    file_span.set_attribute(
                        f""file.{idx}.metadata"",
                        json.dumps(this_file[""metadata""])
                    )
            file_span.set_status(Status(StatusCode.OK))",Handles file operations in the trace,Process and log file attributes from event data using tracing for monitoring.
1104,support_case_data,"def support_case_data() -> Dict[str, Any]:
    
    return {
        ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
        ""displayId"": ""12345678910"",
        ""subject"": ""EC2 instance not starting"",
        ""status"": ""opened"",
        ""serviceCode"": ""amazon-elastic-compute-cloud-linux"",
        ""categoryCode"": ""using-aws"",
        ""severityCode"": ""urgent"",
        ""submittedBy"": ""user@example.com"",
        ""timeCreated"": ""2023-01-01T12:00:00Z"",
        ""recentCommunications"": {
            ""communications"": [
                {
                    ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
                    ""body"": ""My EC2 instance i-1234567890abcdef0 is not starting."",
                    ""submittedBy"": ""user@example.com"",
                    ""timeCreated"": ""2023-01-01T12:00:00Z"",
                }
            ],
            ""nextToken"": None,
        },
        ""ccEmailAddresses"": [""team@example.com""],
        ""language"": ""en"",
        ""nextToken"": None,
    }",Return a dictionary with sample support case data.,Returns a dictionary containing detailed information about a support case.
1105,remove_invalidly_typed_feats,"def remove_invalidly_typed_feats(
    batch: features.BatchDict,
) -> features.BatchDict:
  
  return {
      k: v
      for k, v in batch.items()
      if hasattr(v, 'dtype') and v.dtype in VALID_DTYPES
  }",Remove features of types we don't want to send to the TPU e.g. strings.,Filter features with valid data types from a batch dictionary.
1106,arxiv_search,"def arxiv_search(query: str, max_results: int = 2) -> list:
    
    import arxiv

    max_results = min(max_results, 100)  # prevent abuse or infinite loops
    client = arxiv.Client()
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)

    results = []
    for paper in client.results(search):
        results.append(
            {
                ""title"": paper.title,
                ""authors"": [author.name for author in paper.authors],
                ""published"": paper.published.strftime(""%Y-%m-%d""),
                ""abstract"": paper.summary,
                ""pdf_url"": paper.pdf_url,
            }
        )

    if not results:
        return [{""title"": ""No results found"", ""abstract"": """", ""pdf_url"": """", ""authors"": [], ""published"": """"}]

    return results",Search Arxiv for papers and return the results including abstracts.,Fetch and return a list of relevant academic papers from arXiv based on a search query.
1107,_format_complex_property_types,"def _format_complex_property_types(self, prop_type: str, value: Any) -> str:
        
        if prop_type == ""relation"":
            return f""{len(value)} relation(s)"" if value else ""0 relations""
        elif prop_type == ""rollup"":
            return self._format_rollup_property(value)
        elif prop_type == ""formula"":
            return self._format_formula_property(value)
        elif prop_type == ""unique_id"":
            return self._format_unique_id_property(value)
        elif prop_type == ""verification"":
            return self._format_verification_property(value)
        else:
            return str(value) if value else """"",Format complex property types that need special handling.,Format complex property types into descriptive strings based on type and value
1108,needs_rebuild,"def needs_rebuild(self) -> bool:
        
        if not self.index_path.exists():
            return True
        if not self._index:
            self.load()
        for item in self._index:
            template_path = self.registry.templates_dir / item[""path""]
            try:
                content = template_path.read_text(encoding=""utf-8"")
            except OSError:  # file removed
                return True
            checksum = sha256(content.encode(""utf-8"")).hexdigest()
            if checksum != item.get(""checksum""):
                return True
        # check for new templates not in index
        seen = {(i[""slug""], i[""version""]) for i in self._index}
        for entry in self.registry.list_templates():
            slug = entry[""slug""]
            for version_info in entry.get(""versions"", []):
                if (slug, version_info[""version""]) not in seen:
                    return True
        return False",Return True if stored checksums differ from source files.,"Determine if a template index requires rebuilding based on file existence, content changes, or new entries."
1109,_format_conversation_for_openai,"def _format_conversation_for_openai(
        self, conversation: list[ConversationMessage]
    ) -> list[dict[str, str]]:
        
        messages = []
        for msg in conversation:
            if msg[""from""] == ""human"":
                messages.append({""role"": ""user"", ""content"": msg[""value""]})
            elif msg[""from""] == ""gpt"":
                messages.append({""role"": ""assistant"", ""content"": msg[""value""]})
            # 'system' messages could be handled here if needed
        return messages",Formats the internal conversation list into the OpenAI API messages format.,Transforms conversation messages into OpenAI-compatible role-content format.
1110,setup_repository_name,"def setup_repository_name(
    default_prefix: str = ""genai-app"", non_interactive: bool = False
) -> tuple[str, str]:
    
    if non_interactive:
        timestamp = int(time.time())
        # Return empty string instead of None to match return type
        return f""{default_prefix}-{timestamp}"", """"

    console.print(""\n> Repository Configuration"", style=""bold blue"")

    # Get current GitHub username
    result = run_command([""gh"", ""api"", ""user"", ""--jq"", "".login""], capture_output=True)
    github_username = result.stdout.strip()

    # Get repository name
    repo_name = Prompt.ask(
        ""Enter repository name"", default=f""{default_prefix}-{int(time.time())}""
    )

    # Get repository owner (default to current user)
    repo_owner = Prompt.ask(
        ""Enter repository owner (organization or username)"", default=github_username
    )

    return repo_name, repo_owner",Interactive setup of repository name and owner.,"Configure repository name and owner, optionally using non-interactive defaults."
1111,show_text,"def show_text(self, show_legend=False):
        
        if not is_rich_available():
            raise ImportError(
                ""The `rich` library is required to display text with formatting. Install it using `pip install rich`.""
            )

        text = Text(self.text)
        text.stylize(self.prompt_color, self.text_spans[0][0], self.text_spans[1][0])
        for i, (start, end) in enumerate(self.text_spans[1:]):
            if self.system_spans[i + 1]:
                text.stylize(self.system_color, start, end)
            else:
                text.stylize(self.model_color, start, end)

        text.append(f""\n\nReward: {self.reward}"", style=self.reward_color)
        print(text)

        if show_legend:
            self.show_colour_legend()",Print the text history.,Format and display styled text with optional color legend using the `rich` library.
1112,copy,"def copy(self, **kwargs) -> ""PromptTemplate"":
        
        config = self.get_config()
        new_config = deepcopy(config)
        new_config = {k: kwargs.get(k, v) for k, v in new_config.items()}
        return self.__class__.from_dict(new_config)","Create a deep-copied new PromptTemplate, optionally overriding fields with provided kwargs.",Create a new prompt template with updated configuration parameters.
1113,get_stats,"def get_stats(self):
        
        stats = {k: torch.cat(v, 0).cpu().numpy() for k, v in self.stats.items()}  # to numpy
        self.nt_per_class = np.bincount(stats[""target_cls""].astype(int), minlength=self.nc)
        self.nt_per_image = np.bincount(stats[""target_img""].astype(int), minlength=self.nc)
        stats.pop(""target_img"", None)
        if len(stats) and stats[""tp""].any():
            self.metrics.process(**stats)
        return self.metrics.results_dict",Returns metrics statistics and results dictionary.,Converts and processes statistical data for performance metrics computation.
1114,task_mirror,"def task_mirror(task_result: Optional[dict[str, list[int]]]) -> Optional[dict[str, list[int]]]:
    
    if task_result is None:
        return None
    return {""input"": list(reversed(task_result[""input""])), ""output"": list(reversed(task_result[""output""]))}",Mirror the input and output arrays of a task result.,Reverse input and output lists in a task result dictionary.
1115,load_inputs,"def load_inputs(args):
    
    assert (
        args.prompt_file is not None or args.prompt_dir is not None
    ), ""Error: input file/dir NOT Found!""

    if args.prompt_file is not None:
        assert os.path.exists(args.prompt_file)
        # load inputs for t2v
        prompt_list = load_prompts_from_txt(args.prompt_file)
        num_prompts = len(prompt_list)
        filename_list = [f""prompt-{idx+1:04d}"" for idx in range(num_prompts)]
        image_list = None
    elif args.prompt_dir is not None:
        assert os.path.exists(args.prompt_dir)
        # load inputs for i2v
        filename_list, image_list, prompt_list = load_inputs_i2v(
            args.prompt_dir,
            video_size=(args.height, args.width),
            video_frames=args.frames,
        )
    return prompt_list, image_list, filename_list",load inputs: t2v: prompts i2v: prompts + images,Load and validate input prompts or directories for processing text or image data.
1116,generate_random_prompt,"def generate_random_prompt():
    
    domain = random.choice(DOMAINS)
    metric_type = random.choice(METRIC_TYPES)
    time_period = random.choice(TIME_PERIODS)

    templates = [
        f""Create a metric card for {metric_type} in {domain} for {time_period}."",
        f""Generate a JSON output for a {domain} dashboard showing {metric_type} during {time_period}."",
        f""I need a metric card showing {metric_type} statistics for our {domain} business for {time_period}."",
        f""Design a metric component that displays {metric_type} for our {domain} platform for {time_period}."",
        f""Provide a metric card JSON for {metric_type} in our {domain} analytics for {time_period}."",
    ]

    prompt = random.choice(templates)
    return prompt, domain, metric_type, time_period",Generate a random prompt for metric card generation,Generate a random prompt for creating metric cards across various domains and time periods.
1117,empty_describe_cases_response_data,"def empty_describe_cases_response_data() -> Dict[str, Any]:
    
    return {""cases"": [], ""nextToken"": None}",Return a dictionary with empty describe cases response data.,Function returns a dictionary with empty cases list and null next token.
1118,generate_reply_custom,"def generate_reply_custom(question, original_question, seed, state, stopping_strings=None, is_chat=False):
    
    seed = set_manual_seed(state['seed'])

    t0 = time.time()
    reply = ''
    try:
        if not is_chat:
            yield ''

        if not state['stream']:
            reply = shared.model.generate(question, state)
            yield reply
        else:
            for reply in shared.model.generate_with_streaming(question, state):
                yield reply

    except Exception:
        traceback.print_exc()
    finally:
        t1 = time.time()
        original_tokens = len(encode(original_question)[0])
        new_tokens = len(encode(original_question + reply)[0]) - original_tokens
        print(f'Output generated in {(t1-t0):.2f} seconds ({new_tokens/(t1-t0):.2f} tokens/s, {new_tokens} tokens, context {original_tokens}, seed {seed})')
        return",For models that do not use the transformers library for sampling,Generate a reply to a question using a model with optional streaming and logging.
1119,get_generators,"def get_generators(generators) -> dict:
    
    prefix = ""generate_""
    return {strip_prefix(n, prefix): getattr(generators, n) for n in dir(generators) if n.startswith(prefix)}",returns mapper from task identifiers (keys) to example generator functions,Extracts and maps generator functions from an object by prefix filtering.
1120,forward,"def forward(
        self,
        pix_feat: torch.Tensor,
        masks: torch.Tensor,
        skip_mask_sigmoid: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        
        if not skip_mask_sigmoid:
            masks = F.sigmoid(masks)
        masks = self.mask_downsampler(masks)

        # Fuse pix_feats and downsampled masks, in case the visual features are on CPU, cast them to CUDA
        pix_feat = pix_feat.to(masks.device)

        x = self.pix_feat_proj(pix_feat)
        x = x + masks
        x = self.fuser(x)
        x = self.out_proj(x)

        pos = self.position_encoding(x).to(x.dtype)

        return {""vision_features"": x, ""vision_pos_enc"": [pos]}",Processes pixel features and masks to generate encoded memory representations for segmentation.,"Process and fuse pixel features with masks, applying optional sigmoid and downsampling."
1121,build_graph,"def build_graph(self, repo_operator: RepoOperator) -> None:
        
        self.__graph_ready = True
        self._graph.clear()

        # =====[ Add all files to the graph in parallel ]=====
        syncs = defaultdict(lambda: [])
        if self.config.disable_file_parse:
            logger.warning(""WARNING: File parsing is disabled!"")
        else:
            for filepath, _ in repo_operator.iter_files(subdirs=self.projects[0].subdirectories, extensions=self.extensions, ignore_list=GLOBAL_FILE_IGNORE_LIST):
                syncs[SyncType.ADD].append(self.to_absolute(filepath))
        logger.info(f""> Parsing {len(syncs[SyncType.ADD])} files in {self.projects[0].subdirectories or 'ALL'} subdirectories with {self.extensions} extensions"")
        self._process_diff_files(syncs, incremental=False)
        files: list[SourceFile] = self.get_nodes(NodeType.FILE)
        logger.info(f""> Found {len(files)} files"")
        logger.info(f""> Found {len(self.nodes)} nodes and {len(self.edges)} edges"")
        if self.config.track_graph:
            self.old_graph = self._graph.copy()",Builds a codebase graph based on the current file state of the given repo operator,"Constructs a file dependency graph from repository data, logging file and node statistics."
1122,valid_spec_dict,"def valid_spec_dict():
    
    return {
        ""task_description"": ""Test agent for CLI"",
        ""inputs"": {""data"": ""string""},
        ""outputs"": {""status"": ""string""},
        ""constraints"": [""Must run quickly""],
        ""technical_requirements"": [""Python 3.10+""],
        ""metadata"": {""test_id"": ""cli-001""},
    }",Provides a dictionary representing a valid specification.,Define a specification dictionary for a CLI test agent.
1123,upload_zstd_csv,"def upload_zstd_csv(s3_client, s3_path, lines):
    
    joined_text = ""\n"".join(lines)
    compressor = zstd.ZstdCompressor()
    compressed = compressor.compress(joined_text.encode(""utf-8""))
    put_s3_bytes(s3_client, s3_path, compressed)
    logger.info(f""Uploaded compressed {s3_path}"")",Compress and upload a list of lines as a .zstd CSV file to S3.,Compress and upload CSV data to S3 using Zstandard
1124,put_rewards,"def put_rewards(self, data: RewardsMessage) -> None:
        
        try:
            self.logger.info(""Preparing to put rewards data to Kinesis"")
            self.logger.debug(
                f""Rewards data: {json.dumps(data.model_dump(by_alias=True), cls=DateTimeEncoder)}""
            )
            self._put_record(data.model_dump(by_alias=True), ""swarm-rewards"")
            self.logger.info(""Successfully put rewards data to Kinesis"")
        except Exception as e:
            self.logger.error(f""Failed to put rewards data: {str(e)}"", exc_info=True)
            raise KinesisError(f""Failed to put rewards data: {str(e)}"")",Put rewards data to Kinesis stream,"Log and send rewards data to Kinesis, handling errors with custom exceptions."
1125,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        result = {
            ""time_spent"": self.time_spent,
            ""time_spent_seconds"": self.time_spent_seconds,
        }

        if self.author:
            result[""author""] = self.author.to_simplified_dict()

        if self.comment:
            result[""comment""] = self.comment

        if self.started:
            result[""started""] = self.started

        if self.created:
            result[""created""] = self.created

        if self.updated:
            result[""updated""] = self.updated

        return result",Convert to simplified dictionary for API response.,Convert object attributes to a simplified dictionary format.
1126,json_viewer,"def json_viewer(
    data, title: str | None = None,
    expanded: bool = True,
    size: float = 1.0,
    component_id: str | None = None,
    **kwargs
) -> dict:
    
    # Attempt to ensure JSON is serializable and safe
    try:
        if isinstance(data, str):
            parsed_data = json.loads(data)
        else:
            parsed_data = data
        serializable_data = convert_to_serializable(parsed_data)
    except Exception as e:
        serializable_data = {""error"": f""Invalid JSON: {e!s}""}

    component = {
        ""type"": ""json_viewer"",
        ""id"": component_id,
        ""data"": serializable_data,
        ""title"": title,
        ""expanded"": expanded,
        ""size"": size,
    }

    logger.debug(f""Created JSON viewer component with id {component_id}"")
    return ComponentReturn(component, component)",Create a JSON viewer component with collapsible tree view.,Create a JSON viewer component with error handling and customization options.
1127,load_results,"def load_results(self, filepath: str):
        
        with open(filepath, ""r"") as f:
            data = json.load(f)

        self.test_results = []
        for result_dict in data[""test_results""]:
            result = TestResult(**result_dict)
            self.test_results.append(result)

        print(f""📂 Loaded {len(self.test_results)} test results from {filepath}"")",Load test results from JSON file.,Load and parse test results from a JSON file into a list of objects.
1128,get_artist_album_ids_from_db,"def get_artist_album_ids_from_db(artist_spotify_id: str):
    
    table_name = f""artist_{artist_spotify_id.replace('-', '_')}_albums""
    album_ids: set[str] = set()
    try:
        with _get_artists_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                f""SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}';""
            )
            if cursor.fetchone() is None:
                logger.warning(
                    f""Album table {table_name} for artist {artist_spotify_id} does not exist in {ARTISTS_DB_PATH}. Cannot fetch album IDs.""
                )
                return album_ids
            cursor.execute(f""SELECT album_spotify_id FROM {table_name}"")
            rows = cursor.fetchall()
            for row in rows:
                album_ids.add(row[""album_spotify_id""])
        return album_ids
    except sqlite3.Error as e:
        logger.error(
            f""Error retrieving album IDs for artist {artist_spotify_id} from {ARTISTS_DB_PATH}: {e}"",
            exc_info=True,
        )
        return album_ids",Retrieves all album Spotify IDs from a specific artist's albums table in artists.db.,Retrieve album IDs from a database table for a given artist's Spotify ID.
1129,on_switch_toggled,"def on_switch_toggled(self, switch, gparam):
        
        is_active = switch.get_active()
        self.matugen_enabled = is_active
        # self.scheme_dropdown.set_sensitive(is_active)
        self.custom_color_selector_box.set_visible(not is_active) # Toggle visibility

        # Save the state to the dedicated file
        try:
            with open(data.MATUGEN_STATE_FILE, 'w') as f:
                f.write(str(is_active))
        except Exception as e:
            print(f""Error writing matugen state file: {e}"")",Handles the toggling of the Matugen switch.,"Toggle feature state and update visibility, saving changes to a file."
1130,process_audio_segment,"def process_audio_segment(audio_file):
    
    temp_file = f""{audio_file}_temp.mp3""
    ffmpeg_cmd = [
        'ffmpeg', '-y',
        '-i', audio_file,
        '-ar', '16000',
        '-ac', '1',
        '-b:a', '64k',
        temp_file
    ]
    subprocess.run(ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    audio_segment = AudioSegment.from_mp3(temp_file)
    os.remove(temp_file)
    return audio_segment",Process a single audio segment with MP3 compression,Convert and process audio file to a specific format using FFmpeg.
1131,mock_planning_engine,"def mock_planning_engine():
    
    engine = MagicMock(spec=PlanningEngine)
    engine.analyze_tasks.return_value = {
        ""task_requirements"": [
            {
                ""task_id"": ""task_1"",
                ""tools"": [""coder_tool""],
                ""guardrails"": [],
                ""description"": ""task 1 desc"",
            },
            {
                ""task_id"": ""task_2"",
                ""tools"": [""tester_tool""],
                ""guardrails"": [],
                ""description"": ""task 2 desc"",
            },
        ],
        ""execution_order"": [""task_1"", ""task_2""],
        ""dependencies"": {},
    }
    return engine",Fixture for a mocked PlanningEngine.,Simulate a planning engine to analyze tasks and determine execution order.
1132,compute_steps,"def compute_steps(program, max_steps=100):
    
    steps = [program.copy()]
    seen_states = {tuple(program)}

    for step in range(max_steps):
        current = steps[-1]
        new_program = None

        for i in range(len(current) - 1):
            a, b = current[i], current[i + 1]
            if a == ""A#"" and b == ""#A"":
                new_program = current[:i] + current[i + 2 :]
            elif a == ""A#"" and b == ""#B"":
                new_program = current[:i] + [""#B"", ""A#""] + current[i + 2 :]
            elif a == ""B#"" and b == ""#A"":
                new_program = current[:i] + [""#A"", ""B#""] + current[i + 2 :]
            elif a == ""B#"" and b == ""#B"":
                new_program = current[:i] + current[i + 2 :]

            if new_program is not None:
                break

        if new_program is None:
            # No more transformations possible
            return steps, False

        if tuple(new_program) in seen_states:
            # Detected a loop, meaning non-halting behavior
            return steps, True

        steps.append(new_program)
        seen_states.add(tuple(new_program))

    return steps, True",Computes the transformation steps and detects if the program does not halt.,Simulate transformations on a program to detect halting or looping behavior.
1133,_handle_analyze_query_performance,"def _handle_analyze_query_performance(
        self, keyspace: str, query: str, ctx: Optional[Context] = None
    ) -> str:
        
        try:
            if not keyspace:
                raise Exception('Keyspace name is required')

            if not query:
                raise Exception('Query is required')

            analysis_result = self.query_analysis_service.analyze_query(keyspace, query)

            # Build a user-friendly response
            formatted_text = '## Query Analysis Results\n\n'
            formatted_text += f'**Query:** `{query}`\n\n'
            formatted_text += f'**Table:** `{analysis_result.table_name}`\n\n'
            formatted_text += '### Performance Assessment\n\n'
            formatted_text += f'{analysis_result.performance_assessment}\n\n'

            if analysis_result.recommendations:
                formatted_text += '### Recommendations\n\n'
                for recommendation in analysis_result.recommendations:
                    formatted_text += f'- {recommendation}\n'

            # Add contextual information about query performance in Cassandra
            if ctx:
                ctx.info('Adding contextual information about query performance in Cassandra')  # type: ignore[unused-coroutine]
                formatted_text += build_query_analysis_context(analysis_result)

            return formatted_text
        except Exception as e:
            logger.error(f'Error analyzing query: {str(e)}')
            raise Exception(f'Error analyzing query: {str(e)}')",Handle the analyzeQueryPerformance tool.,Analyze and format query performance results with recommendations for a given keyspace.
1134,current_message_history,"def current_message_history(self) -> List[Dict[str, str]]:
        
        # Convert Message objects to dicts for compatibility with history_command
        return [
            {""role"": msg.role, ""content"": msg.content, ""nanoid"": msg.nanoid}
            for msg in self.conversation_manager.get_history()
        ]",Get the current agent's message history.,Convert message objects to dictionaries for command compatibility.
1135,critique_synthesis,"def critique_synthesis(state: ResearchState) -> ResearchState:
    
    prompt = PromptTemplate(
        input_variables=[""topic"", ""synthesis"", ""answers""],
        template=""Critique this report on '{topic}':\n{synthesis}\nBased on: {answers}\nReturn 'pass' or issues.""
    )
    critique = llm.invoke(prompt.format(
        topic=state[""topic""],
        synthesis=state[""synthesis""],
        answers=""\n"".join([f""Q: {a['question']}\nA: {a['answer']}"" for a in state[""answers""]])
    ))
    return {""criticism"": critique.content}",Critique the synthesis for completeness and accuracy.,Generate a critique of a research synthesis using a language model.
1136,mock_boto3_client,"def mock_boto3_client():
    
    mock_client = MagicMock()

    # Mock search_place_index_for_text response
    mock_client.search_place_index_for_text.return_value = {
        'Results': [
            {
                'Place': {
                    'Label': 'Seattle, WA, USA',
                    'Geometry': {'Point': [-122.3321, 47.6062]},
                    'Country': 'USA',
                    'Region': 'Washington',
                    'Municipality': 'Seattle',
                }
            }
        ]
    }

    with patch('boto3.client', return_value=mock_client):
        yield mock_client",Create a mock boto3 client for testing.,Simulates AWS client to mock geolocation search responses for testing.
1137,get_root,"def get_root():
    
    # Check if a project root is specified in the environment variable
    project_root_env = os.getenv(""METAGPT_PROJECT_ROOT"")
    if project_root_env:
        project_root = Path(project_root_env)
        logger.info(f""PROJECT_ROOT set from environment variable to {str(project_root)}"")
    else:
        # Fallback to package root if no environment variable is set
        project_root = get_package_root()
      
    return project_root",Get the project root directory.,Determine project root directory from environment or default path
1138,reset_prefix_cache,"def reset_prefix_cache(self):
        
        response = self.session.post(url)
        if response.status_code != 200:
            raise Exception(f""Request failed: {response.status_code}, {response.text}"")",Resets the prefix cache for the model.,Clear cache by sending a POST request and handle errors if unsuccessful.
1139,format_plan_result_text,"def format_plan_result_text(plan_result: PlanResult) -> str:
    
    steps_str = (
        ""\n\n"".join(
            f""{i + 1}:\n{format_step_result_text(step)}""
            for i, step in enumerate(plan_result.step_results)
        )
        if plan_result.step_results
        else ""No steps executed yet""
    )

    return PLAN_RESULT_TEMPLATE.format(
        plan_objective=plan_result.objective,
        steps_str=steps_str,
        plan_result=plan_result.result if plan_result.is_complete else ""In Progress"",
    )",Format the full plan execution state as plain text for display,Format a plan's results and steps into a structured text output.
1140,unlink_plex_account,"def unlink_plex_account():
    
    try:
        # Check if user is authenticated via session
        session_id = session.get(SESSION_COOKIE_NAME)
        if not session_id or not verify_session(session_id):
            return jsonify({'success': False, 'error': 'User not authenticated'}), 401
        
        # Since user is authenticated, we can directly unlink without username validation
        if unlink_plex_from_user():
            return jsonify({'success': True, 'message': 'Plex account unlinked successfully'})
        else:
            return jsonify({'success': False, 'error': 'Failed to unlink Plex account'}), 500
            
    except Exception as e:
        logger.error(f""Error unlinking Plex account: {str(e)}"")
        # Check if this is the specific Plex-only user error
        if ""Plex-only user must set a local password"" in str(e):
            return jsonify({
                'success': False, 
                'error': 'You must set a local password before unlinking your Plex account. Please set a password in the account settings first.'
            }), 400
        else:
            return jsonify({'success': False, 'error': 'Internal server error'}), 500",Unlink Plex account from local user,"Handles unlinking a Plex account, ensuring user authentication and error management."
1141,tool_designer_spec,"def tool_designer_spec(self):
        
        return {
            ""name"": ""weather_fetcher"",
            ""purpose"": ""Fetches current weather data for a given city"",
            ""input_parameters"": [
                {
                    ""name"": ""city"",
                    ""type"": ""string"",
                    ""description"": ""Name of the city"",
                    ""required"": True
                },
                {
                    ""name"": ""country_code"",
                    ""type"": ""string"",
                    ""description"": ""ISO country code"",
                    ""required"": False
                }
            ],
            ""output_format"": ""dict""
        }",Tool specification in ToolDesigner format.,Defines a tool specification for fetching current weather data by city.
1142,run_custom,"def run_custom(self, request: BaseModel) -> AgentStatusResponse:
        
        if not self.is_custom_endpoint_available():
            raise ValueError(f""Custom endpoint is not available for this server: {self.server_url}"")
        response = self.start_custom(request)
        max_steps = request.model_dump().get(""max_steps"", max(DEFAULT_MAX_NB_STEPS, 50))
        return asyncio.run(self.watch_logs_and_wait(agent_id=response.agent_id, max_steps=max_steps))",Run an agent with the specified request parameters.,Execute custom request and monitor agent status asynchronously.
1143,_estimate_batch_size,"def _estimate_batch_size(self, sample_row: dict) -> int:
        
        import sys

        # Estimate size of a single row
        row_size = sys.getsizeof(json.dumps(sample_row))

        # Conservative estimate: aim for batches under 100MB to stay well under 512MB limit
        # Account for gRPC overhead and serialization
        target_batch_size_mb = 50  # 50MB per batch
        target_batch_size_bytes = target_batch_size_mb * 1024 * 1024


        # Cap at configured batch_size and minimum of 10
        safe_batch_size = min(self.batch_size, max(10, estimated_batch_size))

        logger.debug(f""Estimated row size: {row_size} bytes, calculated batch size: {safe_batch_size}"")
        return safe_batch_size",Estimate safe batch size based on data size to avoid gRPC limits.,Estimate optimal data batch size for efficient processing within memory limits.
1144,add_pad_mask_dict,"def add_pad_mask_dict(traj: Dict) -> Dict:
    
    traj_len = tf.shape(traj[""action""])[0]

    for key in [""observation"", ""task""]:
        pad_mask_dict = {}
        for subkey in traj[key]:
            # Handles ""language_instruction"", ""image_*"", and ""depth_*""
            if traj[key][subkey].dtype == tf.string:
                pad_mask_dict[subkey] = tf.strings.length(traj[key][subkey]) != 0

            # All other keys should not be treated as padding
            else:
                pad_mask_dict[subkey] = tf.ones([traj_len], dtype=tf.bool)

        traj[key][""pad_mask_dict""] = pad_mask_dict

    return traj",Adds a dictionary indicating which elements of the observation/task should be treated as padding.,Add padding masks to trajectory data for specified keys
1145,delete,"def delete(self, filepath):
        
        if filepath in self.df.index:
            self.df.drop(filepath, inplace=True)
            # self.save_state()
        filepath = path_to_hashed_path(filepath, self.hash_filenames)
        if os.path.exists(filepath):
            logger.debug(f""Deleting file: {filepath}"")
            os.remove(filepath)
        # Validate that we deleted it correctly.
        if self.exists(filepath) or filepath in self.df.index:
            raise Exception(f""Failed to delete {filepath}"")",Delete the specified file.,"Remove file and its record from data structure, ensuring successful deletion."
1146,prepare_batch_data,"def prepare_batch_data(self):
        
        if self.current_size == 0:
            raise ValueError(""No data in buffer to sample from"")

        # No need to stack since data is already in tensors
        all_obs = self.storage_dict[""obses""][""obs""][:self.current_size]
        all_actions = self.storage_dict[""actions""][:self.current_size]
        all_rewards = self.storage_dict[""rewards""][:self.current_size]
        all_dones = self.storage_dict[""dones""][:self.current_size]

        # Limit the number of sequences to sample from to prevent out-of-memory
        valid_range = self.current_size - self.batch_length + 1
        if valid_range <= 0:
            raise ValueError(f""Not enough transitions in buffer to create sequences of length {self.batch_length}"")

        # Sample random starting indices for sequences
        num_seqs = min(self.batch_size, valid_range)
        seq_indices = torch.randint(0, valid_range, (num_seqs,), device=self.device)

        # Create sequences efficiently with vectorized operations
        seq_data = {
            ""obs"": self._create_sequences(all_obs, seq_indices),
            ""action"": self._create_sequences(all_actions, seq_indices),
            ""reward"": self._create_sequences(all_rewards, seq_indices),
            ""done"": self._create_sequences(all_dones, seq_indices),
        }

        self.data_dict = seq_data
        return seq_data",Convert stored transitions to batch format for training,"Generate sequential data batches from stored observations, actions, rewards, and done flags."
1147,with_retry,"def with_retry(max_retries: int = 3):
    

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    print(f""Retrying {func.__name__} after error: {e}"")
            return None

        return wrapper

    return decorator",Decorator factory that creates a retry decorator.,Decorator adds retry logic to functions with customizable attempts.
1148,_prepare_create_graph_request,"def _prepare_create_graph_request(
        self,
        name: str,
        filters: Optional[Dict[str, Any]],
        documents: Optional[List[str]],
        prompt_overrides: Optional[Union[GraphPromptOverrides, Dict[str, Any]]],
        folder_name: Optional[Union[str, List[str]]],
        end_user_id: Optional[str],
    ) -> Dict[str, Any]:
        
        # Convert prompt_overrides to dict if it's a model
        if prompt_overrides and isinstance(prompt_overrides, GraphPromptOverrides):
            prompt_overrides = prompt_overrides.model_dump(exclude_none=True)

        request = {
            ""name"": name,
            ""filters"": filters,
            ""documents"": documents,
            ""prompt_overrides"": prompt_overrides,
        }
        if folder_name:
            request[""folder_name""] = folder_name
        if end_user_id:
            request[""end_user_id""] = end_user_id
        return request",Prepare request for create_graph endpoint,Constructs a graph creation request dictionary with optional parameters.
1149,fixture_env_vars,"def fixture_env_vars():
    

    test_vars = {
        ""TEST_VAR"": ""test_value"",
        ""LIST_VAR"": ""list_value"",
        ""NESTED_VAR"": ""nested_value"",
        ""BOOL_VAR"": ""true"",
        ""FLOAT_VAR"": ""0.0"",
        ""INT_VAR"": ""42"",
        ""FN_LIST_VAR"": ""[fn0, fn1, fn2]""
    }

    # Store original environment variables state
    original_env = {}

    # Set test environment variables and store original values
    for var, value in test_vars.items():
        if var in os.environ:
            original_env[var] = os.environ[var]
        os.environ[var] = value

    # Yield the test variables dctionary to the test
    yield test_vars

    # Clean up: restore original environment
    for var in test_vars:
        if var in original_env:
            os.environ[var] = original_env[var]
        else:
            del os.environ[var]",Fixture to set and clean up environment variables for tests.,Temporarily set and restore environment variables for testing purposes.
1150,configure_connection,"def configure_connection(self, input_list: List[str]) -> None:
        
        if len(input_list) < 2:
            logger.info(""\nPlease specify a connection to configure."")
            logger.info(""Format: configure-connection {connection}"")
            logger.info(""Use 'list-connections' to see available connections."")
            return

        self.agent.connection_manager.configure_connection(connection_name=input_list[1])",Handle configure connection command,Validate and initiate connection configuration based on user input list.
1151,format_reference_for_markdown,"def format_reference_for_markdown(reference_entry: Dict[str, Any]) -> str:
    
    website = reference_entry.get('website', '')
    title = reference_entry.get('title', '')
    url = reference_entry.get('url', '')
    
    # Ensure we have a website name
    if not website or website.strip() == """":
        website = extract_domain_name(url)
    
    # Ensure we have a title
    if not title or title.strip() == """" or title == url:
        # Try to extract a meaningful title from the URL
        title = extract_title_from_url_path(url)
        
        # If still no title, use default format
        if not title:
            title = f""Information from {website}""
    
    # Format: * Website. ""Title."" URL
    return f""* {website}. \""{title}.\"" {url}""",Format a reference entry for markdown output.,Format bibliographic references into Markdown with extracted website and title.
1152,bot_name,"def bot_name(self) -> str:
        
        mapping = {
            Platform.GOOGLE_MEET: ""google_meet"",
            Platform.ZOOM: ""zoom"",
            Platform.TEAMS: ""teams""
        }
        return mapping[self]",Returns the platform name used by the bot containers.,Map platform to corresponding bot name string.
1153,validate_config,"def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        
        required_fields = [""timeline_read_count"", ""cast_interval""]
        missing_fields = [field for field in required_fields if field not in config]
        
        if missing_fields:
            raise ValueError(f""Missing required configuration fields: {', '.join(missing_fields)}"")
            
        if not isinstance(config[""timeline_read_count""], int) or config[""timeline_read_count""] <= 0:
            raise ValueError(""timeline_read_count must be a positive integer"")

        if not isinstance(config[""cast_interval""], int) or config[""cast_interval""] <= 0:
            raise ValueError(""cast_interval must be a positive integer"")
            
        return config",Validate Farcaster configuration from JSON,Validate configuration dictionary for required fields and positive integer values
1154,format_plan_result,"def format_plan_result(plan_result: PlanResult) -> str:
    
    steps_str = (
        ""\n\n"".join(
            f""{i + 1}:\n{format_step_result(step)}""
            for i, step in enumerate(plan_result.step_results)
        )
        if plan_result.step_results
        else ""No steps executed yet""
    )

    return PLAN_RESULT_TEMPLATE.format(
        plan_objective=plan_result.objective,
        steps_str=steps_str,
        plan_status=""Complete"" if plan_result.is_complete else ""In Progress"",
        plan_result=plan_result.result if plan_result.is_complete else ""In Progress"",
    )",Format the full plan execution state for display to planners,Format a plan's execution results into a structured string summary.
1155,get_encryption_key,"def get_encryption_key():
        
        secret_key = os.environ.get(""SECRET_ENCRYPTION_KEY"")
        if not secret_key:
            raise HTTPException(
                status_code=500,
                detail=""SECRET_ENCRYPTION_KEY environment variable is not set"",
            )
        try:
            return Fernet(secret_key.encode(""utf-8""))
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f""Invalid SECRET_ENCRYPTION_KEY: {str(e)}"",
            )",Get Fernet encryption key for local storage.,Retrieve and validate encryption key from environment for secure operations.
1156,query_task,"def query_task(self, task_id: str) -> MeshTaskQueryResponse:
        
        payload = self._prepare_payload(task_id=task_id)

        response = self.client.post(f""{self.base_url}/mesh_task_query"", json=payload)
        response.raise_for_status()
        return MeshTaskQueryResponse(**response.json())",Query the status and result of an asynchronous task.,Fetch and return task details using a prepared payload and HTTP POST request.
1157,docker_exec,"def docker_exec(container_name: str, command: str):
        
        docker_command = f""docker exec {container_name} sh -c '{command}'""

        try:
            out = subprocess.run(
                docker_command,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                shell=True
            )

            if out.stderr or out.returncode != 0:
                error_message = out.stderr.decode(""utf-8"")
                print(f""[ERROR] Docker command execution failed: {error_message}"")
                return error_message
            else:
                output_message = out.stdout.decode(""utf-8"")
                print(output_message)
                return output_message

        except Exception as e:
            raise RuntimeError(f""Failed to execute command in Docker container: {container_name}\nError: {str(e)}"")",Execute a command inside a running Docker container.,Execute shell command in Docker container and handle errors.
1158,get_directory,"def get_directory(
    bucket_file_system_locator: BucketFileSystemLocator,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, Directory]:
    
    params: T_JSON_DICT = dict()
    params[""bucketFileSystemLocator""] = bucket_file_system_locator.to_json()
    cmd_dict: T_JSON_DICT = {
        ""method"": ""FileSystem.getDirectory"",
        ""params"": params,
    }
    json = yield cmd_dict
    return Directory.from_json(json[""directory""])",:param bucket_file_system_locator: :returns: Returns the directory object at the path.,Generate directory object from file system locator using JSON command.
1159,_validate,"def _validate(data: Any) -> pd.DataFrame:
        
        if isinstance(data, pd.DataFrame):
            return data.copy()
        raise ValueError(f""TabularDataset only supports pandas DataFrame input, got {type(data)}"")",Ensure that the input is a pandas DataFrame.,"Ensure input is a pandas DataFrame, else raise an error."
1160,_create_presentation,"def _create_presentation(self, template_path: Optional[str] = None) -> Presentation:
        
        if template_path:
            template_path = self._normalize_path(template_path)
            if not template_path.exists():
                logger.warning(f""Template not found: {template_path}. Using default template."")
                return Presentation()
            return Presentation(template_path)
        return Presentation()","Create a new presentation, optionally from a template.",Generate a presentation using a specified or default template path.
1161,apply_move,"def apply_move(self, move: str) -> bool:
        
        try:
            self.cube.rotate(move)
            self.actions.append(move)
            self.num_steps += 1

            # Record progress after move
            self.progress_history.append(self.cube.count_solved_cubies())

            return True
        except Exception as e:
            logger.error(f""Error applying move {move}: {e}"")
            return False",Apply a move to the cube and return success,"Attempt to execute a cube rotation, logging success or failure."
1162,update_exception_syntax,"def update_exception_syntax(file):
    
    try:
        print(f""🔍 Processing {file.filepath}"")
        for editable in file.find(""except ""):
            try:
                if editable.source.lstrip().startswith(""except"") and "", "" in editable.source and "" as "" not in editable.source:
                    print(f""🔄 Found Python 2 style exception: {editable.source.strip()}"")
                    parts = editable.source.split("","", 1)
                    new_source = f""{parts[0]} as{parts[1]}""
                    print(f""✨ Converting to: {new_source.strip()}"")
                    editable.edit(new_source)
            except Exception as e:
                print(f""⚠️ Error processing except clause: {e!s}"")
    except Exception as e:
        print(f""❌ Error processing file {file.filepath}: {e!s}"")",Update Python 2 exception handling to Python 3 syntax,Refactor Python 2 exception syntax to Python 3 in given file
1163,do_translate,"def do_translate(self, text, rate_limit_params: dict = None):
        
        logger.critical(
            f""Do not call BaseTranslator.do_translate. ""
            f""Translator: {self}. ""
            f""Text: {text}. "",
        )
        raise NotImplementedError","Actual translate text, override this method :param text: text to translate :return: translated text",Logs a critical error and raises an exception for unimplemented translation.
1164,_create_default_profile,"def _create_default_profile(self) -> Dict[str, Any]:
        
        return {
            ""student_id"": ""default_001"",
            ""target_grade"": ""11th grade"",
            ""learning_goal"": ""Master foundational mathematics"",
            ""current_avg_score"": 65,
            ""topics"": [
                {""name"": ""algebra"", ""proficiency"": 0.4},
                {""name"": ""geometry"", ""proficiency"": 0.6},
                {""name"": ""statistics"", ""proficiency"": 0.3},
                {""name"": ""calculus"", ""proficiency"": 0.2},
            ],
            ""preferred_learning_style"": ""visual"",
        }",Create a default student profile if none exists.,Generate a default student profile with academic and learning attributes.
1165,update_info_panels,"def update_info_panels(self) -> None:
		
		try:
			# Update actual content
			self.update_browser_panel()
			self.update_model_panel()
			self.update_tasks_panel()
		except Exception as e:
			logging.error(f'Error in update_info_panels: {str(e)}')
		finally:
			# Always schedule the next update - will update at 1-second intervals
			# This ensures continuous updates even if agent state changes
			self.set_timer(1.0, self.update_info_panels)",Update all information panels with current state.,Periodically refresh UI panels and handle exceptions to ensure continuous updates.
1166,_validate_timeframe,"def _validate_timeframe(self, timeframe: str) -> str:
        
        valid_timeframes = {""1m"", ""5m"", ""15m"", ""1h"", ""1d""}
        timeframe = timeframe.lower()
        if timeframe not in valid_timeframes:
            raise ValueError(f""Invalid timeframe. Must be one of: {', '.join(valid_timeframes)}"")
        return timeframe",Validate and normalize timeframe parameter,Ensure input timeframe is valid by checking against predefined options.
1167,get_description,"def get_description(self) -> str:
        
        descriptions = {
            HistogramMetricName.LLM_LATENCY_MS: ""Latency of the LLM in milliseconds"",
            HistogramMetricName.EMBEDDING_LATENCY_MS: ""Latency of the embedding in milliseconds"",
            HistogramMetricName.REQUEST_LATENCY_MS: ""Latency of the request in milliseconds"",
        }
        return descriptions[self]",Get the description for this metric.,Return latency description based on metric name enumeration
1168,_generate_word_pair,"def _generate_word_pair(self, rng: Random, length: int) -> tuple[str, str, list[str]]:
        
        word_set = self.word_sets[length]
        words_list = sorted(word_set)
        max_attempts = 100

        for _ in range(max_attempts):
            start = rng.choice(words_list)
            end = rng.choice(words_list)

            if start == end:
                continue

            path = self._find_path(start, end, word_set)
            if path:
                return start, end, path

        raise RuntimeError(f""Failed to find valid pair for length {length}"")",Simplified word pair generation,Generate a word pair and path from a word set using random selection and pathfinding
1169,get_dicts,"def get_dicts(self, keys=None, idx=None):
        
        if idx is None:
            if keys is None:
                return [self.info[i] for i in self.out_indices]
            else:
                return [{k: self.info[i][k] for k in keys} for i in self.out_indices]
        if isinstance(idx, (tuple, list)):
            return [self.info[i] if keys is None else {k: self.info[i][k] for k in keys} for i in idx]
        else:
            return self.info[idx] if keys is None else {k: self.info[idx][k] for k in keys}",return info dicts for specified keys (or all if None) at specified indices (or out_indices if None),Retrieve filtered data dictionaries based on optional keys and indices.
1170,_annotate_span_for_completion_response,"def _annotate_span_for_completion_response(
        self, span: trace.Span, response: ResponseMessage, turn: int
    ) -> None:
        
        if not self.context.tracing_enabled:
            return

        event_data = {
            ""completion.response.turn"": turn,
        }

        event_data.update(
            self.extract_response_message_attributes_for_tracing(response)
        )

        # Event name is based on the first choice for now
        event_name = f""completion.response.{turn}""
        if response.choices and len(response.choices) > 0:
            latest_message_role = response.choices[0].message.role
            event_name = f""gen_ai.{latest_message_role}.message""

        span.add_event(event_name, event_data)",Annotate the span with the completion response as an event.,Annotate tracing span with AI response event data if tracing is enabled.
1171,uneven_message_logs,"def uneven_message_logs() -> list[LLMMessageLogType]:
    
    return [
        [  # First sequence (shorter)
            {
                ""input_ids"": torch.tensor([1, 2]),
                ""role"": ""user"",
            }
        ],
        [  # Second sequence (longer)
            {
                ""input_ids"": torch.tensor([3, 4, 5]),
                ""role"": ""assistant"",
            }
        ],
    ]",Fixture for message logs of different lengths.,Generate sample message logs with varying sequence lengths
1172,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        result = {
            ""id"": self.id,
            ""key"": self.key,
        }

        if self.self_url:
            result[""self""] = self.self_url

        if self.fields:
            result[""fields""] = self.fields.to_simplified_dict()

        return result",Convert to simplified dictionary for API response.,Convert object attributes to a simplified dictionary format
1173,load_all_objects_data,"def load_all_objects_data():
    
    if not os.path.isfile(OBJECT_LIST_JSON):
        raise FileNotFoundError(f""Cannot find {OBJECT_LIST_JSON}"")
    with open(OBJECT_LIST_JSON, encoding=""utf-8"") as f:
        data = json.load(f)
    return data",Load the entire objects_init_list.json and return as a dict.,Load and return data from a JSON file if it exists.
1174,_extract_json_strategy_1,"def _extract_json_strategy_1(self, text: str) -> Dict[str, Any]:
        
        start_idx = text.find('{')
        if start_idx == -1:
            raise ValueError(""No opening brace found"")
        
        brace_count = 0
        end_idx = start_idx
        
        for i in range(start_idx, len(text)):
            if text[i] == '{':
                brace_count += 1
            elif text[i] == '}':
                brace_count -= 1
                if brace_count == 0:
                    end_idx = i + 1
                    break
        
        if brace_count != 0:
            raise ValueError(""Unbalanced braces"")
        
        json_str = text[start_idx:end_idx]
        return json.loads(json_str)",Strategy 1: Find first complete JSON object with balanced braces.,Extract and parse JSON from text by identifying balanced braces
1175,get_current_workspace,"def get_current_workspace():
    
    try:
        result = subprocess.run(
            [""hyprctl"", ""activeworkspace""],
            capture_output=True,
            text=True
        )
        # Assume the output similar to: ""ID <number>""
        # Extracting the number from the output
        parts = result.stdout.split()
        for i, part in enumerate(parts):
            if part == ""ID"" and i + 1 < len(parts):
                return int(parts[i+1])
    except Exception as e:
        print(f""Error getting current workspace: {e}"")
    return -1",Get the current workspace ID using hyprctl.,"Retrieve the active workspace ID using a command-line utility, handling errors gracefully."
1176,analyze_documents,"def analyze_documents():
    
    try:
        analyzed_doc_dtos = document_service.analyze_all_documents()
        return jsonify(
            APIResponse.success(
                data={
                    ""total"": len(analyzed_doc_dtos),
                    ""documents"": [doc.dict() for doc in analyzed_doc_dtos],
                }
            )
        )
    except Exception as e:
        logger.error(f""Error analyzing documents: {str(e)}"", exc_info=True)
        return jsonify(
            APIResponse.error(message=f""Error analyzing documents: {str(e)}"")
        )",Analyze all unanalyzed documents,Analyze documents and return results or error response as JSON.
1177,mock_jina_scrape_response,"def mock_jina_scrape_response(mocker):
    
    return {
        ""code"": 200,
        ""status"": 20000,
        ""data"": {
            ""title"": ""Test Page Title"",
            ""description"": ""Test page description"",
            ""content"": ""# Test Page Title\n\nThis is the scraped content from the test page."",
        },
    }",Mock response from Jina Reader API.,Simulate a successful web scrape response with mock data for testing purposes.
1178,_setup_ddp,"def _setup_ddp(self, world_size):
        
        torch.cuda.set_device(RANK)
        self.device = torch.device(""cuda"", RANK)
        # LOGGER.info(f'DDP info: RANK {RANK}, WORLD_SIZE {world_size}, DEVICE {self.device}')
        os.environ[""TORCH_NCCL_BLOCKING_WAIT""] = ""1""  # set to enforce timeout
        dist.init_process_group(
            backend=""nccl"" if dist.is_nccl_available() else ""gloo"",
            timeout=timedelta(seconds=10800),  # 3 hours
            rank=RANK,
            world_size=world_size,
        )",Initializes and sets the DistributedDataParallel parameters for training.,Initialize distributed data parallel processing with specified device and timeout settings.
1179,_review_draft,"def _review_draft(context: dict, **kwargs) -> dict:
    
    task = context.get(""task"")
    if not task.get(""follow_guidelines"", False):
        return {""review"": None, ""result"": """"}

    guidelines = task.get(""guidelines"")
    revision_notes = context.get(""revision_notes"")
    draft = context.get(""draft"")

    revise_prompt = f

    review_prompt = f

    system_prompt = 

    response = execute_agent(system_prompt, review_prompt)
    return {""review"": response, ""result"": """"}",Reviews the draft based on guidelines and determines if revisions are needed.,Function evaluates draft compliance with guidelines and returns review outcome.
1180,list_models,"def list_models(self, **kwargs) -> None:
        
        try:
            client = self._get_client()
            models = client.models.list().data
            
            logger.info(""\nGROK MODELS:"")
            for i, model in enumerate(models):
                logger.info(f""{i+1}. {model.id}"")
                
        except Exception as e:
            raise XAIAPIError(f""Listing models failed: {e}"")",List all available XAI models,"Log available models from a client, handling exceptions."
1181,is_configured,"def is_configured(self, verbose: bool = False) -> bool:
        
        try:
            load_dotenv()
            
            # Check private key exists
            private_key = os.getenv('ETH_PRIVATE_KEY')
            if not private_key:
                if verbose:
                    logger.error(""Missing ETH_PRIVATE_KEY in .env"")
                return False

            # Validate Web3 connection
            if not self._web3 or not self._web3.is_connected():
                if verbose:
                    logger.error(""Not connected to Ethereum network"")
                return False
                
            # Test account access
            account = self._web3.eth.account.from_key(private_key)
            balance = self._web3.eth.get_balance(account.address)
                
            return True

        except Exception as e:
            if verbose:
                logger.error(f""Configuration check failed: {str(e)}"")
            return False",Check if Ethereum connection is properly configured,Check Ethereum configuration by validating private key and network connection
1182,configure_logger,"def configure_logger(log_level: str) -> None:
    
    logger.add(
        sys.stderr,
        level=log_level.upper(),
        format=""<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{process}</cyan> | <magenta>{file}:{line}</magenta> | {message}"",
    )
    logger.debug(f""Log level set to: {log_level}"")",Configure the logger with the specified log level and format.,Configure a logger with specified log level and format for output.
1183,discard_changes,"def discard_changes(self) -> None:
        
        ts1 = perf_counter()
        self.git_cli.head.reset(index=True, working_tree=True)  # discard staged (aka index) + unstaged (aka working tree) changes in tracked files
        ts2 = perf_counter()
        self.git_cli.git.clean(""-fdxq"")  # removes untracked changes and ignored files
        ts3 = perf_counter()
        self.git_cli.git.gc(""--auto"")  # garbage collect
        ts4 = perf_counter()
        logger.info(f""discard_changes took {humanize_duration(ts2 - ts1)} to reset, {humanize_duration(ts3 - ts2)} to clean, {humanize_duration(ts4 - ts3)} to gc"")",Cleans repo dir by discarding any changes in staging/working directory and removes untracked files/dirs.,"Reset and clean Git repository state, logging operation durations."
1184,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        return {
            ""display_name"": self.display_name,
            ""name"": self.display_name,  # Add name for backward compatibility
            ""email"": self.email,
            ""avatar_url"": self.avatar_url,
        }",Convert to simplified dictionary for API response.,Convert object attributes to a simplified dictionary format.
1185,_get_headers,"def _get_headers(self) -> Dict[str, str]:
        
        headers = {""Content-Type"": ""application/json""}
        return headers",Get base headers for API requests,Return a dictionary with JSON content type header.
1186,normalize_project_name,"def normalize_project_name(project_name: str) -> str:
    

    needs_normalization = (
        any(char.isupper() for char in project_name) or ""_"" in project_name
    )

    if needs_normalization:
        normalized_name = project_name
        console.print(
            ""Note: Project names are normalized (lowercase, hyphens only) for better compatibility with cloud resources and tools."",
            style=""dim"",
        )
        if any(char.isupper() for char in normalized_name):
            normalized_name = normalized_name.lower()
            console.print(
                f""Info: Converting to lowercase for compatibility: '{project_name}' -> '{normalized_name}'"",
                style=""bold yellow"",
            )

        if ""_"" in normalized_name:
            # Capture the name state before this specific change
            name_before_hyphenation = normalized_name
            normalized_name = normalized_name.replace(""_"", ""-"")
            console.print(
                f""Info: Replacing underscores with hyphens for compatibility: '{name_before_hyphenation}' -> '{normalized_name}'"",
                style=""yellow"",
            )

        return normalized_name

    return project_name",Normalize project name for better compatibility with cloud resources and tools.,Normalize project names to lowercase with hyphens for cloud compatibility.
1187,get_free_memory_bytes,"def get_free_memory_bytes(device_idx: int) -> float:
    
    global_device_idx = device_id_to_physical_device_id(device_idx)
    with nvml_context():
        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(global_device_idx)
            return pynvml.nvmlDeviceGetMemoryInfo(handle).free
        except pynvml.NVMLError as e:
            raise RuntimeError(
                f""Failed to get free memory for device {device_idx} (global index: {global_device_idx}): {e}""
            )",Get the free memory of a CUDA device in bytes using NVML.,Retrieve available memory in bytes for a specified GPU device index.
1188,echo_context,"def echo_context(custom_request_id: str, ctx: Context[Any, Any, Request]) -> str:
        
        context_data = {
            ""custom_request_id"": custom_request_id,
            ""headers"": {},
            ""method"": None,
            ""path"": None,
        }
        if ctx.request_context.request:
            request = ctx.request_context.request
            context_data[""headers""] = dict(request.headers)
            context_data[""method""] = request.method
            context_data[""path""] = request.url.path
        return json.dumps(context_data)",Returns request context including headers and custom data.,Serialize request context details into a JSON string for logging or debugging.
1189,_get_headers,"def _get_headers(self, api_type: str = 'user') -> Dict[str, str]:
        
        headers = {""Content-Type"": ""application/json""}
        if api_type == 'admin':
            if not self._admin_key:
                raise VexaClientError(""Admin API key is required for this operation but was not provided."")
            headers[""X-Admin-API-Key""] = self._admin_key
        elif api_type == 'user':
            if not self._api_key:
                raise VexaClientError(""User API key is required for this operation but was not provided."")
            headers[""X-API-Key""] = self._api_key
        else:
             raise ValueError(""Invalid api_type specified. Use 'user' or 'admin'."")
        return headers",Prepares headers for the request based on API type.,Generate HTTP headers with API keys based on user or admin access type.
1190,check_flush,"def check_flush():
             
            self.flush_thread_stopped.clear()
            while True:
                if self.container_stop_event.is_set(): # self.shutdown_event.is_set() or 
                    time.sleep(4)
                    if self.container_stop_event.is_set():
                        break
                if self.multi_line_config is False:
                    break
                with self.lock_buffer:
                    if (time.time() - self.log_stream_last_updated > self.log_stream_timeout) and self.buffer:
                        self._handle_and_clear_buffer()
                time.sleep(1)
            self.flush_thread_stopped.set()
            self.logger.debug(f""Flush Thread stopped for Container {self.container_name}"")",When mode is multi-line new lines go into a buffer first to see whether the next line belongs to the same entry or not.,Monitors and manages log buffer flushing based on timeout and stop events.
1191,_identify_chart_patterns,"def _identify_chart_patterns(self, df: pd.DataFrame) -> List[PatternSignal]:
        
        patterns = []
        
        try:
            # Find local maxima and minima
            max_idx = argrelextrema(df['high'].values, np.greater, order=5)[0]
            min_idx = argrelextrema(df['low'].values, np.less, order=5)[0]
            
            # Double Top/Bottom
            patterns.extend(self._find_double_patterns(df, max_idx, min_idx))
            
            # Head and Shoulders
            patterns.extend(self._find_head_shoulders(df, max_idx, min_idx))
            
            # Triangle Patterns
            patterns.extend(self._find_triangle_patterns(df, max_idx, min_idx))
            
            # Flag Patterns
            patterns.extend(self._find_flag_patterns(df))
            
            # Channels
            patterns.extend(self._find_channels(df))
            
            return patterns
            
        except Exception as e:
            logger.error(f""Error identifying chart patterns: {e}"")
            raise",Identify chart patterns using advanced pattern recognition.,Detects and categorizes financial chart patterns from data for analysis.
1192,build_directory_summary,"def build_directory_summary(counts: Dict[str, int]) -> str:
    
    parts = []
    if counts[""new""]:
        parts.append(f""[green]+{counts['new']} new[/green]"")
    if counts[""modified""]:
        parts.append(f""[yellow]~{counts['modified']} modified[/yellow]"")
    if counts[""moved""]:
        parts.append(f""[blue]↔{counts['moved']} moved[/blue]"")
    if counts[""deleted""]:
        parts.append(f""[red]-{counts['deleted']} deleted[/red]"")
    return "" "".join(parts)",Build summary string for directory changes.,Generate a color-coded summary of directory changes based on file status counts.
1193,check_metadata_serializable,"def check_metadata_serializable(metadata: dict):
    
    try:
        json.dumps(metadata)
    except (TypeError, OverflowError) as e:
        print(f""[WARNING] Metadata is not JSON serializable, error: {str(e)}"")
        # Convert non-serializable values to strings
        metadata = {
            ""eval_0"": {
                k: (
                    str(v)
                    if not isinstance(
                        v, (dict, list, str, int, float, bool, type(None))
                    )
                    else v
                )
                for k, v in metadata[""eval_0""].items()
            }
        }
        print(
            f""[WARNING] Metadata now converted to string: {metadata} to be JSON serializable""
        )

    return metadata","Ensure metadata is JSON serializable, if not, convert non-serializable values to strings",Ensure metadata is JSON serializable by converting non-serializable values to strings.
1194,preview,"def preview(self) -> None:
        
        try:
            # Try notebook display first
            if self.display() is None:
                # Fall back to sounddevice if not in notebook
                self.play()
        except Exception as e:
            print(f""Error playing audio: {str(e)}"")",Smart play method that chooses appropriate playback method.,Attempt to preview audio using notebook display or fallback to sound device
1195,has_substantive_content,"def has_substantive_content(content):
    
    # Remove frontmatter
    content_without_frontmatter = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)
    
    # Remove whitespace and common HTML/markdown formatting
    cleaned_content = re.sub(r'\s+', '', content_without_frontmatter)
    cleaned_content = re.sub(r'{:.*?}', '', cleaned_content)
    
    # If there's almost nothing left after cleaning, consider it empty
    return len(cleaned_content) > 20",Check if the processed content has substantive content beyond the frontmatter,Determine if text contains meaningful content after cleaning metadata and formatting.
1196,_get_extension_from_file_type,"def _get_extension_from_file_type(file_type: InputFormat, content_type: str = """") -> str:
    
    # Create a reverse mapping from InputFormat to a default extension
    # We choose the first extension found for each format
    format_to_extension = {}
    for ext, fmt in ExtensionToFormat.items():
        if fmt not in format_to_extension:
            format_to_extension[fmt] = ext

    # Special case for images: use content type to determine exact image format
    if file_type == InputFormat.IMAGE:
        if ""jpeg"" in content_type or ""jpg"" in content_type:
            return "".jpeg""
        elif ""png"" in content_type:
            return "".png""
        elif ""tiff"" in content_type:
            return "".tiff""
        elif ""bmp"" in content_type:
            return "".bmp""
        # Fallback to default image extension
        ext = format_to_extension.get(InputFormat.IMAGE, ""png"")
        return f"".{ext}""

    # For all other formats, use the default extension
    if file_type in format_to_extension:
        return f"".{format_to_extension[file_type]}""

    return "".bin""",Get a file extension based on the file type and content type.,Determine file extension based on file type and content type mapping
1197,convert_actions,"def convert_actions(self, states: np.ndarray, actions: np.ndarray) -> np.ndarray:
        
        # in case of multi robot
        # reshape (N,14) to (N,2,7)
        # or (N,7) to (N,1,7)
        stacked_actions = actions.reshape(*actions.shape[:-1], -1, 7)

        env = self.env
        # generate abs actions
        action_goal_pos = np.zeros(stacked_actions.shape[:-1] + (3,), dtype=stacked_actions.dtype)
        action_goal_ori = np.zeros(stacked_actions.shape[:-1] + (3,), dtype=stacked_actions.dtype)
        action_gripper = stacked_actions[..., [-1]]
        for i in range(len(states)):
            _ = env.reset_to({""states"": states[i]})

            # taken from robot_env.py L#454
            for idx, robot in enumerate(env.env.robots):
                # run controller goal generator
                robot.control(stacked_actions[i, idx], policy_step=True)

                # read pos and ori from robots
                controller = robot.controller
                action_goal_pos[i, idx] = controller.goal_pos
                action_goal_ori[i, idx] = Rotation.from_matrix(controller.goal_ori).as_rotvec()

        stacked_abs_actions = np.concatenate([action_goal_pos, action_goal_ori, action_gripper], axis=-1)
        abs_actions = stacked_abs_actions.reshape(actions.shape)
        return abs_actions",Given state and delta action sequence generate equivalent goal position and orientation for each step keep the original gripper action intact.,Convert relative robot actions to absolute positions and orientations.
1198,assertBondsEqual,"def assertBondsEqual(self, bonds1, bonds2, atom_key1, atom_key2):  # pylint: disable=invalid-name
    
    # An empty bonds table is functionally equivalent to an empty bonds table.
    # NB: this can only ever be None in structure v1.
    if bonds1 is None or not bonds1.size or bonds2 is None or not bonds2.size:
      self.assertTrue(bonds1 is None or not bonds1.size, msg=f'{bonds1=}')
      self.assertTrue(bonds2 is None or not bonds2.size, msg=f'{bonds2=}')
      return

    ptnr1_indices1, ptnr2_indices1 = bonds1.get_atom_indices(atom_key1)
    ptnr1_indices2, ptnr2_indices2 = bonds2.get_atom_indices(atom_key2)
    np.testing.assert_array_equal(ptnr1_indices1, ptnr1_indices2)
    np.testing.assert_array_equal(ptnr2_indices1, ptnr2_indices2)
    np.testing.assert_array_equal(bonds1.type, bonds2.type)
    np.testing.assert_array_equal(bonds1.role, bonds2.role)",Checks whether two Bonds objects are considered equal.,Verify equivalence of two bond structures by comparing atom indices and attributes.
1199,exec,"def exec(self, inputs):
        
        question, context = inputs
        
        print(f""✍️ Crafting final answer..."")
        
        # Create a prompt for the LLM to answer the question
        prompt = f
        # Call the LLM to generate an answer
        answer = call_llm(prompt)
        return answer",Call the LLM to generate a final answer.,Generate an answer using a language model based on input question and context.
1200,_load_and_cache_negative_prompt,"def _load_and_cache_negative_prompt(self) -> None:
        
        if not self.validation or self.neg_metadata is None:
            return

        if self.cached_neg_prompt is not None:
            return

        # Only rank 0 in each SP group should read the negative prompt
        try:
            file_path, row_idx = self.neg_metadata
            parquet_file = pq.ParquetFile(file_path)

            # Since negative prompt is always the first row (row_idx = 0),
            # it's always in the first row group
            row_group_index = 0
            local_index = row_idx  # This will be 0 for the negative prompt

            row_group = parquet_file.read_row_group(row_group_index).to_pydict()
            row_dict = {k: v[local_index] for k, v in row_group.items()}
            del row_group

            # Process the negative prompt row
            self.cached_neg_prompt = self._process_row(row_dict)

        except Exception as e:
            logger.error(""Failed to load negative prompt: %s"", e)
            self.cached_neg_prompt = None",Load and cache the negative prompt.,Load and cache a negative prompt from a Parquet file if validation is enabled.
1201,calculate_execution_score,"def calculate_execution_score(pred_sql, gold_sql, db_path):
    
    try:
        pred_results = execute_sql(pred_sql, db_path)
        gold_results = execute_sql(gold_sql, db_path)
        
        # Compare results as sets to ignore order
        execution_score = 1.0 if set(pred_results) == set(gold_results) else 0.0
        
    except Exception as e:
        print(f""[Error] Error in executing SQL: {e}"")
        execution_score = 0.0

    return execution_score",Calculate score based on SQL execution results.,Evaluate SQL query accuracy by comparing execution results.
1202,_get_prompt_message,"def _get_prompt_message(self) -> HTML:
        
        agent_status = f""({self.agent.name})"" if self.agent else ""(no agent)""
        return HTML(f'<prompt>ZerePy-CLI</prompt> {agent_status} > ')",Generate the prompt message based on current state,Generate a command-line prompt message with agent status for ZerePy-CLI.
1203,load,"def load(self, model_path: str, architecture: str,
             fastvideo_args: FastVideoArgs):
        
        logger.warning(""Using generic loader for %s with library %s"",
                       model_path, self.library)

        if self.library == ""transformers"":
            from transformers import AutoModel

            model = AutoModel.from_pretrained(
                model_path,
                trust_remote_code=fastvideo_args.trust_remote_code,
                revision=fastvideo_args.revision,
            )
            logger.info(""Loaded generic transformers model: %s"",
                        model.__class__.__name__)
            return model
        elif self.library == ""diffusers"":
            logger.warning(
                ""Generic loading for diffusers components is not fully implemented""
            )

            model_config = get_diffusers_config(model=model_path)
            logger.info(""Diffusers Model config: %s"", model_config)
            # This is a placeholder - in a real implementation, you'd need to handle this properly
            return None
        else:
            raise ValueError(f""Unsupported library: {self.library}"")","Load a generic component based on the model path, architecture, and inference args.",Load a model using specified library and configuration parameters.
1204,add_observer,"def add_observer(self, observer: Callable, event_types: List[str]) -> ""CodeActAgent"":
        
        try:
            self._observers.append((observer, event_types))
            return self
        except Exception as e:
            logger.error(f""Failed to add observer: {e}"")
            raise",Add an observer for specific event types.,Add an observer with specified event types to the agent's list.
1205,_parse_style_config,"def _parse_style_config(self, style_config: Optional[str]) -> Dict:
        
        try:
            if not style_config:
                return self.DEFAULT_STYLES.copy()
            
            custom_styles = json.loads(style_config)
            styles = self.DEFAULT_STYLES.copy()
            styles.update(custom_styles)
            return styles
        except json.JSONDecodeError as e:
            logger.error(f""Invalid style configuration JSON: {e}"")
            return self.DEFAULT_STYLES.copy()",Parse and validate style configuration.,Parse and merge style configurations with error handling for JSON decoding
1206,register_module_tools,"def register_module_tools(module_name: str) -> None:
    
    functions = load_module_functions(module_name)
    for func in functions:
        # Skip functions that don't have docstrings
        if not func.__doc__:
            print(f""Skipping function without docstring: {func.__name__}"")
            continue
            
        # Create async wrapper with improved argument handling
        async_func = make_async_wrapper(func)
        
        # Register as MCP tool
        try:
            mcp.tool()(async_func)
            print(f""Registered tool: {func.__name__}"")
        except Exception as e:
            print(f""Failed to register {func.__name__}: {e}"")",Register all suitable functions from a module as MCP tools with improved argument handling.,"Registers module functions as tools with async wrappers, skipping those lacking documentation."
1207,_initialize_client,"def _initialize_client(self):
        
        try:
            import openai

            if self.use_test_client:
                # Import TestClient and app for direct interaction
                try:
                    from fastapi.testclient import TestClient
                    from mlx_omni_server.main import app

                    self.logger.info(
                        ""Using TestClient for direct interaction with MLX Omni Server""
                    )
                    self.client = openai.OpenAI(http_client=TestClient(app))
                except ImportError as e:
                    self.logger.error(
                        f""Failed to import TestClient or MLX Omni Server: {e}""
                    )
                    self.logger.warning(""Falling back to HTTP client"")
                    self._initialize_http_client()
            else:
                self._initialize_http_client()

        except ImportError as e:
            self.logger.error(f""Failed to import OpenAI: {e}"")
            raise ImportError(
                ""OpenAI package is required for MLXOmniClient. Install with 'pip install openai'""
            )",Initialize the appropriate client based on configuration.,Initialize OpenAI client with fallback to HTTP if TestClient unavailable
1208,_get_changed_items,"def _get_changed_items(self) -> set[Symbol]:
        
        if not self.commit_hash:
            return set()

        # Get diffs between base commit and current state
        diffs = self.codebase.get_diffs(self.commit_hash)
        changed_symbols = set()

        # Get all symbols from changed files
        for diff in diffs:
            for path in [diff.a_path, diff.b_path]:
                if not path:
                    continue
                file = self.codebase.get_file(path)
                if file:
                    changed_symbols.update(s for s in file.symbols if s.source)

        logger.info(f""Found {len(changed_symbols)} changed symbols"")
        return changed_symbols",Get set of symbols that have changed since last index.,Identify and return modified code symbols based on commit differences.
1209,on_train_end,"def on_train_end(trainer):
    
    session = getattr(trainer, ""hub_session"", None)
    if session:
        # Upload final model and metrics with exponential standoff
        LOGGER.info(f""{PREFIX}Syncing final model..."")
        session.upload_model(
            trainer.epoch,
            trainer.best,
            map=trainer.metrics.get(""metrics/mAP50-95(B)"", 0),
            final=True,
        )
        session.alive = False  # stop heartbeats
        LOGGER.info(f""{PREFIX}Done ✅\n"" f""{PREFIX}View model at {session.model_url} 🚀"")",Upload final model and metrics to Ultralytics HUB at the end of training.,Finalize and upload the trained model and metrics to the server at the end of training.
1210,create_job,"def create_job(self, job_id: str, inputs: Dict[str, Any]) -> None:
        
        self.jobs.insert_one({
            ""job_id"": job_id,
            ""inputs"": inputs,
            ""status"": ""pending"",
            ""created_at"": datetime.utcnow(),
            ""updated_at"": datetime.utcnow()
        })",Create a new research job record.,Insert a new job entry with pending status into the database.
1211,construct,"def construct(
        self,
        trajectory: list,
        intent: str,
        meta_data: dict[str, Any] = {},
    ):
        
        intro = self.instruction[""intro""]
        examples = self.instruction[""examples""]
        template = self.instruction[""template""]
        keywords = self.instruction[""meta_data""][""keywords""]
        state_info = trajectory[-1]  # type: ignore[assignment]

        obs = state_info[""observation""][self.obs_modality]

        page = state_info[""info""][""page""]
        url = page.url
        previous_action_str = meta_data[""action_history""][-1]

        # input x
        current = template.format(
            objective=intent,
            url=self.map_url_to_real(url),
            observation=obs,
            previous_action=previous_action_str,
        )

        # make sure all keywords are replaced
        assert all([f""{{k}}"" not in current for k in keywords])

        return current",Construct prompt given the trajectory,Generate a formatted string using trajectory data and intent with template placeholders
1212,_extract_complex_type_schema,"def _extract_complex_type_schema(self, subscript: ast.Subscript) -> dict[str, Any]:
        
        if isinstance(subscript.value, ast.Name):
            base_type = subscript.value.id

            if base_type == ""list"":
                # Handle list[ItemType]
                if isinstance(subscript.slice, ast.Name):
                    item_type = self._type_hint_to_json_type(subscript.slice.id)
                    return {""type"": ""array"", ""items"": {""type"": item_type}}
                elif isinstance(subscript.slice, ast.Subscript):
                    # Nested subscript like list[dict[str, Any]]
                    item_schema = self._extract_complex_type_schema(subscript.slice)
                    return {""type"": ""array"", ""items"": item_schema}
                else:
                    # Complex item type, try to parse it
                    item_type_str = ast.unparse(subscript.slice)
                    if ""dict"" in item_type_str.lower():
                        return {""type"": ""array"", ""items"": {""type"": ""object""}}
                    else:
                        item_type = self._type_hint_to_json_type(item_type_str)
                        return {""type"": ""array"", ""items"": {""type"": item_type}}

            elif base_type == ""dict"":
                return {""type"": ""object""}

            elif base_type in [""Optional"", ""Union""]:
                # Handle Optional[Type] or Union[Type, None]
                return self._handle_optional_type(subscript)

        # Fallback
        type_str = ast.unparse(subscript)
        return {""type"": self._type_hint_to_json_type(type_str)}","Extract schema from complex types like list[str], dict[str, Any], etc.",Parse and convert Python type hints into JSON schema representations.
1213,delete,"def delete(self, request: HttpRequest, identifier: str):
        

        redirect_target = f'{self.obj_name}_overview'

        if request.htmx:
            obj = self.get_object(request, identifier)
            obj.delete()

            # try to redirect to current page
            if 'details' not in request.META.get('HTTP_REFERER', ''):
                return HttpResponseClientRefresh()
            # if deleted from the details page the details page will no longer exist
            else:
                return HttpResponseClientRedirect(reverse(redirect_target))

        return redirect(redirect_target)",Delete the specified object.,Handle object deletion and redirect based on request context and referrer.
1214,_convert_code_blocks,"def _convert_code_blocks(self, text: str) -> str:
        
        def replace_code(match):
            code = match.group(2)
            lang = match.group(1) if match.group(1) else ''
            return (
                f""\\begin{{lstlisting}}[language={lang}, ""
                ""basicstyle=\\ttfamily\\small, ""
                ""breaklines=true, ""
                ""commentstyle=\\color{gray}, ""
                ""keywordstyle=\\color{blue}, ""
                ""stringstyle=\\color{green}, ""
                ""numbers=left, ""
                ""frame=single]\n""
                f""{code}\n""
                ""\\end{lstlisting}\n""
            )
        
        return re.sub(
            r'```(\w+)?\n(.*?)\n```',
            replace_code,
            text,
            flags=re.DOTALL
        )",Convert markdown code blocks to LaTeX listings.,Transform markdown code blocks into LaTeX formatted listings with syntax highlighting.
1215,_format_code_preview,"def _format_code_preview(self, code: Dict[str, Any]) -> Dict[str, Any]:
        
        repo = code.get(""repository"", {})
        return {
            ""id"": f""code_{code.get('sha', '')}"",
            ""title"": f""{code.get('name', '')} in {repo.get('full_name', '')}"",
            ""link"": code.get(""html_url"", """"),
            ""snippet"": f""Match in {code.get('path', '')}"",
            ""path"": code.get(""path"", """"),
            ""repo_name"": repo.get(""full_name"", """"),
            ""repo_url"": repo.get(""html_url"", """"),
            ""search_type"": ""code"",
            ""file_url"": code.get(""url"", """"),
        }",Format code search result as preview,Formats code metadata into a structured dictionary for display purposes.
1216,retrieve_essence_markers,"def retrieve_essence_markers(self, days: int = 30) -> List[tuple[str, str]]:
        
        try:
            with self.get_connection() as conn:
                cur = conn.cursor()
                cur.execute(
                    ,
                    (f""-{days} days"",),
                )
                return cur.fetchall()
        except sqlite3.Error as e:
            self.logger.error(f""Database error retrieving essence markers: {e}"")
            return []",Retrieve recent essence markers,Fetch recent essence markers from database with error handling.
1217,build_llm_via_langchain,"def build_llm_via_langchain(provider: str, model: str):
    
    if provider == ""openai"":
        if ""OPENAI_API_KEY"" not in os.environ:
            raise ValueError(""Please set the OPENAI_API_KEY environment variable."")
        return ChatOpenAI(model=model or ""gpt-4"")
    elif provider == ""anthropic"":
        if ""ANTHROPIC_API_KEY"" not in os.environ:
            raise ValueError(""Please set the ANTHROPIC_API_KEY environment variable."")
        return ChatAnthropic(model=model or ""claude-3-opus-20240229"")
    elif provider == ""ollama"":
        return ChatOllama(model=model or ""llama3.1"")
    else:
        raise ValueError(f""Unrecognized LLM provider {provider}. Contributons are welcome!"")",Builds a language model via LangChain.,Configure language model based on provider and validate API key presence.
1218,_count_running_bots_for_user,"def _count_running_bots_for_user(self, user_id: str) -> int:
        
        try:
            containers = self.client.containers.list(
                filters={""label"": f""vexa.user_id={user_id}"", ""status"": ""running""}
            )
            count = len(containers)
            logger.debug(f""Found {count} running bot containers for user {user_id}"")
            return count
        except Exception as e:
            logger.error(f""Error counting running containers for user {user_id}: {e}"")
            # Decide on behavior: raise the error or return 0/error indicator?
            # Returning 0 might be risky if it allows exceeding the limit due to a Docker error.
            # Let's re-raise for now, forcing the request to fail if Docker is inaccessible.
            raise",Counts the number of running bot containers for a specific user using labels.,"Count active bot instances for a specific user, handling errors robustly."
1219,run_events,"def run_events(self, events: list[Event]):
        
        res = async_batch_invoke(self.arun_event, events, num_workers=self.config['num_workers'],
                                 callbacks=self.callbacks, timeout=self.config['timeout'])
        final_result = [{'res': r['result'], 'event_id': events[r['index']].id} for r in res if r['error'] is None]
        cost = sum([r['usage'] for r in res if r['error'] is None])
        return final_result, cost",Run the dialog between the user and the chatbot on the events.,Asynchronously process events with error handling and calculate total cost.
1220,load_models,"def load_models(self) -> Dict[str, nn.Module]:
        
        return {
            'sparse_structure_flow_model': self.get_checkpoint_path(""ss_flow_img_dit_L_16l8_fp16""),
            'slat_flow_model': self.get_checkpoint_path(""slat_flow_img_dit_L_64l8p2_fp16"")
        }",Load all required models with current configuration,Retrieve and return model paths for sparse and slat flow models.
1221,colormap,"def colormap(color):
    
    cmap = {""black"": BLACK}
    return cmap.get(color, color)",Convenience for looking up known colors,Return a mapped color or default to input if not found.
1222,clear,"def clear(self) -> None:
        
        try:
            query = self.CLEAR_TABLE_QUERY.format(index_name=self.index_name)
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(query)
                conn.commit()
        except sqlite3.Error as e:
            raise SQLiteError(f""Error clearing database: {e}"") from e",Clears the SQLite database by deleting all rows in the table.,Clear database table using formatted SQL query with error handling.
1223,get_form_inputs,"def get_form_inputs(self) -> List[str]:
        
        try:
            input_elements = self.find_all_inputs()
            if not input_elements:
                self.logger.info(""No input element on page."")
                return [""No input forms found on the page.""]

            form_strings = []
            for element in input_elements:
                input_type = element.get(""type"") or ""text""
                if input_type in [""hidden"", ""submit"", ""button"", ""image""] or not element[""displayed""]:
                    continue
                input_name = element.get(""text"") or element.get(""id"") or input_type
                if input_type == ""checkbox"" or input_type == ""radio"":
                    try:
                        checked_status = ""checked"" if element.is_selected() else ""unchecked""
                    except Exception as e:
                        continue
                    form_strings.append(f""[{input_name}]({checked_status})"")
                else:
                    form_strings.append(f""[{input_name}]("""")"")
            return form_strings

        except Exception as e:
            raise e",Extract all input from the page and return them.,Extracts and formats visible form input details from a webpage for logging or display.
1224,on_plot,"def on_plot(self, name, data=None):
        
        path = Path(name)
        self.plots[path] = {""data"": data, ""timestamp"": time.time()}",Registers plots (e.g. to be consumed in callbacks).,Store plot data with a timestamp in a dictionary using a path key.
1225,start_service,"def start_service(self):
        
        with self.load_lock:
            if self.model_type == ""hf"":
                if self.pipe is None:
                    print(f""Loading Hugging Face model '{self.model_name}' on device '{self.device}'..."")
                    self.pipe = pipeline(
                        ""text-generation"",
                        model=self.model_name,
                        torch_dtype=""auto"",
                        device_map=self.device
                    )
                    print(""Hugging Face model loaded successfully."")
            elif self.model_type == ""vllm"":
                if self.llm is None:
                    print(f""Loading vLLM model '{self.model_name}' on device '{self.device}'..."")
                    self.llm = LLM(self.model_name, tensor_parallel_size=1)
                    print(""vLLM model loaded successfully."")
            else:
                raise ValueError(""Unsupported model_type. Choose 'hf' for Hugging Face or 'vllm' for vLLM."")",Start the LLM service by loading the model into the chosen pipeline if it's not already loaded.,Initialize and load a specified machine learning model based on its type and device configuration.
1226,_redact_request_headers,"def _redact_request_headers(headers: dict[str, str]) -> dict[str, str]:
  
  redacted_headers = {}
  for header_name, header_value in headers.items():
    if header_name.lower() == 'x-goog-api-key':
      redacted_headers[header_name] = '{REDACTED}'
    elif header_name.lower() == 'user-agent':
      redacted_headers[header_name] = _redact_language_label(
          _redact_version_numbers(header_value)
      )
    elif header_name.lower() == 'x-goog-api-client':
      redacted_headers[header_name] = _redact_language_label(
          _redact_version_numbers(header_value)
      )
    elif header_name.lower() == 'x-goog-user-project':
      continue
    elif header_name.lower() == 'authorization':
      continue
    else:
      redacted_headers[header_name] = header_value
  return redacted_headers",Redacts headers that should not be recorded.,Redacts sensitive information from HTTP request headers for privacy protection.
1227,logger,"def logger(self) -> logging.Logger:
		
		if self._logger is None:
			# Create a child logger with the session ID
			self._logger = logging.getLogger(f'browser_use.BrowserSession[{self.id[-4:]}]')
		return self._logger",Get instance-specific logger with session ID in the name,Initialize or retrieve a session-specific logger instance.
1228,load_episode_log,"def load_episode_log(episode_log_path: str) -> Dict[str, Any]:
    
    try:
        if episode_log_path.endswith('.bk2'):
            # For .bk2 files, just check if it exists and is readable
            if not os.path.exists(episode_log_path):
                raise FileNotFoundError(f""Recording file not found: {episode_log_path}"")
            # Try to open the file to verify it's readable
            with open(episode_log_path, 'rb') as f:
                # Just read a small chunk to verify file is readable
                f.read(1024)
            return {""valid"": True}
        else:
            # For JSON/JSONL files, validate format
            with open(episode_log_path, 'r') as f:
                # Try to read first line to validate format
                first_line = f.readline().strip()
                if first_line:
                    json.loads(first_line)  # Validate JSON format
            return {""valid"": True}
    except Exception as e:
        raise ValueError(f""Error loading episode log: {e}"")",Load episode log from JSON/JSONL file or validate .bk2 file,"Validate and load episode log files, ensuring readability and correct format."
1229,_validate_command,"def _validate_command(self, command: str) -> None:
        
        forbidden_commands = [""rm -rf /"", ""mkfs"", ""dd"", "":(){ :|:& };:""]
        for cmd in forbidden_commands:
            if cmd in command.lower():
                raise ValueError(f""Command '{command}' contains forbidden operation"")",Validate the command for potential security risks.,Ensure command safety by checking against a list of dangerous operations
1230,attach_to_model,"def attach_to_model(self, model: Model | None = None):
        
        if model is None:
            model = self.model
        model_num_layers = len(model.get_cache_layers())
        assert model_num_layers == self.num_layers, \
            f""Cannot attach cache with {self.num_layers} layers to model with {model_num_layers} layers.""
        for layer, module in zip(self.layers, model.get_cache_layers()):
            assert layer not in module.cache_layers, \
                ""Cannot attach cache twice to the same model.""
            module.cache_layers.append(layer)",Attach cache to model.,"Attach cache layers to a model, ensuring layer count consistency and preventing duplicate attachments."
1231,_get_gpu_info,"def _get_gpu_info():
    
    try:
        result = subprocess.run(
            [""nvidia-smi"", ""--query-gpu=gpu_name,memory.total"", ""--format=csv,noheader,nounits""],
            capture_output=True,
            text=True,
            check=True,
        )
        gpu_lines = result.stdout.strip().split(""\n"")
        gpu_count = len(gpu_lines)
        gpu_info = []
        for line in gpu_lines:
            gpu_name, gpu_memory = line.split("", "")
            gpu_info.append(
                {
                    ""type"": gpu_name,
                    ""memory"": float(gpu_memory) / 1024,  # Convert to GB
                }
            )
        return gpu_count, gpu_info
    except subprocess.CalledProcessError:
        print(""Failed to execute nvidia-smi command."")
        return 0, []","Get GPU type, GPU memory, and GPU count using nvidia-smi command.","Retrieve and parse GPU details using nvidia-smi, returning count and specifications."
1232,remove_redundant_questions,"def remove_redundant_questions(dataset, model, similarity_threshold=0.85):
    
    questions = dataset[""question""]
    embeddings, labels = compute_clusters(questions, model, similarity_threshold)

    questions_to_keep = set()

    for i, label in enumerate(labels):
        if label == -1:
            questions_to_keep.add(i)

    for label in set(labels):
        if label == -1:
            continue

        cluster_indices = [i for i, lab in enumerate(labels) if lab == label]

        cluster_embeddings = embeddings[cluster_indices]
        cluster_center = np.mean(cluster_embeddings, axis=0)

        distances_to_center = np.linalg.norm(cluster_embeddings - cluster_center, axis=1)

        closest_question_idx = cluster_indices[np.argmin(distances_to_center)]
        questions_to_keep.add(closest_question_idx)

    # Filter the dataset
    filtered_indices = sorted(questions_to_keep)

    print(f""Original dataset size: {len(dataset)}"")
    print(f""Filtered dataset size: {len(filtered_indices)}"")
    print(f""Removed {len(dataset) - len(filtered_indices)} redundant questions"")

    return dataset.select(filtered_indices)",Removes redundant questions from a Hugging Face dataset with QA pairs.,Filter dataset by removing redundant questions using clustering and similarity threshold.
1233,check_for_updates,"def check_for_updates(update_requested: bool = False):
    
    if not update_requested and not should_update():
        return

    try:
        latest_version: Version = get_latest_version(AGENTSTACK_PACKAGE)
    except Exception as e:
        raise Exception(f""Failed to retrieve package index: {e}"")

    installed_version: Version = parse_version(get_version(AGENTSTACK_PACKAGE))
    if latest_version > installed_version:
        log.info('')  # newline
        if inquirer.confirm(
            f""New version of {AGENTSTACK_PACKAGE} available: {latest_version}! Do you want to install?""
        ):
            try:
                # handle update inside a user project
                conf.assert_project()
                packaging.upgrade(f'{AGENTSTACK_PACKAGE}[{get_framework()}]')
            except conf.NoProjectError:
                # handle update for system version of agentstack
                packaging.set_python_executable(sys.executable)
                packaging.upgrade(AGENTSTACK_PACKAGE, use_venv=False)
            
            log.success(f""{AGENTSTACK_PACKAGE} updated. Re-run your command to use the latest version."")
        else:
            log.info(""Skipping update. Run `agentstack update` to install the latest version."")

    record_update_check()",`update_requested` indicates the user has explicitly requested an update.,"Check and prompt for software updates, handling installation if a newer version is available."
1234,inject_scale_pods_to_zero,"def inject_scale_pods_to_zero(self, microservices: list[str]):
        
        for service in microservices:
            self.kubectl.exec_command(
                f""kubectl scale deployment {service} --replicas=0 -n {self.namespace}""
            )
            print(
                f""Scaled deployment {service} to 0 replicas | namespace: {self.namespace}""
            )",Inject a fault to scale pods to zero for a service.,Scale specified microservices to zero replicas in Kubernetes namespace.
1235,is_parent_child,"def is_parent_child(key1_str: str, key2_str: str, global_map: Dict[str, KeyInfo]) -> bool:
    
    info1 = next((info for info in global_map.values() if info.key_string == key1_str), None)
    info2 = next((info for info in global_map.values() if info.key_string == key2_str), None)


    if not info1 or not info2:
        logger.debug(f""is_parent_child: Could not find KeyInfo for '{key1_str if not info1 else ''}' or '{key2_str if not info2 else ''}'. Returning False."")
        return False # Cannot determine relationship if info is missing

    # Ensure paths are normalized (they should be from KeyInfo, but double-check)
    path1 = normalize_path(info1.norm_path)
    path2 = normalize_path(info2.norm_path)
    parent1 = normalize_path(info1.parent_path) if info1.parent_path else None
    parent2 = normalize_path(info2.parent_path) if info2.parent_path else None

    # Check both directions: info1 is parent of info2 OR info2 is parent of info1
    is_parent1 = parent2 == path1
    is_parent2 = parent1 == path2

    logger.debug(f""is_parent_child check: {key1_str}({path1}) vs {key2_str}({path2}). Is Parent1: {is_parent1}, Is Parent2: {is_parent2}"")
    return is_parent1 or is_parent2",Checks if two keys represent a direct parent-child directory relationship.,Determine if two keys have a parent-child relationship using normalized paths.
1236,run_command,"def run_command(
    cmd: list[str] | str,
    check: bool = True,
    cwd: Path | None = None,
    capture_output: bool = False,
    shell: bool = False,
    input: str | None = None,
) -> subprocess.CompletedProcess:
    
    # Format command for display
    cmd_str = cmd if isinstance(cmd, str) else "" "".join(cmd)
    print(f""\n🔄 Running command: {cmd_str}"")
    if cwd:
        print(f""📂 In directory: {cwd}"")

    # Run the command
    result = subprocess.run(
        cmd,
        check=check,
        cwd=cwd,
        capture_output=capture_output,
        text=True,
        shell=shell,
        input=input,
    )

    # Display output if captured
    if capture_output and result.stdout:
        print(f""📤 Output:\n{result.stdout.strip()}"")
    if capture_output and result.stderr:
        print(f""⚠️ Error output:\n{result.stderr}"")

    return result",Run a command and display it to the user,Execute and manage system commands with optional output capture and error handling.
1237,_format_tool_list,"def _format_tool_list(self, available_tools, selected_tool_name):
        
        display_tool_list = Text()
        for display_tool in available_tools:
            # Handle both OpenAI and Anthropic tool formats
            if isinstance(display_tool, dict):
                if ""function"" in display_tool:
                    # OpenAI format
                    tool_call_name = display_tool[""function""][""name""]
                else:
                    # Anthropic format
                    tool_call_name = display_tool[""name""]
            else:
                # Handle potential object format (e.g., Pydantic models)
                tool_call_name = (
                    display_tool.function.name
                    if hasattr(display_tool, ""function"")
                    else display_tool.name
                )

            parts = (
                tool_call_name.split(SEP)
                if SEP in tool_call_name
                else [tool_call_name, tool_call_name]
            )

            if selected_tool_name.split(SEP)[0] == parts[0]:
                style = ""magenta"" if tool_call_name == selected_tool_name else ""dim white""
                shortened_name = parts[1] if len(parts[1]) <= 12 else parts[1][:11] + ""…""
                display_tool_list.append(f""[{shortened_name}] "", style)

        return display_tool_list","Format the list of available tools, highlighting the selected one.",Formats and styles a list of tools based on selection and naming conventions.
1238,release_animations,"def release_animations(
    animations: typing.List[str],
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""animations""] = [i for i in animations]
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Animation.releaseAnimations"",
        ""params"": params,
    }
    json = yield cmd_dict",Releases a set of animations to no longer be manipulated.,Generate command to release specified animations in JSON format.
1239,on_train_end,"def on_train_end(trainer):
    
    if session := getattr(trainer, ""hub_session"", None):
        # Upload final model and metrics with exponential standoff
        LOGGER.info(f""{PREFIX}Syncing final model..."")
        session.upload_model(
            trainer.epoch,
            trainer.best,
            map=trainer.metrics.get(""metrics/mAP50-95(B)"", 0),
            final=True,
        )
        session.alive = False  # stop heartbeats
        LOGGER.info(f""{PREFIX}Done ✅\n{PREFIX}View model at {session.model_url} 🚀"")",Upload final model and metrics to Ultralytics HUB at the end of training.,Upload final model and metrics to hub session at training completion.
1240,_setup_schema,"def _setup_schema(self) -> None:
        
        composio_toolset = ComposioToolSet(api_key=self.api_key)
        action_schema = composio_toolset.get_action_schemas(actions=[self.action])[0]
        logger.info(f""Tool {self.name} - Setting up input schema for action: {action_schema}"")
        self._input_schema = generate_pydantic_model(
            action_schema.parameters.properties, f""ComposioInput_{self.action.name}""
        )
        self.name = f""Composio Tool - {action_schema.display_name}""",Set up the input schema based on the action.,Initialize input schema for a tool using Composio API and Pydantic model generation.
1241,check_if_file_exists_else_download,"def check_if_file_exists_else_download(path, fname2link=FNAME2LINK, chunk_size=1024):
    
    path = Path(path)
    if not path.exists():
        path.parent.mkdir(exist_ok=True, parents=True)
        link = fname2link.get(path.name, None)
        if link is None:
            raise ValueError(f'Cant find the checkpoint file: {path}.',
                             f'Please download it manually and ensure the path exists.')
        with requests.get(fname2link[path.name], stream=True) as r:
            total_size = int(r.headers.get('content-length', 0))
            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:
                with open(path, 'wb') as f:
                    for data in r.iter_content(chunk_size=chunk_size):
                        if data:
                            f.write(data)
                            pbar.update(chunk_size)","Checks if file exists, if not downloads it from the link to the path","Check if a file exists, otherwise download it from a specified link."
1242,copy_only_git_tracked_and_untracked_files,"def copy_only_git_tracked_and_untracked_files(src_dir: Path, dst_dir: Path, ignore_dir: Optional[str] = None) -> None:
    
    tracked_and_new_files = get_git_tracked_and_untracked_files_in_directory(src_dir)

    if ignore_dir:
        ignore_dir_rel_path = src_dir / ignore_dir

        tracked_and_new_files = list({
            file for file in tracked_and_new_files if not any(parent == ignore_dir_rel_path for parent in file.parents)
        })

    copy_files(src_dir, dst_dir, tracked_and_new_files)",Copy only the files that are tracked by git or newly added from src_dir to dst_dir.,"Copy git-tracked and untracked files from source to destination, optionally ignoring a directory."
1243,_handle_list_tables,"def _handle_list_tables(self, keyspace: str, ctx: Optional[Context] = None) -> str:
        
        try:
            if not keyspace:
                raise Exception('Keyspace name is required')

            tables = self.schema_service.list_tables(keyspace)

            # Format table names as a markdown list for better display
            table_names = [t.name for t in tables]
            formatted_text = f'## Tables in Keyspace `{keyspace}`\n\n'
            if table_names:
                for name in table_names:
                    formatted_text += f'- `{name}`\n'
            else:
                formatted_text += 'No tables found in this keyspace.\n'

            # Add contextual information about tables in Cassandra
            if ctx:
                ctx.info(f'Adding contextual information about tables in keyspace {keyspace}')  # type: ignore[unused-coroutine]
                formatted_text += build_list_tables_context(keyspace, tables)

            return formatted_text
        except Exception as e:
            logger.error(f'Error listing tables: {str(e)}')
            raise Exception(f'Error listing tables: {str(e)}')",Handle the listTables tool.,"Generate a markdown list of tables in a specified keyspace, with optional context."
1244,get_headers,"def get_headers(session: primp.AsyncClient, **kwargs) -> dict:
    
    cookies = session.cookies

    headers = kwargs | {
        ""authorization"": ""Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs=1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA"",
        # ""cookie"": ""; "".join(f""{k}={v}"" for k, v in cookies.items()),
        ""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"",
        ""x-csrf-token"": cookies.get(""ct0"", """"),
        # ""x-guest-token"": cookies.get(""guest_token"", """"),
        ""x-twitter-auth-type"": ""OAuth2Session"" if cookies.get(""auth_token"") else """",
        ""x-twitter-active-user"": ""yes"",
        ""x-twitter-client-language"": ""en"",
    }
    return dict(sorted({k.lower(): v for k, v in headers.items()}.items()))",Get the headers required for authenticated requests,Generate HTTP headers for a session with optional custom parameters.
1245,get_settings,"def get_settings() -> Settings:
    
    settings = Settings()
    publish_configs = load_configs(""PUBLISH"")
    consume_configs = load_configs(""CONSUME"")
    return settings.model_copy(
        update={
            ""publish_configs"": publish_configs,
            ""consume_configs"": consume_configs,
        }
    )",retrieve the singleton instance of the application settings,Initialize settings with updated publish and consume configurations.
1246,show_user_message,"def show_user_message(
        self, message, model: Optional[str], chat_turn: int, name: Optional[str] = None
    ) -> None:
        
        if not self.config or not self.config.logger.show_chat:
            return

        subtitle_text = Text(f""{model or 'unknown'}"", style=""dim white"")
        if chat_turn > 0:
            subtitle_text.append(f"" turn {chat_turn}"", style=""dim white"")

        panel = Panel(
            message,
            title=f""{f'({name}) [USER]' if name else '[USER]'}"",
            title_align=""right"",
            style=""blue"",
            border_style=""bold white"",
            padding=(1, 2),
            subtitle=subtitle_text,
            subtitle_align=""left"",
        )
        console.console.print(panel, markup=self._markup)
        console.console.print(""\n"")",Display a user message in a formatted panel.,Display a styled user message panel with optional model and turn info if logging is enabled.
1247,exif_size,"def exif_size(img: Image.Image):
    
    s = img.size  # (width, height)
    if img.format == ""JPEG"":  # only support JPEG images
        try:
            if exif := img.getexif():
                rotation = exif.get(274, None)  # the EXIF key for the orientation tag is 274
                if rotation in {6, 8}:  # rotation 270 or 90
                    s = s[1], s[0]
        except Exception:
            pass
    return s",Returns exif-corrected PIL size.,Determine image dimensions considering EXIF rotation for JPEG format.
1248,store_report,"def store_report(self, job_id: str, report_data: Dict[str, Any]) -> None:
        
        self.reports.insert_one({
            ""job_id"": job_id,
            ""report_content"": report_data.get(""report"", """"),
            ""references"": report_data.get(""references"", []),
            ""sections"": report_data.get(""sections_completed"", []),
            ""analyst_queries"": report_data.get(""analyst_queries"", {}),
            ""created_at"": datetime.utcnow()
        })",Store the finalized research report.,Store a job report in the database with metadata and timestamps.
1249,send_prompt_to_google_sheet,"def send_prompt_to_google_sheet(prompt: str, terminal_output: str = None) -> bool:
    
    payload = {
        ""entry.1235837381"": prompt,
        ""fvv"": ""1""
    }
    
    # Add terminal output to the payload if provided
    if terminal_output:
        logging.info(f""Including terminal output in Google Form submission (length: {len(terminal_output)})"")
        # Limit the length if necessary to prevent issues
        max_length = 100000  # Set a reasonable max length
        if len(terminal_output) > max_length:
            logging.warning(f""Terminal output exceeds max length, truncating from {len(terminal_output)} to {max_length}"")
            terminal_output = terminal_output[:max_length] + ""\n... (truncated)""
        
        payload[""entry.1645678921""] = terminal_output
    else:
        logging.info(""Sending prompt only, no terminal output included"")
        
    try:
        logging.info(f""Sending data to Google Form: prompt (length: {len(prompt)})"" + 
                    (f"", terminal output (length: {len(terminal_output)})"" if terminal_output else """"))
        response = requests.post(form_url, data=payload)
        success = response.status_code == 200
        logging.info(f""Google Form submission {'succeeded' if success else 'failed'} with status code {response.status_code}"")
        return success
    except Exception as e:
        logging.error(f""Failed to send data to Google Form: {str(e)}"")
        return False","Sends the prompt text and optional terminal output to a Google Form, which appends it to a linked Google Sheet.","Submit prompt and optional output to Google Form, handling length and errors."
1250,load_resources,"def load_resources(resources_file: str = ""resources_info.json"") -> Dict:
    
    global RESOURCES
    
    if not os.path.exists(resources_file):
        raise FileNotFoundError(f""Resources file {resources_file} not found"")
        
    with open(resources_file, 'r') as f:
        RESOURCES = json.load(f)
    
    return RESOURCES",Load resources information from resources.json file.,"Load and return resources from a specified JSON file, raising an error if not found."
1251,cast_to_dict,"def cast_to_dict(object) -> dict:
    
    if isinstance(object, dict):
        return {key: cast_to_dict(val) for key, val in object.items()}
    elif isinstance(object, str):
        extract_json_attempt = parse_json(object)
        if extract_json_attempt:
            return extract_json_attempt
        return object
    else:
        return object",Try to cast an object as a dict.,"Convert objects to nested dictionaries, parsing JSON strings if possible."
1252,create_tool_use_response,"def create_tool_use_response(
        tool_name, tool_args, tool_id, finish_reason=""STOP"", usage=None
    ):
        
        from google.genai import types

        function_call = types.FunctionCall(name=tool_name, args=tool_args, id=tool_id)

        return types.GenerateContentResponse(
            candidates=[
                types.Candidate(
                    content=types.Content(
                        role=""model"", parts=[types.Part(function_call=function_call)]
                    ),
                    finish_reason=finish_reason,
                    safety_ratings=[],
                    citation_metadata=None,
                )
            ],
            prompt_feedback=None,
            usage_metadata=usage
            or {
                ""prompt_token_count"": 150,
                ""candidates_token_count"": 100,
                ""total_token_count"": 250,
            },
        )",Creates a tool use response for testing in Google's format.,Generate a structured response for tool usage with metadata and function call details.
1253,get_scene_info,"def get_scene_info(ctx: Context) -> str:
    
    try:
        blender = get_blender_connection()
        result = blender.send_command(""get_scene_info"")
        
        # Just return the JSON representation of what Blender sent us
        return json.dumps(result, indent=2)
    except Exception as e:
        logger.error(f""Error getting scene info from Blender: {str(e)}"")
        return f""Error getting scene info: {str(e)}""",Get detailed information about the current Blender scene,"Retrieve and return scene information from Blender, handling errors gracefully."
1254,is_iambic_pair,"def is_iambic_pair(stress1: int, stress2: int) -> bool:
    
    valid_pairs = {(2, 1), (0, 2), (0, 1), (0, 0), (1, 1), (2, 2)}
    return (stress1, stress2) in valid_pairs",Whether the pair of stresses is a valid iambic pair.,Determine if stress pattern forms an iambic pair
1255,get_environment,"def get_environment() -> str:
    
    try:
        logger.debug(""Retrieving environment details."")
        shell = os.getenv(""SHELL"", ""bash"")
        current_dir = os.getcwd()
        operating_system = os.name
        date_time = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")

        environment_details = (
            f""Current shell: {shell}\n""
            f""Current directory: {current_dir}\n""
            f""Operating system: {operating_system}\n""
            f""Date and time: {date_time}""
        )
        logger.debug(f""Environment details:\n{environment_details}"")
        return environment_details
    except Exception as e:
        logger.error(f""Error retrieving environment details: {str(e)}"")
        return ""Environment details unavailable.""",Retrieve the current environment details.,"Retrieve and log system environment details including shell, directory, OS, and timestamp."
1256,compute_cost_info,"def compute_cost_info(
    attributes: Mapping[str, AttributeValue],
) -> CostInfo | None:
    
    if not any(
        key in attributes
        for key in [""gen_ai.usage.input_tokens"", ""gen_ai.usage.output_tokens""]
    ):
        return None

    new_info: dict[str, float] = {}
    try:
        cost_prompt, cost_completion = cost_per_token(
            model=str(attributes.get(""gen_ai.request.model"", """")),
            prompt_tokens=int(attributes.get(""gen_ai.usage.input_tokens"", 0)),  # type: ignore[arg-type]
            completion_tokens=int(attributes.get(""gen_ai.usage.output_tokens"", 0)),  # type: ignore[arg-type]
        )
        new_info[""input_cost""] = cost_prompt
        new_info[""output_cost""] = cost_completion
    except Exception as e:
        msg = f""Error computing cost_per_token: {e}""
        logger.warning(msg)
        new_info[""input_cost""] = 0.0
        new_info[""output_cost""] = 0.0
    return CostInfo.model_validate(new_info)",Use litellm to compute cost.,"Calculate AI usage costs from token attributes, handling errors gracefully."
1257,_reconstruct_attributes,"def _reconstruct_attributes(attrs: list[str]) -> list[str]:
    

    def is_attr(attr: str) -> bool:
        if ""="" in attr:
            _, value = attr.split(""="", 1)
            if value.startswith(""'"") or value.startswith('""'):
                return True
        return False

    reconstructed = []
    found_attr = False
    for attr in attrs:
        if is_attr(attr):
            reconstructed.append(attr)
            found_attr = True
        else:
            if found_attr:
                reconstructed[-1] += f"" {attr}""
                found_attr = True
            elif reconstructed:
                reconstructed[-1] += f"" {attr}""
            else:
                reconstructed.append(attr)
    return reconstructed",Reconstructs attributes from a list of strings where some attributes may be split across multiple elements.,Reconstructs attribute strings by merging split components in a list.
1258,get_params,"def get_params(
        self,
        query: str | None = None,
        url: str | None = None,
        search_type: SearchType | None = None,
        output: str | None = None,
        include_html: bool | None = None,
        **kwargs,
    ) -> dict[str, Any]:
        
        params = {""api_key"": self.connection.api_key, **kwargs}

        current_search_type = search_type or self.search_type

        if current_search_type != SearchType.WEB:
            params[""search_type""] = current_search_type

        if query:
            params[""q""] = query
        elif url:
            params[""url""] = url

        if output:
            params[""output""] = output

        if include_html is not None:
            params[""include_html""] = include_html

        return {k: v for k, v in params.items() if v is not None}",Prepare the parameters for the API request.,Constructs a dictionary of search parameters based on input criteria and defaults.
1259,run_as_cli,"def run_as_cli():
    
    try:
        # Import required modules
        import threading
        from primary.background import start_huntarr, stop_event
        from primary.web_server import app
        from waitress import serve
        
        print(""Starting Huntarr in command-line mode..."")
        
        # Start background tasks in a thread
        background_thread = threading.Thread(
            target=start_huntarr, 
            name=""HuntarrBackground"", 
            daemon=True
        )
        background_thread.start()
        
        # Run the web server directly (blocking)
        print(""Press Ctrl+C to stop"")
        serve(app, host='0.0.0.0', port=9705, threads=8)
    except KeyboardInterrupt:
        print(""\nStopping Huntarr..."")
        if not stop_event.is_set():
            stop_event.set()
        # Give threads time to clean up
        time.sleep(2)
    except Exception as e:
        print(f""Error running Huntarr in command-line mode: {e}"")",Run Huntarr as a command-line application (non-service fallback),Initialize and manage a background service with a web server in command-line mode.
1260,register_actions,"def register_actions(self) -> None:
        
        self.actions = {
            ""generate-text"": Action(
                name=""generate-text"",
                parameters=[
                    ActionParameter(""prompt"", True, str, ""The input prompt for text generation""),
                    ActionParameter(""system_prompt"", True, str, ""System prompt to guide the model""),
                    ActionParameter(""model"", False, str, ""Model to use for generation"")
                ],
                description=""Generate text using Galadriel models""
            ),
        }",Register available Galadriel actions,Define actions for text generation with customizable parameters.
1261,parse_html,"def parse_html(self, html_content: str) -> BeautifulSoup:
        
        try:
            return BeautifulSoup(html_content, ""html.parser"")
        except Exception as e:
            logger.error(f""Error parsing HTML: {e}"")
            raise ValueError(f""Error parsing HTML: {e}"")",Parse HTML content using BeautifulSoup.,"Convert HTML content into a BeautifulSoup object, handling parsing errors."
1262,load_checkpoint,"def load_checkpoint(self):
        
        super().load_checkpoint()

        checkpoint_path = f""{self.checkpoint_dir}/env_checkpoints/{self.wandb_prepend}/step-{self.curr_step}.json""
        try:
            with open(checkpoint_path, ""r"") as f:
                data = json.load(f)

            if ""curriculum_level"" in data:
                level = data[""curriculum_level""]
                self.curriculum.current_level = level

            if ""performance_history"" in data:
                self.curriculum.performance_history = {
                    int(k): v for k, v in data[""performance_history""].items()
                }
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.warning(f""Failed to load checkpoint: {e}"")",Load curriculum state from checkpoint.,Load and apply saved curriculum data from a JSON checkpoint file
1263,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""DOMStorage.disable"",
    }
    json = yield cmd_dict","Disables storage tracking, prevents storage events from being sent to the client.",Disables DOM storage by sending a command and yielding a JSON response.
1264,split,"def split(self):
        
        train_ratio = 0.7
        test_ratio = 0.2
        val_ratio = 0.1

        df = self.dataset

        train_data, temp_data = train_test_split(df, test_size=(1 - train_ratio), random_state=42)
        test_size = test_ratio / (test_ratio + val_ratio)
        test_data, val_data = train_test_split(temp_data, test_size=test_size, random_state=42)

        os.makedirs(self.file_path + '/split_indices/', exist_ok=True)
        np.save(self.file_path + '/split_indices/train_indices.npy', train_data.index.to_numpy())
        np.save(self.file_path + '/split_indices/test_indices.npy', test_data.index.to_numpy())
        np.save(self.file_path + '/split_indices/val_indices.npy', val_data.index.to_numpy())

        os.makedirs(self.file_path + '/split_data/', exist_ok=True)
        train_data.to_numpy().dump(self.file_path + '/split_data/train_data.npy')
        test_data.to_numpy().dump(self.file_path + '/split_data/test_data.npy')
        val_data.to_numpy().dump(self.file_path + '/split_data/val_data.npy')

        self.train_df = train_data
        self.test_df = test_data
        self.val_df = val_data

        logging.info(
            f""Data split completed: {len(train_data)} train, {len(test_data)} test, {len(val_data)} validation samples."")","Split the dataset into training, validation, and test sets.","Split dataset into training, testing, and validation sets, saving indices and data files."
1265,handle_mention,"def handle_mention(event: dict[str, Any], say: Any) -> None:
        
        print(""#####[ Received Event ]#####"")
        print(event)

        # Skip if we've already answered this question
        # Seems like Slack likes to double-send events while debugging (?)
        if event[""ts""] in responded:
            return
        responded[event[""ts""]] = True

        # Get message text without the bot mention
        query = event[""text""].split("">"", 1)[1].strip()
        if not query:
            say(""Please ask a question about FastAPI!"")
            return

        try:
            # Add typing indicator emoji
            slack_app.client.reactions_add(
                channel=event[""channel""],
                timestamp=event[""ts""],
                name=""writing_hand"",
            )

            # Get answer using RAG
            answer, context = answer_question(query)

            # Format and send response in thread
            response = format_response(answer, context)
            say(text=response, thread_ts=event[""ts""])

        except Exception as e:
            # Send error message in thread
            say(text=f""Error: {str(e)}"", thread_ts=event[""ts""])",Handle mentions of the bot in channels.,Handle Slack mention events by responding to FastAPI queries with contextual answers.
1266,has_capacity,"def has_capacity(self, token_estimate: _TokenUsage) -> bool:
        
        self.update_capacity()
        if self.token_limit_strategy == TokenLimitStrategy.combined:
            has_capacity = self._check_combined_capacity(token_estimate)
        else:
            has_capacity = self._check_seperate_capacity(token_estimate)

        # This is a very loud log, enable only when needed
        # if not has_capacity:
        #     logger.debug(
        #         f""No capacity for request with {token_estimate} tokens.""
        #         f""Available capacity: {self.available_token_capacity} tokens, ""
        #         f""{int(self.available_request_capacity)} requests.""
        #     )
        return has_capacity",Check if there's enough capacity for a request.,Determine if token usage fits within current capacity limits.
1267,_load_yaml_or_json_from_tar,"def _load_yaml_or_json_from_tar(tar, yaml_path: str, json_path: str):
    
    members = [m.name for m in tar.getmembers()]
    if yaml_path in members:
        content = tar.extractfile(yaml_path).read().decode(""utf-8"")
        return yaml.load(content, Loader=FallbackNoneLoader)
    elif json_path in members:
        content = tar.extractfile(json_path).read().decode(""utf-8"")
        return json.loads(content)
    else:
        raise FileNotFoundError(f""Neither {yaml_path} nor {json_path} found in archive"")","Load from YAML if available, fallback to JSON for backward compatibility.","Extract and parse YAML or JSON from a tar archive, raising an error if neither is found."
1268,extract_json_from_model_output,"def extract_json_from_model_output(content: str) -> dict:
	
	try:
		# If content is wrapped in code blocks, extract just the JSON part
		if '```' in content:
			# Find the JSON content between code blocks
			content = content.split('```')[1]
			# Remove language identifier if present (e.g., 'json\n')
			if '\n' in content:
				content = content.split('\n', 1)[1]
		# Parse the cleaned content
		result_dict = json.loads(content)

		if isinstance(result_dict, list) and len(result_dict) == 1 and isinstance(result_dict[0], dict):
			result_dict = result_dict[0]

		assert isinstance(result_dict, dict), f'Expected JSON dictionary in response, got JSON {type(result_dict)} instead'
		return result_dict
	except json.JSONDecodeError as e:
		logger.warning(f'Failed to parse model output: {content} {str(e)}')
		raise ValueError('Could not parse response.')","Extract JSON from model output, handling both plain JSON and code-block-wrapped JSON.","Extract and parse JSON data from model output, handling code block formatting."
1269,get_cdp_config,"def get_cdp_config(self) -> Optional[ConfigDict]:
        
        cdp_endpoint_url = self._config.get(""CDP_ENDPOINT_URL"")
        if cdp_endpoint_url:
            return {""endpoint_url"": cdp_endpoint_url}
        return None",Return CDP config if `CDP_ENDPOINT_URL` is set.,Retrieve CDP configuration endpoint URL from settings if available.
1270,save_generations,"def save_generations(responses, model_name, dataset_name):
    
    output_dir = ""results/generations_no_reasoning""
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, f""{dataset_name}_{model_name}_generations.json"")
    with open(output_file, 'w') as f:
        json.dump(responses, f, indent=2)
    print(f""Saved generations to {output_file}"")",Save generation results to a JSON file.,Save AI model outputs to a structured JSON file in a specified directory.
1271,sample_ddl_queries,"def sample_ddl_queries() -> dict[str, str]:
    
    return {
        ""create_table"": ""CREATE TABLE users (id SERIAL PRIMARY KEY, name TEXT, email TEXT UNIQUE)"",
        ""alter_table"": ""ALTER TABLE users ADD COLUMN active BOOLEAN DEFAULT false"",
        ""drop_table"": ""DROP TABLE users"",
        ""truncate_table"": ""TRUNCATE TABLE users"",
        ""create_index"": ""CREATE INDEX idx_user_email ON users (email)"",
    }","Sample DDL (CREATE, ALTER, DROP) queries for testing.",Provides sample SQL Data Definition Language queries for table management.
1272,create_eval_set,"def create_eval_set(self, app_name, eval_set_id):
      
      if app_name not in eval_sets:
        eval_sets[app_name] = {}

      if eval_set_id in eval_sets[app_name]:
        raise ValueError(f""Eval set {eval_set_id} already exists."")

      eval_sets[app_name][eval_set_id] = EvalSet(
          eval_set_id=eval_set_id, eval_cases=[]
      )
      return eval_set_id",Create an eval set.,Initialize a new evaluation set for an application if it doesn't already exist.
1273,from_yaml,"def from_yaml(cls, yaml_path: str) -> Self:
        
        try:
            with open(yaml_path, ""r"") as f:
                config_data = yaml.safe_load(f)
            config = AuthConfig(**config_data)
            return cls(config)
        except Exception as e:
            logger.error(f""Failed to load auth config from {yaml_path}: {str(e)}"")
            raise ConfigurationException(f""Failed to load auth config: {str(e)}"") from ConfigurationException()",Create AuthManager from YAML config file.,"Load and parse YAML configuration into an authentication object, handling errors."
1274,set_script_execution_disabled,"def set_script_execution_disabled(
    value: bool,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""value""] = value
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Emulation.setScriptExecutionDisabled"",
        ""params"": params,
    }
    json = yield cmd_dict",Switches script execution in the page.,Disable script execution via emulation command generator.
1275,get_scale_friendly_name,"def get_scale_friendly_name(scale_values):
    
    scale_tuple = tuple(scale_values)
    if scale_tuple not in SCALE_TABLE:
        SCALE_TABLE.append(scale_tuple)
    index = SCALE_TABLE.index(scale_tuple)
    return f""Scale{index}""","Given a list of scale values, return a friendly name based on an index in the SCALE_TABLE.","Convert scale values to a unique, user-friendly scale name."
1276,input_spec,"def input_spec(self) -> list:
        
        specs = []
        for i, inp in enumerate(self.inputs):
            specs.append((inp[""name""], inp[""shape""], inp[""dtype""]))
            if self.debug:
                print(f""trt input {i} -> {inp['name']} -> {inp['shape']} -> {inp['dtype']}"")
        return specs","Restituisce le specifiche degli input (nome, shape, dtype) utili per preparare gli array.",Generate input specifications list with optional debug output.
1277,calculate_answer_score,"def calculate_answer_score(pred_sql, gold_sql, db_path, do_print=False):
    
    try:
        pred_results = execute_sql(pred_sql, db_path)
        gold_results = execute_sql(gold_sql, db_path)
        
        # answer_score = 2 if set(pred_results) == set(gold_results) else 0.5
        # answer_score = 1 if set(pred_results) == set(gold_results) else 0
        answer_score = 1 if set(pred_results) == set(gold_results) else 0.3
        
    except Exception as e:
        if do_print:
            print(f""[Error] Error in executing SQL: {e}"")
        pred_results = []
        gold_results = []

        answer_score = 0
        if 'syntax' in str(e):
            answer_score = 0
        else:
            answer_score = 0.1

    if do_print:
        # print(f""Retrieved results: {pred_results}"")
        # print(f""Target: {gold_results} "")
        print(f""Answer score: {answer_score}"")

    
    return answer_score",Calculate answer score based on final_prediction idx.,"Evaluate SQL query accuracy by comparing predicted and gold results, returning a score."
1278,_ensure_log_directory,"def _ensure_log_directory(self):
        
        log_dir = self.config[""logging""].get(""log_directory"", ""logs"")
        
        # Check for NLWEB_OUTPUT_DIR environment variable
        output_dir = os.getenv('NLWEB_OUTPUT_DIR')
        if output_dir:
            # Create logs directory under the output directory
            log_dir = os.path.join(output_dir, os.path.basename(log_dir))
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
                print(f""Created log directory: {log_dir}"")
            
        # Store the resolved directory (but don't create it yet)
        self.log_directory = log_dir",Resolve log directory path (but don't create it yet),"Ensure logging directory exists, using environment variable if specified"
1279,validate_api_key,"def validate_api_key(
    db_session: Annotated[Session, Depends(yield_db_session)],
    api_key_key: Annotated[str, Security(api_key_header)],
) -> UUID:
    
    api_key = crud.projects.get_api_key(db_session, api_key_key)
    if api_key is None:
        logger.error(
            ""api key not found"",
            extra={""partial_api_key"": f""{api_key_key[:4]}****{api_key_key[-4:]}""},
        )
        raise InvalidAPIKey(""api key not found"")

    elif api_key.status == APIKeyStatus.DISABLED:
        logger.error(""api key is disabled"", extra={""api_key_id"": api_key.id})
        raise InvalidAPIKey(""API key is disabled"")

    elif api_key.status == APIKeyStatus.DELETED:
        logger.error(""api key is deleted"", extra={""api_key_id"": api_key.id})
        raise InvalidAPIKey(""API key is deleted"")

    else:
        api_key_id: UUID = api_key.id
        logger.info(""api key validation successful"", extra={""api_key_id"": api_key_id})
        return api_key_id",Validate API key and return the API key ID.,"Validate and log API key status, raising errors for invalid or disabled keys."
1280,clean_system_prompts,"def clean_system_prompts(messages):
    
    system_prompt = None
    cleaned = []
    for msg in messages:
        if msg.get(""role"") == ""system"":
            if system_prompt is None:
                system_prompt = msg
                cleaned.append(msg)
            else:
                continue
        else:
            cleaned.append(msg)
    if system_prompt is None:
        cleaned.insert(0, {""role"": ""system"", ""content"": SYSTEM_PROMPT})
    else:
        if cleaned[0].get(""role"") != ""system"":
            cleaned.remove(system_prompt)
            cleaned.insert(0, system_prompt)
    return cleaned",Removes duplicate system prompts and ensures exactly one system message at top.,Reorganize messages to ensure a single system prompt is at the start.
1281,on_error,"def on_error(self, error: Exception, context: str = """", **kwargs: Any) -> None:
        
        try:
            error_event = {
                ""timestamp"": datetime.now(),
                ""error"": str(error),
                ""error_type"": type(error).__name__,
                ""context"": context,
                ""additional_kwargs"": kwargs,
            }
            self.current_trace[""errors""].append(error_event)
            logger.error(f""Error in {context}: {error}"")
        except Exception as e:
            logger.critical(f""Error in error handler: {e}"")",Enhanced error handling with context,Log and handle errors with context and additional details.
1282,_get_config_value,"def _get_config_value(
        webui_manager: WebuiManager,
        comp_dict: Dict[gr.components.Component, Any],
        comp_id_suffix: str,
        default: Any = None,
) -> Any:
    
    # Assumes component ID format is ""tab_name.comp_name""
    tab_name = ""browser_use_agent""  # Hardcode or derive if needed
    comp_id = f""{tab_name}.{comp_id_suffix}""
    # Need to find the component object first using the ID from the manager
    try:
        comp = webui_manager.get_component_by_id(comp_id)
        return comp_dict.get(comp, default)
    except KeyError:
        # Try accessing settings tabs as well
        for prefix in [""agent_settings"", ""browser_settings""]:
            try:
                comp_id = f""{prefix}.{comp_id_suffix}""
                comp = webui_manager.get_component_by_id(comp_id)
                return comp_dict.get(comp, default)
            except KeyError:
                continue
        logger.warning(
            f""Component with suffix '{comp_id_suffix}' not found in manager for value lookup.""
        )
        return default",Safely get value from component dictionary using its ID suffix relative to the tab.,Retrieve configuration value by component ID with fallback options.
1283,_format_agent_info,"def _format_agent_info(self, agent_name: str) -> str:
        
        agent = self.agents.get(agent_name)
        if not agent:
            return """"

        servers = ""\n"".join(
            [
                f""- {self._format_server_info(server_name)}""
                for server_name in agent.server_names
            ]
        )

        return f""Agent Name: {agent.name}\nDescription: {agent.instruction}\nServers in Agent: {servers}""",Format Agent information for display to planners,Format agent details and associated servers into a structured string.
1284,get_model_specifiction_cls,"def get_model_specifiction_cls(model_name: str, inference_type: InferenceType) -> ModelSpecification:
    
    if model_name not in SUPPORTED_MODEL_CONFIGS:
        raise ValueError(
            f""Model {model_name} not supported. Supported models are: {list(SUPPORTED_MODEL_CONFIGS.keys())}""
        )
    if inference_type not in SUPPORTED_MODEL_CONFIGS[model_name]:
        raise ValueError(
            f""Inference type {inference_type} not supported for model {model_name}. Supported inference types are: {list(SUPPORTED_MODEL_CONFIGS[model_name].keys())}""
        )
    return SUPPORTED_MODEL_CONFIGS[model_name][inference_type]",Get the model specification class for the given model name and inference type.,Validate and retrieve model configuration based on name and inference type
1285,log_move_and_thought,"def log_move_and_thought(move, thought, latency, cache_dir=None):
    
    cache_dir = cache_dir or DEFAULT_CACHE_DIR
    log_file_path = os.path.join(cache_dir, ""ace_attorney_moves.log"")
    
    log_entry = f""[{time.strftime('%Y-%m-%d %H:%M:%S')}] Move: {move}, Thought: {thought}, Latency: {latency:.2f} sec\n""
    
    try:
        with open(log_file_path, ""a"", encoding='utf-8') as log_file:
            log_file.write(log_entry)
    except Exception as e:
        print(f""[ERROR] Failed to write log entry: {e}"")",Logs the move and thought process into a log file inside the cache directory.,"Log player actions and thoughts with timestamps to a file, handling errors."
1286,_load_component,"def _load_component(name: str) -> Any:
    
    try:
        # Add some randomization to import paths
        if sys.modules.get(f""cai.internal.components.{name}""):
            return sys.modules[f""cai.internal.components.{name}""]
        return importlib.import_module(f""cai.internal.components.{name}"")
    except:
        return None",Load a system component,"Dynamically import and return a component module by name, handling errors."
1287,__store_chunk_topics,"def __store_chunk_topics(session, new_version: int, chunk_topics_dict: dict) -> None:
    
    if not isinstance(chunk_topics_dict, dict):
        logger.warning(f""Invalid chunk_topics format: {type(chunk_topics_dict)}"")
        return

    logger.info(f""Found chunk topics dict: {chunk_topics_dict}"")

    for cluster_id, cluster_data in chunk_topics_dict.items():
        # for each chunkId, create an unique record
        for chunk_id in cluster_data.get(""chunkIds"", []):
            topic_data = L1ChunkTopic(
                version=new_version,
                chunk_id=chunk_id,
                topic=cluster_data.get(""topic""),
                tags=cluster_data.get(""tags""),
                create_time=datetime.now(),
            )
            session.add(topic_data)",Store Chunk Topics data,Store and log chunk topics in a database session with validation and timestamping.
1288,print_welcome_banner,"def print_welcome_banner(agent_name: str, mode: str, high_contrast: bool = False) -> None:
    
    console = Console()
    
    # Use high contrast colors if enabled
    border_color = ""bright_blue"" if high_contrast else ""blue""
    _text_color = ""bright_white"" if high_contrast else ""white""
    
    welcome_message = Text.from_markup(get_welcome_message(agent_name, mode))
    
    # Main panel with consistent colors
    console.print(
        Panel(
            welcome_message,
            title=""[bright_white]»»—— QUANTALOGIC QUANTUM SYSTEM ——««[/]"",
            title_align=""center"",
            border_style=Style(color=border_color, bold=True),
            box=box.DOUBLE,
            padding=(1, 4),
            subtitle=f""[bright_cyan]QUANTUM INTELLIGENCE READY | {get_version()}[/]""
        )
    )
    
    # Print quantum equations panel at the bottom
    console.print(
        Panel(
            Text.from_markup(f""[bright_cyan]{QUANTUM_SYMBOL}[/]""),
            box=box.SIMPLE,
            style=Style(color=""bright_cyan"", bold=True),
            padding=(0, 2),
            title=""[bright_white]QUANTUM STATE[/]"",
            title_align=""center""
        )
    )",Print the welcome banner to the console with consistent colors.,Display a styled welcome banner for a quantum system interface.
1289,create_environment,"def create_environment(task_id=0, dataset_type=""validation"", tool_list=None):
    
    print_step(f""Creating environment (task_id={task_id}, dataset={dataset_type}, tools={tool_list if tool_list else 'default'})..."")
    
    data = {""id"": task_id, ""dataset_type"": dataset_type}
    if tool_list:
        data[""tool_list""] = tool_list
    
    env_id = make_request(""POST"", ""create"", data=data)
    
    if env_id:
        print_success(f""Environment created with ID: {env_id}"")
        return env_id
    else:
        print_error(""Failed to create environment"")
        return None",Create a test environment,Function initializes a task-specific environment using provided parameters and returns its ID.
1290,parse,"def parse(
        cls,
        response: str,
        answer_key: str,
        *,
        global_description: str = """",
        query_question: str = """",
        is_single_line_ans: bool = None,
    ) -> dict:
        
        response_parsed = cls._parse(
            response,
            is_ascii_art=False,
            global_description=global_description,
            query_question=query_question,
            is_single_line_ans=is_single_line_ans,
        )
        results = {answer_key: response_parsed}
        return results",Try to parse a single answer.,Parse a response string into a dictionary using specified parameters.
1291,_init_actions,"def _init_actions(self):
        
        self._actions = {
            ""plan"": self._plan,
            ""assign"": self._assign,
            ""final"": self._final,
            ""handle_input"": self._handle_input,
        }",Initializes the default actions for the manager.,Initialize a dictionary mapping action names to their corresponding methods.
1292,mock_jwks_data,"def mock_jwks_data(self, rsa_key_pair: RSAKeyPair) -> JWKSData:
        
        from authlib.jose import JsonWebKey

        # Create JWK from the RSA public key
        jwk = JsonWebKey.import_key(rsa_key_pair.public_key)  # type: ignore
        jwk_data: JWKData = jwk.as_dict()  # type: ignore
        jwk_data[""kid""] = ""test-key-1""
        jwk_data[""alg""] = ""RS256""

        return {""keys"": [jwk_data]}",Create mock JWKS data from RSA key pair.,Generate mock JSON Web Key Set data from an RSA public key.
1293,_identify_patterns,"def _identify_patterns(self, market_data: MarketData) -> None:
        
        df = market_data.data
        patterns = {}
        
        try:
            # Candlestick Patterns
            patterns['doji'] = ta.candlestick.doji(df['open'], df['high'], df['low'], df['close'])
            patterns['hammer'] = ta.candlestick.hammer(df['open'], df['high'], df['low'], df['close'])
            patterns['shooting_star'] = ta.candlestick.shooting_star(df['open'], df['high'], df['low'], df['close'])
            patterns['morning_star'] = ta.candlestick.morning_star(df['open'], df['high'], df['low'], df['close'])
            patterns['evening_star'] = ta.candlestick.evening_star(df['open'], df['high'], df['low'], df['close'])
            
            # Custom Pattern Detection
            patterns['double_top'] = self._detect_double_top(df)
            patterns['double_bottom'] = self._detect_double_bottom(df)
            patterns['head_shoulders'] = self._detect_head_shoulders(df)
            
            market_data.patterns = patterns
            
        except Exception as e:
            logger.error(f""Error identifying patterns: {e}"")
            raise",Identify chart patterns and candlestick patterns.,Identify and store candlestick and custom patterns in market data
1294,disable_freeu,"def disable_freeu(self):
        
        freeu_keys = {""s1"", ""s2"", ""b1"", ""b2""}
        for i, upsample_block in enumerate(self.up_blocks):
            for k in freeu_keys:
                if (
                    hasattr(upsample_block, k)
                    or getattr(upsample_block, k, None) is not None
                ):
                    setattr(upsample_block, k, None)",Disables the FreeU mechanism.,Disable specific attributes in upsample blocks by setting them to None.
1295,search_pubmed,"def search_pubmed(self, pubmed_api_url, query: str) -> str:
        
        params = {
            ""db"": ""pubmed"",
            ""term"": query,
            ""retmode"": ""json"",
            ""retmax"": 5
        }
        
        try:
            response = requests.get(pubmed_api_url, params=params)
            data = response.json()
            article_ids = data.get(""esearchresult"", {}).get(""idlist"", [])
            if not article_ids:
                return ""No relevant PubMed articles found.""
            
            return ""\n"".join(article_links)
        except Exception as e:
            return f""Error retrieving PubMed articles: {e}""",Search PubMed for relevant medical articles.,Search PubMed for articles using a query and return results or error message.
1296,manual_run,"def manual_run():
    
    try:
        from src.primary.apps.swaparr.handler import run_swaparr
        
        settings = load_settings(""swaparr"")
        if not settings or not settings.get(""enabled"", False):
            return jsonify({
                ""success"": False, 
                ""message"": ""Swaparr is not enabled""
            }), 400
        
        # Run Swaparr
        run_swaparr()
        
        # Get updated session stats
        session_stats = get_session_stats()
        
        return jsonify({
            ""success"": True,
            ""message"": ""Swaparr run completed successfully"",
            ""session_stats"": session_stats
        })
        
    except Exception as e:
        swaparr_logger.error(f""Error during manual Swaparr run: {str(e)}"")
        return jsonify({
            ""success"": False, 
            ""message"": f""Manual run failed: {str(e)}""
        }), 500",Manually trigger a Swaparr run,"Execute a manual Swaparr operation, handling settings validation and error logging."
1297,reset_state,"def reset_state(self, item_id: str) -> dict:
        
        if item_id in self.episodes_state:
            return self.episodes_state[item_id]
        else:
            logger.error(
                f""No state found for item_id {item_id}. Creating a default state.""
            )
            return self._initialize_workflow_state(item_id, """", None)",Retrieves the workflow state for the given item_id.,Retrieve or initialize state for a given item identifier.
1298,on_apply_color_clicked,"def on_apply_color_clicked(self, button):
        
        hue_value = self.hue_slider.get_value() # Get value from 0-360
        hex_color = self.hsl_to_rgb_hex(hue_value) # Convert HSL(hue, 1.0, 0.5) to HEX
        print(f""Applying color from slider: H={hue_value}, HEX={hex_color}"")
        selected_scheme = self.scheme_dropdown.get_active_id()
        # Run matugen with the chosen hex color and selected scheme
        exec_shell_command_async(f'matugen color hex ""{hex_color}"" -t {selected_scheme}')",Applies the color selected by the hue slider via matugen.,Convert hue slider value to HEX and apply color scheme asynchronously
1299,context_without_results,"def context_without_results():
    
    return {
        ""topic"": ""Empty Topic"",
        ""timeframe"": ""1d"",
        ""has_results"": False,
        ""hierarchical_results"": [],
    }",Create a sample context without results for testing.,Create a default context dictionary indicating no search results available.
1300,do_CS,"def do_CS(self, name: PDFStackT) -> None:
        
        try:
            self.il_creater.on_stroking_color_space(literal_name(name))
            self.scs = self.csmap[literal_name(name)]
        except KeyError:
            if settings.STRICT:
                raise PDFInterpreterError(f""Undefined ColorSpace: {name!r}"") from None
        return",Set color space for stroking operations,Handle color space setting with error handling for undefined spaces.
1301,save_bash_state_by_id,"def save_bash_state_by_id(thread_id: str, bash_state_dict: dict[str, Any]) -> None:
    
    if not thread_id:
        return

    bash_state_dir = get_bash_state_dir_xdg()
    state_file = os.path.join(bash_state_dir, f""{thread_id}_bash_state.json"")

    with open(state_file, ""w"") as f:
        json.dump(bash_state_dict, f, indent=2)",Save bash state to XDG directory with the given thread_id.,Save thread-specific bash state to a JSON file.
1302,run_browsecomp_benchmark,"def run_browsecomp_benchmark(
    num_examples, output_dir, model=None, provider=None, endpoint_url=None, api_key=None
):
    
    from local_deep_research.benchmarks.benchmark_functions import evaluate_browsecomp

    logger.info(f""Starting BrowseComp benchmark with {num_examples} examples"")
    start_time = time.time()

    # Run the benchmark
    results = evaluate_browsecomp(
        num_examples=num_examples,
        search_iterations=3,
        questions_per_iteration=3,
        search_strategy=""source_based"",
        search_tool=""searxng"",
        search_model=model,
        search_provider=provider,
        endpoint_url=endpoint_url,
        output_dir=os.path.join(output_dir, ""browsecomp""),
        evaluation_provider=""ANTHROPIC"",
        evaluation_model=""claude-3-7-sonnet-20250219"",
    )

    duration = time.time() - start_time
    logger.info(f""BrowseComp benchmark completed in {duration:.1f} seconds"")

    if results and isinstance(results, dict):
        logger.info(f""BrowseComp accuracy: {results.get('accuracy', 'N/A')}"")

    return results",Run BrowseComp benchmark with specified number of examples.,Execute a benchmarking process for BrowseComp with specified parameters and log results.
1303,_format_child_database_block,"def _format_child_database_block(
        self, block_content: dict, block: dict, page_breadcrumbs: List[Breadcrumb]
    ) -> str:
        
        title = block_content.get(""title"", ""Untitled Database"")
        database_id = block[""id""]

        # Queue this child database for processing with proper breadcrumbs
        self._child_databases_to_process.add(database_id)

        # Create breadcrumbs for the child database (parent page + current page)
        child_db_breadcrumbs = page_breadcrumbs.copy()
        self._child_database_breadcrumbs[database_id] = child_db_breadcrumbs

        self._stats[""child_databases_found""] += 1
        breadcrumb_names = [b.name for b in child_db_breadcrumbs]
        logger.info(
            f""Found child database: {title} ({database_id}) in page breadcrumbs: {breadcrumb_names}""
        )

        # Include a reference in the page content
        return f""🗃️ **[{title}]** (Child Database)""",Format child database blocks.,"Format and log child database details, updating processing queue and breadcrumbs."
1304,_keepalive_thread_run,"def _keepalive_thread_run(self):
        
        while self._keepalive_running:
            time.sleep(self.KEEPALIVE_POLL_FREQUENCY)
            with self._lock:
                for name, entry in list(self._entries.items()):
                    if (
                        entry.keeper is not None
                        and entry.keepalive_ttl is not None
                        and entry.lease_id is not None
                        and entry.keeper.check()
                    ):
                        try:
                            # Refresh the lease
                            self._client.refresh_lease(entry.lease_id)
                        except Exception as e:
                            logger.error(
                                f""Failed to refresh lease for key: K={name} V={entry.value}. Error: {e}""
                            )",Background thread to keep leases alive.,Periodically refresh leases for active entries while handling exceptions
1305,_create_websearch_agent,"def _create_websearch_agent(self):
        
        try:
            # Choose websearch tool type based on premium setting
            websearch_tool_type = ""web_search_premium"" if self.websearch_premium else ""web_search""
            
            self.websearch_agent = self.client.beta.agents.create(
                model=self.model_name,
                description=""Agent able to search information over the web, such as news, weather, sport results..."",
                name=""Websearch Agent"",
                instructions=""You have the ability to perform web searches to find up-to-date information. Use web search when you need current information that may not be in your training data."",
                tools=[{""type"": websearch_tool_type}],
                completion_args={
                    ""temperature"": self.temperature,
                    ""top_p"": 0.95,
                    ""max_tokens"": self.max_tokens,
                }
            )
            self.logger.info(f""Created websearch agent with ID: {self.websearch_agent.id}"")
        except Exception as e:
            self.logger.error(f""Error creating websearch agent: {e}"")
            raise",Create a websearch agent with the specified configuration.,Create a web search agent with premium or standard tools based on settings
1306,set_bypass_csp,"def set_bypass_csp(enabled: bool) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""enabled""] = enabled
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Page.setBypassCSP"",
        ""params"": params,
    }
    json = yield cmd_dict",Enable page Content Security Policy by-passing.,Enable or disable Content Security Policy bypass in a web page.
1307,format_code_output,"def format_code_output(execution_dict: dict[str, str],
                       code_output_begin: str,
                       code_output_end: str,
                       code_output_format: str = 'llama'):
    
    if code_output_format == 'llama':
        output = execution_dict[""process_status""]
        if execution_dict['stdout']:
            output += f""\n[stdout]\n{execution_dict['stdout']}[/stdout]""
        if execution_dict['stderr']:
            output += f""\n[stderr]\n{execution_dict['stderr']}[/stderr]""
        output = f""{code_output_begin}\n\n{output}{code_output_end}\n\n""
    elif code_output_format == 'qwen':
        output = """"
        if execution_dict['stdout']:
            output += f""{execution_dict['stdout']}""
        if execution_dict['stderr']:
            output += f""{execution_dict['stderr']}""
        if execution_dict['stderr'] and execution_dict['stdout']:
            LOG.warning(""Both stdout and stderr are not empty. This shouldn't normally happen! %s"", execution_dict)
        output = f""{code_output_begin}{output}{code_output_end}""
    else:
        raise ValueError(f""Unknown code_output_format: {code_output_format}"")

    # wrapping with code output separators
    return output",Formatting code output to be displayed as an llm expects it.,Format execution results with specified output style and separators
1308,_pull_data_worker,"def _pull_data_worker(self):
        
        # Initialize the puller inside the worker thread
        stream = NameResolvingZmqPuller(
            constants.experiment_name(),
            constants.trial_name(),
            puller_index=self.util.dp_rank,
        )
        try:
            while not self._stop_event.is_set():
                try:
                    data = stream.pull(timeout_ms=self.pull_timeout_ms)
                    processed_data = [
                        SequenceSample.from_json_compatible(x) for x in data
                    ]
                    logger.debug(
                        f""Get data {[x.ids[0] for x in processed_data]} from puller stream.""
                    )
                    self.data_queue.put(processed_data)
                except queue.Empty:
                    logger.debug(f""No data from puller stream."")
                    time.sleep(0.1)
                    continue
        finally:
            # Ensure socket is closed in the same thread
            del stream
            # Exit if this thread has an error
            sys.exit(1)",Worker thread that creates its own ZMQ puller and streams data.,"Worker thread pulls and processes data from a stream, handling timeouts and errors."
1309,load_products,"def load_products(num=None):
    
    with open(ITEMS_PATH) as f:
        all_products = json.load(f)
        if num is not None:
            random.shuffle(all_products)
            all_products = all_products[:num]
        products = dict()
        asins = set()
        for p in all_products:
            asin = p['asin']
            if asin in asins:
                continue
            asins.add(asin)
            products[asin] = p

    with open(REVIEWS_PATH) as f:
        reviews = json.load(f)
        reviews = {r['asin']: r for r in reviews}

    for asin, p in products.items():
        if asin in reviews:
            p['review'] = reviews[asin]
        else:
            p['review'] = None
    return products",Loads products from the `items.json` file and combine them with reviews through `asin`.,"Load and merge product data with reviews, optionally shuffling and limiting entries."
1310,set_simulated_central_state,"def set_simulated_central_state(
    state: CentralState,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""state""] = state.to_json()
    cmd_dict: T_JSON_DICT = {
        ""method"": ""BluetoothEmulation.setSimulatedCentralState"",
        ""params"": params,
    }
    json = yield cmd_dict",Set the state of the simulated central.,Simulate Bluetooth central state by yielding command dictionary
1311,get_ssl_verify_setting,"def get_ssl_verify_setting() -> bool:
    
    try:
        general_settings = settings_manager.load_settings(""general"")
        return general_settings.get(""ssl_verify"", True)  # Default to True for security
    except Exception as e:
        lidarr_logger.warning(f""Error getting SSL verify setting: {e}. Using default (True)."")
        return True",Get SSL verification setting from general configuration.,Retrieve SSL verification setting with a default fallback for security
1312,save_module,"def save_module(self, path: str, ignore: List[str] = [], **kwargs):
        
        logger.info(""Saving {} to {}"", self.__class__.__name__, path)
        config = self.get_graph_info()
        for ignore_key in ignore:
            config.pop(ignore_key, None)
        with open(path, ""w"", encoding=""utf-8"") as f:
            json.dump(config, f, indent=4)

        return path",Save the workflow graph to a module file.,"Serialize and save configuration to file, excluding specified keys."
1313,with_retries,"def with_retries(func):
    
    @functools.wraps(func)
    async def wrapper(self, *args, **kwargs):
        attempts = getattr(self.config.SETTINGS, 'ATTEMPTS', 5)  # Default to 5 if not set
        pause_range = getattr(self.config.SETTINGS, 'PAUSE_BETWEEN_ATTEMPTS', [5, 15])  # Default to [5, 15] if not set
        last_exception = None
        
        for attempt in range(attempts):
            try:
                return await func(self, *args, **kwargs)
            except Exception as e:
                last_exception = e
                logger.warning(f""[{self.account_index}] Attempt {attempt + 1}/{attempts} failed for {func.__name__}: {str(e)}"")
                if attempt < attempts - 1:  # Don't sleep on the last attempt
                    pause_time = random.uniform(pause_range[0], pause_range[1])
                    logger.info(f""[{self.account_index}] Waiting {pause_time:.2f} seconds before next attempt..."")
                    await asyncio.sleep(pause_time)
                
        logger.error(f""[{self.account_index}] All {attempts} attempts failed for {func.__name__}"")
        raise last_exception

    return wrapper",Decorator to add retry functionality to async methods.,Decorator adds retry logic with configurable attempts and pauses for asynchronous functions.
1314,sample_tcl_queries,"def sample_tcl_queries() -> dict[str, str]:
    
    return {
        ""begin_transaction"": ""BEGIN"",
        ""commit_transaction"": ""COMMIT"",
        ""rollback_transaction"": ""ROLLBACK"",
        ""savepoint"": ""SAVEPOINT my_savepoint"",
        ""mixed_case_transaction"": ""Begin Transaction"",
    }","Sample TCL (BEGIN, COMMIT, ROLLBACK) queries for testing.",Map transaction operations to corresponding SQL commands in a dictionary.
1315,find_missing_pdfs,"def find_missing_pdfs(pdf_sources, created_files, base_bench_path):
    
    subdirs = set()

    for source_file in pdf_sources:
        if not source_file:
            continue

        subdir, _ = get_subdir_and_pdf_name(source_file)
        if subdir:
            subdirs.add(subdir)

    print(f""Found PDF subdirectories: {sorted(subdirs)}"")

    missing_pdfs = []

    for subdir in subdirs:
        pdf_dir = Path(base_bench_path) / ""bench_data"" / ""pdfs"" / subdir

        if not pdf_dir.exists():
            print(f""Warning: Directory {pdf_dir} does not exist"")
            continue

        pdf_files = list(pdf_dir.glob(""*.pdf""))
        print(f""Found {len(pdf_files)} PDF files in {subdir}/"")

        for pdf_file in pdf_files:
            pdf_name = pdf_file.stem

            if (subdir, pdf_name) not in created_files:
                missing_pdfs.append({""pdf_name"": pdf_name, ""full_path"": pdf_file, ""subdir"": subdir})

    print(f""Found {len(missing_pdfs)} missing PDFs"")
    return missing_pdfs",Find PDFs that exist in directories but are missing from JSONL data.,Identify and list missing PDF files from specified directories.
1316,precalculate_safetensors_hashes,"def precalculate_safetensors_hashes(tensors, metadata):
    

    # Because writing user metadata to the file can change the result of
    # sd_models.model_hash(), only retain the training metadata for purposes of
    # calculating the hash, as they are meant to be immutable
    metadata = {k: v for k, v in metadata.items() if k.startswith(""ss_"")}

    bytes = safetensors.torch.save(tensors, metadata)
    b = BytesIO(bytes)

    model_hash = addnet_hash_safetensors(b)
    legacy_hash = addnet_hash_legacy(b)
    return model_hash, legacy_hash",Precalculate the model hashes needed by sd-webui-additional-networks to save time on indexing the model later.,Compute immutable hashes for tensors using filtered metadata.
1317,print_extracted_result,"def print_extracted_result(result: ScrapedResult):
    
    if result.success:
        print(f""\n=={result.name} Results ==="")
        print(f""Extracted content: {result.content}"")
        print(f""Raw markdown length: {result.raw_markdown_len}"")
        print(f""Citations markdown length: {result.citations_markdown_len}"")
    else:
        print(f""Error in {result.name}: {result.error}"")",Method to print out the extracted results,Display extracted data or error message based on scraping success status.
1318,validate_output,"def validate_output(output_data, expected_batch_size):
    
    if len(output_data) != expected_batch_size:
        raise ValueError(f""Output size {len(output_data)} does not match expected batch size {expected_batch_size}."")
    print(f""Output size is valid: {len(output_data)}"")",Validate the size of the output data.,"Ensure output data size matches expected batch size, raising error if not."
1319,a3m_to_aligned_dataframe,"def a3m_to_aligned_dataframe(
    a3m_path: Path | str,
    source_database: MSADataSource,
    insert_pairing_key: bool = True,
) -> pd.DataFrame:
    
    alignments = read_fasta(a3m_path)

    records: list[dict[str, str]] = []
    for i, alignment in enumerate(alignments):
        # Assume first entry in a3m is the query
        src = MSADataSource.QUERY.value if i == 0 else source_database.value
        record = {
            ""sequence"": alignment.sequence,
            ""source_database"": src,
            # Empty pairing keys get encoded as UNKNOWN
            ""pairing_key"": (
                get_tax_names([alignment.header], source_database).pop()
                if insert_pairing_key
                else """"
            ),
            ""comment"": alignment.header,
        }
        records.append(record)
    assert records[0][""source_database""] == ""query""
    retval = pd.DataFrame.from_records(records)
    AlignedParquetModel.validate(retval)
    return retval",Reformat the a3m as a parquet.,Convert A3M file to aligned DataFrame with optional pairing keys.
1320,docstring,"def docstring(description: typing.Optional[str]) -> str:
    
    if not description:
        return """"
    # if original description uses escape sequences it should be generated as a raw docstring
    description = escape_backticks(description)
    if ""\\"" in description:
        return dedent("""").format(description)
    else:
        return dedent("""").format(description)",Generate a docstring from a description.,Generate a formatted docstring from an optional description input.
1321,log_reader_thread,"def log_reader_thread(log_file):
    
    try:
        with open(log_file, ""r"", encoding=""utf-8"") as f:
            # Move to the end of file
            f.seek(0, 2)

            while not STOP_LOG_THREAD.is_set():
                line = f.readline()
                if line:
                    LOG_QUEUE.put(line)  # Add to conversation record queue
                else:
                    # No new lines, wait for a short time
                    time.sleep(0.1)
    except Exception as e:
        logging.error(f""Log reader thread error: {str(e)}"")",Background thread that continuously reads the log file and adds new lines to the queue,"Monitors log file for new entries, adding them to a processing queue."
1322,get_node,"def get_node(self, node_id: UUID) -> DagNode:
        
        for node in self.nodes:
            if node.id == node_id:
                return node
        raise ValueError(f""Node with ID {node_id} not found"")",Get a node by ID.,"Retrieve a node by its unique identifier from a collection, raising an error if not found."
1323,check_correctness,"def check_correctness(problem, generation):
    
    try:
        return run_test_with_timeout(problem, generation)
    except timeout_decorator.TimeoutError:
        print(""Test execution timed out"")
        return False
    except Exception as e:
        print(f""Error in check_correctness: {e}"")
        return False",Check if the code is correct.,Evaluate solution validity with timeout and error handling.
1324,_get_default_states,"def _get_default_states(self):
        
        return [
            {
                self.env.handler.robot.name: {
                    ""dof_pos"": {j: 0.0 for j in self.env.handler.robot.joint_limits.keys()},
                    ""pos"": torch.tensor([0.0, 0.0, 0.0]),
                    ""rot"": torch.tensor([1.0, 0.0, 0.0, 0.0]),
                }
            }
            for _ in range(self.num_envs)
        ]",Generate default reset states,Initialize default robot states with zero positions and identity rotation for multiple environments.
1325,map_env_action_to_agent_action,"def map_env_action_to_agent_action(self, env_action_idx: int) -> str:
        
        # Try adapter's mapping first (which should be loaded from game_env_config.json)
        if env_action_idx in self.adapter.action_idx_to_move:
            return self.adapter.action_idx_to_move[env_action_idx]
        # Fallback to environment's internal generation
        if env_action_idx in self.env_action_idx_to_move:
            return self.env_action_idx_to_move[env_action_idx]
        return f""action_index_{env_action_idx}""",Maps an internal environment action index to its string representation.,Map environment action index to agent's action using adapter or fallback.
1326,handle_export_tracker,"def handle_export_tracker(args: argparse.Namespace) -> int:
    
    try:
        export_result_path_or_msg = export_tracker(args.tracker_file, args.format, args.output)
        if export_result_path_or_msg.startswith(""Error:""):
            print(export_result_path_or_msg); return 1
        print(f""Tracker exported to {export_result_path_or_msg}""); return 0
    except Exception as e_export: logger.exception(f""Error export_tracker: {e_export}""); print(f""Error: {e_export}""); return 1",Handle the export-tracker command.,"Handles tracker export, logging errors, and returning status code"
1327,_process_custom_functions,"def _process_custom_functions(self, functions: List[Callable]) -> Dict[str, Callable]:
        
        function_map = {}
        for func in functions:
            if not callable(func):
                raise ValueError(f""Provided function {func} is not callable"")

            name = getattr(func, ""__name__"", None)
            if not name:
                raise ValueError(f""Function {func} must have a __name__ attribute"")

            # Add metadata if missing
            if not func.__doc__:
                func.__doc__ = f""Execute {name} operation""
                logger.warning(f""Added default docstring to function: {name}"")

            function_map[name] = func
            logger.info(f""Registered custom function: {name}"")

        # Ensure print_answer is always present
        if ""print_answer"" not in function_map:
            function_map[""print_answer""] = self._create_print_answer()
            logger.info(""Added default print_answer function"")

        return function_map",Process user-provided functions with validation.,"Validate and register custom functions, ensuring metadata and default operations."
1328,process_user_data,"def process_user_data(users: list[User], include_inactive: bool = False, transform_func: callable | None = None) -> dict[str, Any]:
    
    result: dict[str, Any] = {""users"": [], ""total"": 0, ""admin_count"": 0}

    for user in users:
        if transform_func:
            user_data = transform_func(user.to_dict())
        else:
            user_data = user.to_dict()

        result[""users""].append(user_data)
        result[""total""] += 1

        if ""admin"" in user.roles:
            result[""admin_count""] += 1

    return result",Process user data with optional transformations.,"Aggregate and transform user data, optionally including inactive users."
1329,_generate_generic_name,"def _generate_generic_name(self, statement: ValidatedStatement) -> str:
        
        command = statement.command.value.lower()
        schema = statement.schema_name.lower() if statement.schema_name else ""public""
        object_type = statement.object_type.lower() if statement.object_type else ""object""

        name = f""{command}_{schema}_{object_type}""
        return self.sanitize_name(name)",Generate a name for other statement types.,"Create a sanitized name by combining command, schema, and object type."
1330,send_sonic,"def send_sonic(agent, **kwargs):
    
    try:
        to_address = kwargs.get(""to_address"")
        amount = float(kwargs.get(""amount""))

        # Direct passthrough to connection method - add your logic before/after this call!
        agent.connection_manager.connections[""sonic""].transfer(
            to_address=to_address,
            amount=amount
        )
        return

    except Exception as e:
        logger.error(f""Failed to send $S: {str(e)}"")
        return None",Send $S tokens to an address.,Function facilitates a secure transfer of funds using a specified connection method.
1331,load_source_dataset,"def load_source_dataset(config: Dict[str, Any]) -> datasets.Dataset:
    
    try:
        dataset = datasets.load_dataset(
            config[""name""],
            config.get(""subset"")
        )
        return dataset
    except Exception as e:
        print(f""Error loading dataset {config['name']}: {str(e)}"")
        return None",Load a source dataset with error handling,"Load a dataset using configuration parameters, handling errors gracefully."
1332,import_modules_from_path,"def import_modules_from_path(path: str) -> None:
    
    path = os.path.abspath(os.path.normpath(path))
    importlib.invalidate_caches()
    tmp = path.rsplit(os.sep, 1)
    if len(tmp) == 1:
        module_path = "".""
        package_name = tmp[0]
    else:
        module_path, package_name = tmp
    append_python_path(module_path)
    # Import at top level
    module = importlib.import_module(package_name)
    path = list(getattr(module, ""__path__"", []))
    path_string = """" if not path else path[0]
    # walk_packages only finds immediate children, so need to recurse.
    for module_finder, name, _ in pkgutil.walk_packages(path):
        # Sometimes when you import third-party libraries that are on your path,
        # `pkgutil.walk_packages` returns those too, so we need to skip them.
        if path_string and module_finder.path != path_string:
            continue
        # subpackage = f""{package_name}.{name}""
        subpackage = f""{path_string}/{name}""

        import_modules_from_path(subpackage)",Import all submodules under the given package.,Recursively import all modules from a specified directory path using dynamic path adjustments.
1333,file_path,"def file_path(self):
        
        if self.content_type == ""text/markdown"":
            return f""{self.folder}/{self.title}.md"" if self.folder else f""{self.title}.md""
        else:
            return f""{self.folder}/{self.title}"" if self.folder else self.title",Get the file path for this entity based on its permalink.,Determine file path based on content type and folder presence
1334,_resolve_model,"def _resolve_model(self, model_str: str) -> Type[BaseModel]:
        
        try:
            module_name, class_name = model_str.split("":"")
            module = importlib.import_module(module_name)
            model_class = getattr(module, class_name)
            if not issubclass(model_class, BaseModel):
                raise ValueError(f""{model_str} is not a Pydantic model"")
            return model_class
        except (ValueError, ImportError, AttributeError) as e:
            raise ValueError(f""Failed to resolve response_model '{model_str}': {e}"")",Resolve a string to a Pydantic model class for structured_llm_node.,Resolve and validate a Pydantic model class from a string identifier.
1335,get_graph_nodes,"def get_graph_nodes(self) -> list[ast.Call]:
        

        def _get_node_name(node: ast.expr) -> str:
            if isinstance(node, ast.Str):
                return node.s
            raise ValidationError(f""Could not determine name of node `{node}` in {ENTRYPOINT}"")

        nodes = asttools.find_method_calls(self.get_run_method(), 'add_node')
        for node in nodes:
            source, target = node.args
            source_name = _get_node_name(source)
            # target_name = _get_node_name(target)
            if source_name == GRAPH_NODE_TOOLS:  # TODO this is a bit brittle
                nodes.remove(node)
            # if target_name == GRAPH_NODE_TOOLS:
            #     nodes.remove(node)
        return nodes",Get all of the AST Call nodes that create the graph nodes.,Extracts and filters graph nodes from method calls in an abstract syntax tree.
1336,initialize_trackers,"def initialize_trackers(
    trackers: List[str], experiment_name: str, config: Dict[str, Any], log_dir: str
) -> Union[BaseTracker, SequentialTracker]:
    

    logger.info(f""Initializing trackers: {trackers}. Logging to {log_dir=}"")

    if len(trackers) == 0:
        return BaseTracker()

    if any(tracker_name not in _SUPPORTED_TRACKERS for tracker_name in set(trackers)):
        raise ValueError(f""Unsupported tracker(s) provided. Supported trackers: {_SUPPORTED_TRACKERS}"")

    tracker_instances = []
    for tracker_name in set(trackers):
        if tracker_name == Trackers.NONE:
            tracker = BaseTracker()
        elif tracker_name == Trackers.WANDB:
            tracker = WandbTracker(experiment_name, log_dir, config)
        tracker_instances.append(tracker)

    tracker = SequentialTracker(tracker_instances)
    return tracker",Initialize loggers based on the provided configuration.,Initialize and validate experiment trackers based on configuration and supported types
1337,get_thumbnail_path,"def get_thumbnail_path(instance, _):
    

    file_name = f'thumbnails/{instance.id}.png'
    file_path = '/'.join([str(instance.owner.user.id), file_name])

    return str(file_path)",Get the file path for the thumbnail of a PDF.,Generate a file path for a user's thumbnail image based on instance ID
1338,parse_candy_crush_textual_board,"def parse_candy_crush_textual_board(text_rep: str) -> Optional[List[List[str]]]:
    
    if not text_rep:
        return None
    
    try:
        lines = text_rep.strip().split('\n')
        board = []
        
        for line in lines:
            line = line.strip()
            if '|' in line and any(char.isdigit() for char in line.split('|')[0]):
                # This is a board row like ""0| R R C P G P R C""
                parts = line.split('|', 1)
                if len(parts) == 2:
                    row_data = parts[1].strip().split()
                    if row_data:  # Only add non-empty rows
                        board.append(row_data)
        
        # Validate board dimensions (should be 8x8 for candy crush)
        if len(board) == 8 and all(len(row) == 8 for row in board):
            return board
        elif board:  # Return whatever board we found, even if not 8x8
            print(f""Warning: Candy crush board dimensions are {len(board)}x{len(board[0]) if board else 0}, expected 8x8"")
            return board
        else:
            return None
            
    except Exception as e:
        print(f""Error parsing candy crush textual representation: {e}"")
        return None",Parse candy crush textual representation to extract board,"Parse a textual representation of a Candy Crush board into a 2D list format, ensuring valid dimensions."
1339,_make_api_request,"def _make_api_request(self):
        
        try:
            auth = None
            if self.config.auth:
                if ""type"" in self.config.auth and self.config.auth[""type""] == ""basic"":
                    auth = HTTPBasicAuth(
                        self.config.auth[""username""], self.config.auth[""password""]
                    )
                elif (
                    ""type"" in self.config.auth and self.config.auth[""type""] == ""bearer""
                ):
                    headers = self.config.headers or {}
                    headers[""Authorization""] = f""Bearer {self.config.auth['token']}""

            response = requests.request(
                method=self.config.method,
                url=self.config.url,
                headers=self.config.headers,
                params=self.config.params,
                auth=auth,
            )
            response.raise_for_status()
            return response
        except Exception as e:
            logger.error(f""Error making API request: {e}"")
            raise",Make an API request based on the configuration,Handle API requests with optional basic or bearer authentication
1340,process_image,"def process_image(image):
    
    if isinstance(image, str):
        # Check if it's a URL or base64 string
        if image.startswith('data:'):
            return image  # Already in base64 format
        elif urlparse(image).scheme in ('http', 'https'):
            return image  # It's a URL
        else:
            # Assume it's a local file path
            encoded = encode_image_file(image)
            return f""data:image/jpeg;base64,{encoded}"" if encoded else None
    return None",Process image input to the format expected by Mistral API.,Determine image format and convert local files to base64 if necessary.
1341,chdir_to_tensorzero_example,"def chdir_to_tensorzero_example(project_root):
    
    original_cwd = Path.cwd()
    example_dir = project_root / ""examples"" / ""tensorzero""
    if not example_dir.is_dir():
        pytest.skip(
            f""TensorZero example directory not found at {example_dir}""
        )  # Use skip instead of fail
        return
    os.chdir(example_dir)
    print(f""\nChanged CWD to: {example_dir}"")
    yield
    os.chdir(original_cwd)
    print(f""\nRestored CWD to: {original_cwd}"")",Change CWD to the tensorzero example directory for a test.,"Change working directory to TensorZero example, skip if not found, then restore original path."
1342,_log_sensitive_data_usage,"def _log_sensitive_data_usage(self, placeholders_used: set[str], current_url: str | None) -> None:
		
		if placeholders_used:
			url_info = f' on {current_url}' if current_url and current_url != 'about:blank' else ''
			logger.info(f'🔒 Using sensitive data placeholders: {"", "".join(sorted(placeholders_used))}{url_info}')",Log when sensitive data is being used on a page,Log sensitive data usage with placeholders and URL context.
1343,uninstall,"def uninstall(manifest_id: str) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    params[""manifestId""] = manifest_id
    cmd_dict: T_JSON_DICT = {
        ""method"": ""PWA.uninstall"",
        ""params"": params,
    }
    json = yield cmd_dict",Uninstalls the given manifest_id and closes any opened app windows.,Generate a command to uninstall a PWA using a manifest ID.
1344,_generate_multiple_samples,"def _generate_multiple_samples(
        self, 
        prompt: str, 
        num_samples: int, 
        temperature: float, 
        top_p: float
    ) -> List[str]:
        
        samples = []
        
        for i in range(num_samples):
            logger.debug(f""Generating sample {i+1}/{num_samples}"")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{""role"": ""user"", ""content"": prompt}],
                max_tokens=self.max_tokens,
                temperature=temperature,
                top_p=top_p
            )
            
            self.completion_tokens += response.usage.completion_tokens
            samples.append(response.choices[0].message.content.strip())
        
        return samples",Generate multiple samples by calling the API multiple times.,Generate multiple text samples using AI model with specified parameters
1345,build_instruction_kwargs,"def build_instruction_kwargs(row: dict) -> dict:
    
    kwargs = row[""kwargs""]

    if kwargs is None:
        return {""valid_kwargs_json"": False}

    try:
        kwargs = json.loads(row[""kwargs""])
    except json.JSONDecodeError:
        return {""valid_kwargs_json"": False}

    instruction_id_list = row[""instruction_id_list""]
    kwargs_list = []
    for instruction_id in instruction_id_list:
        args = INSTRUCTION_ARGS[instruction_id]
        instruction_kwargs = {}
        for arg in args:
            value = kwargs[arg]
            # Fix ""English"" instead of ""en""
            if arg == ""language"":
                if value in LANGUAGE_TO_CODE:
                    value = LANGUAGE_TO_CODE[value]
                else:
                    return {""valid_kwargs_json"": False}
            instruction_kwargs[arg] = value
        kwargs_list.append(instruction_kwargs)

    return {""kwargs"": json.dumps(kwargs_list), ""valid_kwargs_json"": True}",Builds the list of `kwargs` for each instruction in `instruction_id_list`.,"Parse and validate JSON arguments, mapping language codes for instructions."
1346,plot_predictions,"def plot_predictions(self, batch, preds, ni):
        
        plot_images(
            batch[""img""],
            batch_idx=torch.arange(len(batch[""img""])),
            cls=torch.argmax(preds, dim=1),
            fname=self.save_dir / f""val_batch{ni}_pred.jpg"",
            names=self.names,
            on_plot=self.on_plot,
        )",Plots predicted bounding boxes on input images and saves the result.,Visualize model predictions on a batch of images and save the plot.
1347,discover_all_files,"def discover_all_files(self):
        
        logger.info(
            f""{self.rank_info}(id={self.id}) Listing all text embed cache entries""
        )
        # This isn't returned, because we merely check if it's stored, or, store it.
        (
            StateTracker.get_text_cache_files(data_backend_id=self.id)
            or StateTracker.set_text_cache_files(
                self.data_backend.list_files(
                    instance_data_dir=self.cache_dir,
                    file_extensions=[""pt""],
                ),
                data_backend_id=self.id,
            )
        )
        self.debug_log("" -> done listing all text embed cache entries"")",Identify all files in the data backend.,Log and manage text cache files in a data backend system.
1348,apply_compile,"def apply_compile(model: torch.nn.Module, compile_scope: str) -> torch.nn.Module:
    
    if getattr(model, ""_torch_compiled"", False):
        return model  # Already compiled

    if compile_scope == ""full"":
        model = torch.compile(model)
        setattr(model, ""_torch_compiled"", True)
    elif compile_scope == ""regional"":
        if isinstance(model, torch.nn.ModuleList):
            for name, module in model.named_children():
                if not getattr(module, ""_torch_compiled"", False):
                    compiled_module = torch.compile(module)
                    setattr(compiled_module, ""_torch_compiled"", True)
                    model.register_module(name, compiled_module)
        else:
            for name, module in model.named_children():
                apply_compile(module, compile_scope)
    else:
        raise ValueError(f""Unknown compile mode: {compile_scope}. Use 'full' or 'regional'."")

    return model",Apply  to a model or its submodules if not already compiled.,Optimize neural network model by compiling based on specified scope.
1349,generate_ddp_command,"def generate_ddp_command(world_size, trainer):
    

    if not trainer.resume:
        shutil.rmtree(trainer.save_dir)  # remove the save_dir
    file = generate_ddp_file(trainer)
    dist_cmd = ""torch.distributed.run"" if TORCH_1_9 else ""torch.distributed.launch""
    port = find_free_network_port()
    cmd = [sys.executable, ""-m"", dist_cmd, ""--nproc_per_node"", f""{world_size}"", ""--master_port"", f""{port}"", file]
    return cmd, file",Generates and returns command for distributed training.,Generate a distributed training command for PyTorch with dynamic port and file setup.
1350,handle,"def handle(self, response, messages) -> None:
        
        msg = {""role"": ""assistant"", ""content"": []}
        tool_used = False
        for content in response.content:
            if content.type == ""tool_use"" and content.name == self.name:
                msg[""content""].append(
                    {
                        ""type"": ""tool_use"",
                        ""id"": content.id,
                        ""name"": content.name,
                        ""input"": content.input,
                    }
                )
                # Function execution:
                self.function_result = str(self.raw_func(**content.input))
                self.tool_id = content.id
                tool_used = True
            elif content.type == ""text"":
                msg[""content""].append({""type"": ""text"", ""text"": content.text})

        if tool_used:
            messages.append(msg)
            messages.append(
                {""role"": ""user"", ""content"": [self.get_response_schema()]}
            )",Handle the tool execution result from an API response.,"Process response content, execute functions, and update messages accordingly."
1351,get_upload_path,"def get_upload_path(instance, filename):
    
    owner_email = instance.collection.owner.email
    # Sanitize email to be safe for use in paths
    safe_email = owner_email.replace(""@"", ""_at_"")
    if settings.DEBUG:
        upload_path = f""dev-documents/{safe_email}/{filename}""  # pragma: no cover
    else:
        upload_path = f""documents/{safe_email}/{filename}""

    MAX_UPLOAD_PATH_LENGTH = 116
    if len(upload_path) <= MAX_UPLOAD_PATH_LENGTH:
        return upload_path

    extension = os.path.splitext(upload_path)[1]
    trimmed_upload_path = (
        upload_path[: MAX_UPLOAD_PATH_LENGTH - len(extension)] + extension
    )

    logger.info(f""Trimmed upload path to {trimmed_upload_path}"")
    return trimmed_upload_path",Generate the upload path for document files.,"Generate a safe, length-limited file upload path based on user email and environment."
1352,get_video,"def get_video(self, video_id):
        
        video = self.collection.get_video(video_id)
        return {
            ""id"": video.id,
            ""name"": video.name,
            ""description"": video.description,
            ""collection_id"": video.collection_id,
            ""stream_url"": video.stream_url,
            ""length"": video.length,
            ""thumbnail_url"": video.thumbnail_url,
        }",Get a video by ID.,Retrieve and return detailed video information by video ID.
1353,process_audio_input,"def process_audio_input(conv_state, audio_path, text):
            

            if not audio_path and not text.strip():
                return conv_state

            if audio_path:
                text = preprocess_ref_audio_text(audio_path, text)[1]
            if not text.strip():
                return conv_state

            conv_state.append({""role"": ""user"", ""content"": text})
            return conv_state",Handle audio or text input from user,Update conversation state with processed audio or text input
1354,_get_fundamental_data,"def _get_fundamental_data(self, ticker: str) -> Dict:
        
        try:
            url = f""{self.BASE_URL}/{ticker}""
            response = requests.get(url)
            response.raise_for_status()
            
            # Parse the response to extract fundamental data
            data = response.json()
            return {
                'market_cap': data.get('marketCap'),
                'pe_ratio': data.get('peRatio'),
                'dividend_yield': data.get('dividendYield'),
                'eps': data.get('eps'),
                'high_52week': data.get('high52Week'),
                'low_52week': data.get('low52Week')
            }
            
        except requests.RequestException as e:
            logger.error(f""Error fetching fundamental data: {e}"")
            return {}",Fetch fundamental data from Google Finance.,Fetch and parse stock fundamental data from an API for a given ticker.
1355,load_jsonl,"def load_jsonl(file_path: str) -> List[Dict]:
    
    try:
        with open(file_path, ""r"", encoding=""utf-8"") as f:
            return [json.loads(line) for line in f]
    except FileNotFoundError:
        print(f""ERROR: JSONL file not found: {file_path}"")
        raise
    except json.JSONDecodeError as e:
        print(f""ERROR: JSON parsing failed in {file_path}: {str(e)}"")
        raise",Load JSONL file with validation,"Load and parse JSONL file into a list of dictionaries, handling errors."
1356,update_difficulty,"def update_difficulty(self, dataset_name: str, method: Literal[""increment"", ""decrement""]):
        
        if method not in [""increment"", ""decrement""]:
            raise ValueError(f""Invalid method: {method}"")

        if method == ""increment"":
            self.curricula[dataset_name].increment_global_level()
        elif method == ""decrement"":
            self.curricula[dataset_name].decrement_global_level()

        config = self.curricula[dataset_name].get_global_level()
        self.composite.update_dataset_config(dataset_name, config)",Update difficulty levels based on performance metrics,Adjust dataset difficulty level based on specified method and update configuration.
1357,process_node,"def process_node(
        self, node, current_class, definitions, process_method, process_function, process_class, process_class_variable
    ):
        
        if node.type == ""function_definition"":
            if current_class:
                process_method(node, definitions[""classes""][current_class][""methods""])
            else:
                process_function(node, definitions[""functions""])
            return ""function""
        elif node.type == ""class_specifier"":
            class_name = process_class(node)
            definitions[""classes""][class_name] = {
                ""line"": (node.start_point[0] + 1, node.end_point[0] + 1),
                ""methods"": [],
                ""variables"": [],
            }
            return ""class""",Processes a node in a C++ syntax tree.,"Classifies and processes nodes as functions or classes, updating definitions accordingly."
1358,save_images,"def save_images(images_list, fold, index):
    
    image_paths = []
    image_folder = f'SAT_images_{fold}'
    os.makedirs(image_folder, exist_ok=True)
    
    for idx, im_bytes in enumerate(images_list):
        im_bytes = im_bytes.strip().encode().decode('unicode_escape').encode('raw_unicode_escape')
        image = Image.open(io.BytesIO(im_bytes))
        image_path = os.path.join(image_folder, f'{index}_{idx}.png')
        image.save(image_path)
        image_paths.append(image_path)
    
    return image_paths",Save images from byte format to PNG files.,Save and return file paths of decoded images in a specified directory.
1359,init_fallback_handler,"def init_fallback_handler(agent: RAgents, tools: List[Any]):
    
    if not get_config_repository().get(""experimental_fallback_handler"", False):
        return None
    agent_type = get_agent_type(agent)
    if agent_type == ""React"":
        # Create a dict with only the necessary config values for the FallbackHandler
        fallback_tool_model_limit = get_config_repository().get(
            ""fallback_tool_model_limit"", None
        )
        retry_fallback_count = get_config_repository().get(""retry_fallback_count"", None)
        provider = get_config_repository().get(""provider"", ""anthropic"")
        model = get_config_repository().get(""model"", """")

        config_for_fallback = {
            ""fallback_tool_model_limit"": fallback_tool_model_limit,
            ""retry_fallback_count"": retry_fallback_count,
            ""provider"": provider,
            ""model"": model,
        }

        return FallbackHandler(config_for_fallback, tools)
    return None","Initialize fallback handler if agent is of type ""React"" and experimental_fallback_handler is enabled; otherwise return None.",Initialize fallback handler for React agents using configuration settings
1360,to_domain,"def to_domain(self) -> Cluster:
        
        return Cluster(
            id=self.id,
            name=self.name,
            clusterCenter=self.center_embedding,
            memoryList=[{""memoryId"": mid} for mid in self.memory_ids],
        )",Convert to domain object,Transforms object attributes into a structured Cluster data representation.
1361,verify_outputs,"def verify_outputs(test_cases: List[str]) -> bool:
    
    normalizers = {
        ""inline"": TextNormalizerInline(),
        ""compiled"": TextNormalizerCompiled(),
        ""hybrid"": TextNormalizerHybrid(),
    }

    for test in test_cases:
        results = [norm.normalize(test) for norm in normalizers.values()]
        if not all(r == results[0] for r in results):
            return False
    return True",Verify that all implementations produce identical output,Verify consistency of text normalization across multiple strategies.
1362,process_tools,"def process_tools(tools: List[Union[Tool, Callable]]) -> List[Tool]:
    
    processed_tools: List[Tool] = []
    for tool in tools:
        if isinstance(tool, Tool) or (
            hasattr(tool, 'name') and 
            hasattr(tool, 'description') and 
            hasattr(tool, 'async_execute')
        ):
            processed_tools.append(tool)
        elif callable(tool):
            if not inspect.iscoroutinefunction(tool):
                tool_name = getattr(tool, 'name', getattr(tool, '__name__', str(tool)))
                raise ValueError(f""Callable '{tool_name}' must be an async function to be used as a tool."")
            processed_tools.append(create_tool(tool))
        else:
            raise ValueError(f""Invalid item type: {type(tool)}. Expected Tool, tool-like instance, or async function."")
    return processed_tools",Convert a list of tools or callables into Tool instances.,Validate and convert tools or async functions into a standardized tool list.
1363,perform_agent_graph_action,"def perform_agent_graph_action(
        self,
        action_name: str,
        arguments: dict[str, Any],
    ):
        
        if ""unfollow"" in action_name:
            followee_id: int | None = arguments.get(""followee_id"", None)
            if followee_id is None:
                return
            self.agent_graph.remove_edge(self.social_agent_id, followee_id)
            agent_log.info(
                f""Agent {self.social_agent_id} unfollowed Agent {followee_id}"")
        elif ""follow"" in action_name:
            followee_id: int | None = arguments.get(""followee_id"", None)
            if followee_id is None:
                return
            self.agent_graph.add_edge(self.social_agent_id, followee_id)
            agent_log.info(
                f""Agent {self.social_agent_id} followed Agent {followee_id}"")",Remove edge if action is unfollow or add edge if action is follow to the agent graph.,Handle social network follow/unfollow actions by updating agent connections.
1364,parse_output_string,"def parse_output_string(output_str, brick_lib):
    
    bricks = []
    for line in output_str.strip().split('\n'):
        if not line.strip():
            continue
        dims, coords = line.split(' ')
        h_raw, w_raw = map(int, dims.split('x'))
        x, y, z = map(int, coords.strip('()').split(','))
        if h_raw < w_raw:
            height, width = h_raw, w_raw
            ori = 0
        else:
            height, width = w_raw, h_raw
            ori = 1
        brick_id = get_brick_id_from_dimensions(height, width, brick_lib)
        brick = {
            'x': x,
            'y': y,
            'z': z,
            'ori': ori,
            'height': height,
            'width': width,
            'brick_id': brick_id
        }
        bricks.append(brick)
    return bricks",Parse the 'output' string from the new JSON format into a list of brick dicts.,Parse formatted string to extract and organize brick attributes.
1365,get_system_prompt,"def get_system_prompt() -> str:
    
    
    current_date = datetime.now().strftime(""%Y-%m-%d"")
     
    return f",Get the enhanced system prompt for the stock analysis assistant.,Returns a formatted string with the current date.
1366,process_jsonl_file_order,"def process_jsonl_file_order(input_file, output_file):
    
    with open(input_file, ""r"") as infile, open(output_file, ""w"") as outfile:
        for line in infile:
            if line.strip():  # Skip empty lines
                data = json.loads(line)
                image = data[""image""]
                original_text = data[""text""]
                num_cases = random.randint(1, 3)

                for _ in range(num_cases):
                    before_text, after_text = extract_ordered_segments(original_text)
                    if not before_text or not after_text:
                        continue
                    processed_num = random.randint(11, 16)
                    processed_id = f""{image}_processed{processed_num:02d}""
                    max_diffs = random.randint(1, 3)

                    new_case = {
                        ""pdf"": f""{image}.pdf"",
                        ""page"": 1,
                        ""id"": processed_id,
                        ""type"": ""order"",
                        ""before"": before_text,
                        ""after"": after_text,
                        ""max_diffs"": max_diffs,
                        ""checked"": ""verified"",
                    }

                    outfile.write(json.dumps(new_case) + ""\n"")",Process a JSONL file and create order-type cases.,Process JSONL file to generate ordered text segments with metadata and save to output
1367,_fetch_tp_shard_tensor_vocab,"def _fetch_tp_shard_tensor_vocab(tensor, name, chunk_dim=0, mutate_func=None) -> torch.Tensor:
        
        nonlocal state_dict
        tp_rank = mpu.get_tensor_model_parallel_rank()
        tp_size = mpu.get_tensor_model_parallel_world_size()
        if name in state_dict:
            full_weight = state_dict[name]

            if mutate_func is not None:
                full_weight = mutate_func(full_weight)
            tensor_chunk = torch.chunk(full_weight, tp_size, dim=chunk_dim)
            if tensor is not None:
                tensor = tensor.data.copy_(tensor_chunk[tp_rank], non_blocking=True)
        else:
            print(f""tp_shard tensor:[{name}] not in state_dict, skip loading"")",fetch tensor in tp shards,Load and process a tensor shard from a distributed state dictionary
1368,upload_to_huggingface,"def upload_to_huggingface(structured_emails, repo_id=""corbt/enron-emails""):
    
    print(f""Preparing dataset for upload to {repo_id}..."")

    # Convert to Hugging Face dataset directly
    dataset = Dataset.from_list(structured_emails)

    # Push to Hugging Face
    print(f""Uploading dataset to Hugging Face ({repo_id})..."")
    dataset.push_to_hub(repo_id, private=False)
",Upload the structured emails to a HuggingFace dataset,Upload structured emails to a specified Hugging Face repository.
1369,create_sample_files,"def create_sample_files():
    
    files = []
    sample_texts = [
        ""Artificial Intelligence is transforming various industries."",
        ""Machine Learning models require significant amounts of data."",
        ""Natural Language Processing enables computers to understand human language."",
        ""Computer Vision systems can detect objects in images and videos."",
        ""Reinforcement Learning is used in robotics and game-playing AI."",
    ]

    # Create temporary directory
    temp_dir = tempfile.mkdtemp()
    print(f""Created temporary directory: {temp_dir}"")

    # Create text files
    for i, text in enumerate(sample_texts):
        file_path = os.path.join(temp_dir, f""sample_{i+1}.txt"")
        with open(file_path, ""w"") as f:
            f.write(text)
        files.append(file_path)

    return temp_dir, files",Create temporary text files for demonstration,Generate temporary text files with AI-related content in a new directory.
1370,__next__,"def __next__(self):
        
        self.count += 1

        images = []
        for i, x in enumerate(self.imgs):
            # Wait until a frame is available in each buffer
            while not x:
                if not self.threads[i].is_alive() or cv2.waitKey(1) == ord(""q""):  # q to quit
                    self.close()
                    raise StopIteration
                time.sleep(1 / min(self.fps))
                x = self.imgs[i]
                if not x:
                    LOGGER.warning(f""WARNING ⚠️ Waiting for stream {i}"")

            # Get and remove the first frame from imgs buffer
            if self.buffer:
                images.append(x.pop(0))

            # Get the last frame, and clear the rest from the imgs buffer
            else:
                images.append(x.pop(-1) if x else np.zeros(self.shape[i], dtype=np.uint8))
                x.clear()

        return self.sources, images, [""""] * self.bs",Returns the next batch of frames from multiple video streams for processing.,"Iterate through image streams, handling frame retrieval and buffer management."
1371,concat_carryover,"def concat_carryover(chat_message: str, carryover_message: Union[str, list[dict[str, Any]]]) -> str:
            
            prefix = f""{chat_message}\n"" if chat_message else """"

            if isinstance(carryover_message, str):
                content = carryover_message
            elif isinstance(carryover_message, list):
                content = ""\n"".join(
                    msg[""content""] for msg in carryover_message if ""content"" in msg and msg[""content""] is not None
                )
            else:
                raise ValueError(""Carryover message must be a string or a list of dictionaries"")

            return f""{prefix}Context:\n{content}""",Concatenate the carryover message to the chat message.,Combine chat and context messages into a formatted string output.
1372,_hire_staffs,"def _hire_staffs(
        self,
        record_cost: bool,
        language_model: LLM | AsyncLLM,
        vision_model: LLM | AsyncLLM,
    ) -> dict[str, Agent]:
        
        llm_mapping = {
            ""language"": language_model,
            ""vision"": vision_model,
        }
        self.staffs = {
            role: Agent(
                role,
                record_cost=record_cost,
                text_model=self.text_embedder,
                llm_mapping=llm_mapping,
            )
            for role in [""planner""] + self.roles
        }",Initialize agent roles and their models,Initialize agents with language and vision models for various roles.
1373,get_mdx_route_for_class,"def get_mdx_route_for_class(cls_doc: ClassDoc) -> str:
    
    lower_class_name = cls_doc.title.lower()
    if lower_class_name.startswith(""py""):
        return f""codebase-sdk/python/{cls_doc.title}""
    elif lower_class_name.startswith((""ts"", ""jsx"")):
        return f""codebase-sdk/typescript/{cls_doc.title}""
    else:
        return f""codebase-sdk/core/{cls_doc.title}""","Get the expected MDX route for a class split by /core, /python, and /typescript",Determine documentation path based on class name prefix
1374,set_lang,"def set_lang(self, lang=None):
        
        if not lang:
            lang = get_system_language()
            self.log.info(f""No language specified, using system language: {lang}"")

        if lang != self.lang:
            if self.lang: self.log.info(f""Switching language from {self.lang} to: {lang}"")
            else: self.log.info(f""Setting language to: {lang}"")
            self.lang = lang
            self.load_messages()",Set the current language.,"Set or switch the application's language, defaulting to system language if unspecified."
1375,build_call_tree_per_example,"def build_call_tree_per_example(df: pd.DataFrame) -> list[ConcurrencyCallNode]:
    
    req_cols = {""example_number"", ""event_type"", ""UUID"", ""event_timestamp""}
    missing = req_cols - set(df.columns)
    if missing:
        raise ValueError(f""DataFrame missing required columns: {missing}"")

    dfc = df.copy()
    dfc.sort_values([""example_number"", ""event_timestamp""], inplace=True)

    all_roots: list[ConcurrencyCallNode] = []
    for _, grp in dfc.groupby(""example_number""):
        r = build_call_tree_for_example(grp)
        all_roots.extend(r)
    return all_roots","Groups by example_number, builds separate call trees, returns combined list of top-level calls.",Constructs concurrency call trees from DataFrame event data per example.
1376,_log_agent_run,"def _log_agent_run(self) -> None:
		
		logger.info(f'🚀 Starting task: {self.task}')

		logger.debug(f'Version: {self.version}, Source: {self.source}')
		self.telemetry.capture(
			AgentRunTelemetryEvent(
				agent_id=self.agent_id,
				use_vision=self.use_vision,
				task=self.task,
				model_name=self.model_name,
				chat_model_library=self.chat_model_library,
				version=self.version,
				source=self.source,
			)
		)",Log the agent run,Log and capture telemetry data for an agent's task execution.
1377,is_empty,"def is_empty(self) -> bool:
        
        if self._vector_store is None:
            raise WeaviateMemoryError(""Weaviate vector store not initialized."")
        try:
            count = self._vector_store.count_documents()
            return count == 0
        except Exception as e:
            logger.error(f""Error checking if Weaviate memory is empty: {e}"")
            raise WeaviateMemoryError(f""Error checking if Weaviate memory is empty: {e}"") from e",Checks if the Weaviate collection associated with this memory is empty.,Check if the Weaviate vector store is initialized and contains documents.
1378,_build_eval_prompt,"def _build_eval_prompt(
        self, original_request: str, current_response: str, iteration: int
    ) -> str:
        
        return f",Build the evaluation prompt for the evaluator,"Constructs an evaluation prompt using request, response, and iteration data."
1379,format_hotkey,"def format_hotkey(hotkey_str: str) -> str:
    
    parts = hotkey_str.lower().split(""+"")
    formatted_parts = []
    for part in parts:
        if part in [""ctrl"", ""shift"", ""alt""]:
            formatted_parts.append(f""<{part}>"")
        else:
            formatted_parts.append(part)
    return ""+"".join(formatted_parts)","Converts a hotkey string such as 'ctrl+k' to the format expected by GlobalHotKeys, for example, '+k'.",Format keyboard shortcuts by enclosing modifiers in angle brackets.
1380,reset_permissions,"def reset_permissions(
    browser_context_id: typing.Optional[BrowserContextID] = None,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    if browser_context_id is not None:
        params[""browserContextId""] = browser_context_id.to_json()
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Browser.resetPermissions"",
        ""params"": params,
    }
    json = yield cmd_dict",Reset all permission management for all origins.,Reset browser permissions for a given context ID.
1381,check_dangerous_functions,"def check_dangerous_functions(code: str) -> List[Dict[str, Any]]:
    
    dangerous_patterns = [
        'exec(',
        'eval(',
        'subprocess.',
        'os.system',
        'os.popen',
        '__import__',
        'pickle.loads',
    ]

    results = []
    lines = code.splitlines()

    for i, line in enumerate(lines):
        for pattern in dangerous_patterns:
            if pattern in line:
                results.append(
                    {
                        'function': pattern.rstrip('('),
                        'line': i + 1,
                        'code': line.strip(),
                    }
                )

    return results","Check for dangerous functions like exec, eval, etc.",Detects and reports potentially dangerous code patterns in a given script.
1382,_log_plots,"def _log_plots(experiment, trainer):
    
    plot_filenames = None
    if isinstance(trainer.validator.metrics, SegmentMetrics) and trainer.validator.metrics.task == ""segment"":
        plot_filenames = [
            trainer.save_dir / f""{prefix}{plots}.png""
            for plots in EVALUATION_PLOT_NAMES
            for prefix in SEGMENT_METRICS_PLOT_PREFIX
        ]
    elif isinstance(trainer.validator.metrics, PoseMetrics):
        plot_filenames = [
            trainer.save_dir / f""{prefix}{plots}.png""
            for plots in EVALUATION_PLOT_NAMES
            for prefix in POSE_METRICS_PLOT_PREFIX
        ]
    elif isinstance(trainer.validator.metrics, (DetMetrics, OBBMetrics)):
        plot_filenames = [trainer.save_dir / f""{plots}.png"" for plots in EVALUATION_PLOT_NAMES]

    if plot_filenames is not None:
        _log_images(experiment, plot_filenames, None)

    confusion_matrix_filenames = [trainer.save_dir / f""{plots}.png"" for plots in CONFUSION_MATRIX_PLOT_NAMES]
    _log_images(experiment, confusion_matrix_filenames, None)

    if not isinstance(trainer.validator.metrics, ClassifyMetrics):
        label_plot_filenames = [trainer.save_dir / f""{labels}.jpg"" for labels in LABEL_PLOT_NAMES]
        _log_images(experiment, label_plot_filenames, None)",Logs evaluation plots and label plots for the experiment.,Log evaluation plots and images based on metrics type for an experiment
1383,call_model,"def call_model(state: MessagesState, config: RunnableConfig) -> dict[str, BaseMessage]:
    
    system_message = ""You are a helpful AI assistant.""
    messages_with_system = [{""type"": ""system"", ""content"": system_message}] + state[
        ""messages""
    ]
    # Forward the RunnableConfig object to ensure the agent is capable of streaming the response.
    response = llm.invoke(messages_with_system, config)
    return {""messages"": response}",Calls the language model and returns the response.,Invoke AI model with system message and configuration for response
1384,trigger_playlist_check_endpoint,"def trigger_playlist_check_endpoint():
    
    watch_config = get_watch_config()
    if not watch_config.get(""enabled"", False):
        return jsonify(
            {
                ""error"": ""Watch feature is currently disabled globally. Cannot trigger check.""
            }
        ), 403

    logger.info(""Manual trigger for playlist check received for all playlists."")
    try:
        # Run check_watched_playlists without an ID to check all
        thread = threading.Thread(target=check_watched_playlists, args=(None,))
        thread.start()
        return jsonify(
            {
                ""message"": ""Playlist check triggered successfully in the background for all playlists.""
            }
        ), 202
    except Exception as e:
        logger.error(
            f""Error manually triggering playlist check for all: {e}"", exc_info=True
        )
        return jsonify(
            {""error"": f""Could not trigger playlist check for all: {str(e)}""}
        ), 500",Manually triggers the playlist checking mechanism for all watched playlists.,"Initiates a background playlist check if the watch feature is enabled, handling errors gracefully."
1385,evaluate_final_gate,"def evaluate_final_gate(gate_type: str, term_values: list) -> int:
        
        if gate_type == ""AND"":
            return 1 if all(v == 1 for v in term_values) else 0
        elif gate_type == ""OR"":
            return 1 if any(v == 1 for v in term_values) else 0
        elif gate_type == ""XOR"":
            return sum(term_values) % 2
        elif gate_type == ""NOR"":
            return 0 if any(v == 1 for v in term_values) else 1
        else:
            raise ValueError(f""Unknown gate type: {gate_type}"")",Evaluate the final gate with given term values,Determine logic gate output based on type and input values.
1386,chat_example,"def chat_example(client):
    
    print(""\n=== Chat Example ==="")

    # Single message example
    messages = [
        {""role"": ""system"", ""content"": ""You are a helpful AI assistant.""},
        {""role"": ""user"", ""content"": ""What is the capital of France?""},
    ]

    responses, usage, done_reasons = client.chat(messages)

    print(f""Response: {responses[0]}"")
    print(f""Usage: {usage}"")
    print(f""Done reason: {done_reasons[0]}"")",Example of using the chat API.,Simulates a chat interaction with an AI assistant to demonstrate response handling.
1387,convert_html_to_text,"def convert_html_to_text(self, html, simple=False):
        
        texts = self._parse_html(html).findAll(text=True)
        visible_texts = filter(tag_visible, texts)
        if simple:
            # For `simple` mode, return just [SEP] separators
            return ' [SEP] '.join(t.strip() for t in visible_texts if t != '\n')
        else:
            # Otherwise, return an observation with tags mapped to specific, unique separators
            observation = ''
            for t in visible_texts:
                if t == '\n': continue
                if t.parent.name == 'button':  # button
                    processed_t = f'[button] {t} [button_]'
                elif t.parent.name == 'label':  # options
                    if f'""{t}""' in self.state['url']:
                        processed_t = f'  [clicked button] {t} [clicked button_]'
                        observation = f'You have clicked {t}.\n' + observation
                    else:
                        processed_t = f'  [button] {t} [button_]'
                elif t.parent.get('class') == [""product-link""]: # product asins
                    if f'{t}' in self.server.user_sessions[self.session]['asins']:
                        processed_t = f'\n[clicked button] {t} [clicked button_]'
                    else:
                        processed_t = f'\n[button] {t} [button_]'
                else: # regular, unclickable text
                    processed_t =  str(t)
                observation += processed_t + '\n'
            return observation",Strip HTML of tags and add separators to convert observation into simple mode,Convert HTML content to text with optional tagging for interactive elements.
1388,chat_component,"def chat_component():
    
    st.subheader(f""{t('Welcome to ')}Vanilla RAG"")

    # Setup chat container
    his_container = st.container(height=500)
    
    with his_container:
        # Display chat history
        for (query, response) in st.session_state.simple_rag_history:
            with st.chat_message(name=""user"", avatar=""user""):
                st.markdown(query, unsafe_allow_html=True)
            with st.chat_message(name=""assistant"", avatar=""assistant""):
                st.markdown(response, unsafe_allow_html=True)
        container = st.empty()
        container_a = st.empty()
        
    # Handle user input
    chat_input_container = st.container()
    with chat_input_container:
        if query := st.chat_input(t(""Please input your question"")):
            container.chat_message(""user"").markdown(query, unsafe_allow_html=True)
            response_with_extra_info, pure_response = asyncio.run(listen(query,container_a))
            
            # Update chat history
            st.session_state.simple_rag_messages.append({""role"": ""user"", ""content"": query})
            st.session_state.simple_rag_messages.append({""role"": ""assistant"", ""content"": pure_response})
            st.session_state.simple_rag_history.append((query, response_with_extra_info))
            st.rerun()",Display the chat interface component.,Implements a chat interface with history and user input handling in a web app.
1389,mock_envs,"def mock_envs(ray_init):
    
    math_env = MockEnvironment.remote(rewards=[1.0, 2.0])
    code_env = MockEnvironment.remote(rewards=[3.0, 4.0])
    yield {""math"": math_env, ""code"": code_env}
    ray.kill(math_env)
    ray.kill(code_env)",Create mock environments for multiple task tests.,Initialize and manage mock environments with specified rewards using Ray
1390,click_element_with_text,"def click_element_with_text(text: str) -> str:
    
    print(f""🖱️ Clicking element with text: '{text}'"")  # Added print statement

    try:
        element.click()
        return f""Clicked element with text: {text}""
    except selenium.common.exceptions.NoSuchElementException:
        return ""Element not found, cannot click.""
    except selenium.common.exceptions.ElementNotInteractableException:
        return ""Element not interactable, cannot click.""
    except selenium.common.exceptions.ElementClickInterceptedException:
        return ""Element click intercepted, cannot click.""",Clicks on an element on the page with the given text.,"Handles clicking a web element by text, with error handling for common exceptions."
1391,_make_request,"def _make_request(self, method: str, url: str, **kwargs) -> Any:
        
        headers = {
            ""Content-Type"": ""application/json"",
            ""x-api-key"": self.api_key
        }
        kwargs['headers'] = headers

        for attempt in range(3):
            try:
                response = requests.request(method, url, timeout=10, **kwargs)
                if response.status_code == 429:  # Rate limit
                    retry_after = int(response.headers.get('Retry-After', 60))
                    logger.warning(f""Rate limit hit, waiting {retry_after}s"")
                    time.sleep(retry_after)
                    continue
                response.raise_for_status()
                return response.json()
            except requests.Timeout:
                logger.error(f""Timeout on attempt {attempt + 1}"")
                time.sleep(2 ** attempt)  # Exponential backoff
            except requests.RequestException as e:
                if attempt == 2:
                    raise EchochambersAPIError(f""Failed after 3 attempts: {str(e)}"")
                logger.warning(f""Attempt {attempt + 1} failed: {str(e)}"")
                time.sleep(2 ** attempt)",Make HTTP request with retries and error handling,"Perform an HTTP request with retries, handling rate limits and timeouts."
1392,_format_date_property,"def _format_date_property(self, value: dict) -> str:
        
        if value and value.get(""start""):
            start = value[""start""]
            end = value.get(""end"")
            return f""{start} - {end}"" if end else start
        return """"",Format date property values.,Formats date range from dictionary with start and optional end dates.
1393,reset,"def reset(self, session=None, instruction_text=None):
        
        session_int = None
        if session is not None:
            self.session = str(session)
            if isinstance(session, int):
                session_int = session
        else:
            self.session = """".join(random.choices(string.ascii_lowercase, k=10))
        if self.session_prefix is not None:
            self.session = self.session_prefix + self.session

        init_url = f""{self.base_url}/{self.session}""
        self.browser.get(init_url, session_id=self.session, session_int=session_int)

        self.text_to_clickable = None
        self.instruction_text = (
            self.get_instruction_text()
            if instruction_text is None
            else instruction_text
        )
        obs = self.observation
        self.prev_obs = [obs]
        self.prev_actions = []
        return obs, None",Create a new session and reset environment variables,Initialize or reset a session with optional parameters and return initial observation.
1394,get_account_info,"def get_account_info(self):
        
        return {
            ""email"": self.generate_email(),
            ""password"": self.default_password,
            ""first_name"": self.default_first_name,
            ""last_name"": self.default_last_name,
        }",Get complete account information,Generate and return a dictionary with user account details.
1395,get_part_working_dir,"def get_part_working_dir(self, part_index: int) -> Path:
        
        if part_index not in self._part_working_dirs:
            if self.working_dir:
                part_dir = Path(self.working_dir) / f""part_{part_index}""
            else:
                part_dir = Path(tempfile.mkdtemp()) / f""part_{part_index}""
            part_dir.mkdir(parents=True, exist_ok=True)
            self._part_working_dirs[part_index] = part_dir
        return self._part_working_dirs[part_index]",Get working directory for a specific part,Ensure directory exists for a specific part index and return its path.
1396,initialize_weights,"def initialize_weights(model):
    
    for m in model.modules():
        t = type(m)
        if t is nn.Conv2d:
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif t is nn.BatchNorm2d:
            m.eps = 1e-3
            m.momentum = 0.03
        elif t in {nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU}:
            m.inplace = True",Initialize model weights to random values.,Initialize neural network layers with specific configurations for optimal performance.
1397,get_all_env_vars,"def get_all_env_vars(self) -> Dict[str, str]:
        
        env_vars = {}
        for module_name, module_config in self.config[""logging""].get(""modules"", {}).items():
            if ""env_var"" in module_config:
                env_var = module_config[""env_var""]
                env_vars[env_var] = os.getenv(env_var, module_config.get(""default_level"", ""INFO""))
        return env_vars",Get all environment variables and their current values,Extract environment variables for logging modules with defaults.
1398,skip_if_no_db,"def skip_if_no_db():
    
    # Use a test database URL, trying the environment first, then fallback to localhost
    test_db_url = os.environ.get(
    )

    # Parse the URL to get the connection details
    creds = parts[0].split("":"")
    host_parts = parts[1].split(""/"")
    host_port = host_parts[0].split("":"")
    db_name = host_parts[1] if len(host_parts) > 1 else ""postgres""

    user = creds[0]
    password = creds[1] if len(creds) > 1 else """"
    host = host_port[0]
    port = int(host_port[1]) if len(host_port) > 1 else 5432

    try:
        # For simplicity, use a simple sync check rather than async in this fixture
        import psycopg2

        psycopg2.connect(dbname=db_name, user=user, password=password, host=host, port=port).close()

        # If we get here, connection was successful
        return True
    except Exception as e:
        pytest.skip(f""Database connection not available: {e}"")",Skip tests that require a database if connection is not available.,Check database connectivity and skip tests if unavailable.
1399,_process_image,"def _process_image(self, image_path: str) -> Optional[str]:
        
        try:
            if not os.path.exists(image_path):
                self.logger.error(f""Image file not found: {image_path}"")
                return None

            # Get the MIME type
            mime_type, _ = mimetypes.guess_type(image_path)
            if not mime_type:
                mime_type = ""application/octet-stream""  # Default MIME type

            # Read and encode the image
            with open(image_path, ""rb"") as img_file:
                img_data = base64.b64encode(img_file.read()).decode(""utf-8"")

            self.logger.info(f""✅ Image processed successfully: {image_path}"")
            return img_data

        except Exception as e:
            self.logger.error(f""❌ Error processing image: {str(e)}"")
            return None",Process an image file and return a data URL or None if processing fails,"Process and encode image to base64, logging success or errors."
1400,initialize_islands,"def initialize_islands(self, initial_programs: List[Program]) -> None:
        
        if settings.DEBUG:
            logger.debug(f""Initializing {self.num_islands} islands with {programs_per_island} programs each"")
        
        for i in range(self.num_islands):
            start_idx = i * programs_per_island
            end_idx = start_idx + programs_per_island if i < self.num_islands - 1 else len(initial_programs)
            island_programs = initial_programs[start_idx:end_idx]
            self.islands[i] = Island(i, island_programs)
            if settings.DEBUG:
                logger.debug(f""Initialized Island {i} with {len(island_programs)} programs"")",Initialize islands with the initial population.,"Distribute programs across islands, logging initialization details if debugging is enabled."
1401,add,"def add(self, message: Message) -> None:
        
        try:
            query = self.INSERT_MESSAGE_QUERY.format(index_name=self.index_name)
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                message_id = str(uuid.uuid4())
                cursor.execute(
                    query,
                    (
                        message_id,
                        message.role.value,
                        message.content,
                        json.dumps(message.metadata),
                        message.metadata.get(""timestamp"", 0),
                    ),
                )
                conn.commit()

        except sqlite3.Error as e:
            raise SQLiteError(f""Error adding message to database: {e}"") from e",Stores a message in the SQLite database.,Insert a message into a SQLite database with error handling for database operations.
1402,_get_items_to_index_for_files,"def _get_items_to_index_for_files(self, files: list[File]) -> list[tuple[str, str]]:
        
        items_to_index = []

        # Filter out binary files and files without content
        files_to_process = []
        for f in files:
            try:
                if f.content:  # This will raise ValueError for binary files
                    files_to_process.append(f)
            except ValueError:
                logger.debug(f""Skipping binary file: {f.filepath}"")

        if len(files) == 1:
            logger.info(f""Processing file: {files[0].filepath}"")
        else:
            logger.info(f""Found {len(files_to_process)} indexable files out of {len(files)} total files"")

        # Collect all chunks that need to be processed
        for file in files_to_process:
            chunks = self._split_by_tokens(file.content)
            if len(chunks) == 1:
                items_to_index.append((file.filepath, file.content))
            else:
                # For multi-chunk files, create virtual items
                for i, chunk in enumerate(chunks):
                    chunk_id = f""{file.filepath}#chunk{i}""
                    items_to_index.append((chunk_id, chunk))

        if items_to_index:
            logger.info(f""Total chunks to process: {len(items_to_index)}"")
        return items_to_index",Get items to index for specific files.,"Filter and index text content from files, handling binary files and chunking large texts."
1403,_format_data_for_client,"def _format_data_for_client(self, data: Dict[str, Any]) -> Dict[str, Any]:
        
        if isinstance(data, dict):
            return {k: str(v) if isinstance(v, datetime | bytes) else v for k, v in data.items()}
        return data",Format data for client consumption.,Convert specific data types to strings for client compatibility.
1404,basic_server,"def basic_server(basic_server_port: int) -> Generator[None, None, None]:
    
    proc = multiprocessing.Process(
        target=run_server, kwargs={""port"": basic_server_port}, daemon=True
    )
    proc.start()

    # Wait for server to be running
    max_attempts = 20
    attempt = 0
    while attempt < max_attempts:
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.connect((""127.0.0.1"", basic_server_port))
                break
        except ConnectionRefusedError:
            time.sleep(0.1)
            attempt += 1
    else:
        raise RuntimeError(f""Server failed to start after {max_attempts} attempts"")

    yield

    # Clean up
    proc.kill()
    proc.join(timeout=2)",Start a basic server.,"Launches and monitors a server process, ensuring it starts successfully."
1405,_ensure_single_record,"def _ensure_single_record(self):
        
        # This is a safety measure to ensure we only have one record
        # In normal operation, this should never be needed
        count = self.repository.count()
        if count != 1:
            # If we have more than one record, we need to clean up
            # This is a rare case that should not happen in normal operation
            # Implementation would depend on how we want to handle this case
            # For now, we'll just log a warning
            from lpm_kernel.common.logging import logger
            logger.warning(f""Found {count} LLM configurations in the database. Only one should exist."")",Ensure that only one configuration record exists in the database,"Ensure only one configuration record exists, logging a warning if multiple are found."
1406,_current_file_path_and_line,"def _current_file_path_and_line() -> str:
  
  current_frame = inspect.currentframe()
  if (
      current_frame is not None
      and current_frame.f_back is not None
      and current_frame.f_back.f_back is not None
  ):
    frame = current_frame.f_back.f_back
    filepath = inspect.getfile(frame)
    lineno = frame.f_lineno
    return f'File: {filepath}, Line: {lineno}'
  return ''",Prints the current file path and line number.,Retrieve the file path and line number of the caller's caller.
1407,create_collection,"def create_collection(self, name, description=""""):
        
        if not name:
            raise ValueError(""Collection name is required to create a collection."")

        try:
            new_collection = self.conn.create_collection(name, description)
            return {
                ""success"": True,
                ""message"": f""Collection '{new_collection.id}' created successfully"",
                ""collection"": {
                    ""id"": new_collection.id,
                    ""name"": new_collection.name,
                    ""description"": new_collection.description,
                },
            }
        except Exception as e:
            logging.error(f""Failed to create collection '{name}': {e}"")
            raise Exception(f""Failed to create collection '{name}': {str(e)}"") from e",Create a new collection with the given name and description.,Create a new collection with validation and error handling
1408,prompt_description,"def prompt_description(self) -> str:
		
		skip_keys = ['title']
		s = f'{self.description}: \n'
		s += '{' + str(self.name) + ': '
		s += str(
			{
				k: {sub_k: sub_v for sub_k, sub_v in v.items() if sub_k not in skip_keys}
				for k, v in self.param_model.schema()['properties'].items()
			}
		)
		s += '}'
		return s",Get a description of the action for the prompt,Generate a formatted string excluding specific keys from a schema dictionary.
1409,_validate_chat_history,"def _validate_chat_history(
    messages: Sequence[BaseMessage],
) -> None:
    
    all_tool_calls = [
        tool_call
        for message in messages
        if isinstance(message, AIMessage)
        for tool_call in message.tool_calls
    ]
    tool_call_ids_with_results = {
        message.tool_call_id for message in messages if isinstance(message, ToolMessage)
    }
    tool_calls_without_results = [
        tool_call
        for tool_call in all_tool_calls
        if tool_call[""id""] not in tool_call_ids_with_results
    ]
    if not tool_calls_without_results:
        return

    error_message = create_error_message(
        message=""Found AIMessages with tool_calls that do not have a corresponding ToolMessage. ""
        f""Here are the first few of those tool calls: {tool_calls_without_results[:3]}.\n\n""
        ""Every tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage ""
        ""(result of a tool invocation to return to the LLM) - this is required by most LLM providers."",
        error_code=ErrorCode.INVALID_CHAT_HISTORY,
    )
    raise ValueError(error_message)",Validate that all tool calls in AIMessages have a corresponding ToolMessage.,"Ensure all AI tool calls in chat history have corresponding results, raising an error if not."
1410,generate_patch,"def generate_patch(original_code: str, fixed_code: str, file_path: str) -> str:
    
    from difflib import unified_diff

    # Split both code versions into lines
    original_lines = original_code.splitlines(keepends=True)
    fixed_lines = fixed_code.splitlines(keepends=True)

    # Generate unified diff
    patch_lines = list(
        unified_diff(original_lines, fixed_lines, fromfile=f'a/{file_path}', tofile=f'b/{file_path}', lineterm=''))

    return ''.join(patch_lines)",Generate a unified diff patch from original and fixed code.,Generate a unified diff patch between original and fixed code versions.
1411,save_dataset_cache_file,"def save_dataset_cache_file(prefix, path, x, version):
    
    x[""version""] = version  # add cache version
    if is_dir_writeable(path.parent):
        if path.exists():
            path.unlink()  # remove *.cache file if exists
        np.save(str(path), x)  # save cache for next time
        path.with_suffix("".cache.npy"").rename(path)  # remove .npy suffix
        LOGGER.info(f""{prefix}New cache created: {path}"")
    else:
        LOGGER.warning(f""{prefix}WARNING ⚠️ Cache directory {path.parent} is not writeable, cache not saved."")",Save an Ultralytics dataset *.cache dictionary x to path.,Save dataset cache file with versioning and handle directory permissions.
1412,exec,"def exec(self, event_data):
        
        try:
            event = create_event(
                summary=event_data['summary'],
                description=event_data['description'],
                start_time=event_data['start_time'],
                end_time=event_data['end_time']
            )
            return {'success': True, 'event': event}
        except Exception as e:
            return {'success': False, 'error': str(e)}",Creates a new calendar event.,"Attempt to create and return an event object from provided data, handling errors."
1413,right_foot_height,"def right_foot_height(envstate, robot_name: str):
    
    # return envstate[f""{_METASIM_SITE_PREFIX}right_foot""][""pos""][2] # Only for mujoco
    return envstate[""robots""][robot_name][""body""][""right_ankle_link""][""pos""][2]",Returns the height of the right foot.,Retrieve the vertical position of a robot's right foot from environment data.
1414,change_username,"def change_username(current_username: str, new_username: str, password: str) -> bool:
    
    user_data = get_user_data()
    
    # Verify current username and password
    current_username_hash = hash_username(current_username)
    if user_data.get(""username"") != current_username_hash:
        logger.warning(f""Username change failed: Current username '{current_username}' does not match stored hash."")
        return False
    
    if not verify_password(user_data.get(""password"", """"), password):
        logger.warning(f""Username change failed for '{current_username}': Invalid password provided."")
        return False
    
    # Update username
    user_data[""username""] = hash_username(new_username)
    if save_user_data(user_data):
        logger.info(f""Username changed successfully from '{current_username}' to '{new_username}'."")
        return True
    else:
        logger.error(f""Failed to save user data after changing username for '{current_username}'."")
        return False",Change the username for the current user,Function securely updates a user's username after verifying credentials and saves changes.
1415,_init_api_clients,"def _init_api_clients(self):
        
        try:
            # News API
            self.news_api = NewsApiClient(api_key=os.getenv('NEWS_API_KEY'))
            
            # Alpha Vantage
            self.alpha_vantage_fd = FundamentalData(key=os.getenv('ALPHA_VANTAGE_KEY'))
            self.alpha_vantage_ts = TimeSeries(key=os.getenv('ALPHA_VANTAGE_KEY'))
            
            # Twitter API
            auth = tweepy.OAuthHandler(
                os.getenv('TWITTER_API_KEY'),
                os.getenv('TWITTER_API_SECRET')
            )
            auth.set_access_token(
                os.getenv('TWITTER_ACCESS_TOKEN'),
                os.getenv('TWITTER_ACCESS_TOKEN_SECRET')
            )
            self.twitter_api = tweepy.API(auth)
            
            # SerpAPI
            self.serp_api = GoogleSearch({
                ""api_key"": os.getenv('SERPAPI_KEY')
            })
            
            # CCXT
            self.exchange = ccxt.binance()
            
        except Exception as e:
            logger.error(f""Error initializing API clients: {e}"")
            raise",Initialize various API clients.,"Initialize various API clients for news, finance, social media, and search services."
1416,workflow,"def workflow(
    directory: Path = typer.Argument(
        Path("".""),
        help=""Directory where workflow examples will be created"",
    ),
    force: bool = typer.Option(False, ""--force"", ""-f"", help=""Force overwrite existing files""),
) -> None:
    
    target_dir = directory.resolve()
    if not target_dir.exists():
        target_dir.mkdir(parents=True)
        console.print(f""Created directory: {target_dir}"")

    created = copy_example_files(""workflow"", target_dir, force)
    _show_completion_message(""workflow"", created)",Create workflow pattern examples.,"Create workflow examples in a specified directory, optionally overwriting existing files."
1417,_update_active_tools,"def _update_active_tools(self) -> None:
        
        excluded_tool_classes: set[type[Tool]] = set()
        # modes
        for mode in self._modes:
            excluded_tool_classes.update(mode.get_excluded_tool_classes())
        # context
        excluded_tool_classes.update(self._context.get_excluded_tool_classes())
        # project config
        if self._active_project is not None:
            excluded_tool_classes.update(self._active_project.project_config.get_excluded_tool_classes())
            if self._active_project.project_config.read_only:
                for tool_class in self._all_tools:
                    if tool_class.can_edit():
                        excluded_tool_classes.add(tool_class)

        self._active_tools = {
            tool_class: tool_instance for tool_class, tool_instance in self._all_tools.items() if tool_class not in excluded_tool_classes
        }

        log.info(f""Active tools after all exclusions ({len(self._active_tools)}): {', '.join(self.get_active_tool_names())}"")","Update the active tools based on context, modes, and project configuration.","Determine active tools by excluding based on modes, context, and project settings."
1418,create_agent,"def create_agent(model_name: str = None, agent_name: str = ""Assistant"", system_prompt: str = None):
    
    model = OpenAIChat(model_name=model_name) if model_name else OpenAIChat()
    
    return Agent(
        agent_name=agent_name,
        system_prompt=system_prompt or ""You are a helpful AI assistant."",
        llm=model,
        max_loops=1,
        autosave=False,
        dashboard=False,
        verbose=True,
        dynamic_temperature_enabled=True,
        streaming_on=True,
        saved_state_path=f""{agent_name.lower()}_state.json"",
        user_name=""gradio_user"",
        retry_attempts=1,
        context_length=200000,
        return_step_meta=False,
        output_type=""string""
    )",Create a Swarms Agent with the specified model and configuration,Creates a customizable AI assistant agent with specified model and settings.
1419,benchmark_status,"def benchmark_status(job_id):
    
    if job_id not in running_jobs:
        return jsonify({""error"": ""Job not found""}), 404

    job = running_jobs[job_id]

    # Calculate runtime if job is running
    if job[""status""] == ""running"":
        job[""runtime""] = import_time().time() - job[""start_time""]

    return jsonify(job)",Get status of a benchmark job.,"Check job status and runtime, return JSON response"
1420,status,"def status(
    verbose: bool = typer.Option(False, ""--verbose"", ""-v"", help=""Show detailed file information""),
):
    
    try:
        asyncio.run(run_status(verbose))  # pragma: no cover
    except Exception as e:
        logger.error(f""Error checking status: {e}"")
        typer.echo(f""Error checking status: {e}"", err=True)
        raise typer.Exit(code=1)",Show sync status between files and database.,Execute asynchronous status check with optional verbosity and error handling.
1421,conditioning_pixels,"def conditioning_pixels(
    filepaths,
    training_filepaths,
    conditioning_data_backend_id: str,
    training_data_backend_id: str,
):
    
    try:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            pixels = list(
                executor.map(
                    fetch_conditioning_pixel_values,
                    filepaths,
                    training_filepaths,
                    [conditioning_data_backend_id] * len(filepaths),
                    [training_data_backend_id] * len(filepaths),
                )
            )
    except Exception as e:
        logger.error(
            f""(conditioning_data_backend_id={conditioning_data_backend_id}) Error while retrieving or transforming pixels (training data id={training_data_backend_id}): {e}""
        )
        raise
    pixels = torch.stack(pixels)
    pixels = pixels.to(memory_format=torch.contiguous_format).float()

    return pixels",For pixel-based conditioning images that must be prepared matching a paired image's metadata..,Parallel fetch and transform pixel data from files using specified backend IDs.
1422,call_tool,"def call_tool(self, tool_json: str, **kwargs: Any) -> str:
        
        try:
            command = json.loads(tool_json)
            if not isinstance(command, dict):
                return ""Error: Tool command must be a JSON object, e.g. '{\""name\"": \""tool_name\"", \""args\"": {\""arg1\"": \""value1\"", \""arg2\"": \""value2\""}}'""
            
            tool_name = command.get(""name"")
            if not tool_name:
                return ""Error: Tool command must specify 'name', e.g. '{\""name\"": \""tool_name\"", \""args\"": {\""arg1\"": \""value1\"", \""arg2\"": \""value2\""}}'""
            
            if tool_name not in self.tools:
                return f""Error: Unknown tool '{tool_name}'. "" + ""Please format your tool call as '{\""name\"": \""tool_name\"", \""args\"": {\""arg1\"": \""value1\"", \""arg2\"": \""value2\""}}'""
            
            tool = self.tools[tool_name]
            tool_args = command.get(""args"", {})
            if isinstance(tool_args, str):
                return f""Error: Arguments for {tool_name} must be a JSON object matching the tool's input schema, not a string."" 
            
            # Call the tool object with arguments
            result = tool(**tool_args)
            return str(result)
        except json.JSONDecodeError:
            return ""Error: Invalid JSON format. Please format your tool call as '{\""name\"": \""tool_name\"", \""args\"": {\""arg1\"": \""value1\"", \""arg2\"": \""value2\""}}'""
        except Exception as e:
            return f""Error: {str(e)}. "" + ""Please format your tool call as '{\""name\"": \""tool_name\"", \""args\"": {\""arg1\"": \""value1\"", \""arg2\"": \""value2\""}}'""",Call a SmolaAgents Tool object based on JSON command.,"Validate and execute tool commands from JSON input, handling errors and returning results."
1423,_parse_rate_limit,"def _parse_rate_limit(self, rate_limit: str) -> Tuple[int, int]:
        
        if not rate_limit or rate_limit == ""0"":
            return 0, 0

        # Try compound format first (e.g., ""1/10s"")
        compound_pattern = r""^(\d+)/(\d+)([smhd])$""
        compound_match = re.match(compound_pattern, rate_limit)

        if compound_match:
            requests = int(compound_match.group(1))
            multiplier = int(compound_match.group(2))
            unit = compound_match.group(3)
            return requests, multiplier * self.TIME_UNITS[unit]

        # Simple format (e.g., ""100/m"")
        simple_pattern = r""^(\d+)/([smhd])$""
        simple_match = re.match(simple_pattern, rate_limit)

        if simple_match:
            requests = int(simple_match.group(1))
            unit = simple_match.group(2)
            return requests, self.TIME_UNITS[unit]

        logger.warning(f""Invalid rate limit format: {rate_limit}"")
        return 0, 0",Parse rate limit string into numeric values.,Parse and convert rate limit strings into request and time interval values.
1424,_extract_tool_calls_from_chunk,"def _extract_tool_calls_from_chunk(self, chunk: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        
        try:
            if isinstance(chunk, dict):
                # Direct tool_calls field
                if ""tool_calls"" in chunk:
                    return chunk[""tool_calls""]
                
                # OpenAI-style delta format
                if ""choices"" in chunk and chunk[""choices""]:
                    choice = chunk[""choices""][0]
                    if ""delta"" in choice:
                        delta = choice[""delta""]
                        if ""tool_calls"" in delta:
                            return delta[""tool_calls""]
                        # Sometimes tool_calls come in function_call format
                        if ""function_call"" in delta:
                            return {""function_call"": delta[""function_call""]}
                
                # Alternative formats
                if ""function_call"" in chunk:
                    return {""function_call"": chunk[""function_call""]}
                    
        except Exception as e:
            logger.debug(f""Error extracting tool calls from chunk: {e}"")
            
        return None",Extract tool call data from a streaming chunk.,Extracts tool call data from various structured input formats
1425,_extract_template_variables,"def _extract_template_variables(self, text: str) -> Set[str]:
        
        variable_pattern = r""{{([^}]+)}}""
        matches = re.findall(variable_pattern, text)
        return set(matches)",Extract template variables from text using regex,Extracts unique placeholders from text using regex pattern matching.
1426,serialize_attribute,"def serialize_attribute(key: str, value: Any) -> Dict[str, Any]:
    
    serialized = {}

    if is_otel_serializable(value):
        serialized[key] = value

    elif isinstance(value, dict):
        for sub_key, sub_value in value.items():
            serialized.update(serialize_attribute(f""{key}.{sub_key}"", sub_value))

    elif isinstance(value, Callable):
        serialized[f""{key}_callable_name""] = getattr(value, ""__qualname__"", str(value))
        serialized[f""{key}_callable_module""] = getattr(value, ""__module__"", ""unknown"")
        serialized[f""{key}_is_coroutine""] = asyncio.iscoroutinefunction(value)

    elif inspect.iscoroutine(value):
        serialized[f""{key}_coroutine""] = str(value)
        serialized[f""{key}_is_coroutine""] = True

    return serialized",Serialize a single attribute value into a flat dict of OpenTelemetry-compatible values.,Serialize complex attributes into a structured dictionary format.
1427,prepare_openai_json_schema,"def prepare_openai_json_schema(schema: dict[str, Any]) -> dict[str, Any]:
    
    schema = deepcopy(schema)
    defs = schema.get(""$defs"", {})

    _sanitize_json_schema_inner(schema, in_defs=False, defs=defs)

    # Remove orphan definitions in $defs that are not referenced in the non-$defs part of the schema
    if ""$defs"" in schema:
        non_defs = {k: schema[k] for k in schema if k != ""$defs""}
        top_level_refs = _collect_refs(non_defs)
        schema[""$defs""] = {k: v for k, v in schema[""$defs""].items() if k in top_level_refs}

    return schema",Sanitize and prepare a JSON schema for OpenAI outputs by removing unsupported keys and adjusting types.,Refine JSON schema by sanitizing and removing unused definitions.
1428,print_multiturn_stats_by_provider,"def print_multiturn_stats_by_provider(multiturn_results):
    
    from collections import defaultdict
    provider_times = defaultdict(list)
    for r in multiturn_results:
        if r['turn'] != 'ALL' and r['elapsed'] is not None:
            provider = r['provider']
            provider_times[provider].append(r['elapsed'])
    print(""\n=== Multi-turn (per turn) Timing Stats by Provider ==="")
    for provider, times in provider_times.items():
        if not times:
            print(f""Provider: {provider} - No successful timings."")
            continue
        print(f""Provider: {provider}"")
        print(f""  Count: {len(times)}"")
        print(f""  Min: {min(times):.3f} s"")
        print(f""  Max: {max(times):.3f} s"")
        print(f""  Mean: {statistics.mean(times):.3f} s"")
        print(f""  Median: {statistics.median(times):.3f} s"")
        print(f""  Stddev: {statistics.stdev(times):.3f} s"" if len(times) > 1 else ""  Stddev: N/A"")",Print timing stats for multi-turn results by provider.,Summarizes timing statistics for each provider from multi-turn results.
1429,verify,"def verify(self, data):
        
        # batched scoring
        prompt_ids = data.batch[""prompts""]

        response_ids = data.batch[""responses""]
        sequences_str = self.tokenizer.batch_decode(response_ids, skip_special_tokens=True)
        ground_truth = [data_item.non_tensor_batch[""reward_model""][""ground_truth""] for data_item in data]
        data_sources = data.non_tensor_batch[self.reward_fn_key]
        extra_info = data.non_tensor_batch.get(""extra_info"", None)

        assert len(sequences_str) == len(ground_truth) == len(data_sources)
        try:
            scores = run_reward_scoring(
                self.compute_score,
                completions=sequences_str,
                references=ground_truth,
                tasks=data_sources,
                extra_info=extra_info,
                num_processes=64,
            )
        except asyncio.TimeoutError:
            print(""[Timeout] Global reward scoring timed out. Setting all as 0."")
            scores = [0.0 for _ in range(len(sequences_str))]
        except Exception as e:
            print(f""[Error] Unexpected error during scoring. Setting all as 0. {e}"")
            scores = [0.0 for _ in range(len(sequences_str))]
        data.batch[""acc""] = torch.tensor(scores, dtype=torch.float32, device=prompt_ids.device)
        return scores",verify the batch and save as ``acc`` tensor,Evaluate and score model-generated responses against ground truth using a reward function.
1430,cfg2task,"def cfg2task(cfg):
        
        m = cfg[""head""][-1][-2].lower()  # output module name
        if m in {""classify"", ""classifier"", ""cls"", ""fc""}:
            return ""classify""
        if ""detect"" in m:
            return ""detect""
        if m == ""segment"":
            return ""segment""
        if m == ""pose"":
            return ""pose""
        if m == ""obb"":
            return ""obb""",Guess from YAML dictionary.,Map configuration to task type based on module name.
1431,update_session_status,"def update_session_status(self, session_id: int, status: str) -> SessionModel:
        
        try:
            session = Session.get_by_id(session_id)
            session.status = status
            session.save()
            logger.info(f""Updated session {session_id} status to '{status}'"")
            # Fetch the updated session again to ensure consistency and include display_name
            updated_session_model = self.get(session_id)
            if updated_session_model is None:
                 # Should ideally not happen if we just updated it, but handle defensively
                 raise SessionNotFoundError(f""Session with ID {session_id} not found after update."")
            return updated_session_model
        except Session.DoesNotExist:
            logger.error(f""Attempted to update status for non-existent session ID {session_id}"")
            raise SessionNotFoundError(f""Session with ID {session_id} not found."")
        except peewee.DatabaseError as e:
            logger.error(f""Database error updating session {session_id} status: {str(e)}"")
            raise",Updates the status of a specific session.,Update session status in database and handle potential errors
1432,_handle_command,"def _handle_command(self, input_string: str) -> None:
        
        try:
            input_list = shlex.split(input_string)
        except ValueError as e:
            logger.error(f""Error parsing command: {e}"")
            return

        command_string = input_list[0].lower()

        try:
            command = self.commands.get(command_string)
            if command:
                command.handler(input_list)
            else:
                self._handle_unknown_command(command_string)
        except Exception as e:
            logger.error(f""Error executing command: {e}"")",Parse and handle a command input,"Parse and execute commands from input strings, handling errors gracefully."
1433,get_model_input_tokens,"def get_model_input_tokens(model):
    
    model_tokens = {
        ""gpt"": 128000,
        ""o1"": 200000,
        ""claude"": 200000,
    }
    for model_type, tokens in model_tokens.items():
        if model_type in model:
            return tokens
    return model_tokens[""gpt""]",Get the number of input tokens for max context window capacity for a given model.,Determine token limit based on model type identifier
1434,_get_comments,"def _get_comments(self, comments_url) -> List[GitHubIssueComment]:
        
        try:
            response = requests.get(
                comments_url,
                headers={
                    ""Authorization"": f""Bearer {self.access_token}"",
                    ""X-GitHub-Api-Version"": ""2022-11-28"",
                },
            )
        except requests.exceptions.ConnectTimeout:
            logging.warn(f""Timeout fetching comments from {comments_url}"")
            return []
        comments = []
        for comment in response.json():
            comments.append(
                GitHubIssueComment(
                    url=comment[""url""],
                    html_url=comment[""html_url""],
                    body=comment[""body""],
                )
            )
        return comments",Downloads all the comments associated with an issue; returns an empty list if the request times out.,Fetch and parse GitHub issue comments using an API request with authentication.
1435,get_headers_dict,"def get_headers_dict(self, complete_request) -> Dict[str, str]:
        
        headers_dict = {}
        try:
            if b""\r\n\r\n"" not in complete_request:
                return {}

            headers_end = complete_request.index(b""\r\n\r\n"")
            headers = complete_request[:headers_end].split(b""\r\n"")[1:]

            for header in headers:
                try:
                    name, value = header.decode(""utf-8"").split("":"", 1)
                    headers_dict[name.strip().lower()] = value.strip()
                    if name == ""user-agent"":
                        logger.debug(f""User-Agent header received: {value} from {self.peername}"")
                except ValueError:
                    continue

            return headers_dict
        except Exception as e:
            logger.error(f""Error parsing headers: {e}"")
            return {}",Convert raw headers to dictionary format,Extract and parse HTTP headers from a request into a dictionary.
1436,alert,"def alert(message: str, level: str = ""info"", size: float = 1.0, component_id: str | None = None, **kwargs) -> ComponentReturn:
    

    logger.debug(f""Creating alert component with id {component_id}, message: {message}"")

    component = {
        ""type"": ""alert"",
        ""id"": component_id,
        ""message"": message,
        ""level"": level,
        ""size"": size,
    }

    return ComponentReturn(message, component)",Create an alert component.,"Create an alert component with customizable message, level, and size"
1437,to_simplified_dict,"def to_simplified_dict(self) -> dict[str, Any]:
        
        result = {
            ""filename"": self.filename,
            ""size"": self.size,
            ""url"": self.url,
        }

        if self.content_type:
            result[""content_type""] = self.content_type

        if self.author:
            result[""author""] = self.author.to_simplified_dict()

        if self.thumbnail_url:
            result[""thumbnail_url""] = self.thumbnail_url

        if self.created:
            result[""created""] = self.created

        return result",Convert to simplified dictionary for API response.,Converts object attributes into a simplified dictionary format.
1438,_find_high_entropy_matches,"def _find_high_entropy_matches(cls, line: str, line_num: int, found_values: set) -> List[Match]:
        
        matches = []
        assignment_pattern = re.findall(
            r""([A-Za-z_][A-Za-z0-9_]*)\s*=\s*([\""']?([A-Za-z0-9_\-\.+/=]{8,})[\""']?)"", line
        )

        for key, _, word in assignment_pattern:
            pattern = f""{key}:{word}""
            if pattern in found_values or word.startswith(""REDACTED""):
                continue
            if cls._calculate_entropy(word) >= cls.HIGH_ENTROPY_THRESHOLD:
                found_values.add(pattern)
                matches.append(
                    Match(
                        ""High Entropy"",
                        ""Potential Secret"",
                        key,
                        word,
                        line_num,
                        line.find(word),
                        line.find(word) + len(word),
                    )
                )

        return matches",Find matches based on high entropy values.,Identify high-entropy potential secrets in code lines and return matches.
1439,forward,"def forward(self, x):
        
        N, C, H, W = x.size()
        g = self.groups
        assert C % g == 0, ""Incompatible group size {} for input channel {}"".format(
            g, C
        )
        return (
            x.view(N, g, int(C / g), H, W)
            .permute(0, 2, 1, 3, 4)
            .contiguous()
            .view(N, C, H, W)
        )","Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]",Reshape and permute input tensor for grouped processing in neural network layer.
1440,format_model_status,"def format_model_status(
    status: Dict[str, Any], config: QueryPipelineConfig
) -> Optional[Dict[str, Any]]:
    
    model = status.get(""model"", ""unknown"")
    status_type = status.get(""status"")

    if status_type == ""pulling"":
        content = f""Starting to download model {model}...""
    elif status_type == ""progress"":
        percentage = status.get(""percentage"", 0)
        content = f""Downloading model {model}: `{percentage}%` complete""
    elif status_type == ""complete"":
        content = f""Successfully downloaded model {model}""
    elif status_type == ""error"" and ""pull"" in status.get(""error"", """").lower():
        error_msg = status.get(""error"", ""Unknown error"")
        content = f""Error downloading model {model}: {error_msg}""
    else:
        return None

    content += ""\n""
    return format_stream_response(config, content=content)",Format model status updates for streaming response.,Formats model download status messages based on current progress and errors.
1441,enable,"def enable(
    time_domain: typing.Optional[str] = None,
) -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    params: T_JSON_DICT = dict()
    if time_domain is not None:
        params[""timeDomain""] = time_domain
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Performance.enable"",
        ""params"": params,
    }
    json = yield cmd_dict",Enable collecting and reporting metrics.,Enable performance monitoring with optional time domain configuration
1442,policy_setup,"def policy_setup(tokenizer, num_gpus):
    
    policy = None
    cluster = None

    cluster_name = f""test-init-{num_gpus}gpu""
    print(f""Creating virtual cluster '{cluster_name}' for {num_gpus} GPUs..."")

    cluster = RayVirtualCluster(
        name=cluster_name,
        bundle_ct_per_node_list=[num_gpus],
        use_gpus=True,
        num_gpus_per_node=num_gpus,
        max_colocated_worker_groups=1,
    )

    config = basic_llama_test_config
    config[""generation""] = configure_generation_config(config[""generation""], tokenizer)

    print(""Creating HfPolicy..."")
    policy = HfPolicy(cluster=cluster, config=config, tokenizer=tokenizer)

    yield policy, cluster

    # Clean up after the test
    print(""Cleaning up resources for test"")
    cluster.shutdown()
    policy.worker_group.shutdown()",Setup and teardown for policy tests - creates a virtual cluster and policy.,Initialize and manage a virtual GPU cluster and policy for testing with a tokenizer.
1443,get_http_client,"def get_http_client(config: Config, cancellation_token: CancellationToken) -> httpx.AsyncClient:
    
    headers = {}

    # Set auth based on provided credentials
    auth = None
    if config.api_key:
        # Bearer token authentication
        headers[""Authorization""] = f""Bearer {config.api_key}""
    elif config.username and config.password:
        # Basic authentication
        auth = httpx.BasicAuth(config.username, config.password)

    return httpx.AsyncClient(base_url=config.base_url, auth=auth, headers=headers)",Create an HTTP client for the API with appropriate authentication,Create an asynchronous HTTP client with authentication based on configuration.
1444,editor_agent,"def editor_agent(context: dict, query: str) -> dict:
    
    llm = OpenAI(
        connection=OpenAIConnection(),
        model=""gpt-4o-mini"",
        temperature=0.1,
    )

    orchestrator = GraphOrchestrator(
        name=""Reviser graph orchestrator"",
        manager=GraphAgentManager(llm=llm),
    )

    def orchestrate(context: dict, **kwargs) -> str:
        return END if context.get(""review"") is None else ""reviser""

    orchestrator.add_state_by_tasks(""researcher"", [_run_in_depth_research])
    orchestrator.add_state_by_tasks(""reviewer"", [_review_draft])
    orchestrator.add_state_by_tasks(""reviser"", [_revise_draft])

    orchestrator.add_edge(START, ""researcher"")
    orchestrator.add_edge(""researcher"", ""reviewer"")
    orchestrator.add_conditional_edge(""reviewer"", [""reviser"", END], orchestrate)
    orchestrator.add_edge(""reviser"", END)

    orchestrator.context = {
        ""query"": query,
        ""task"": context.get(""task""),
    }
    orchestrator.run(input_data={})
    return {""draft"": orchestrator.context.get(""draft""), ""result"": """"}",Runs a research orchestration process for a given query.,Automates document revision using AI-driven task orchestration and state transitions.
1445,_calculate_volatility_indicators,"def _calculate_volatility_indicators(self, df: pd.DataFrame) -> Dict[str, TechnicalSignal]:
        
        signals = {}
        
        try:
            # Bollinger Bands
            df['bb_upper'] = ta.volatility.bollinger_hband(df['close'])
            df['bb_middle'] = ta.volatility.bollinger_mavg(df['close'])
            df['bb_lower'] = ta.volatility.bollinger_lband(df['close'])
            
            # ATR
            df['atr'] = ta.volatility.average_true_range(df['high'], df['low'], df['close'])
            
            # Keltner Channel
            df['kc_upper'] = ta.volatility.keltner_channel_hband(df['high'], df['low'], df['close'])
            df['kc_lower'] = ta.volatility.keltner_channel_lband(df['high'], df['low'], df['close'])
            
            # Donchian Channel
            df['dc_upper'] = df['high'].rolling(20).max()
            df['dc_lower'] = df['low'].rolling(20).min()
            
            # Volatility Signals
            signals['bollinger'] = self._analyze_bollinger_bands(df)
            signals['atr'] = self._analyze_atr(df)
            signals['keltner'] = self._analyze_keltner_channel(df)
            signals['donchian'] = self._analyze_donchian_channel(df)
            
            return signals
            
        except Exception as e:
            logger.error(f""Error calculating volatility indicators: {e}"")
            raise",Calculate comprehensive volatility indicators.,Compute and analyze financial volatility indicators for trading signals.
1446,_load_example_file,"def _load_example_file(self, example_name):
        
        file_path = os.path.join(self.examples_dir, f""{example_name}.py"")
        with open(file_path, ""r"") as f:
            return f.read()",Load an example from a file.,Load and return the content of a specified example Python file.
1447,disable,"def disable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Audits.disable"",
    }
    json = yield cmd_dict","Disables issues domain, prevents further issues from being reported to the client.",Disables audit functionality by sending a command dictionary.
1448,read_config,"def read_config(path: Union[None, str]) -> Dict[str, Any]:
    
    if path is None:
        return {}

    try:
        with smart_open.open(path, mode=""rt"") as f:
            return dict(safe_load(f))
    except FileNotFoundError as ex:
        raise DolmaRefineError(f""Config file not found: {path}"") from ex
    except Exception as ex:
        raise DolmaRefineError(f""Error while reading config file: {path}"") from ex",Read a configuration file if it exists,"Load configuration from a file, handling errors and missing paths."
1449,load_toolboxes,"def load_toolboxes(self, toolbox_names: List[str] = []) -> None:
        
        try:
            entry_points = importlib.metadata.entry_points(group=""quantalogic.tools"")
        except Exception as e:
            loguru.logger.error(f""Failed to retrieve entry points: {e}"")
            entry_points = []

        try:
            if toolbox_names:  # Only filter if toolbox_names is non-empty
                entry_points = [ep for ep in entry_points if ep.name in toolbox_names]
            # If no toolbox_names specified, load all available toolboxes
            loguru.logger.debug(f""Found {len(entry_points)} toolbox entry points"")
            for ep in entry_points:
                try:
                    module = ep.load()
                    # normalize toolbox names to valid Python identifiers
                    normalized = ep.name.replace('-', '_')
                    self.register_tools_from_module(module, toolbox_name=normalized)
                    loguru.logger.info(f""Successfully loaded toolbox: {ep.name}"")
                except ImportError as e:
                    loguru.logger.error(f""Failed to import toolbox {ep.name}: {e}"")
                except Exception as e:
                    loguru.logger.error(f""Failed to load toolbox {ep.name}: {e}"")
        except Exception as e:
            loguru.logger.error(f""Error loading toolboxes: {e}"")","Load toolboxes from registered entry points, optionally filtering by name.","Load and register specified or all available toolboxes, handling errors gracefully."
1450,login,"def login():
    
    try:
        # check if already logged in
        token = get_stored_token()
        if token:
            log.success(""You are already authenticated!"")
            if not inquirer.confirm('Would you like to log in with a different account?'):
                return

        # Start the local server
        server, port = start_auth_server()

        # Create server thread
        server_thread = threading.Thread(target=server.serve_forever)
        server_thread.daemon = True
        server_thread.start()

        # Open the browser to the login page
        auth_url = f""{auth_url_base}/login?callback_port={port}""
        webbrowser.open(auth_url)

        # Wait for authentication to complete
        while not server.authentication_successful:
            pass

        # Cleanup
        server.shutdown()
        server_thread.join()

        log.success(""🔐 Authentication successful! Token has been stored."")
        return True

    except Exception as e:
        log.warn(f""Authentication failed: {str(e)}"", err=True)
        return False",Log in to AgentStack,Initiates user authentication via local server and browser interaction.
1451,_generate_mcp_tools_info,"def _generate_mcp_tools_info(self):
        
        mcp_tools_info = ""### Available MCP Tools\n\n""
        for tool in self.mcp_client.available_tools:
            mcp_tools_info += f""## {tool['name']}\n\n""
            mcp_tools_info += f""**Description**: {tool['description']}\n\n""

            # Create parameter list from schema
            params = []
            if ""properties"" in tool[""input_schema""]:
                for param_name in tool[""input_schema""][""properties""].keys():
                    params.append(param_name)

            mcp_tools_info += f
        return mcp_tools_info",Generate explanation of available MCP tools for supervisor.,Generate a formatted list of available tools with descriptions and parameters.
1452,cli_print_tool_call,"def cli_print_tool_call(tool_name="""", args="""", output="""", prefix=""  ""):
    
    if not tool_name:
        return

    print(f""{prefix}{color('Tool Call:', fg='cyan')}"")
    print(f""{prefix}{color('Name:', fg='cyan')} {tool_name}"")
    if args:
        print(f""{prefix}{color('Args:', fg='cyan')} {args}"")
    if output:
        print(f""{prefix}{color('Output:', fg='cyan')} {output}"")",Print a tool call with pretty formatting,Formats and prints tool call details with optional arguments and output.
1453,simple_message_log,"def simple_message_log() -> LLMMessageLogType:
    
    return [
        {
            ""input_ids"": torch.tensor([1, 2, 3]),
            ""attention_mask"": torch.tensor([1, 1, 1]),
            ""text"": ""test"",
        }
    ]",Fixture for a single message with tensor and text data.,"Create a basic message log with input IDs, attention mask, and text."
1454,_clone,"def _clone(repo_url: str, target_path: str, commit_sha: str) -> None:
  
  process = subprocess.Popen(
    [""git"", ""clone"", ""--progress"", repo_url, target_path],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    universal_newlines=True,
  )

  while True:
    # Read output line by line.
    if not process.stderr.readline() and process.poll() is not None:
      break

  if process.returncode != 0:
    raise subprocess.CalledProcessError(process.returncode, [""git"", ""clone""])

  # checkout specific commit
  print(f""Checking out commit {commit_sha}"")
  subprocess.run([""git"", ""-C"", target_path, ""checkout"", commit_sha], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)",Clone a git repo with progress bar.,Clone a Git repository and checkout a specific commit using subprocess.
1455,get_value,"def get_value(self, path: str) -> Any:
        
        try:
            if path in self.overrides:
                return self.overrides[path]

            parts = path.split('.')
            current = self.base_config

            for i, part in enumerate(parts):
                if not isinstance(current, dict):
                    current_path = '.'.join(parts[:i])
                    raise click.BadParameter(f""Cannot access '{path}': '{current_path}' is not a dictionary"")
                if part not in current:
                    raise click.BadParameter(f""Path '{path}' not found: '{part}' does not exist"")
                current = current[part]

            return current

        except Exception as e:
            logger.error(""Error accessing path %s: %s"", path, e)
            raise",Get value with better error messages,Retrieve configuration value from nested structure with error handling
1456,mask,"def mask(markdown_content: str) -> MaskedDocument:
        
        # Initialize storage for links and images
        links: dict[str, str] = {}
        images: dict[str, str] = {}

        def replace_images(content: str) -> str:
            
            return re.sub(
                MarkdownPruningPipe.image_pattern, lambda match: MarkdownPruningPipe.image_mask(match, images), content
            )

        def replace_links(content: str) -> str:
            
            return re.sub(
                MarkdownPruningPipe.link_pattern, lambda match: MarkdownPruningPipe.link_mask(match, links), content
            )

        # Apply transformations in sequence
        process = compose(replace_images, replace_links)
        processed_content = process(markdown_content)

        return MaskedDocument(content=processed_content, links=links, images=images)",Process markdown content to replace links and images with placeholders.,"Transform markdown by masking links and images, returning structured data."
1457,find_all_aliases_for_agent,"def find_all_aliases_for_agent(self, agent_id):
        
        aliases = []
        try:
            response = self.bedrock_agent.list_agent_aliases(
                agentId=agent_id,
                maxResults=100
            )
            
            for alias in response.get('agentAliasSummaries', []):
                aliases.append({
                    'id': alias['agentAliasId'],
                    'name': alias['agentAliasName'],
                    'arn': alias['agentAliasArn'] if 'agentAliasArn' in alias else None
                })
                
            return aliases
        except Exception as e:
            print(f""Error finding aliases for agent {agent_id}: {str(e)}"")
            return []",Find all aliases for a given agent,"Retrieve and return all alias details for a specified agent ID, handling exceptions."
1458,_missing_,"def _missing_(cls, value: str) -> Optional[""LogLevel""]:
        
        try:
            # Convert to uppercase and look up directly
            return cls[value.upper()]
        except (KeyError, AttributeError):
            raise ValueError(
                f""'{value}' is not a valid LogLevel. ""
                f""Valid levels are: {', '.join(level.value for level in cls)}""
            )",Handle case-insensitive lookup of enum values.,Handle invalid log level by converting input to uppercase and checking against valid levels
1459,get_edit_form_get,"def get_edit_form_get(self, field_name: str, pdf: Pdf):
        

        form_dict = self.get_edit_form_dict()

        initial_dict = {
            'name': {'name': pdf.name},
            'description': {'description': pdf.description},
            'notes': {'notes': pdf.notes},
            'file_directory': {'file_directory': pdf.file_directory},
            'tags': {'tag_string': ' '.join(sorted([tag.name for tag in pdf.tags.all()]))},
        }

        form = form_dict[field_name](initial=initial_dict[field_name])

        return form",Get the form belonging to the specified field.,Generate a pre-filled form for editing PDF metadata based on field name.
1460,multiply,"def multiply(a: int, b: int) -> int:  # Changed return type hint to int
    
    logger.info(f""Executing multiply tool with a={a}, b={b}"")
    return a * b",Multiply two numbers and return the result,Logs and returns the product of two integers.
1461,save_metrics,"def save_metrics(metrics, model_name, dataset_name):
    
    output_dir = ""results/metrics""
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, f""{dataset_name}_{model_name}_metrics.txt"")
    with open(output_file, 'w') as f:
        for metric_name, value in metrics.items():
            f.write(f""{metric_name}: {value}\n"")
    print(f""Saved metrics to {output_file}"")",Save evaluation metrics to a text file.,Save model evaluation metrics to a structured text file.
1462,command_wrapper,"def command_wrapper(
                    *,
                    config_file: str = typer.Option(""server_config.json"", help=""Configuration file path""),
                    server: str | None = typer.Option(None, help=""Server to connect to""),
                    provider: str = typer.Option(""openai"", help=""LLM provider name""),
                    model: str | None = typer.Option(None, help=""Model name""),
                    disable_filesystem: bool = typer.Option(False, help=""Disable filesystem access""),
                ) -> None:
                    
                    from mcp_cli.cli_options import process_options

                    servers, _, server_names = process_options(
                        server, disable_filesystem, provider, model, config_file
                    )

                    extra_params = {
                        ""provider"": provider,
                        ""model"": model,
                        ""server_names"": server_names,
                    }

                    run_cmd(
                        _orig_exec,
                        config_file,
                        servers,
                        extra_params=extra_params,
                    )",Dynamically generated Typer wrapper.,Wraps command execution with configuration and server options for LLM interaction.
1463,index,"def index(self, session_id, **kwargs):
        
        html = map_action_to_html(
            ""start"",
            session_id=session_id,
            instruction_text=kwargs[""instruction_text""],
        )
        url = f""{self.base_url}/{session_id}""
        return html, url",Redirect to the search page with the given session ID,Generate HTML and URL for session initiation with instructions.
1464,handle_block,"def handle_block(self, element: lxml_html.HtmlElement) -> str:
        
        content = self.process_children(element)
        return f""\n\n{content}\n\n""",Handle block-level elements like div,Formats HTML element content with newlines for readability.
1465,_list_cached_images,"def _list_cached_images(self):
        
        # Extract array of tuple into just, an array of files:
        pt_files = StateTracker.get_vae_cache_files(data_backend_id=self.id)
        # Extract just the base filename without the extension
        results = {os.path.splitext(f)[0] for f in pt_files}
        # self.debug_log(
        #     f""Found {len(pt_files)} cached files in {self.cache_dir} (truncated): {list(results)[:5]}""
        # )
        return results",Return a set of filenames (without the .pt extension) that have been processed.,Retrieve cached image filenames without extensions from storage
1466,print_tool_parameters,"def print_tool_parameters():
    
    tool_names = [
        'SearchAwsProviderDocs',
        'ExecuteTerraformCommand',
        'SearchAwsccProviderDocs',
        'SearchSpecificAwsIaModules',
        'RunCheckovScan',
    ]

    print('\n=== Current Tool Parameter Schemas ===\n')
    for tool_name in tool_names:
        try:
            tool = mcp._tool_manager.get_tool(tool_name)
            if tool is None:
                print(f'Tool {tool_name} not found')
                continue

            if not hasattr(tool, 'parameters') or tool.parameters is None:
                print(f'Tool {tool_name} has no parameters schema')
                continue

            print(f'=== {tool_name} Parameters Schema ===')
            print(json.dumps(tool.parameters, indent=2))
            print('\n')
        except Exception as e:
            print(f'Error getting tool {tool_name}: {e}')",Print the parameters for each tool after annotations are added.,"Display parameter schemas for specified tools, handling errors gracefully."
1467,list_models,"def list_models(self, **kwargs) -> None:
        
        try:
            client = self._get_client()
            response = client.models.list().data
        
            model_ids= [model.id for model in response]

            logger.info(""\nAVAILABLE MODELS:"")
            for i, model_id in enumerate(model_ids, start=1):
                logger.info(f""{i}. {model_id}"")
                    
        except Exception as e:
            raise GroqAPIError(f""Listing models failed: {e}"")",List all available Groq models,Logs available model identifiers from a client response list
1468,_save_debug_image,"def _save_debug_image(self, image: np.ndarray, layout, page_number: int):
        
        if not self.translation_config.debug:
            return

        debug_dir = Path(self.translation_config.get_working_file_path(""ocr-box-image""))
        debug_dir.mkdir(parents=True, exist_ok=True)

        # Draw boxes on the image
        debug_image = image.copy()
        for box in layout.boxes:
            x0, y0, x1, y1 = box.xyxy
            cv2.rectangle(
                debug_image,
                (int(x0), int(y0)),
                (int(x1), int(y1)),
                (0, 255, 0),
                2,
            )
            # Add text label
            cv2.putText(
                debug_image,
                layout.names[box.cls],
                (int(x0), int(y0) - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 255, 0),
                1,
            )

        # Save the image
        output_path = debug_dir / f""{page_number}.jpg""
        cv2.imwrite(str(output_path), debug_image)",Save debug image with drawn boxes if debug mode is enabled.,Save annotated debug images with bounding boxes if debugging is enabled.
1469,_display_bloat_analysis,"def _display_bloat_analysis(self, answer_lengths: List[int]):
        
        bloat_ratio = (
            answer_lengths[-1] / answer_lengths[0] if answer_lengths[0] > 0 else 1.0
        )

        if bloat_ratio > 2.0:
            bloat_color = ""red""
            bloat_icon = ""🔴""
        elif bloat_ratio > 1.5:
            bloat_color = ""yellow""
            bloat_icon = ""🟡""
        else:
            bloat_color = ""green""
            bloat_icon = ""🟢""

        self.console.print(
            f""[bold]Answer Bloat:[/bold] [{bloat_color}]{bloat_ratio:.1f}x {bloat_icon}[/{bloat_color}]""
        )

        # Show progression
        lengths_display = "" → "".join(str(length) for length in answer_lengths)
        self.console.print(f""[dim]Length progression: {lengths_display} chars[/dim]\n"")",Display answer bloat analysis,Analyze and display answer length growth with color-coded bloat indicators.
1470,attach_tool,"def attach_tool(self, tool: Tool) -> None:
        
        # Check if the agent's instructions say to not use tools. If so, we will rewrite them.
        instructions = agents_helper.get_agent_instructions_by_name(self.name)
        if Agent.NO_TOOL_USE_INSTRUCTION in instructions:
            print(f""Replacing instructions to not use tools..."")
            instructions = instructions.replace(
                Agent.NO_TOOL_USE_INSTRUCTION, Agent.TOOL_USE_INSTRUCTION
            )
            self.update(new_instructions=instructions)

        # add_action_group_with_lambda() doesn't check if the lambda already exists, we need to
        if self.has_action_group(tool.name):
            print(f""Action group {tool.name} already exists, skipping..."")
            return

        tool_defs = [tool.to_action_group_definition()]
        agents_helper.add_action_group_with_lambda(
            self.name,
            tool.name,
            tool.code_file,
            tool_defs,  # One function for now, generalize to handle groups later
            tool.name,  # Using tool name as the action group name
            f""actions for {tool.description}"",
        )",Attach a tool to this agent.,"Attach a tool to an agent, updating instructions and action groups as needed."
1471,_checkpoint_filter_fn,"def _checkpoint_filter_fn(state_dict, model):
    
    if 'stem.0.weight' in state_dict:
        return state_dict  # non-original checkpoint, no remapping needed

    out_dict = {}
    import re
    stage_idx = 0
    for k, v in state_dict.items():
        if k.startswith('patch_embed'):
            k = k.replace('patch_embed.0', 'stem.conv1')
            k = k.replace('patch_embed.1', 'stem.norm1')
            k = k.replace('patch_embed.3', 'stem.conv2')
            k = k.replace('patch_embed.4', 'stem.norm2')

        if re.match(r'network\.(\d+)\.proj\.weight', k):
            stage_idx += 1
        k = re.sub(r'network.(\d+).(\d+)', f'stages.{stage_idx}.blocks.\\2', k)
        k = re.sub(r'network.(\d+).proj', f'stages.{stage_idx}.downsample.conv', k)
        k = re.sub(r'network.(\d+).norm', f'stages.{stage_idx}.downsample.norm', k)

        k = re.sub(r'layer_scale_([0-9])', r'ls\1.gamma', k)
        k = k.replace('dist_head', 'head_dist')
        out_dict[k] = v
    return out_dict",Remap original checkpoints -> timm,Transforms model state dictionary keys for compatibility with a new architecture.
1472,initialize_session,"def initialize_session():
    
    if 'count' not in st.session_state:
        st.session_state['count'] = 1

        # Refresh agent IDs and aliases
        for idx, config in enumerate(bot_configs):
            try:
                agent_id = agents_helper.get_agent_id_by_name(config['agent_name'])
                agent_alias_id = agents_helper.get_agent_latest_alias_id(agent_id)
                bot_configs[idx]['agent_id'] = agent_id
                bot_configs[idx]['agent_alias_id'] = agent_alias_id
            except Exception as e:
                print(f""Could not find agent named:{config['agent_name']}, skipping..."")
                continue

        # Get bot configuration
        bot_name = os.environ.get('BOT_NAME', 'Mortgages Assistant')
        bot_config = next((config for config in bot_configs if config['bot_name'] == bot_name), None)
        
        if bot_config:
            st.session_state['bot_config'] = bot_config
            
            # Load tasks if any
            task_yaml_content = {}
            if 'tasks' in bot_config:
                with open(bot_config['tasks'], 'r') as file:
                    task_yaml_content = yaml.safe_load(file)
            st.session_state['task_yaml_content'] = task_yaml_content

            # Initialize session ID and message history
            st.session_state['session_id'] = str(uuid.uuid4())
            st.session_state.messages = []",Initialize session state and bot configuration.,"Initialize session state with bot configurations, agent IDs, and task data for a chatbot application."
1473,wait_for_callback,"def wait_for_callback(self, timeout=300):
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            if self.callback_data[""authorization_code""]:
                return self.callback_data[""authorization_code""]
            elif self.callback_data[""error""]:
                raise Exception(f""OAuth error: {self.callback_data['error']}"")
            time.sleep(0.1)
        raise Exception(""Timeout waiting for OAuth callback"")",Wait for OAuth callback with timeout.,"Waits for OAuth callback, returns code or raises error on timeout."
1474,write,"def write(self, filepath: Union[str, Path], data: Any) -> None:
        
        if isinstance(filepath, str):
            assert not filepath.startswith(
                ""http""
            ), f""writing to {filepath} is not allowed as it has http in it""
            filepath = Path(filepath)
        # Not a huge fan of auto-shortening filenames, as we hash things for that in other cases.
        # However, this is copied in from the original Arcade-AI CSV backend implementation for compatibility.
        filepath = path_to_hashed_path(filepath, self.hash_filenames)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        with open(filepath, ""wb"") as file:
            # Check if data is a Tensor, and if so, save it appropriately
            if isinstance(data, torch.Tensor):
                # logger.debug(f""Writing a torch file to disk."")
                return self.torch_save(data, file)
            if isinstance(data, str):
                # logger.debug(f""Writing a string to disk as {filepath}: {data}"")
                data = data.encode(""utf-8"")
            else:
                logger.debug(
                    f""Received an unknown data type to write to disk. Doing our best: {type(data)}""
                )
            file.write(data)",Write the provided data to the specified filepath.,"Function writes data to a file, ensuring path validity and handling different data types."
1475,_init_beacon_embed,"def _init_beacon_embed(self, missing_keys):
        
        if is_deepspeed_zero3_enabled():
            import deepspeed
            params = [self.beacon_embed_tokens.weight, self.embed_tokens.weight]
            with deepspeed.zero.GatheredParameters(params, modifier_rank=0):
                # deepspeed will initialize the parameters to zero
                if (self.beacon_embed_tokens.weight == 0).all():
                    if self.config.beacon_embed_init == ""bos"":
                        self.beacon_embed_tokens.weight.data[:] = self.embed_tokens.weight.data[self.config.bos_token_id]
                    elif self.config.beacon_embed_init == ""eos"":
                        if isinstance(self.config.eos_token_id, list):
                            eos_token_id = self.config.eos_token_id[0]
                        else:
                            eos_token_id = self.config.eos_token_id
                        self.beacon_embed_tokens.weight.data[:] = self.embed_tokens.weight.data[eos_token_id]
                    else:
                        raise NotImplementedError(f""Make sure beacon_embed_init is either eos or bos, found {self.config.beacon_embed_init}"")
        else:
            if any(""beacon_embed_tokens"" in missing_key for missing_key in missing_keys):
                if self.config.beacon_embed_init == ""bos"":
                    self.beacon_embed_tokens.weight.data[:] = self.embed_tokens.weight.data[self.config.bos_token_id]
                elif self.config.beacon_embed_init == ""eos"":
                    if isinstance(self.config.eos_token_id, list):
                        eos_token_id = self.config.eos_token_id[0]
                    else:
                        eos_token_id = self.config.eos_token_id
                    self.beacon_embed_tokens.weight.data[:] = self.embed_tokens.weight.data[eos_token_id]
                else:
                    raise NotImplementedError(f""Make sure beacon_embed_init is either eos or bos, found {self.config.beacon_embed_init}"")",Initialize the beacon token embedding with that of the eos token.,Initialize beacon embeddings based on configuration and DeepSpeed zero3 state
1476,cancel_download,"def cancel_download():
    
    task_id = request.args.get(""task_id"")
    if not task_id:
        return Response(
            json.dumps({""error"": ""Missing process id (task_id) parameter""}),
            status=400,
            mimetype=""application/json"",
        )

    # Use the queue manager's cancellation method.
    result = download_queue_manager.cancel_task(task_id)
    status_code = 200 if result.get(""status"") == ""cancelled"" else 404

    return Response(json.dumps(result), status=status_code, mimetype=""application/json"")",Cancel a running download process by its task id.,"Cancel a download task based on a provided task identifier, returning status and result."
1477,describe_cases_response_data,"def describe_cases_response_data() -> Dict[str, Any]:
    
    return {
        ""cases"": [
            {
                ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
                ""displayId"": ""12345678910"",
                ""subject"": ""EC2 instance not starting"",
                ""status"": ""opened"",
                ""serviceCode"": ""amazon-elastic-compute-cloud-linux"",
                ""categoryCode"": ""using-aws"",
                ""severityCode"": ""urgent"",
                ""submittedBy"": ""user@example.com"",
                ""timeCreated"": ""2023-01-01T12:00:00Z"",
                ""recentCommunications"": {
                    ""communications"": [
                        {
                            ""caseId"": ""case-12345678910-2013-c4c1d2bf33c5cf47"",
                            ""body"": ""My EC2 instance i-1234567890abcdef0 is not starting."",
                            ""submittedBy"": ""user@example.com"",
                            ""timeCreated"": ""2023-01-01T12:00:00Z"",
                        }
                    ],
                    ""nextToken"": None,
                },
            }
        ],
        ""nextToken"": None,
    }",Return a dictionary with sample describe cases response data.,Generate a mock response for AWS support case details with communication history.
1478,_delete_pv,"def _delete_pv(self):
        
        pv_name = self._get_pv_name_from_file(self.pvc_config_file)
        result = KubeCtl().exec_command(f""kubectl get pv {pv_name} --ignore-not-found"")

        if result:
            print(f""Deleting PersistentVolume {pv_name}"")
            KubeCtl().exec_command(f""kubectl delete pv {pv_name}"")
            print(f""Successfully deleted PersistentVolume from {pv_name}"")
        else:
            print(f""PersistentVolume {pv_name} not found. Skipping deletion."")",Delete the PersistentVolume and associated PersistentVolumeClaim.,"Delete a Kubernetes PersistentVolume if it exists, using command-line execution."
1479,task_reflect_block_with_border_pixel,"def task_reflect_block_with_border_pixel(rng: Random, size: int) -> Optional[dict[str, list[int]]]:
    
    block_size = rng.randint(2, size)

    c1 = rng.randint(1, 9)
    c2 = rng.choice(tuple(c for c in range(1, 9) if c != c1))

    side = ""left"" if rng.random() < 0.5 else ""right""
    pos = rng.randint(0, size - block_size)

    block = [c1] * block_size
    if side == ""left"":
        block[0] = c2
    else:
        block[block_size - 1] = c2

    question = write_block(pos, block, gen_field(size))
    reversed_block = block[::-1]  # Reverse the block
    answer = write_block(pos, reversed_block, gen_field(size))

    return {""input"": question, ""output"": answer}",Generate a task where a block with a border pixel is reflected.,Generate a mirrored block with border pixel and return input-output mapping.
1480,enable,"def enable() -> typing.Generator[T_JSON_DICT, T_JSON_DICT, None]:
    
    cmd_dict: T_JSON_DICT = {
        ""method"": ""Security.enable"",
    }
    json = yield cmd_dict",Enables tracking security state changes.,Initiates a security feature by sending a command and awaiting a JSON response.
1481,query_documentation,"def query_documentation(
        self, query_text: str, product_name: str, version: str | None = None, limit: int = 4, db_path: str | None = None
    ) -> List[Dict[str, Any]]:
        
        if query_text == """" or product_name == """":
            raise ValueError(""Both query and product must be specified"")

        try:
            if product_name not in PRODUCT_DB_MAP:
                raise ValueError(
                    f""Unsupported product: {product_name}. Supported product: {', '.join(PRODUCT_DB_MAP.keys())}""
                )

            version = version if version and version.strip() else None

            query_embedding = self.create_embeddings(query_text)

            filter = {""product_name"": product_name}
            if version:
                filter[""version""] = version

            results = self.query_collection(query_embedding, filter, limit, db_path)
            # Filter results based on min_similarity threshold
            filtered_results = [qr for qr in results if qr.distance <= (1 - self.config.min_similarity)]
            return [{""distance"": qr.distance, ""content"": qr.content} for qr in filtered_results]

        except Exception as e:
            logging.error(""An error occurred: %s"", e)
            raise",Query documentation for a specific product.,Search product documentation using query embeddings and filter by product and version
1482,parse,"def parse(self, input: dict, response: str) -> dict:
        
        instruction = input[""conversation""][0][""content""]
        return {""instruction"": instruction, ""new_response"": response}",Parse the model response along with the input to the model into the desired output format..,Extracts initial conversation content and pairs it with a new response.
1483,find_timeout_in_source,"def find_timeout_in_source():
    
    print(""\n📋 Source code analysis:"")
    
    try:
        from chuk_tool_processor.execution.strategies.inprocess_strategy import InProcessStrategy
        import inspect
        
        # Get the source code
        source = inspect.getsource(InProcessStrategy)
        
        # Look for timeout references
        lines = source.split('\n')
        for i, line in enumerate(lines):
            if 'timeout' in line.lower() and ('10' in line or '30' in line):
                print(f""   Line {i+1}: {line.strip()}"")
                
    except Exception as e:
        print(f""   Could not analyze source: {e}"")",Find the actual timeout value in source code.,Analyzes source code to identify timeout settings in a specific strategy class.
1484,mock_terminal,"def mock_terminal():
    
    with (
        patch(""termios.tcgetattr"") as mock_get_attr,
        patch(""termios.tcsetattr"") as mock_set_attr,
        patch(""tty.setraw"") as mock_setraw,
        patch(""sys.stdin"") as mock_stdin,
    ):

        # Setup mock return values
        mock_get_attr.return_value = [1, 2, 3]  # Mock terminal settings
        mock_stdin.fileno.return_value = 0
        mock_stdin.read.return_value = ""x""

        yield {
            ""get_attr"": mock_get_attr,
            ""set_attr"": mock_set_attr,
            ""setraw"": mock_setraw,
            ""stdin"": mock_stdin,
        }",Fixture to mock terminal-related functions and modules.,Simulate terminal behavior using mocked system calls for testing purposes.
1485,_evaluate_confidence,"def _evaluate_confidence(self, sample_data: List[Dict[str, Any]]) -> float:
        
        if len(sample_data) < 2:
            return 0.5  # Neutral confidence for single sample
        
        # Extract answers and thinking for analysis
        answers = [sample[""answer""] for sample in sample_data if sample[""answer""]]
        thinking_texts = [sample[""thinking""] for sample in sample_data if sample[""thinking""]]
        
        if not answers:
            return 0.1  # Very low confidence if no answers extracted
        
        # Evaluate answer consistency
        answer_consistency = self._calculate_answer_consistency(answers)
        
        # Evaluate reasoning consistency  
        reasoning_consistency = self._calculate_reasoning_consistency(thinking_texts)
        
        # Combine metrics (weighted average)
        confidence = (0.6 * answer_consistency + 0.4 * reasoning_consistency)
        
        logger.debug(f""Answer consistency: {answer_consistency:.3f} (weight: 0.6)"")
        logger.debug(f""Reasoning consistency: {reasoning_consistency:.3f} (weight: 0.4)"")
        logger.debug(f""Combined confidence: {confidence:.3f}"")
        
        # Log additional details for debugging low confidence
        if confidence < 0.5:
            logger.debug(f""Low confidence detected. Sample count: {len(sample_data)}"")
            logger.debug(f""Answers found: {len(answers)}, Thinking texts: {len(thinking_texts)}"")
            if answers:
                logger.debug(f""Sample answers: {answers}"")
            if len(answers) >= 2:
                logger.debug(f""Most common answer appears {max(Counter(answers).values())} times out of {len(answers)}"")
        
        return confidence",Evaluate confidence based on consistency across samples.,Calculate confidence score based on answer and reasoning consistency in sample data.
1486,_worker,"def _worker(self):
        
        last_flush = time.time()
        
        while not self.shutdown_event.is_set():
            try:
                # Try to get a log record with timeout
                try:
                    log_record = self.log_queue.get(timeout=0.1)
                except queue.Empty:
                    # Check if we need to flush
                    if time.time() - last_flush > self.flush_interval:
                        self._flush_all_loggers()
                        last_flush = time.time()
                    continue
                
                # Process the log record
                module_name, level, message, args, kwargs = log_record
                
                # Get or create the real logger
                real_logger = self._get_real_logger(module_name)
                
                # Write the log message
                self._dispatch_log(real_logger, level, message, args, kwargs)
                self.log_queue.task_done()
                
            except Exception as e:
                # Don't let exceptions in logging crash the worker thread
                print(f""Error in async log processor: {e}"")
                continue
        
        # Process remaining items in queue during shutdown
        self._drain_queue()",Background worker that processes queued log messages,"Asynchronously processes log records, dispatching them to appropriate loggers and handling shutdown gracefully."
1487,mount,"def mount(nfs_server_ip):
    
    remote_path = f""{nfs_server_ip}:/public/datasets/aiq""
    mount_point = ""/mnt/nfs/aiq""

    # Install NFS common tools
    run_command(""sudo apt -y update"")
    run_command(""sudo apt -y install nfs-common"")

    # Create the mount point
    run_command(f""sudo mkdir -p {mount_point}"")
    # Mount the NFS share
    run_command(f""sudo mount -v -t nfs -o nfsvers=3 {remote_path} {mount_point}"")

    # Print only the AIQ Toolkit mount
    print(""\nAIQ Toolkit mount details:"")
    run_command(f""mount | grep {mount_point}"")
    print(""NFS mount completed successfully"")",Mount the NFS share for aiq datasets.,Mounts an NFS share from a server to a local directory for accessing datasets.
1488,create_table,"def create_table(data: Dict, nested: bool = False) -> str:
        
        table = ['| Key | Value |', '|-----|-------|']

        for key, value in data.items():
            formatted_key = key.replace('_', ' ').title()
            formatted_value = _format_value(key, value)
            table.append(f'| {formatted_key} | {formatted_value} |')

        return '\n'.join(table)",Create a markdown table from dictionary data.,Generate a formatted table from dictionary data with optional nesting.
1489,start_ollama_server,"def start_ollama_server() -> bool:
    
    if is_ollama_server_running():
        print(f""{Fore.GREEN}Ollama server is already running.{Style.RESET_ALL}"")
        return True

    system = platform.system().lower()

    try:
        if system == ""darwin"" or system == ""linux"":  # macOS or Linux
            subprocess.Popen([""ollama"", ""serve""], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        elif system == ""windows"":  # Windows
            subprocess.Popen([""ollama"", ""serve""], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        else:
            print(f""{Fore.RED}Unsupported operating system: {system}{Style.RESET_ALL}"")
            return False

        # Wait for server to start
        for _ in range(10):  # Try for 10 seconds
            if is_ollama_server_running():
                print(f""{Fore.GREEN}Ollama server started successfully.{Style.RESET_ALL}"")
                return True
            time.sleep(1)

        print(f""{Fore.RED}Failed to start Ollama server. Timed out waiting for server to become available.{Style.RESET_ALL}"")
        return False
    except Exception as e:
        print(f""{Fore.RED}Error starting Ollama server: {e}{Style.RESET_ALL}"")
        return False",Start the Ollama server if it's not already running.,"Attempt to start the Ollama server, checking its status and handling different operating systems."
1490,convert_html_to_text,"def convert_html_to_text(self, html, simple=False):
        
        texts = self._parse_html(html).findAll(text=True)
        visible_texts = filter(tag_visible, texts)
        if simple:
            # For `simple` mode, return just [SEP] separators
            return "" [SEP] "".join(t.strip() for t in visible_texts if t != ""\n"")
        else:
            # Otherwise, return an observation with tags mapped to specific, unique separators
            observation = """"
            for t in visible_texts:
                if t == ""\n"":
                    continue
                if t.parent.name == ""button"":  # button
                    processed_t = f""[button] {t} [button_]""
                elif t.parent.name == ""label"":  # options
                    if f'""{t}""' in self.state[""url""]:
                        processed_t = f""  [clicked button] {t} [clicked button_]""
                        observation = f""You have clicked {t}.\n"" + observation
                    else:
                        processed_t = f""  [button] {t} [button_]""
                elif t.parent.get(""class"") == [""product-link""]:  # product asins
                    if f""{t}"" in self.server.user_sessions[self.session][""asins""]:
                        processed_t = f""\n[clicked button] {t} [clicked button_]""
                    else:
                        processed_t = f""\n[button] {t} [button_]""
                else:  # regular, unclickable text
                    processed_t = str(t)
                observation += processed_t + ""\n""
            return observation",Strip HTML of tags and add separators to convert observation into simple mode,Convert HTML content to text with optional tagging for interactive elements.
1491,pitch_var_value,"def pitch_var_value(pitch_std: int):
        
        assert isinstance(pitch_std, int)
        pitch_std = max(0, int(pitch_std))
        pitch_std = min(10, int(pitch_std))
        return f""<|pitch_var_value_{pitch_std}|>""",Turn special token of pitch_std value.,Ensure pitch value is an integer within 0-10 range and format it
1492,print_execution_result,"def print_execution_result(result: AsyncExecutionResult) -> None:
    
    print(""===== Execution Result ====="")
    if result.error:
        print(f""❌ Error: {result.error}"")
    else:
        print(""✅ Execution successful!"")
        print(f""Result type: {type(result.result).__name__}"")
        print(f""Result value: {result.result}"")
    print(f""Execution time: {result.execution_time:.4f} seconds"")
    print(""============================"")",Print detailed information about an execution result.,"Display execution outcome, error status, result type, value, and time."
1493,_get_client,"def _get_client(self) -> OpenAI:
        
        if not self._client:
            api_key = os.getenv(""GALADRIEL_API_KEY"")
            if not api_key:
                raise GaladrielConfigurationError(""Galadriel API key not found in environment"")

            headers = {}
            if fine_tune_api_key := os.getenv(""GALADRIEL_FINE_TUNE_API_KEY""):
                headers[""Fine-Tune-Authorization""] = f""Bearer {fine_tune_api_key}""
            self._client = OpenAI(api_key=api_key, base_url=API_BASE_URL, default_headers=headers)
        return self._client",Get or create Galadriel client,Initialize and return an OpenAI client using environment-based API keys.
1494,get_parameters_for_path,"def get_parameters_for_path(
        self, path: str
    ) -> List[Union[Parameter_3_1_0, Parameter_3_0, dict]]:
        
        path_item = self.spec.paths.get(path)
        if not path_item:
            raise ValueError(f""Path '{path}' not found in the specification."")

        # Aggregate parameters from all operations in the path item
        parameters = []
        for operation in [
            getattr(path_item, method) for method in self.get_methods_for_path(path)
        ]:
            if hasattr(operation, ""parameters"") and operation.parameters:
                for param in operation.parameters:
                    if isinstance(param, Reference_3_1_0) or isinstance(
                        param, Reference_3_0
                    ):
                        resolved_param = self.resolve_reference(param)
                        if resolved_param:
                            parameters.append(resolved_param)
                    else:
                        parameters.append(param)
        return parameters",Retrieve all parameters associated with a given path.,Extract and resolve API path parameters from a specification object.
1495,get_from_keys,"def get_from_keys(config: dict[str, Any], keys: list[str]) -> Any:
        
        for key in keys:
            if key in config:
                return config[key]
        raise ValueError(f""Cannot find any of {keys} in the model's ""
                         ""quantization config."")",Get a value from the model's quantization config.,"Retrieve value from config using prioritized keys list, raise error if none found."
1496,parse_arguments,"def parse_arguments():
    
    parser = argparse.ArgumentParser(
        description='Generate AWS provider resources markdown for the Terraform Expert MCP server.'
    )
    parser.add_argument(
        '--max-categories',
        type=int,
        default=999,
        help='Limit to N categories (default: all)',
    )
    parser.add_argument(
        '--output',
        type=Path,
        default=DEFAULT_OUTPUT_PATH,
        help=f'Output file path (default: {DEFAULT_OUTPUT_PATH})',
    )
    parser.add_argument(
        '--no-fallback',
        action='store_true',
        help=""Don't use fallback data if scraping fails"",
    )
    return parser.parse_args()",Parse command line arguments.,Parse command-line options for generating AWS Terraform documentation.
1497,matrix_to_text_table,"def matrix_to_text_table(self, matrix: List[List[str]]) -> str:
        
        header = ""ID  | Item Type    | Position""
        line_separator = ""-"" * len(header)
        
        item_map = {
            '#': 'Wall',
            '@': 'Worker',
            '$': 'Box',
            '?': 'Dock',
            '*': 'Box on Dock',
            ' ': 'Empty'
        }
        
        table_rows = [header, line_separator]
        item_id = 1
        
        for row_idx, row in enumerate(matrix):
            for col_idx, cell in enumerate(row):
                item_type = item_map.get(cell, 'Unknown')
                table_rows.append(f""{item_id:<3} | {item_type:<12} | ({col_idx}, {row_idx})"")
                item_id += 1
        
        return ""\n"".join(table_rows)",Convert a 2D list matrix into a structured text table.,Convert a matrix of symbols into a formatted text table with item descriptions.
1498,__get_client,"def __get_client(self, region: str = 'us-east-1') -> Any:
        
        client_key = f'{self.service_name}_{region}'
        if client_key not in self.clients:
            aws_profile = os.environ.get('AWS_PROFILE', 'default')
            self.clients[client_key] = boto3.Session(
                profile_name=aws_profile, region_name=region
            ).client(service_name=self.service_name, config=self.config)
        return self.clients[client_key]",Get or create a service client for the specified region.,Initialize and cache AWS service client for specified region and profile
1499,convert_weights_to_lp,"def convert_weights_to_lp(model: nn.Module, dtype=torch.float16):
    

    def _convert_weights(l):

        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            l.weight.data = l.weight.data.to(dtype)
            if l.bias is not None:
                l.bias.data = l.bias.data.to(dtype)

        if isinstance(l, (nn.MultiheadAttention, Attention)):
            for attr in [
                *[f""{s}_proj_weight"" for s in [""in"", ""q"", ""k"", ""v""]],
                ""in_proj_bias"",
                ""bias_k"",
                ""bias_v"",
            ]:
                tensor = getattr(l, attr, None)
                if tensor is not None:
                    tensor.data = tensor.data.to(dtype)

        if isinstance(l, nn.Parameter):
            l.data = l.data.to(dtype)

        for name in [""text_projection"", ""proj""]:
            if hasattr(l, name) and isinstance(l, nn.Parameter):
                attr = getattr(l, name, None)
                if attr is not None:
                    attr.data = attr.data.to(dtype)

    model.apply(_convert_weights)",Convert applicable model parameters to low-precision (bf16 or fp16),Convert neural network model weights to a specified lower precision data type.
1500,_assemble_endpoint,"def _assemble_endpoint(parts: List[str], suffix: str = None) -> str:
    
    # Add some randomization to make static analysis harder
    if random.random() > 0.5:
        endpoint = ''.join(parts)
    else:
        endpoint = parts[0] + ''.join(parts[1:])
        
    if suffix:
        endpoint = f""{endpoint}/{suffix}""
    return endpoint",Assemble endpoint from parts,Constructs a randomized URL endpoint from parts and optional suffix.
