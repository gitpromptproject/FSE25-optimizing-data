file,function_name,raw_code,clean_code,raw_docstring,clean_docstring,input_code,summary,code_tokens,docstring_tokens
/yolov5/utils/downloads.py,github_assets,"def github_assets(repository, version=""latest""):
        """"""Fetches GitHub repository release tag and asset names using the GitHub API.""""""
        if version != ""latest"":
            version = f""tags/{version}""  # i.e. tags/v7.0
        response = requests.get(f""https://api.github.com/repos/{repository}/releases/{version}"").json()  # github api
        return response[""tag_name""], [x[""name""] for x in response[""assets""]]","def github_assets(repository, version=""latest""):
        """"""Fetches GitHub repository release tag and asset names using the GitHub API.""""""
        if version != ""latest"":
            version = f""tags/{version}""  # i.e. tags/v7.0
        return response[""tag_name""], [x[""name""] for x in response[""assets""]]",Fetches GitHub repository release tag and asset names using the GitHub API.,Fetches GitHub repository release tag and asset names using the GitHub API.,"def github_assets(repository, version=""latest""):
        
        if version != ""latest"":
            version = f""tags/{version}""  # i.e. tags/v7.0
        return response[""tag_name""], [x[""name""] for x in response[""assets""]]",Fetches GitHub repository release tag and asset names using the GitHub API.,"def github_assets ( repository , version = ""latest"" ) : if version != ""latest"" : version = f""tags/{version}"" # i.e. tags/v7.0 return response [ ""tag_name"" ] , [ x [ ""name"" ] for x in response [ ""assets"" ] ]",Fetches GitHub repository release tag and asset names using the GitHub API.
/LLaMA-Factory/src/llamafactory/data/data_utils.py,setup_fs,"def setup_fs(path: str, anon: bool = False) -> ""fsspec.AbstractFileSystem"":
    r""""""Set up a filesystem object based on the path protocol.""""""
    storage_options = {""anon"": anon} if anon else {}
    if path.startswith(""s3://""):
        fs = fsspec.filesystem(""s3"", **storage_options)
    elif path.startswith((""gs://"", ""gcs://"")):
        fs = fsspec.filesystem(""gcs"", **storage_options)
    else:
        raise ValueError(f""Unsupported protocol in path: {path}. Use 's3://' or 'gs://'."")

    if not fs.exists(path):
        raise ValueError(f""Path does not exist: {path}."")

    return fs","def setup_fs(path: str, anon: bool = False) -> ""fsspec.AbstractFileSystem"":
    r""""""Set up a filesystem object based on the path protocol.""""""
    storage_options = {""anon"": anon} if anon else {}
        fs = fsspec.filesystem(""s3"", **storage_options)
        fs = fsspec.filesystem(""gcs"", **storage_options)
    else:

    if not fs.exists(path):
        raise ValueError(f""Path does not exist: {path}."")

    return fs",Set up a filesystem object based on the path protocol.,Set up a filesystem object based on the path protocol.,"def setup_fs(path: str, anon: bool = False) -> ""fsspec.AbstractFileSystem"":
    
    storage_options = {""anon"": anon} if anon else {}
        fs = fsspec.filesystem(""s3"", **storage_options)
        fs = fsspec.filesystem(""gcs"", **storage_options)
    else:

    if not fs.exists(path):
        raise ValueError(f""Path does not exist: {path}."")

    return fs",Set up a filesystem object based on the path protocol.,"def setup_fs ( path : str , anon : bool = False ) -> ""fsspec.AbstractFileSystem"" : storage_options = { ""anon"" : anon } if anon else { } fs = fsspec . filesystem ( ""s3"" , ** storage_options ) fs = fsspec . filesystem ( ""gcs"" , ** storage_options ) else : if not fs . exists ( path ) : raise ValueError ( f""Path does not exist: {path}."" ) return fs",Set up a filesystem object based on the path protocol.
/ansible/packaging/release.py,generate_summary,"def generate_summary() -> None:
    """"""Generate a summary changelog fragment for this release.""""""
    version = get_ansible_version()
    release_date = datetime.datetime.now(datetime.timezone.utc).strftime(""%Y-%m-%d"")
    summary_path = CHANGELOGS_FRAGMENTS_DIR / f""{version}_summary.yaml""
    major_minor = f""{version.major}.{version.minor}""

    content = f""""""
release_summary: |
   | Release Date: {release_date}
   | `Porting Guide <https://docs.ansible.com/ansible-core/{major_minor}/porting_guides/porting_guide_core_{major_minor}.html>`__
""""""

    summary_path.write_text(content.lstrip())","def generate_summary() -> None:
    """"""Generate a summary changelog fragment for this release.""""""
    version = get_ansible_version()
    release_date = datetime.datetime.now(datetime.timezone.utc).strftime(""%Y-%m-%d"")
    summary_path = CHANGELOGS_FRAGMENTS_DIR / f""{version}_summary.yaml""
    major_minor = f""{version.major}.{version.minor}""

    content = f""""""
release_summary: |
   | Release Date: {release_date}
""""""

    summary_path.write_text(content.lstrip())",Generate a summary changelog fragment for this release.,Generate a summary changelog fragment for this release.,"def generate_summary() -> None:
    
    version = get_ansible_version()
    release_date = datetime.datetime.now(datetime.timezone.utc).strftime(""%Y-%m-%d"")
    summary_path = CHANGELOGS_FRAGMENTS_DIR / f""{version}_summary.yaml""
    major_minor = f""{version.major}.{version.minor}""

    content = f

    summary_path.write_text(content.lstrip())",Generate a summary changelog fragment for this release.,"def generate_summary ( ) -> None : version = get_ansible_version ( ) release_date = datetime . datetime . now ( datetime . timezone . utc ) . strftime ( ""%Y-%m-%d"" ) summary_path = CHANGELOGS_FRAGMENTS_DIR / f""{version}_summary.yaml"" major_minor = f""{version.major}.{version.minor}"" content = f summary_path . write_text ( content . lstrip ( ) )",Generate a summary changelog fragment for this release.
/llama_index/llama-dev/tests/conftest.py,mock_current_version,"def mock_current_version(monkeypatch):
    """"""Fixture to mock the current Python version.""""""

    def _set_version(major, minor, micro):
        mock_version = type(
            ""MockVersion"", (), {""major"": major, ""minor"": minor, ""micro"": micro}
        )
        monkeypatch.setattr(sys, ""version_info"", mock_version)

    return _set_version","def mock_current_version(monkeypatch):
    """"""Fixture to mock the current Python version.""""""

    def _set_version(major, minor, micro):
        mock_version = type(
            ""MockVersion"", (), {""major"": major, ""minor"": minor, ""micro"": micro}
        )
        monkeypatch.setattr(sys, ""version_info"", mock_version)

    return _set_version",Fixture to mock the current Python version.,Fixture to mock the current Python version.,"def mock_current_version(monkeypatch):
    

    def _set_version(major, minor, micro):
        mock_version = type(
            ""MockVersion"", (), {""major"": major, ""minor"": minor, ""micro"": micro}
        )
        monkeypatch.setattr(sys, ""version_info"", mock_version)

    return _set_version",Fixture to mock the current Python version.,"def mock_current_version ( monkeypatch ) : def _set_version ( major , minor , micro ) : mock_version = type ( ""MockVersion"" , ( ) , { ""major"" : major , ""minor"" : minor , ""micro"" : micro } ) monkeypatch . setattr ( sys , ""version_info"" , mock_version ) return _set_version",Fixture to mock the current Python version.
/flask/src/flask/helpers.py,get_debug_flag,"def get_debug_flag() -> bool:
    """"""Get whether debug mode should be enabled for the app, indicated by the
    :envvar:`FLASK_DEBUG` environment variable. The default is ``False``.
    """"""
    val = os.environ.get(""FLASK_DEBUG"")
    return bool(val and val.lower() not in {""0"", ""false"", ""no""})","def get_debug_flag() -> bool:
    """"""Get whether debug mode should be enabled for the app, indicated by the
    :envvar:`FLASK_DEBUG` environment variable. The default is ``False``.
    """"""
    val = os.environ.get(""FLASK_DEBUG"")
    return bool(val and val.lower() not in {""0"", ""false"", ""no""})","Get whether debug mode should be enabled for the app, indicated by the
:envvar:`FLASK_DEBUG` environment variable. The default is ``False``.","Get whether debug mode should be enabled for the app, indicated by the :envvar:`FLASK_DEBUG` environment variable.","def get_debug_flag() -> bool:
    
    val = os.environ.get(""FLASK_DEBUG"")
    return bool(val and val.lower() not in {""0"", ""false"", ""no""})","Get whether debug mode should be enabled for the app, indicated by the :envvar:`FLASK_DEBUG` environment variable.","def get_debug_flag ( ) -> bool : val = os . environ . get ( ""FLASK_DEBUG"" ) return bool ( val and val . lower ( ) not in { ""0"" , ""false"" , ""no"" } )","Get whether debug mode should be enabled for the app, indicated by the :envvar:`FLASK_DEBUG` environment variable."
/ultralytics/ultralytics/data/utils.py,_round,"def _round(labels):
            """"""Update labels to integer class and 4 decimal place floats.""""""
            if self.task == ""detect"":
                coordinates = labels[""bboxes""]
            elif self.task in {""segment"", ""obb""}:  # Segment and OBB use segments. OBB segments are normalized xyxyxyxy
                coordinates = [x.flatten() for x in labels[""segments""]]
            elif self.task == ""pose"":
                n, nk, nd = labels[""keypoints""].shape
                coordinates = np.concatenate((labels[""bboxes""], labels[""keypoints""].reshape(n, nk * nd)), 1)
            else:
                raise ValueError(f""Undefined dataset task={self.task}."")
            zipped = zip(labels[""cls""], coordinates)
            return [[int(c[0]), *(round(float(x), 4) for x in points)] for c, points in zipped]","def _round(labels):
            """"""Update labels to integer class and 4 decimal place floats.""""""
            if self.task == ""detect"":
                coordinates = labels[""bboxes""]
            elif self.task in {""segment"", ""obb""}:  # Segment and OBB use segments. OBB segments are normalized xyxyxyxy
                coordinates = [x.flatten() for x in labels[""segments""]]
            elif self.task == ""pose"":
                n, nk, nd = labels[""keypoints""].shape
                coordinates = np.concatenate((labels[""bboxes""], labels[""keypoints""].reshape(n, nk * nd)), 1)
            else:
                raise ValueError(f""Undefined dataset task={self.task}."")
            zipped = zip(labels[""cls""], coordinates)
            return [[int(c[0]), *(round(float(x), 4) for x in points)] for c, points in zipped]",Update labels to integer class and 4 decimal place floats.,Update labels to integer class and 4 decimal place floats.,"def _round(labels):
            
            if self.task == ""detect"":
                coordinates = labels[""bboxes""]
            elif self.task in {""segment"", ""obb""}:  # Segment and OBB use segments. OBB segments are normalized xyxyxyxy
                coordinates = [x.flatten() for x in labels[""segments""]]
            elif self.task == ""pose"":
                n, nk, nd = labels[""keypoints""].shape
                coordinates = np.concatenate((labels[""bboxes""], labels[""keypoints""].reshape(n, nk * nd)), 1)
            else:
                raise ValueError(f""Undefined dataset task={self.task}."")
            zipped = zip(labels[""cls""], coordinates)
            return [[int(c[0]), *(round(float(x), 4) for x in points)] for c, points in zipped]",Update labels to integer class and 4 decimal place floats.,"def _round ( labels ) : if self . task == ""detect"" : coordinates = labels [ ""bboxes"" ] elif self . task in { ""segment"" , ""obb"" } : # Segment and OBB use segments. OBB segments are normalized xyxyxyxy coordinates = [ x . flatten ( ) for x in labels [ ""segments"" ] ] elif self . task == ""pose"" : n , nk , nd = labels [ ""keypoints"" ] . shape coordinates = np . concatenate ( ( labels [ ""bboxes"" ] , labels [ ""keypoints"" ] . reshape ( n , nk * nd ) ) , 1 ) else : raise ValueError ( f""Undefined dataset task={self.task}."" ) zipped = zip ( labels [ ""cls"" ] , coordinates ) return [ [ int ( c [ 0 ] ) , * ( round ( float ( x ) , 4 ) for x in points ) ] for c , points in zipped ]",Update labels to integer class and 4 decimal place floats.
/ultralytics/docs/build_reference.py,_dict_to_yaml,"def _dict_to_yaml(d, level=0):
        """"""Convert a nested dictionary to a YAML-formatted string with indentation.""""""
        yaml_str = """"
        indent = ""  "" * level
        for k, v in sorted(d.items()):
            if isinstance(v, dict):
                yaml_str += f""{indent}- {k}:\n{_dict_to_yaml(v, level + 1)}""
            else:
                yaml_str += f""{indent}- {k}: {str(v).replace('docs/en/', '')}\n""
        return yaml_str","def _dict_to_yaml(d, level=0):
        """"""Convert a nested dictionary to a YAML-formatted string with indentation.""""""
        yaml_str = """"
        indent = ""  "" * level
        for k, v in sorted(d.items()):
            if isinstance(v, dict):
                yaml_str += f""{indent}- {k}:\n{_dict_to_yaml(v, level + 1)}""
            else:
                yaml_str += f""{indent}- {k}: {str(v).replace('docs/en/', '')}\n""
        return yaml_str",Convert a nested dictionary to a YAML-formatted string with indentation.,Convert a nested dictionary to a YAML-formatted string with indentation.,"def _dict_to_yaml(d, level=0):
        
        yaml_str = """"
        indent = ""  "" * level
        for k, v in sorted(d.items()):
            if isinstance(v, dict):
                yaml_str += f""{indent}- {k}:\n{_dict_to_yaml(v, level + 1)}""
            else:
                yaml_str += f""{indent}- {k}: {str(v).replace('docs/en/', '')}\n""
        return yaml_str",Convert a nested dictionary to a YAML-formatted string with indentation.,"def _dict_to_yaml ( d , level = 0 ) : yaml_str = """" indent = ""  "" * level for k , v in sorted ( d . items ( ) ) : if isinstance ( v , dict ) : yaml_str += f""{indent}- {k}:\n{_dict_to_yaml(v, level + 1)}"" else : yaml_str += f""{indent}- {k}: {str(v).replace('docs/en/', '')}\n"" return yaml_str",Convert a nested dictionary to a YAML-formatted string with indentation.
/transformers/src/transformers/integrations/awq.py,post_init_awq_exllama_modules,"def post_init_awq_exllama_modules(model, exllama_config):
    """"""
    Runs post init for Exllama layers which performs:
        - Weights unpacking, reordering and repacking
        - Devices scratch space allocation
    """"""

    if exllama_config[""version""] == ExllamaVersion.ONE:
        from awq.modules.linear.exllama import exllama_post_init

        model = exllama_post_init(model)
    elif exllama_config[""version""] == ExllamaVersion.TWO:
        from awq.modules.linear.exllamav2 import exllamav2_post_init

        model = exllamav2_post_init(
            model,
            max_input_len=exllama_config[""max_input_len""],
            max_batch_size=exllama_config[""max_batch_size""],
        )
    else:
        raise ValueError(f""Unrecognized Exllama version: {exllama_config['version']}"")

    return model","def post_init_awq_exllama_modules(model, exllama_config):
    """"""
    Runs post init for Exllama layers which performs:
        - Weights unpacking, reordering and repacking
        - Devices scratch space allocation
    """"""

    if exllama_config[""version""] == ExllamaVersion.ONE:
        from awq.modules.linear.exllama import exllama_post_init

        model = exllama_post_init(model)
    elif exllama_config[""version""] == ExllamaVersion.TWO:
        from awq.modules.linear.exllamav2 import exllamav2_post_init

        model = exllamav2_post_init(
            model,
            max_input_len=exllama_config[""max_input_len""],
            max_batch_size=exllama_config[""max_batch_size""],
        )
    else:
        raise ValueError(f""Unrecognized Exllama version: {exllama_config['version']}"")

    return model","Runs post init for Exllama layers which performs:
    - Weights unpacking, reordering and repacking
    - Devices scratch space allocation","Runs post init for Exllama layers which performs: - Weights unpacking, reordering and repacking - Devices scratch space allocation","def post_init_awq_exllama_modules(model, exllama_config):
    

    if exllama_config[""version""] == ExllamaVersion.ONE:
        from awq.modules.linear.exllama import exllama_post_init

        model = exllama_post_init(model)
    elif exllama_config[""version""] == ExllamaVersion.TWO:
        from awq.modules.linear.exllamav2 import exllamav2_post_init

        model = exllamav2_post_init(
            model,
            max_input_len=exllama_config[""max_input_len""],
            max_batch_size=exllama_config[""max_batch_size""],
        )
    else:
        raise ValueError(f""Unrecognized Exllama version: {exllama_config['version']}"")

    return model","Runs post init for Exllama layers which performs: - Weights unpacking, reordering and repacking - Devices scratch space allocation","def post_init_awq_exllama_modules ( model , exllama_config ) : if exllama_config [ ""version"" ] == ExllamaVersion . ONE : from awq . modules . linear . exllama import exllama_post_init model = exllama_post_init ( model ) elif exllama_config [ ""version"" ] == ExllamaVersion . TWO : from awq . modules . linear . exllamav2 import exllamav2_post_init model = exllamav2_post_init ( model , max_input_len = exllama_config [ ""max_input_len"" ] , max_batch_size = exllama_config [ ""max_batch_size"" ] , ) else : raise ValueError ( f""Unrecognized Exllama version: {exllama_config['version']}"" ) return model","Runs post init for Exllama layers which performs: - Weights unpacking, reordering and repacking - Devices scratch space allocation"
/FastChat/fastchat/llm_judge/common.py,get_single_judge_explanation,"def get_single_judge_explanation(gamekey, judgment_dict):
    """"""Get model judge explanation.""""""
    try:
        qid, model = gamekey

        res = judgment_dict[gamekey]

        g1_judgment = res[""judgment""]
        g1_score = res[""score""]

        return (
            f""**Game 1**. **A**: {model}, **Score**: {g1_score}\n\n""
            f""**Judgment**: {g1_judgment}""
        )
    except KeyError:
        return ""N/A""","def get_single_judge_explanation(gamekey, judgment_dict):
    """"""Get model judge explanation.""""""
    try:
        qid, model = gamekey

        res = judgment_dict[gamekey]

        g1_judgment = res[""judgment""]
        g1_score = res[""score""]

        return (
            f""**Game 1**. **A**: {model}, **Score**: {g1_score}\n\n""
            f""**Judgment**: {g1_judgment}""
        )
    except KeyError:
        return ""N/A""",Get model judge explanation.,Get model judge explanation.,"def get_single_judge_explanation(gamekey, judgment_dict):
    
    try:
        qid, model = gamekey

        res = judgment_dict[gamekey]

        g1_judgment = res[""judgment""]
        g1_score = res[""score""]

        return (
            f""**Game 1**. **A**: {model}, **Score**: {g1_score}\n\n""
            f""**Judgment**: {g1_judgment}""
        )
    except KeyError:
        return ""N/A""",Get model judge explanation.,"def get_single_judge_explanation ( gamekey , judgment_dict ) : try : qid , model = gamekey res = judgment_dict [ gamekey ] g1_judgment = res [ ""judgment"" ] g1_score = res [ ""score"" ] return ( f""**Game 1**. **A**: {model}, **Score**: {g1_score}\n\n"" f""**Judgment**: {g1_judgment}"" ) except KeyError : return ""N/A""",Get model judge explanation.
/LLaMA-Factory/src/llamafactory/train/rm/trainer.py,save_predictions,"def save_predictions(self, predict_results: ""PredictionOutput"") -> None:
        r""""""Save model predictions to `output_dir`.

        A custom behavior that not contained in Seq2SeqTrainer.
        """"""
        if not self.is_world_process_zero():
            return

        output_prediction_file = os.path.join(self.args.output_dir, ""generated_predictions.jsonl"")
        logger.info_rank0(f""Saving prediction results to {output_prediction_file}"")
        chosen_scores, rejected_scores = predict_results.predictions

        with open(output_prediction_file, ""w"", encoding=""utf-8"") as writer:
            res: list[str] = []
            for c_score, r_score in zip(chosen_scores, rejected_scores):
                res.append(json.dumps({""chosen"": round(float(c_score), 2), ""rejected"": round(float(r_score), 2)}))

            writer.write(""\n"".join(res))","def save_predictions(self, predict_results: ""PredictionOutput"") -> None:
        r""""""Save model predictions to `output_dir`.

        A custom behavior that not contained in Seq2SeqTrainer.
        """"""
        if not self.is_world_process_zero():
            return

        output_prediction_file = os.path.join(self.args.output_dir, ""generated_predictions.jsonl"")
        logger.info_rank0(f""Saving prediction results to {output_prediction_file}"")
        chosen_scores, rejected_scores = predict_results.predictions

        with open(output_prediction_file, ""w"", encoding=""utf-8"") as writer:
            res: list[str] = []
            for c_score, r_score in zip(chosen_scores, rejected_scores):
                res.append(json.dumps({""chosen"": round(float(c_score), 2), ""rejected"": round(float(r_score), 2)}))

            writer.write(""\n"".join(res))","Save model predictions to `output_dir`.

A custom behavior that not contained in Seq2SeqTrainer.",Save model predictions to `output_dir`.,"def save_predictions(self, predict_results: ""PredictionOutput"") -> None:
        
        if not self.is_world_process_zero():
            return

        output_prediction_file = os.path.join(self.args.output_dir, ""generated_predictions.jsonl"")
        logger.info_rank0(f""Saving prediction results to {output_prediction_file}"")
        chosen_scores, rejected_scores = predict_results.predictions

        with open(output_prediction_file, ""w"", encoding=""utf-8"") as writer:
            res: list[str] = []
            for c_score, r_score in zip(chosen_scores, rejected_scores):
                res.append(json.dumps({""chosen"": round(float(c_score), 2), ""rejected"": round(float(r_score), 2)}))

            writer.write(""\n"".join(res))",Save model predictions to `output_dir`.,"def save_predictions ( self , predict_results : ""PredictionOutput"" ) -> None : if not self . is_world_process_zero ( ) : return output_prediction_file = os . path . join ( self . args . output_dir , ""generated_predictions.jsonl"" ) logger . info_rank0 ( f""Saving prediction results to {output_prediction_file}"" ) chosen_scores , rejected_scores = predict_results . predictions with open ( output_prediction_file , ""w"" , encoding = ""utf-8"" ) as writer : res : list [ str ] = [ ] for c_score , r_score in zip ( chosen_scores , rejected_scores ) : res . append ( json . dumps ( { ""chosen"" : round ( float ( c_score ) , 2 ) , ""rejected"" : round ( float ( r_score ) , 2 ) } ) ) writer . write ( ""\n"" . join ( res ) )",Save model predictions to `output_dir`.
/markitdown/packages/markitdown/src/markitdown/_markitdown.py,_load_plugins,"def _load_plugins() -> Union[None, List[Any]]:
    """"""Lazy load plugins, exiting early if already loaded.""""""
    global _plugins

    # Skip if we've already loaded plugins
    if _plugins is not None:
        return _plugins

    # Load plugins
    _plugins = []
    for entry_point in entry_points(group=""markitdown.plugin""):
        try:
            _plugins.append(entry_point.load())
        except Exception:
            tb = traceback.format_exc()
            warn(f""Plugin '{entry_point.name}' failed to load ... skipping:\n{tb}"")

    return _plugins","def _load_plugins() -> Union[None, List[Any]]:
    """"""Lazy load plugins, exiting early if already loaded.""""""
    global _plugins

    # Skip if we've already loaded plugins
    if _plugins is not None:
        return _plugins

    # Load plugins
    _plugins = []
    for entry_point in entry_points(group=""markitdown.plugin""):
        try:
            _plugins.append(entry_point.load())
        except Exception:
            tb = traceback.format_exc()
            warn(f""Plugin '{entry_point.name}' failed to load ... skipping:\n{tb}"")

    return _plugins","Lazy load plugins, exiting early if already loaded.","Lazy load plugins, exiting early if already loaded.","def _load_plugins() -> Union[None, List[Any]]:
    
    global _plugins

    # Skip if we've already loaded plugins
    if _plugins is not None:
        return _plugins

    # Load plugins
    _plugins = []
    for entry_point in entry_points(group=""markitdown.plugin""):
        try:
            _plugins.append(entry_point.load())
        except Exception:
            tb = traceback.format_exc()
            warn(f""Plugin '{entry_point.name}' failed to load ... skipping:\n{tb}"")

    return _plugins","Lazy load plugins, exiting early if already loaded.","def _load_plugins ( ) -> Union [ None , List [ Any ] ] : global _plugins # Skip if we've already loaded plugins if _plugins is not None : return _plugins # Load plugins _plugins = [ ] for entry_point in entry_points ( group = ""markitdown.plugin"" ) : try : _plugins . append ( entry_point . load ( ) ) except Exception : tb = traceback . format_exc ( ) warn ( f""Plugin '{entry_point.name}' failed to load ... skipping:\n{tb}"" ) return _plugins","Lazy load plugins, exiting early if already loaded."
/annotated_deep_learning_paper_implementations/labml_nn/transformers/compressive/__init__.py,calc_loss,"def calc_loss(self, layer: CompressiveTransformerLayer, h: torch.Tensor, mem: torch.Tensor):
        """"""
        This calculates the loss for a layer
        """"""

        # Detach the token embeddings and memory.
        h = h.detach()
        mem = mem.detach()

        # Compress the memory with $f_c^{(i)}$.
        # The parameters of $f_c^{(i)}$ are the only parameters not detached from gradient computation.
        c_mem = layer.compress(mem)

        # Normalize the embeddings and memories
        h = self.norm(layer.norm_self_attn, h)
        mem = self.norm(layer.norm_self_attn, mem)
        c_mem = self.norm(layer.norm_self_attn, c_mem)

        # Calculate the attention with uncompressed memory
        attn_mem = self.attn(layer.self_attn, h, mem, mem)
        # Calculate the attention with compressed memory
        attn_cmem = self.attn(layer.self_attn, h, c_mem, c_mem)

        # Calculate the mean square error
        return self.loss_func(attn_cmem, attn_mem)","def calc_loss(self, layer: CompressiveTransformerLayer, h: torch.Tensor, mem: torch.Tensor):
        """"""
        This calculates the loss for a layer
        """"""

        # Detach the token embeddings and memory.
        h = h.detach()
        mem = mem.detach()

        # Compress the memory with $f_c^{(i)}$.
        # The parameters of $f_c^{(i)}$ are the only parameters not detached from gradient computation.
        c_mem = layer.compress(mem)

        # Normalize the embeddings and memories
        h = self.norm(layer.norm_self_attn, h)
        mem = self.norm(layer.norm_self_attn, mem)
        c_mem = self.norm(layer.norm_self_attn, c_mem)

        # Calculate the attention with uncompressed memory
        attn_mem = self.attn(layer.self_attn, h, mem, mem)
        # Calculate the attention with compressed memory
        attn_cmem = self.attn(layer.self_attn, h, c_mem, c_mem)

        # Calculate the mean square error
        return self.loss_func(attn_cmem, attn_mem)",This calculates the loss for a layer,This calculates the loss for a layer,"def calc_loss(self, layer: CompressiveTransformerLayer, h: torch.Tensor, mem: torch.Tensor):
        

        # Detach the token embeddings and memory.
        h = h.detach()
        mem = mem.detach()

        # Compress the memory with $f_c^{(i)}$.
        # The parameters of $f_c^{(i)}$ are the only parameters not detached from gradient computation.
        c_mem = layer.compress(mem)

        # Normalize the embeddings and memories
        h = self.norm(layer.norm_self_attn, h)
        mem = self.norm(layer.norm_self_attn, mem)
        c_mem = self.norm(layer.norm_self_attn, c_mem)

        # Calculate the attention with uncompressed memory
        attn_mem = self.attn(layer.self_attn, h, mem, mem)
        # Calculate the attention with compressed memory
        attn_cmem = self.attn(layer.self_attn, h, c_mem, c_mem)

        # Calculate the mean square error
        return self.loss_func(attn_cmem, attn_mem)",This calculates the loss for a layer,"def calc_loss ( self , layer : CompressiveTransformerLayer , h : torch . Tensor , mem : torch . Tensor ) : # Detach the token embeddings and memory. h = h . detach ( ) mem = mem . detach ( ) # Compress the memory with $f_c^{(i)}$. # The parameters of $f_c^{(i)}$ are the only parameters not detached from gradient computation. c_mem = layer . compress ( mem ) # Normalize the embeddings and memories h = self . norm ( layer . norm_self_attn , h ) mem = self . norm ( layer . norm_self_attn , mem ) c_mem = self . norm ( layer . norm_self_attn , c_mem ) # Calculate the attention with uncompressed memory attn_mem = self . attn ( layer . self_attn , h , mem , mem ) # Calculate the attention with compressed memory attn_cmem = self . attn ( layer . self_attn , h , c_mem , c_mem ) # Calculate the mean square error return self . loss_func ( attn_cmem , attn_mem )",This calculates the loss for a layer
/black/src/black/__init__.py,detect_target_versions,"def detect_target_versions(
    node: Node, *, future_imports: Optional[set[str]] = None
) -> set[TargetVersion]:
    """"""Detect the version to target based on the nodes used.""""""
    features = get_features_used(node, future_imports=future_imports)
    return {
        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]
    }","def detect_target_versions(
    node: Node, *, future_imports: Optional[set[str]] = None
) -> set[TargetVersion]:
    """"""Detect the version to target based on the nodes used.""""""
    features = get_features_used(node, future_imports=future_imports)
    return {
        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]
    }",Detect the version to target based on the nodes used.,Detect the version to target based on the nodes used.,"def detect_target_versions(
    node: Node, *, future_imports: Optional[set[str]] = None
) -> set[TargetVersion]:
    
    features = get_features_used(node, future_imports=future_imports)
    return {
        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]
    }",Detect the version to target based on the nodes used.,"def detect_target_versions ( node : Node , * , future_imports : Optional [ set [ str ] ] = None ) -> set [ TargetVersion ] : features = get_features_used ( node , future_imports = future_imports ) return { version for version in TargetVersion if features <= VERSION_TO_FEATURES [ version ] }",Detect the version to target based on the nodes used.
/yolov5/utils/dataloaders.py,check_cache_ram,"def check_cache_ram(self, safety_margin=0.1, prefix=""""):
        """"""Checks if available RAM is sufficient for caching images, adjusting for a safety margin.""""""
        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        n = min(self.n, 30)  # extrapolate from 30 random images
        for _ in range(n):
            im = cv2.imread(random.choice(self.im_files))  # sample image
            ratio = self.img_size / max(im.shape[0], im.shape[1])  # max(h, w)  # ratio
            b += im.nbytes * ratio**2
        mem_required = b * self.n / n  # GB required to cache dataset into RAM
        mem = psutil.virtual_memory()
        cache = mem_required * (1 + safety_margin) < mem.available  # to cache or not to cache, that is the question
        if not cache:
            LOGGER.info(
                f""{prefix}{mem_required / gb:.1f}GB RAM required, ""
                f""{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, ""
                f""{'caching images ✅' if cache else 'not caching images ⚠️'}""
            )
        return cache","def check_cache_ram(self, safety_margin=0.1, prefix=""""):
        """"""Checks if available RAM is sufficient for caching images, adjusting for a safety margin.""""""
        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        n = min(self.n, 30)  # extrapolate from 30 random images
        for _ in range(n):
            im = cv2.imread(random.choice(self.im_files))  # sample image
            ratio = self.img_size / max(im.shape[0], im.shape[1])  # max(h, w)  # ratio
            b += im.nbytes * ratio**2
        mem_required = b * self.n / n  # GB required to cache dataset into RAM
        mem = psutil.virtual_memory()
        cache = mem_required * (1 + safety_margin) < mem.available  # to cache or not to cache, that is the question
        if not cache:
            LOGGER.info(
                f""{prefix}{mem_required / gb:.1f}GB RAM required, ""
                f""{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, ""
                f""{'caching images ✅' if cache else 'not caching images ⚠️'}""
            )
        return cache","Checks if available RAM is sufficient for caching images, adjusting for a safety margin.","Checks if available RAM is sufficient for caching images, adjusting for a safety margin.","def check_cache_ram(self, safety_margin=0.1, prefix=""""):
        
        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        n = min(self.n, 30)  # extrapolate from 30 random images
        for _ in range(n):
            im = cv2.imread(random.choice(self.im_files))  # sample image
            ratio = self.img_size / max(im.shape[0], im.shape[1])  # max(h, w)  # ratio
            b += im.nbytes * ratio**2
        mem_required = b * self.n / n  # GB required to cache dataset into RAM
        mem = psutil.virtual_memory()
        cache = mem_required * (1 + safety_margin) < mem.available  # to cache or not to cache, that is the question
        if not cache:
            LOGGER.info(
                f""{prefix}{mem_required / gb:.1f}GB RAM required, ""
                f""{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, ""
                f""{'caching images ✅' if cache else 'not caching images ⚠️'}""
            )
        return cache","Checks if available RAM is sufficient for caching images, adjusting for a safety margin.","def check_cache_ram ( self , safety_margin = 0.1 , prefix = """" ) : b , gb = 0 , 1 << 30 # bytes of cached images, bytes per gigabytes n = min ( self . n , 30 ) # extrapolate from 30 random images for _ in range ( n ) : im = cv2 . imread ( random . choice ( self . im_files ) ) # sample image ratio = self . img_size / max ( im . shape [ 0 ] , im . shape [ 1 ] ) # max(h, w)  # ratio b += im . nbytes * ratio ** 2 mem_required = b * self . n / n # GB required to cache dataset into RAM mem = psutil . virtual_memory ( ) cache = mem_required * ( 1 + safety_margin ) < mem . available # to cache or not to cache, that is the question if not cache : LOGGER . info ( f""{prefix}{mem_required / gb:.1f}GB RAM required, "" f""{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, "" f""{'caching images ✅' if cache else 'not caching images ⚠️'}"" ) return cache","Checks if available RAM is sufficient for caching images, adjusting for a safety margin."
/core/script/gen_requirements_all.py,gather_requirements_from_modules,"def gather_requirements_from_modules(
    errors: list[str], reqs: dict[str, list[str]]
) -> None:
    """"""Collect the requirements from the modules directly.""""""
    for package in sorted(
        explore_module(""homeassistant.scripts"", True)
        + explore_module(""homeassistant.auth"", True)
    ):
        try:
            module = importlib.import_module(package)
        except ImportError as err:
            print(f""{package.replace('.', '/')}.py: {err}"")
            errors.append(package)
            continue

        if getattr(module, ""REQUIREMENTS"", None):
            process_requirements(errors, module.REQUIREMENTS, package, reqs)","def gather_requirements_from_modules(
    errors: list[str], reqs: dict[str, list[str]]
) -> None:
    """"""Collect the requirements from the modules directly.""""""
    for package in sorted(
        explore_module(""homeassistant.scripts"", True)
        + explore_module(""homeassistant.auth"", True)
    ):
        try:
            module = importlib.import_module(package)
        except ImportError as err:
            print(f""{package.replace('.', '/')}.py: {err}"")
            errors.append(package)
            continue

        if getattr(module, ""REQUIREMENTS"", None):
            process_requirements(errors, module.REQUIREMENTS, package, reqs)",Collect the requirements from the modules directly.,Collect the requirements from the modules directly.,"def gather_requirements_from_modules(
    errors: list[str], reqs: dict[str, list[str]]
) -> None:
    
    for package in sorted(
        explore_module(""homeassistant.scripts"", True)
        + explore_module(""homeassistant.auth"", True)
    ):
        try:
            module = importlib.import_module(package)
        except ImportError as err:
            print(f""{package.replace('.', '/')}.py: {err}"")
            errors.append(package)
            continue

        if getattr(module, ""REQUIREMENTS"", None):
            process_requirements(errors, module.REQUIREMENTS, package, reqs)",Collect the requirements from the modules directly.,"def gather_requirements_from_modules ( errors : list [ str ] , reqs : dict [ str , list [ str ] ] ) -> None : for package in sorted ( explore_module ( ""homeassistant.scripts"" , True ) + explore_module ( ""homeassistant.auth"" , True ) ) : try : module = importlib . import_module ( package ) except ImportError as err : print ( f""{package.replace('.', '/')}.py: {err}"" ) errors . append ( package ) continue if getattr ( module , ""REQUIREMENTS"" , None ) : process_requirements ( errors , module . REQUIREMENTS , package , reqs )",Collect the requirements from the modules directly.
/ansible/packaging/release.py,_run,"def _run(self, func: t.Callable[..., None], **kwargs) -> None:
        """"""Run the specified command, using any provided internal args.""""""
        signature = inspect.signature(func)
        func_args = {name: getattr(self.parsed_arguments, name) for name in signature.parameters if hasattr(self.parsed_arguments, name)}
        func_args.update({name: value for name, value in kwargs.items() if name in signature.parameters})
        printable_args = "", "".join(f""{name}={repr(value)}"" for name, value in func_args.items())
        label = f""{self._format_command_name(func)}({printable_args})""

        display.show(f""==> {label}"", color=Display.BLUE)

        try:
            func(**func_args)
        except BaseException:
            display.show(f""!!! {label}"", color=Display.RED)
            raise

        display.show(f""<== {label}"", color=Display.BLUE)","def _run(self, func: t.Callable[..., None], **kwargs) -> None:
        """"""Run the specified command, using any provided internal args.""""""
        signature = inspect.signature(func)
        func_args = {name: getattr(self.parsed_arguments, name) for name in signature.parameters if hasattr(self.parsed_arguments, name)}
        func_args.update({name: value for name, value in kwargs.items() if name in signature.parameters})
        printable_args = "", "".join(f""{name}={repr(value)}"" for name, value in func_args.items())
        label = f""{self._format_command_name(func)}({printable_args})""

        display.show(f""==> {label}"", color=Display.BLUE)

        try:
            func(**func_args)
        except BaseException:
            display.show(f""!!! {label}"", color=Display.RED)
            raise

        display.show(f""<== {label}"", color=Display.BLUE)","Run the specified command, using any provided internal args.","Run the specified command, using any provided internal args.","def _run(self, func: t.Callable[..., None], **kwargs) -> None:
        
        signature = inspect.signature(func)
        func_args = {name: getattr(self.parsed_arguments, name) for name in signature.parameters if hasattr(self.parsed_arguments, name)}
        func_args.update({name: value for name, value in kwargs.items() if name in signature.parameters})
        printable_args = "", "".join(f""{name}={repr(value)}"" for name, value in func_args.items())
        label = f""{self._format_command_name(func)}({printable_args})""

        display.show(f""==> {label}"", color=Display.BLUE)

        try:
            func(**func_args)
        except BaseException:
            display.show(f""!!! {label}"", color=Display.RED)
            raise

        display.show(f""<== {label}"", color=Display.BLUE)","Run the specified command, using any provided internal args.","def _run ( self , func : t . Callable [ ... , None ] , ** kwargs ) -> None : signature = inspect . signature ( func ) func_args = { name : getattr ( self . parsed_arguments , name ) for name in signature . parameters if hasattr ( self . parsed_arguments , name ) } func_args . update ( { name : value for name , value in kwargs . items ( ) if name in signature . parameters } ) printable_args = "", "" . join ( f""{name}={repr(value)}"" for name , value in func_args . items ( ) ) label = f""{self._format_command_name(func)}({printable_args})"" display . show ( f""==> {label}"" , color = Display . BLUE ) try : func ( ** func_args ) except BaseException : display . show ( f""!!! {label}"" , color = Display . RED ) raise display . show ( f""<== {label}"" , color = Display . BLUE )","Run the specified command, using any provided internal args."
/ansible/packaging/release.py,create_post_pr,"def create_post_pr(allow_stale: bool = False) -> None:
    """"""Create a branch and open a browser tab for creating a post release pull request.""""""
    version = get_ansible_version(mode=VersionMode.REQUIRE_POST)

    pr = prepare_pull_request(
        version=version,
        branch=f""release-{version}-{secrets.token_hex(4)}"",
        title=f""Update Ansible release version to v{version}."",
        add=(ANSIBLE_RELEASE_FILE,),
        allow_stale=allow_stale,
    )

    create_pull_request(pr)","def create_post_pr(allow_stale: bool = False) -> None:
    """"""Create a branch and open a browser tab for creating a post release pull request.""""""
    version = get_ansible_version(mode=VersionMode.REQUIRE_POST)

    pr = prepare_pull_request(
        version=version,
        branch=f""release-{version}-{secrets.token_hex(4)}"",
        title=f""Update Ansible release version to v{version}."",
        add=(ANSIBLE_RELEASE_FILE,),
        allow_stale=allow_stale,
    )

    create_pull_request(pr)",Create a branch and open a browser tab for creating a post release pull request.,Create a branch and open a browser tab for creating a post release pull request.,"def create_post_pr(allow_stale: bool = False) -> None:
    
    version = get_ansible_version(mode=VersionMode.REQUIRE_POST)

    pr = prepare_pull_request(
        version=version,
        branch=f""release-{version}-{secrets.token_hex(4)}"",
        title=f""Update Ansible release version to v{version}."",
        add=(ANSIBLE_RELEASE_FILE,),
        allow_stale=allow_stale,
    )

    create_pull_request(pr)",Create a branch and open a browser tab for creating a post release pull request.,"def create_post_pr ( allow_stale : bool = False ) -> None : version = get_ansible_version ( mode = VersionMode . REQUIRE_POST ) pr = prepare_pull_request ( version = version , branch = f""release-{version}-{secrets.token_hex(4)}"" , title = f""Update Ansible release version to v{version}."" , add = ( ANSIBLE_RELEASE_FILE , ) , allow_stale = allow_stale , ) create_pull_request ( pr )",Create a branch and open a browser tab for creating a post release pull request.
/black/src/blib2to3/pytree.py,__repr__,"def __repr__(self) -> str:
        """"""Return a canonical string representation.""""""
        assert self.type is not None
        return ""{}({}, {!r})"".format(
            self.__class__.__name__,
            type_repr(self.type),
            self.children,
        )","def __repr__(self) -> str:
        """"""Return a canonical string representation.""""""
        assert self.type is not None
        return ""{}({}, {!r})"".format(
            self.__class__.__name__,
            type_repr(self.type),
            self.children,
        )",Return a canonical string representation.,Return a canonical string representation.,"def __repr__(self) -> str:
        
        assert self.type is not None
        return ""{}({}, {!r})"".format(
            self.__class__.__name__,
            type_repr(self.type),
            self.children,
        )",Return a canonical string representation.,"def __repr__ ( self ) -> str : assert self . type is not None return ""{}({}, {!r})"" . format ( self . __class__ . __name__ , type_repr ( self . type ) , self . children , )",Return a canonical string representation.
/ultralytics/docs/build_docs.py,update_page_title,"def update_page_title(file_path: Path, new_title: str):
    """"""Update the title of an HTML file.""""""
    with open(file_path, encoding=""utf-8"") as file:
        content = file.read()

    # Replace the existing title with the new title
    updated_content = re.sub(r""<title>.*?</title>"", f""<title>{new_title}</title>"", content)

    # Write the updated content back to the file
    with open(file_path, ""w"", encoding=""utf-8"") as file:
        file.write(updated_content)","def update_page_title(file_path: Path, new_title: str):
    """"""Update the title of an HTML file.""""""
    with open(file_path, encoding=""utf-8"") as file:
        content = file.read()

    # Replace the existing title with the new title
    updated_content = re.sub(r""<title>.*?</title>"", f""<title>{new_title}</title>"", content)

    # Write the updated content back to the file
    with open(file_path, ""w"", encoding=""utf-8"") as file:
        file.write(updated_content)",Update the title of an HTML file.,Update the title of an HTML file.,"def update_page_title(file_path: Path, new_title: str):
    
    with open(file_path, encoding=""utf-8"") as file:
        content = file.read()

    # Replace the existing title with the new title
    updated_content = re.sub(r""<title>.*?</title>"", f""<title>{new_title}</title>"", content)

    # Write the updated content back to the file
    with open(file_path, ""w"", encoding=""utf-8"") as file:
        file.write(updated_content)",Update the title of an HTML file.,"def update_page_title ( file_path : Path , new_title : str ) : with open ( file_path , encoding = ""utf-8"" ) as file : content = file . read ( ) # Replace the existing title with the new title updated_content = re . sub ( r""<title>.*?</title>"" , f""<title>{new_title}</title>"" , content ) # Write the updated content back to the file with open ( file_path , ""w"" , encoding = ""utf-8"" ) as file : file . write ( updated_content )",Update the title of an HTML file.
/ultralytics/ultralytics/data/build.py,build_yolo_dataset,"def build_yolo_dataset(cfg, img_path, batch, data, mode=""train"", rect=False, stride=32, multi_modal=False):
    """"""Build and return a YOLO dataset based on configuration parameters.""""""
    dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    return dataset(
        img_path=img_path,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == ""train"",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == ""train"" else 0.5,
        prefix=colorstr(f""{mode}: ""),
        task=cfg.task,
        classes=cfg.classes,
        data=data,
        fraction=cfg.fraction if mode == ""train"" else 1.0,
    )","def build_yolo_dataset(cfg, img_path, batch, data, mode=""train"", rect=False, stride=32, multi_modal=False):
    """"""Build and return a YOLO dataset based on configuration parameters.""""""
    dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    return dataset(
        img_path=img_path,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == ""train"",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == ""train"" else 0.5,
        prefix=colorstr(f""{mode}: ""),
        task=cfg.task,
        classes=cfg.classes,
        data=data,
        fraction=cfg.fraction if mode == ""train"" else 1.0,
    )",Build and return a YOLO dataset based on configuration parameters.,Build and return a YOLO dataset based on configuration parameters.,"def build_yolo_dataset(cfg, img_path, batch, data, mode=""train"", rect=False, stride=32, multi_modal=False):
    
    dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    return dataset(
        img_path=img_path,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == ""train"",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == ""train"" else 0.5,
        prefix=colorstr(f""{mode}: ""),
        task=cfg.task,
        classes=cfg.classes,
        data=data,
        fraction=cfg.fraction if mode == ""train"" else 1.0,
    )",Build and return a YOLO dataset based on configuration parameters.,"def build_yolo_dataset ( cfg , img_path , batch , data , mode = ""train"" , rect = False , stride = 32 , multi_modal = False ) : dataset = YOLOMultiModalDataset if multi_modal else YOLODataset return dataset ( img_path = img_path , imgsz = cfg . imgsz , batch_size = batch , augment = mode == ""train"" , # augmentation hyp = cfg , # TODO: probably add a get_hyps_from_cfg function rect = cfg . rect or rect , # rectangular batches cache = cfg . cache or None , single_cls = cfg . single_cls or False , stride = int ( stride ) , pad = 0.0 if mode == ""train"" else 0.5 , prefix = colorstr ( f""{mode}: "" ) , task = cfg . task , classes = cfg . classes , data = data , fraction = cfg . fraction if mode == ""train"" else 1.0 , )",Build and return a YOLO dataset based on configuration parameters.
/cpython/Tools/wasm/emscripten/__main__.py,build_python_path,"def build_python_path():
    """"""The path to the build Python binary.""""""
    binary = NATIVE_BUILD_DIR / ""python""
    if not binary.is_file():
        binary = binary.with_suffix("".exe"")
        if not binary.is_file():
            raise FileNotFoundError(""Unable to find `python(.exe)` in "" f""{NATIVE_BUILD_DIR}"")

    return binary","def build_python_path():
    """"""The path to the build Python binary.""""""
    binary = NATIVE_BUILD_DIR / ""python""
    if not binary.is_file():
        binary = binary.with_suffix("".exe"")
        if not binary.is_file():
            raise FileNotFoundError(""Unable to find `python(.exe)` in "" f""{NATIVE_BUILD_DIR}"")

    return binary",The path to the build Python binary.,The path to the build Python binary.,"def build_python_path():
    
    binary = NATIVE_BUILD_DIR / ""python""
    if not binary.is_file():
        binary = binary.with_suffix("".exe"")
        if not binary.is_file():
            raise FileNotFoundError(""Unable to find `python(.exe)` in "" f""{NATIVE_BUILD_DIR}"")

    return binary",The path to the build Python binary.,"def build_python_path ( ) : binary = NATIVE_BUILD_DIR / ""python"" if not binary . is_file ( ) : binary = binary . with_suffix ( "".exe"" ) if not binary . is_file ( ) : raise FileNotFoundError ( ""Unable to find `python(.exe)` in "" f""{NATIVE_BUILD_DIR}"" ) return binary",The path to the build Python binary.
/yolov5/utils/metrics.py,plot_pr_curve,"def plot_pr_curve(px, py, ap, save_dir=Path(""pr_curve.png""), names=()):
    """"""Plots precision-recall curve, optionally per class, saving to `save_dir`; `px`, `py` are lists, `ap` is Nx2
    array, `names` optional.
    """"""
    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)
    py = np.stack(py, axis=1)

    if 0 < len(names) < 21:  # display per-class legend if < 21 classes
        for i, y in enumerate(py.T):
            ax.plot(px, y, linewidth=1, label=f""{names[i]} {ap[i, 0]:.3f}"")  # plot(recall, precision)
    else:
        ax.plot(px, py, linewidth=1, color=""grey"")  # plot(recall, precision)

    ax.plot(px, py.mean(1), linewidth=3, color=""blue"", label=f""all classes {ap[:, 0].mean():.3f} mAP@0.5"")
    ax.set_xlabel(""Recall"")
    ax.set_ylabel(""Precision"")
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.legend(bbox_to_anchor=(1.04, 1), loc=""upper left"")
    ax.set_title(""Precision-Recall Curve"")
    fig.savefig(save_dir, dpi=250)
    plt.close(fig)","def plot_pr_curve(px, py, ap, save_dir=Path(""pr_curve.png""), names=()):
    """"""Plots precision-recall curve, optionally per class, saving to `save_dir`; `px`, `py` are lists, `ap` is Nx2
    array, `names` optional.
    """"""
    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)
    py = np.stack(py, axis=1)

    if 0 < len(names) < 21:  # display per-class legend if < 21 classes
        for i, y in enumerate(py.T):
            ax.plot(px, y, linewidth=1, label=f""{names[i]} {ap[i, 0]:.3f}"")  # plot(recall, precision)
    else:
        ax.plot(px, py, linewidth=1, color=""grey"")  # plot(recall, precision)

    ax.plot(px, py.mean(1), linewidth=3, color=""blue"", label=f""all classes {ap[:, 0].mean():.3f} mAP@0.5"")
    ax.set_xlabel(""Recall"")
    ax.set_ylabel(""Precision"")
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.legend(bbox_to_anchor=(1.04, 1), loc=""upper left"")
    ax.set_title(""Precision-Recall Curve"")
    fig.savefig(save_dir, dpi=250)
    plt.close(fig)","Plots precision-recall curve, optionally per class, saving to `save_dir`; `px`, `py` are lists, `ap` is Nx2
array, `names` optional.","Plots precision-recall curve, optionally per class, saving to `save_dir`; `px`, `py` are lists, `ap` is Nx2 array, `names` optional.","def plot_pr_curve(px, py, ap, save_dir=Path(""pr_curve.png""), names=()):
    
    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)
    py = np.stack(py, axis=1)

    if 0 < len(names) < 21:  # display per-class legend if < 21 classes
        for i, y in enumerate(py.T):
            ax.plot(px, y, linewidth=1, label=f""{names[i]} {ap[i, 0]:.3f}"")  # plot(recall, precision)
    else:
        ax.plot(px, py, linewidth=1, color=""grey"")  # plot(recall, precision)

    ax.plot(px, py.mean(1), linewidth=3, color=""blue"", label=f""all classes {ap[:, 0].mean():.3f} mAP@0.5"")
    ax.set_xlabel(""Recall"")
    ax.set_ylabel(""Precision"")
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.legend(bbox_to_anchor=(1.04, 1), loc=""upper left"")
    ax.set_title(""Precision-Recall Curve"")
    fig.savefig(save_dir, dpi=250)
    plt.close(fig)","Plots precision-recall curve, optionally per class, saving to `save_dir`; `px`, `py` are lists, `ap` is Nx2 array, `names` optional.","def plot_pr_curve ( px , py , ap , save_dir = Path ( ""pr_curve.png"" ) , names = ( ) ) : fig , ax = plt . subplots ( 1 , 1 , figsize = ( 9 , 6 ) , tight_layout = True ) py = np . stack ( py , axis = 1 ) if 0 < len ( names ) < 21 : # display per-class legend if < 21 classes for i , y in enumerate ( py . T ) : ax . plot ( px , y , linewidth = 1 , label = f""{names[i]} {ap[i, 0]:.3f}"" ) # plot(recall, precision) else : ax . plot ( px , py , linewidth = 1 , color = ""grey"" ) # plot(recall, precision) ax . plot ( px , py . mean ( 1 ) , linewidth = 3 , color = ""blue"" , label = f""all classes {ap[:, 0].mean():.3f} mAP@0.5"" ) ax . set_xlabel ( ""Recall"" ) ax . set_ylabel ( ""Precision"" ) ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , 1 ) ax . legend ( bbox_to_anchor = ( 1.04 , 1 ) , loc = ""upper left"" ) ax . set_title ( ""Precision-Recall Curve"" ) fig . savefig ( save_dir , dpi = 250 ) plt . close ( fig )","Plots precision-recall curve, optionally per class, saving to `save_dir`; `px`, `py` are lists, `ap` is Nx2 array, `names` optional."
/ansible/packaging/release.py,create_release_pr,"def create_release_pr(allow_stale: bool = False) -> None:
    """"""Create a branch and open a browser tab for creating a release pull request.""""""
    version = get_ansible_version()

    pr = prepare_pull_request(
        version=version,
        branch=f""release-{version}-{secrets.token_hex(4)}"",
        title=f""New release v{version}"",
        add=(
            CHANGELOGS_DIR,
            ANSIBLE_RELEASE_FILE,
            ANSIBLE_PYPROJECT_TOML_FILE,
        ),
        allow_stale=allow_stale,
    )

    create_pull_request(pr)","def create_release_pr(allow_stale: bool = False) -> None:
    """"""Create a branch and open a browser tab for creating a release pull request.""""""
    version = get_ansible_version()

    pr = prepare_pull_request(
        version=version,
        branch=f""release-{version}-{secrets.token_hex(4)}"",
        title=f""New release v{version}"",
        add=(
            CHANGELOGS_DIR,
            ANSIBLE_RELEASE_FILE,
            ANSIBLE_PYPROJECT_TOML_FILE,
        ),
        allow_stale=allow_stale,
    )

    create_pull_request(pr)",Create a branch and open a browser tab for creating a release pull request.,Create a branch and open a browser tab for creating a release pull request.,"def create_release_pr(allow_stale: bool = False) -> None:
    
    version = get_ansible_version()

    pr = prepare_pull_request(
        version=version,
        branch=f""release-{version}-{secrets.token_hex(4)}"",
        title=f""New release v{version}"",
        add=(
            CHANGELOGS_DIR,
            ANSIBLE_RELEASE_FILE,
            ANSIBLE_PYPROJECT_TOML_FILE,
        ),
        allow_stale=allow_stale,
    )

    create_pull_request(pr)",Create a branch and open a browser tab for creating a release pull request.,"def create_release_pr ( allow_stale : bool = False ) -> None : version = get_ansible_version ( ) pr = prepare_pull_request ( version = version , branch = f""release-{version}-{secrets.token_hex(4)}"" , title = f""New release v{version}"" , add = ( CHANGELOGS_DIR , ANSIBLE_RELEASE_FILE , ANSIBLE_PYPROJECT_TOML_FILE , ) , allow_stale = allow_stale , ) create_pull_request ( pr )",Create a branch and open a browser tab for creating a release pull request.
/yolov5/models/common.py,gd_outputs,"def gd_outputs(gd):
                """"""Generates a sorted list of graph outputs excluding NoOp nodes and inputs, formatted as '<name>:0'.""""""
                name_list, input_list = [], []
                for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef
                    name_list.append(node.name)
                    input_list.extend(node.input)
                return sorted(f""{x}:0"" for x in list(set(name_list) - set(input_list)) if not x.startswith(""NoOp""))","def gd_outputs(gd):
                """"""Generates a sorted list of graph outputs excluding NoOp nodes and inputs, formatted as '<name>:0'.""""""
                name_list, input_list = [], []
                for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef
                    name_list.append(node.name)
                    input_list.extend(node.input)
                return sorted(f""{x}:0"" for x in list(set(name_list) - set(input_list)) if not x.startswith(""NoOp""))","Generates a sorted list of graph outputs excluding NoOp nodes and inputs, formatted as '<name>:0'.","Generates a sorted list of graph outputs excluding NoOp nodes and inputs, formatted as ':0'.","def gd_outputs(gd):
                
                name_list, input_list = [], []
                for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef
                    name_list.append(node.name)
                    input_list.extend(node.input)
                return sorted(f""{x}:0"" for x in list(set(name_list) - set(input_list)) if not x.startswith(""NoOp""))","Generates a sorted list of graph outputs excluding NoOp nodes and inputs, formatted as ':0'.","def gd_outputs ( gd ) : name_list , input_list = [ ] , [ ] for node in gd . node : # tensorflow.core.framework.node_def_pb2.NodeDef name_list . append ( node . name ) input_list . extend ( node . input ) return sorted ( f""{x}:0"" for x in list ( set ( name_list ) - set ( input_list ) ) if not x . startswith ( ""NoOp"" ) )","Generates a sorted list of graph outputs excluding NoOp nodes and inputs, formatted as ':0'."
/odoo/odoo/_monkeypatches/werkzeug_urls.py,encode_netloc,"def encode_netloc(self) -> str:
        """"""Encodes the netloc part to an ASCII safe URL as bytes.""""""
        rv = self.ascii_host or """"
        if "":"" in rv:
            rv = f""[{rv}]""
        port = self.port
        if port is not None:
            rv = f""{rv}:{port}""
        auth = "":"".join(
            filter(
                None,
                [
                    url_quote(self.raw_username or """", ""utf-8"", ""strict"", ""/:%""),
                    url_quote(self.raw_password or """", ""utf-8"", ""strict"", ""/:%""),
                ],
            )
        )
        if auth:
            rv = f""{auth}@{rv}""
        return rv","def encode_netloc(self) -> str:
        """"""Encodes the netloc part to an ASCII safe URL as bytes.""""""
        rv = self.ascii_host or """"
        if "":"" in rv:
            rv = f""[{rv}]""
        port = self.port
        if port is not None:
            rv = f""{rv}:{port}""
        auth = "":"".join(
            filter(
                None,
                [
                    url_quote(self.raw_username or """", ""utf-8"", ""strict"", ""/:%""),
                    url_quote(self.raw_password or """", ""utf-8"", ""strict"", ""/:%""),
                ],
            )
        )
        if auth:
            rv = f""{auth}@{rv}""
        return rv",Encodes the netloc part to an ASCII safe URL as bytes.,Encodes the netloc part to an ASCII safe URL as bytes.,"def encode_netloc(self) -> str:
        
        rv = self.ascii_host or """"
        if "":"" in rv:
            rv = f""[{rv}]""
        port = self.port
        if port is not None:
            rv = f""{rv}:{port}""
        auth = "":"".join(
            filter(
                None,
                [
                    url_quote(self.raw_username or """", ""utf-8"", ""strict"", ""/:%""),
                    url_quote(self.raw_password or """", ""utf-8"", ""strict"", ""/:%""),
                ],
            )
        )
        if auth:
            rv = f""{auth}@{rv}""
        return rv",Encodes the netloc part to an ASCII safe URL as bytes.,"def encode_netloc ( self ) -> str : rv = self . ascii_host or """" if "":"" in rv : rv = f""[{rv}]"" port = self . port if port is not None : rv = f""{rv}:{port}"" auth = "":"" . join ( filter ( None , [ url_quote ( self . raw_username or """" , ""utf-8"" , ""strict"" , ""/:%"" ) , url_quote ( self . raw_password or """" , ""utf-8"" , ""strict"" , ""/:%"" ) , ] , ) ) if auth : rv = f""{auth}@{rv}"" return rv",Encodes the netloc part to an ASCII safe URL as bytes.
/yolov5/utils/autoanchor.py,check_anchor_order,"def check_anchor_order(m):
    """"""Checks and corrects anchor order against stride in YOLOv5 Detect() module if necessary.""""""
    a = m.anchors.prod(-1).mean(-1).view(-1)  # mean anchor area per output layer
    da = a[-1] - a[0]  # delta a
    ds = m.stride[-1] - m.stride[0]  # delta s
    if da and (da.sign() != ds.sign()):  # same order
        LOGGER.info(f""{PREFIX}Reversing anchor order"")
        m.anchors[:] = m.anchors.flip(0)","def check_anchor_order(m):
    """"""Checks and corrects anchor order against stride in YOLOv5 Detect() module if necessary.""""""
    a = m.anchors.prod(-1).mean(-1).view(-1)  # mean anchor area per output layer
    da = a[-1] - a[0]  # delta a
    ds = m.stride[-1] - m.stride[0]  # delta s
    if da and (da.sign() != ds.sign()):  # same order
        LOGGER.info(f""{PREFIX}Reversing anchor order"")
        m.anchors[:] = m.anchors.flip(0)",Checks and corrects anchor order against stride in YOLOv5 Detect() module if necessary.,Checks and corrects anchor order against stride in YOLOv5 Detect() module if necessary.,"def check_anchor_order(m):
    
    a = m.anchors.prod(-1).mean(-1).view(-1)  # mean anchor area per output layer
    da = a[-1] - a[0]  # delta a
    ds = m.stride[-1] - m.stride[0]  # delta s
    if da and (da.sign() != ds.sign()):  # same order
        LOGGER.info(f""{PREFIX}Reversing anchor order"")
        m.anchors[:] = m.anchors.flip(0)",Checks and corrects anchor order against stride in YOLOv5 Detect() module if necessary.,"def check_anchor_order ( m ) : a = m . anchors . prod ( - 1 ) . mean ( - 1 ) . view ( - 1 ) # mean anchor area per output layer da = a [ - 1 ] - a [ 0 ] # delta a ds = m . stride [ - 1 ] - m . stride [ 0 ] # delta s if da and ( da . sign ( ) != ds . sign ( ) ) : # same order LOGGER . info ( f""{PREFIX}Reversing anchor order"" ) m . anchors [ : ] = m . anchors . flip ( 0 )",Checks and corrects anchor order against stride in YOLOv5 Detect() module if necessary.
/OpenManus/app/tool/create_chat_completion.py,_create_union_schema,"def _create_union_schema(self, types: tuple) -> dict:
        """"""Create schema for Union types.""""""
        return {
            ""type"": ""object"",
            ""properties"": {
                ""response"": {""anyOf"": [self._get_type_info(t) for t in types]}
            },
            ""required"": self.required,
        }","def _create_union_schema(self, types: tuple) -> dict:
        """"""Create schema for Union types.""""""
        return {
            ""type"": ""object"",
            ""properties"": {
                ""response"": {""anyOf"": [self._get_type_info(t) for t in types]}
            },
            ""required"": self.required,
        }",Create schema for Union types.,Create schema for Union types.,"def _create_union_schema(self, types: tuple) -> dict:
        
        return {
            ""type"": ""object"",
            ""properties"": {
                ""response"": {""anyOf"": [self._get_type_info(t) for t in types]}
            },
            ""required"": self.required,
        }",Create schema for Union types.,"def _create_union_schema ( self , types : tuple ) -> dict : return { ""type"" : ""object"" , ""properties"" : { ""response"" : { ""anyOf"" : [ self . _get_type_info ( t ) for t in types ] } } , ""required"" : self . required , }",Create schema for Union types.
/FastChat/fastchat/train/train_flant5.py,safe_save_model_for_hf_trainer,"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    """"""Collects the state dict and dump to disk.""""""
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)","def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    """"""Collects the state dict and dump to disk.""""""
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,Collects the state dict and dump to disk.,"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,"def safe_save_model_for_hf_trainer ( trainer : transformers . Trainer , output_dir : str ) : state_dict = trainer . model . state_dict ( ) if trainer . args . should_save : cpu_state_dict = { key : value . cpu ( ) for key , value in state_dict . items ( ) } del state_dict trainer . _save ( output_dir , state_dict = cpu_state_dict )",Collects the state dict and dump to disk.
/core/script/gen_requirements_all.py,process_action_requirement,"def process_action_requirement(req: str, action: str) -> str:
    """"""Process requirement for a specific github action.""""""
    normalized_package_name = normalize_package_name(req)
    if normalized_package_name in OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""exclude""]:
        return f""# {req}""
    if normalized_package_name in OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""include""]:
        return req
    if normalized_package_name in EXCLUDED_REQUIREMENTS_ALL:
        return f""# {req}""
    if markers := OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""markers""].get(
        normalized_package_name, None
    ):
        return f""{req};{markers}""
    return req","def process_action_requirement(req: str, action: str) -> str:
    """"""Process requirement for a specific github action.""""""
    normalized_package_name = normalize_package_name(req)
    if normalized_package_name in OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""exclude""]:
        return f""# {req}""
    if normalized_package_name in OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""include""]:
        return req
    if normalized_package_name in EXCLUDED_REQUIREMENTS_ALL:
        return f""# {req}""
    if markers := OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""markers""].get(
        normalized_package_name, None
    ):
        return f""{req};{markers}""
    return req",Process requirement for a specific github action.,Process requirement for a specific github action.,"def process_action_requirement(req: str, action: str) -> str:
    
    normalized_package_name = normalize_package_name(req)
    if normalized_package_name in OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""exclude""]:
        return f""# {req}""
    if normalized_package_name in OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""include""]:
        return req
    if normalized_package_name in EXCLUDED_REQUIREMENTS_ALL:
        return f""# {req}""
    if markers := OVERRIDDEN_REQUIREMENTS_ACTIONS[action][""markers""].get(
        normalized_package_name, None
    ):
        return f""{req};{markers}""
    return req",Process requirement for a specific github action.,"def process_action_requirement ( req : str , action : str ) -> str : normalized_package_name = normalize_package_name ( req ) if normalized_package_name in OVERRIDDEN_REQUIREMENTS_ACTIONS [ action ] [ ""exclude"" ] : return f""# {req}"" if normalized_package_name in OVERRIDDEN_REQUIREMENTS_ACTIONS [ action ] [ ""include"" ] : return req if normalized_package_name in EXCLUDED_REQUIREMENTS_ALL : return f""# {req}"" if markers := OVERRIDDEN_REQUIREMENTS_ACTIONS [ action ] [ ""markers"" ] . get ( normalized_package_name , None ) : return f""{req};{markers}"" return req",Process requirement for a specific github action.
/ultralytics/docs/build_docs.py,update_subdir_edit_links,"def update_subdir_edit_links(subdir: str = """", docs_url: str = """"):
    """"""Update the HTML head section of each file.""""""
    if str(subdir[0]) == ""/"":
        subdir = str(subdir[0])[1:]
    html_files = (SITE / subdir).rglob(""*.html"")
    for html_file in tqdm(html_files, desc=""Processing subdir files"", mininterval=1.0):
        with html_file.open(""r"", encoding=""utf-8"") as file:
            soup = BeautifulSoup(file, ""html.parser"")

        # Find the anchor tag and update its href attribute
        a_tag = soup.find(""a"", {""class"": ""md-content__button md-icon""})
        if a_tag and a_tag[""title""] == ""Edit this page"":
            a_tag[""href""] = f""{docs_url}{a_tag['href'].rpartition(subdir)[-1]}""

        # Write the updated HTML back to the file
        with open(html_file, ""w"", encoding=""utf-8"") as file:
            file.write(str(soup))","def update_subdir_edit_links(subdir: str = """", docs_url: str = """"):
    """"""Update the HTML head section of each file.""""""
    if str(subdir[0]) == ""/"":
        subdir = str(subdir[0])[1:]
    html_files = (SITE / subdir).rglob(""*.html"")
    for html_file in tqdm(html_files, desc=""Processing subdir files"", mininterval=1.0):
        with html_file.open(""r"", encoding=""utf-8"") as file:
            soup = BeautifulSoup(file, ""html.parser"")

        # Find the anchor tag and update its href attribute
        a_tag = soup.find(""a"", {""class"": ""md-content__button md-icon""})
        if a_tag and a_tag[""title""] == ""Edit this page"":
            a_tag[""href""] = f""{docs_url}{a_tag['href'].rpartition(subdir)[-1]}""

        # Write the updated HTML back to the file
        with open(html_file, ""w"", encoding=""utf-8"") as file:
            file.write(str(soup))",Update the HTML head section of each file.,Update the HTML head section of each file.,"def update_subdir_edit_links(subdir: str = """", docs_url: str = """"):
    
    if str(subdir[0]) == ""/"":
        subdir = str(subdir[0])[1:]
    html_files = (SITE / subdir).rglob(""*.html"")
    for html_file in tqdm(html_files, desc=""Processing subdir files"", mininterval=1.0):
        with html_file.open(""r"", encoding=""utf-8"") as file:
            soup = BeautifulSoup(file, ""html.parser"")

        # Find the anchor tag and update its href attribute
        a_tag = soup.find(""a"", {""class"": ""md-content__button md-icon""})
        if a_tag and a_tag[""title""] == ""Edit this page"":
            a_tag[""href""] = f""{docs_url}{a_tag['href'].rpartition(subdir)[-1]}""

        # Write the updated HTML back to the file
        with open(html_file, ""w"", encoding=""utf-8"") as file:
            file.write(str(soup))",Update the HTML head section of each file.,"def update_subdir_edit_links ( subdir : str = """" , docs_url : str = """" ) : if str ( subdir [ 0 ] ) == ""/"" : subdir = str ( subdir [ 0 ] ) [ 1 : ] html_files = ( SITE / subdir ) . rglob ( ""*.html"" ) for html_file in tqdm ( html_files , desc = ""Processing subdir files"" , mininterval = 1.0 ) : with html_file . open ( ""r"" , encoding = ""utf-8"" ) as file : soup = BeautifulSoup ( file , ""html.parser"" ) # Find the anchor tag and update its href attribute a_tag = soup . find ( ""a"" , { ""class"" : ""md-content__button md-icon"" } ) if a_tag and a_tag [ ""title"" ] == ""Edit this page"" : a_tag [ ""href"" ] = f""{docs_url}{a_tag['href'].rpartition(subdir)[-1]}"" # Write the updated HTML back to the file with open ( html_file , ""w"" , encoding = ""utf-8"" ) as file : file . write ( str ( soup ) )",Update the HTML head section of each file.
/mitmproxy/examples/contrib/xss_scanner.py,get_cookies,"def get_cookies(flow: http.HTTPFlow) -> Cookies:
    """"""Return a dict going from cookie names to cookie values
    - Note that it includes both the cookies sent in the original request and
      the cookies sent by the server""""""
    return {name: value for name, value in flow.request.cookies.fields}","def get_cookies(flow: http.HTTPFlow) -> Cookies:
    """"""Return a dict going from cookie names to cookie values
    - Note that it includes both the cookies sent in the original request and
      the cookies sent by the server""""""
    return {name: value for name, value in flow.request.cookies.fields}","Return a dict going from cookie names to cookie values
- Note that it includes both the cookies sent in the original request and
  the cookies sent by the server",Return a dict going from cookie names to cookie values - Note that it includes both the cookies sent in the original request and the cookies sent by the server,"def get_cookies(flow: http.HTTPFlow) -> Cookies:
    
    return {name: value for name, value in flow.request.cookies.fields}",Return a dict going from cookie names to cookie values - Note that it includes both the cookies sent in the original request and the cookies sent by the server,"def get_cookies ( flow : http . HTTPFlow ) -> Cookies : return { name : value for name , value in flow . request . cookies . fields }",Return a dict going from cookie names to cookie values - Note that it includes both the cookies sent in the original request and the cookies sent by the server
/yolov5/utils/dataloaders.py,__next__,"def __next__(self):
        """"""Captures and returns the next screen frame as a BGR numpy array, cropping to only the first three channels
        from BGRA.
        """"""
        im0 = np.array(self.sct.grab(self.monitor))[:, :, :3]  # [:, :, :3] BGRA to BGR
        s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: ""

        if self.transforms:
            im = self.transforms(im0)  # transforms
        else:
            im = letterbox(im0, self.img_size, stride=self.stride, auto=self.auto)[0]  # padded resize
            im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB
            im = np.ascontiguousarray(im)  # contiguous
        self.frame += 1
        return str(self.screen), im, im0, None, s","def __next__(self):
        """"""Captures and returns the next screen frame as a BGR numpy array, cropping to only the first three channels
        from BGRA.
        """"""
        im0 = np.array(self.sct.grab(self.monitor))[:, :, :3]  # [:, :, :3] BGRA to BGR
        s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: ""

        if self.transforms:
            im = self.transforms(im0)  # transforms
        else:
            im = letterbox(im0, self.img_size, stride=self.stride, auto=self.auto)[0]  # padded resize
            im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB
            im = np.ascontiguousarray(im)  # contiguous
        self.frame += 1
        return str(self.screen), im, im0, None, s","Captures and returns the next screen frame as a BGR numpy array, cropping to only the first three channels
from BGRA.","Captures and returns the next screen frame as a BGR numpy array, cropping to only the first three channels from BGRA.","def __next__(self):
        
        im0 = np.array(self.sct.grab(self.monitor))[:, :, :3]  # [:, :, :3] BGRA to BGR
        s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: ""

        if self.transforms:
            im = self.transforms(im0)  # transforms
        else:
            im = letterbox(im0, self.img_size, stride=self.stride, auto=self.auto)[0]  # padded resize
            im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB
            im = np.ascontiguousarray(im)  # contiguous
        self.frame += 1
        return str(self.screen), im, im0, None, s","Captures and returns the next screen frame as a BGR numpy array, cropping to only the first three channels from BGRA.","def __next__ ( self ) : im0 = np . array ( self . sct . grab ( self . monitor ) ) [ : , : , : 3 ] # [:, :, :3] BGRA to BGR s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: "" if self . transforms : im = self . transforms ( im0 ) # transforms else : im = letterbox ( im0 , self . img_size , stride = self . stride , auto = self . auto ) [ 0 ] # padded resize im = im . transpose ( ( 2 , 0 , 1 ) ) [ : : - 1 ] # HWC to CHW, BGR to RGB im = np . ascontiguousarray ( im ) # contiguous self . frame += 1 return str ( self . screen ) , im , im0 , None , s","Captures and returns the next screen frame as a BGR numpy array, cropping to only the first three channels from BGRA."
/flask/tests/conftest.py,site_packages,"def site_packages(modules_tmp_path, monkeypatch):
    """"""Create a fake site-packages.""""""
    py_dir = f""python{sys.version_info.major}.{sys.version_info.minor}""
    rv = modules_tmp_path / ""lib"" / py_dir / ""site-packages""
    rv.mkdir(parents=True)
    monkeypatch.syspath_prepend(os.fspath(rv))
    return rv","def site_packages(modules_tmp_path, monkeypatch):
    """"""Create a fake site-packages.""""""
    py_dir = f""python{sys.version_info.major}.{sys.version_info.minor}""
    rv = modules_tmp_path / ""lib"" / py_dir / ""site-packages""
    rv.mkdir(parents=True)
    monkeypatch.syspath_prepend(os.fspath(rv))
    return rv",Create a fake site-packages.,Create a fake site-packages.,"def site_packages(modules_tmp_path, monkeypatch):
    
    py_dir = f""python{sys.version_info.major}.{sys.version_info.minor}""
    rv = modules_tmp_path / ""lib"" / py_dir / ""site-packages""
    rv.mkdir(parents=True)
    monkeypatch.syspath_prepend(os.fspath(rv))
    return rv",Create a fake site-packages.,"def site_packages ( modules_tmp_path , monkeypatch ) : py_dir = f""python{sys.version_info.major}.{sys.version_info.minor}"" rv = modules_tmp_path / ""lib"" / py_dir / ""site-packages"" rv . mkdir ( parents = True ) monkeypatch . syspath_prepend ( os . fspath ( rv ) ) return rv",Create a fake site-packages.
/ansible/packaging/release.py,get_commit,"def get_commit(rev: str | None = None) -> str:
    """"""Return the commit associated with the given rev, or HEAD if no rev is given.""""""
    try:
        return git(""rev-parse"", ""--quiet"", ""--verify"", ""--end-of-options"", f""{rev or 'HEAD'}^{{commit}}"", capture_output=True).stdout.strip()
    except CalledProcessError as ex:
        if ex.status == 1 and not ex.stdout and not ex.stderr:
            raise ApplicationError(f""Could not find commit: {rev}"") from None

        raise","def get_commit(rev: str | None = None) -> str:
    """"""Return the commit associated with the given rev, or HEAD if no rev is given.""""""
    try:
        return git(""rev-parse"", ""--quiet"", ""--verify"", ""--end-of-options"", f""{rev or 'HEAD'}^{{commit}}"", capture_output=True).stdout.strip()
    except CalledProcessError as ex:
        if ex.status == 1 and not ex.stdout and not ex.stderr:
            raise ApplicationError(f""Could not find commit: {rev}"") from None

        raise","Return the commit associated with the given rev, or HEAD if no rev is given.","Return the commit associated with the given rev, or HEAD if no rev is given.","def get_commit(rev: str | None = None) -> str:
    
    try:
        return git(""rev-parse"", ""--quiet"", ""--verify"", ""--end-of-options"", f""{rev or 'HEAD'}^{{commit}}"", capture_output=True).stdout.strip()
    except CalledProcessError as ex:
        if ex.status == 1 and not ex.stdout and not ex.stderr:
            raise ApplicationError(f""Could not find commit: {rev}"") from None

        raise","Return the commit associated with the given rev, or HEAD if no rev is given.","def get_commit ( rev : str | None = None ) -> str : try : return git ( ""rev-parse"" , ""--quiet"" , ""--verify"" , ""--end-of-options"" , f""{rev or 'HEAD'}^{{commit}}"" , capture_output = True ) . stdout . strip ( ) except CalledProcessError as ex : if ex . status == 1 and not ex . stdout and not ex . stderr : raise ApplicationError ( f""Could not find commit: {rev}"" ) from None raise","Return the commit associated with the given rev, or HEAD if no rev is given."
/odoo/odoo/models.py,expand_ids,"def expand_ids(id0, ids):
    """""" Return an iterator of unique ids from the concatenation of ``[id0]`` and
        ``ids``, and of the same kind (all real or all new).
    """"""
    yield id0
    seen = {id0}
    kind = bool(id0)
    for id_ in ids:
        if id_ not in seen and bool(id_) == kind:
            yield id_
            seen.add(id_)","def expand_ids(id0, ids):
    """""" Return an iterator of unique ids from the concatenation of ``[id0]`` and
        ``ids``, and of the same kind (all real or all new).
    """"""
    yield id0
    seen = {id0}
    kind = bool(id0)
    for id_ in ids:
        if id_ not in seen and bool(id_) == kind:
            yield id_
            seen.add(id_)","Return an iterator of unique ids from the concatenation of ``[id0]`` and
``ids``, and of the same kind (all real or all new).","Return an iterator of unique ids from the concatenation of ``[id0]`` and ``ids``, and of the same kind (all real or all new).","def expand_ids(id0, ids):
    
    yield id0
    seen = {id0}
    kind = bool(id0)
    for id_ in ids:
        if id_ not in seen and bool(id_) == kind:
            yield id_
            seen.add(id_)","Return an iterator of unique ids from the concatenation of ``[id0]`` and ``ids``, and of the same kind (all real or all new).","def expand_ids ( id0 , ids ) : yield id0 seen = { id0 } kind = bool ( id0 ) for id_ in ids : if id_ not in seen and bool ( id_ ) == kind : yield id_ seen . add ( id_ )","Return an iterator of unique ids from the concatenation of ``[id0]`` and ``ids``, and of the same kind (all real or all new)."
/open-interpreter/interpreter/core/computer/files/files.py,edit,"def edit(self, path, original_text, replacement_text):
        """"""
        Edits a file on the filesystem, replacing the original text with the replacement text.
        """"""
        with open(path, ""r"") as file:
            filedata = file.read()

        if original_text not in filedata:
            matches = get_close_matches_in_text(original_text, filedata)
            if matches:
                suggestions = "", "".join(matches)
                raise ValueError(
                    f""Original text not found. Did you mean one of these? {suggestions}""
                )

        filedata = filedata.replace(original_text, replacement_text)

        with open(path, ""w"") as file:
            file.write(filedata)","def edit(self, path, original_text, replacement_text):
        """"""
        Edits a file on the filesystem, replacing the original text with the replacement text.
        """"""
        with open(path, ""r"") as file:
            filedata = file.read()

        if original_text not in filedata:
            matches = get_close_matches_in_text(original_text, filedata)
            if matches:
                suggestions = "", "".join(matches)
                raise ValueError(
                    f""Original text not found. Did you mean one of these? {suggestions}""
                )

        filedata = filedata.replace(original_text, replacement_text)

        with open(path, ""w"") as file:
            file.write(filedata)","Edits a file on the filesystem, replacing the original text with the replacement text.","Edits a file on the filesystem, replacing the original text with the replacement text.","def edit(self, path, original_text, replacement_text):
        
        with open(path, ""r"") as file:
            filedata = file.read()

        if original_text not in filedata:
            matches = get_close_matches_in_text(original_text, filedata)
            if matches:
                suggestions = "", "".join(matches)
                raise ValueError(
                    f""Original text not found. Did you mean one of these? {suggestions}""
                )

        filedata = filedata.replace(original_text, replacement_text)

        with open(path, ""w"") as file:
            file.write(filedata)","Edits a file on the filesystem, replacing the original text with the replacement text.","def edit ( self , path , original_text , replacement_text ) : with open ( path , ""r"" ) as file : filedata = file . read ( ) if original_text not in filedata : matches = get_close_matches_in_text ( original_text , filedata ) if matches : suggestions = "", "" . join ( matches ) raise ValueError ( f""Original text not found. Did you mean one of these? {suggestions}"" ) filedata = filedata . replace ( original_text , replacement_text ) with open ( path , ""w"" ) as file : file . write ( filedata )","Edits a file on the filesystem, replacing the original text with the replacement text."
/python-patterns/patterns/dependency_injection.py,production_code_time_provider,"def production_code_time_provider() -> str:
    """"""
    Production code version of the time provider (just a wrapper for formatting
    datetime for this example).
    """"""
    current_time = datetime.datetime.now()
    current_time_formatted = f""{current_time.hour}:{current_time.minute}""
    return current_time_formatted","def production_code_time_provider() -> str:
    """"""
    Production code version of the time provider (just a wrapper for formatting
    datetime for this example).
    """"""
    current_time = datetime.datetime.now()
    current_time_formatted = f""{current_time.hour}:{current_time.minute}""
    return current_time_formatted","Production code version of the time provider (just a wrapper for formatting
datetime for this example).",Production code version of the time provider (just a wrapper for formatting datetime for this example).,"def production_code_time_provider() -> str:
    
    current_time = datetime.datetime.now()
    current_time_formatted = f""{current_time.hour}:{current_time.minute}""
    return current_time_formatted",Production code version of the time provider (just a wrapper for formatting datetime for this example).,"def production_code_time_provider ( ) -> str : current_time = datetime . datetime . now ( ) current_time_formatted = f""{current_time.hour}:{current_time.minute}"" return current_time_formatted",Production code version of the time provider (just a wrapper for formatting datetime for this example).
/OpenManus/app/schema.py,to_dict,"def to_dict(self) -> dict:
        """"""Convert message to dictionary format""""""
        message = {""role"": self.role}
        if self.content is not None:
            message[""content""] = self.content
        if self.tool_calls is not None:
            message[""tool_calls""] = [tool_call.dict() for tool_call in self.tool_calls]
        if self.name is not None:
            message[""name""] = self.name
        if self.tool_call_id is not None:
            message[""tool_call_id""] = self.tool_call_id
        if self.base64_image is not None:
            message[""base64_image""] = self.base64_image
        return message","def to_dict(self) -> dict:
        """"""Convert message to dictionary format""""""
        message = {""role"": self.role}
        if self.content is not None:
            message[""content""] = self.content
        if self.tool_calls is not None:
            message[""tool_calls""] = [tool_call.dict() for tool_call in self.tool_calls]
        if self.name is not None:
            message[""name""] = self.name
        if self.tool_call_id is not None:
            message[""tool_call_id""] = self.tool_call_id
        if self.base64_image is not None:
            message[""base64_image""] = self.base64_image
        return message",Convert message to dictionary format,Convert message to dictionary format,"def to_dict(self) -> dict:
        
        message = {""role"": self.role}
        if self.content is not None:
            message[""content""] = self.content
        if self.tool_calls is not None:
            message[""tool_calls""] = [tool_call.dict() for tool_call in self.tool_calls]
        if self.name is not None:
            message[""name""] = self.name
        if self.tool_call_id is not None:
            message[""tool_call_id""] = self.tool_call_id
        if self.base64_image is not None:
            message[""base64_image""] = self.base64_image
        return message",Convert message to dictionary format,"def to_dict ( self ) -> dict : message = { ""role"" : self . role } if self . content is not None : message [ ""content"" ] = self . content if self . tool_calls is not None : message [ ""tool_calls"" ] = [ tool_call . dict ( ) for tool_call in self . tool_calls ] if self . name is not None : message [ ""name"" ] = self . name if self . tool_call_id is not None : message [ ""tool_call_id"" ] = self . tool_call_id if self . base64_image is not None : message [ ""base64_image"" ] = self . base64_image return message",Convert message to dictionary format
/odoo/odoo/tests/form.py,set,"def set(self, records):
        """""" Set the field value to be ``records``. """"""
        self._assert_editable()
        comodel_name = self._field_info['relation']
        assert isinstance(records, BaseModel) and records._name == comodel_name, \
            f""trying to assign a {records._name!r} object to a {comodel_name!r} field""

        if set(records.ids) != set(self._field_value):
            self._field_value.clear()
            for id_ in records.ids:
                self._field_value.add(id_, {'id': id_})
            self._form._perform_onchange(self._field)","def set(self, records):
        """""" Set the field value to be ``records``. """"""
        self._assert_editable()
        comodel_name = self._field_info['relation']
        assert isinstance(records, BaseModel) and records._name == comodel_name, \
            f""trying to assign a {records._name!r} object to a {comodel_name!r} field""

        if set(records.ids) != set(self._field_value):
            self._field_value.clear()
            for id_ in records.ids:
                self._field_value.add(id_, {'id': id_})
            self._form._perform_onchange(self._field)",Set the field value to be ``records``.,Set the field value to be ``records``.,"def set(self, records):
        
        self._assert_editable()
        comodel_name = self._field_info['relation']
        assert isinstance(records, BaseModel) and records._name == comodel_name, \
            f""trying to assign a {records._name!r} object to a {comodel_name!r} field""

        if set(records.ids) != set(self._field_value):
            self._field_value.clear()
            for id_ in records.ids:
                self._field_value.add(id_, {'id': id_})
            self._form._perform_onchange(self._field)",Set the field value to be ``records``.,"def set ( self , records ) : self . _assert_editable ( ) comodel_name = self . _field_info [ 'relation' ] assert isinstance ( records , BaseModel ) and records . _name == comodel_name , f""trying to assign a {records._name!r} object to a {comodel_name!r} field"" if set ( records . ids ) != set ( self . _field_value ) : self . _field_value . clear ( ) for id_ in records . ids : self . _field_value . add ( id_ , { 'id' : id_ } ) self . _form . _perform_onchange ( self . _field )",Set the field value to be ``records``.
/pytorch/functorch/benchmarks/chrome_trace_parser.py,get_model_name,"def get_model_name(filename):
    """"""
    Get model name from a file in format {model_name}_chrome_trace_*.json
    """"""
    _, tail = os.path.split(filename)
    modelname = tail[: tail.find(""_chrome_trace"")]
    return modelname","def get_model_name(filename):
    """"""
    Get model name from a file in format {model_name}_chrome_trace_*.json
    """"""
    _, tail = os.path.split(filename)
    modelname = tail[: tail.find(""_chrome_trace"")]
    return modelname",Get model name from a file in format {model_name}_chrome_trace_*.json,Get model name from a file in format {model_name}_chrome_trace_*.json,"def get_model_name(filename):
    
    _, tail = os.path.split(filename)
    modelname = tail[: tail.find(""_chrome_trace"")]
    return modelname",Get model name from a file in format {model_name}_chrome_trace_*.json,"def get_model_name ( filename ) : _ , tail = os . path . split ( filename ) modelname = tail [ : tail . find ( ""_chrome_trace"" ) ] return modelname",Get model name from a file in format {model_name}_chrome_trace_*.json
/python-patterns/patterns/structural/mvc.py,show_item_information,"def show_item_information(self, item_name: str) -> None:
        """"""
        Show information about a {item_type} item.
        :param str item_name: the name of the {item_type} item to show information about
        """"""
        item_type: str = self.model.item_type
        try:
            item_info: dict = self.model.get(item_name)
        except Exception:
            self.view.item_not_found(item_type, item_name)
        else:
            self.view.show_item_information(item_type, item_name, item_info)","def show_item_information(self, item_name: str) -> None:
        """"""
        Show information about a {item_type} item.
        :param str item_name: the name of the {item_type} item to show information about
        """"""
        item_type: str = self.model.item_type
        try:
            item_info: dict = self.model.get(item_name)
        except Exception:
            self.view.item_not_found(item_type, item_name)
        else:
            self.view.show_item_information(item_type, item_name, item_info)","Show information about a {item_type} item.
:param str item_name: the name of the {item_type} item to show information about",Show information about a {item_type} item.,"def show_item_information(self, item_name: str) -> None:
        
        item_type: str = self.model.item_type
        try:
            item_info: dict = self.model.get(item_name)
        except Exception:
            self.view.item_not_found(item_type, item_name)
        else:
            self.view.show_item_information(item_type, item_name, item_info)",Show information about a {item_type} item.,"def show_item_information ( self , item_name : str ) -> None : item_type : str = self . model . item_type try : item_info : dict = self . model . get ( item_name ) except Exception : self . view . item_not_found ( item_type , item_name ) else : self . view . show_item_information ( item_type , item_name , item_info )",Show information about a {item_type} item.
/FastChat/fastchat/llm_judge/common.py,normalize_game_key_single,"def normalize_game_key_single(gamekey, result):
    """"""Make the model names sorted in a game key.""""""
    qid, model_1, model_2 = gamekey
    if model_1 < model_2:
        return gamekey, result
    else:
        new_gamekey = (qid, model_2, model_1)
        new_result = {
            ""winners"": tuple(reverse_model_map.get(x, x) for x in result[""winners""]),
            ""g1_judgment"": result[""g2_judgment""],
            ""g2_judgment"": result[""g1_judgment""],
        }
        return new_gamekey, new_result","def normalize_game_key_single(gamekey, result):
    """"""Make the model names sorted in a game key.""""""
    qid, model_1, model_2 = gamekey
    if model_1 < model_2:
        return gamekey, result
    else:
        new_gamekey = (qid, model_2, model_1)
        new_result = {
            ""winners"": tuple(reverse_model_map.get(x, x) for x in result[""winners""]),
            ""g1_judgment"": result[""g2_judgment""],
            ""g2_judgment"": result[""g1_judgment""],
        }
        return new_gamekey, new_result",Make the model names sorted in a game key.,Make the model names sorted in a game key.,"def normalize_game_key_single(gamekey, result):
    
    qid, model_1, model_2 = gamekey
    if model_1 < model_2:
        return gamekey, result
    else:
        new_gamekey = (qid, model_2, model_1)
        new_result = {
            ""winners"": tuple(reverse_model_map.get(x, x) for x in result[""winners""]),
            ""g1_judgment"": result[""g2_judgment""],
            ""g2_judgment"": result[""g1_judgment""],
        }
        return new_gamekey, new_result",Make the model names sorted in a game key.,"def normalize_game_key_single ( gamekey , result ) : qid , model_1 , model_2 = gamekey if model_1 < model_2 : return gamekey , result else : new_gamekey = ( qid , model_2 , model_1 ) new_result = { ""winners"" : tuple ( reverse_model_map . get ( x , x ) for x in result [ ""winners"" ] ) , ""g1_judgment"" : result [ ""g2_judgment"" ] , ""g2_judgment"" : result [ ""g1_judgment"" ] , } return new_gamekey , new_result",Make the model names sorted in a game key.
/open-interpreter/interpreter/core/utils/lazy_import.py,lazy_import,"def lazy_import(name, optional=True):
    """"""Lazily import a module, specified by the name. Useful for optional packages, to speed up startup times.""""""
    # Check if module is already imported
    if name in sys.modules:
        return sys.modules[name]

    # Find the module specification from the module name
    spec = importlib.util.find_spec(name)
    if spec is None:
        if optional:
            return None  # Do not raise an error if the module is optional
        else:
            raise ImportError(f""Module '{name}' cannot be found"")

    # Use LazyLoader to defer the loading of the module
    loader = importlib.util.LazyLoader(spec.loader)
    spec.loader = loader

    # Create a module from the spec and set it up for lazy loading
    module = importlib.util.module_from_spec(spec)
    sys.modules[name] = module
    loader.exec_module(module)

    return module","def lazy_import(name, optional=True):
    """"""Lazily import a module, specified by the name. Useful for optional packages, to speed up startup times.""""""
    # Check if module is already imported
    if name in sys.modules:
        return sys.modules[name]

    # Find the module specification from the module name
    spec = importlib.util.find_spec(name)
    if spec is None:
        if optional:
            return None  # Do not raise an error if the module is optional
        else:
            raise ImportError(f""Module '{name}' cannot be found"")

    # Use LazyLoader to defer the loading of the module
    loader = importlib.util.LazyLoader(spec.loader)
    spec.loader = loader

    # Create a module from the spec and set it up for lazy loading
    module = importlib.util.module_from_spec(spec)
    sys.modules[name] = module
    loader.exec_module(module)

    return module","Lazily import a module, specified by the name. Useful for optional packages, to speed up startup times.","Lazily import a module, specified by the name.","def lazy_import(name, optional=True):
    
    # Check if module is already imported
    if name in sys.modules:
        return sys.modules[name]

    # Find the module specification from the module name
    spec = importlib.util.find_spec(name)
    if spec is None:
        if optional:
            return None  # Do not raise an error if the module is optional
        else:
            raise ImportError(f""Module '{name}' cannot be found"")

    # Use LazyLoader to defer the loading of the module
    loader = importlib.util.LazyLoader(spec.loader)
    spec.loader = loader

    # Create a module from the spec and set it up for lazy loading
    module = importlib.util.module_from_spec(spec)
    sys.modules[name] = module
    loader.exec_module(module)

    return module","Lazily import a module, specified by the name.","def lazy_import ( name , optional = True ) : # Check if module is already imported if name in sys . modules : return sys . modules [ name ] # Find the module specification from the module name spec = importlib . util . find_spec ( name ) if spec is None : if optional : return None # Do not raise an error if the module is optional else : raise ImportError ( f""Module '{name}' cannot be found"" ) # Use LazyLoader to defer the loading of the module loader = importlib . util . LazyLoader ( spec . loader ) spec . loader = loader # Create a module from the spec and set it up for lazy loading module = importlib . util . module_from_spec ( spec ) sys . modules [ name ] = module loader . exec_module ( module ) return module","Lazily import a module, specified by the name."
/sentry/src/sentry/runner/commands/workstations.py,_notify,"def _notify(text: str) -> None:
    """"""
    Prints simple status updates to the console in a highlighted style.
    """"""

    lines = text.splitlines()
    if len(lines) == 0:
        return

    head = lines[0]
    tail = lines[1:]
    click.echo(click.style(f""➤  {head}"", bold=True, italic=True, fg=""cyan""))
    for line in tail:
        click.echo(click.style(f""   {line.strip()}"", bold=True, fg=""cyan""))","def _notify(text: str) -> None:
    """"""
    Prints simple status updates to the console in a highlighted style.
    """"""

    lines = text.splitlines()
    if len(lines) == 0:
        return

    head = lines[0]
    tail = lines[1:]
    click.echo(click.style(f""➤  {head}"", bold=True, italic=True, fg=""cyan""))
    for line in tail:
        click.echo(click.style(f""   {line.strip()}"", bold=True, fg=""cyan""))",Prints simple status updates to the console in a highlighted style.,Prints simple status updates to the console in a highlighted style.,"def _notify(text: str) -> None:
    

    lines = text.splitlines()
    if len(lines) == 0:
        return

    head = lines[0]
    tail = lines[1:]
    click.echo(click.style(f""➤  {head}"", bold=True, italic=True, fg=""cyan""))
    for line in tail:
        click.echo(click.style(f""   {line.strip()}"", bold=True, fg=""cyan""))",Prints simple status updates to the console in a highlighted style.,"def _notify ( text : str ) -> None : lines = text . splitlines ( ) if len ( lines ) == 0 : return head = lines [ 0 ] tail = lines [ 1 : ] click . echo ( click . style ( f""➤  {head}"" , bold = True , italic = True , fg = ""cyan"" ) ) for line in tail : click . echo ( click . style ( f""   {line.strip()}"" , bold = True , fg = ""cyan"" ) )",Prints simple status updates to the console in a highlighted style.
/black/tests/util.py,get_case_path,"def get_case_path(
    subdir_name: str, name: str, data: bool = True, suffix: str = PYTHON_SUFFIX
) -> Path:
    """"""Get case path from name""""""
    case_path = get_base_dir(data) / subdir_name / name
    if not name.endswith(ALLOWED_SUFFIXES):
        case_path = case_path.with_suffix(suffix)
    assert case_path.is_file(), f""{case_path} is not a file.""
    return case_path","def get_case_path(
    subdir_name: str, name: str, data: bool = True, suffix: str = PYTHON_SUFFIX
) -> Path:
    """"""Get case path from name""""""
    case_path = get_base_dir(data) / subdir_name / name
    if not name.endswith(ALLOWED_SUFFIXES):
        case_path = case_path.with_suffix(suffix)
    assert case_path.is_file(), f""{case_path} is not a file.""
    return case_path",Get case path from name,Get case path from name,"def get_case_path(
    subdir_name: str, name: str, data: bool = True, suffix: str = PYTHON_SUFFIX
) -> Path:
    
    case_path = get_base_dir(data) / subdir_name / name
    if not name.endswith(ALLOWED_SUFFIXES):
        case_path = case_path.with_suffix(suffix)
    assert case_path.is_file(), f""{case_path} is not a file.""
    return case_path",Get case path from name,"def get_case_path ( subdir_name : str , name : str , data : bool = True , suffix : str = PYTHON_SUFFIX ) -> Path : case_path = get_base_dir ( data ) / subdir_name / name if not name . endswith ( ALLOWED_SUFFIXES ) : case_path = case_path . with_suffix ( suffix ) assert case_path . is_file ( ) , f""{case_path} is not a file."" return case_path",Get case path from name
/Fooocus/ldm_patched/k_diffusion/utils.py,append_dims,"def append_dims(x, target_dims):
    """"""Appends dimensions to the end of a tensor until it has target_dims dimensions.""""""
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')
    expanded = x[(...,) + (None,) * dims_to_append]
    # MPS will get inf values if it tries to index into the new axes, but detaching fixes this.
    # https://github.com/pytorch/pytorch/issues/84364
    return expanded.detach().clone() if expanded.device.type == 'mps' else expanded","def append_dims(x, target_dims):
    """"""Appends dimensions to the end of a tensor until it has target_dims dimensions.""""""
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')
    expanded = x[(...,) + (None,) * dims_to_append]
    # MPS will get inf values if it tries to index into the new axes, but detaching fixes this.
    return expanded.detach().clone() if expanded.device.type == 'mps' else expanded",Appends dimensions to the end of a tensor until it has target_dims dimensions.,Appends dimensions to the end of a tensor until it has target_dims dimensions.,"def append_dims(x, target_dims):
    
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')
    expanded = x[(...,) + (None,) * dims_to_append]
    # MPS will get inf values if it tries to index into the new axes, but detaching fixes this.
    return expanded.detach().clone() if expanded.device.type == 'mps' else expanded",Appends dimensions to the end of a tensor until it has target_dims dimensions.,"def append_dims ( x , target_dims ) : dims_to_append = target_dims - x . ndim if dims_to_append < 0 : raise ValueError ( f'input has {x.ndim} dims but target_dims is {target_dims}, which is less' ) expanded = x [ ( ... , ) + ( None , ) * dims_to_append ] # MPS will get inf values if it tries to index into the new axes, but detaching fixes this. return expanded . detach ( ) . clone ( ) if expanded . device . type == 'mps' else expanded",Appends dimensions to the end of a tensor until it has target_dims dimensions.
/Fooocus/ldm_patched/unipc/uni_pc.py,cond_grad_fn,"def cond_grad_fn(x, t_input):
        """"""
        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).
        """"""
        with torch.enable_grad():
            x_in = x.detach().requires_grad_(True)
            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)
            return torch.autograd.grad(log_prob.sum(), x_in)[0]","def cond_grad_fn(x, t_input):
        """"""
        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).
        """"""
        with torch.enable_grad():
            x_in = x.detach().requires_grad_(True)
            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)
            return torch.autograd.grad(log_prob.sum(), x_in)[0]","Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).","Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).","def cond_grad_fn(x, t_input):
        
        with torch.enable_grad():
            x_in = x.detach().requires_grad_(True)
            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)
            return torch.autograd.grad(log_prob.sum(), x_in)[0]","Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).","def cond_grad_fn ( x , t_input ) : with torch . enable_grad ( ) : x_in = x . detach ( ) . requires_grad_ ( True ) log_prob = classifier_fn ( x_in , t_input , condition , ** classifier_kwargs ) return torch . autograd . grad ( log_prob . sum ( ) , x_in ) [ 0 ]","Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t)."
/yolov5/utils/metrics.py,plot_mc_curve,"def plot_mc_curve(px, py, save_dir=Path(""mc_curve.png""), names=(), xlabel=""Confidence"", ylabel=""Metric""):
    """"""Plots a metric-confidence curve for model predictions, supporting per-class visualization and smoothing.""""""
    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)

    if 0 < len(names) < 21:  # display per-class legend if < 21 classes
        for i, y in enumerate(py):
            ax.plot(px, y, linewidth=1, label=f""{names[i]}"")  # plot(confidence, metric)
    else:
        ax.plot(px, py.T, linewidth=1, color=""grey"")  # plot(confidence, metric)

    y = smooth(py.mean(0), 0.05)
    ax.plot(px, y, linewidth=3, color=""blue"", label=f""all classes {y.max():.2f} at {px[y.argmax()]:.3f}"")
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.legend(bbox_to_anchor=(1.04, 1), loc=""upper left"")
    ax.set_title(f""{ylabel}-Confidence Curve"")
    fig.savefig(save_dir, dpi=250)
    plt.close(fig)","def plot_mc_curve(px, py, save_dir=Path(""mc_curve.png""), names=(), xlabel=""Confidence"", ylabel=""Metric""):
    """"""Plots a metric-confidence curve for model predictions, supporting per-class visualization and smoothing.""""""
    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)

    if 0 < len(names) < 21:  # display per-class legend if < 21 classes
        for i, y in enumerate(py):
            ax.plot(px, y, linewidth=1, label=f""{names[i]}"")  # plot(confidence, metric)
    else:
        ax.plot(px, py.T, linewidth=1, color=""grey"")  # plot(confidence, metric)

    y = smooth(py.mean(0), 0.05)
    ax.plot(px, y, linewidth=3, color=""blue"", label=f""all classes {y.max():.2f} at {px[y.argmax()]:.3f}"")
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.legend(bbox_to_anchor=(1.04, 1), loc=""upper left"")
    ax.set_title(f""{ylabel}-Confidence Curve"")
    fig.savefig(save_dir, dpi=250)
    plt.close(fig)","Plots a metric-confidence curve for model predictions, supporting per-class visualization and smoothing.","Plots a metric-confidence curve for model predictions, supporting per-class visualization and smoothing.","def plot_mc_curve(px, py, save_dir=Path(""mc_curve.png""), names=(), xlabel=""Confidence"", ylabel=""Metric""):
    
    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)

    if 0 < len(names) < 21:  # display per-class legend if < 21 classes
        for i, y in enumerate(py):
            ax.plot(px, y, linewidth=1, label=f""{names[i]}"")  # plot(confidence, metric)
    else:
        ax.plot(px, py.T, linewidth=1, color=""grey"")  # plot(confidence, metric)

    y = smooth(py.mean(0), 0.05)
    ax.plot(px, y, linewidth=3, color=""blue"", label=f""all classes {y.max():.2f} at {px[y.argmax()]:.3f}"")
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.legend(bbox_to_anchor=(1.04, 1), loc=""upper left"")
    ax.set_title(f""{ylabel}-Confidence Curve"")
    fig.savefig(save_dir, dpi=250)
    plt.close(fig)","Plots a metric-confidence curve for model predictions, supporting per-class visualization and smoothing.","def plot_mc_curve ( px , py , save_dir = Path ( ""mc_curve.png"" ) , names = ( ) , xlabel = ""Confidence"" , ylabel = ""Metric"" ) : fig , ax = plt . subplots ( 1 , 1 , figsize = ( 9 , 6 ) , tight_layout = True ) if 0 < len ( names ) < 21 : # display per-class legend if < 21 classes for i , y in enumerate ( py ) : ax . plot ( px , y , linewidth = 1 , label = f""{names[i]}"" ) # plot(confidence, metric) else : ax . plot ( px , py . T , linewidth = 1 , color = ""grey"" ) # plot(confidence, metric) y = smooth ( py . mean ( 0 ) , 0.05 ) ax . plot ( px , y , linewidth = 3 , color = ""blue"" , label = f""all classes {y.max():.2f} at {px[y.argmax()]:.3f}"" ) ax . set_xlabel ( xlabel ) ax . set_ylabel ( ylabel ) ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , 1 ) ax . legend ( bbox_to_anchor = ( 1.04 , 1 ) , loc = ""upper left"" ) ax . set_title ( f""{ylabel}-Confidence Curve"" ) fig . savefig ( save_dir , dpi = 250 ) plt . close ( fig )","Plots a metric-confidence curve for model predictions, supporting per-class visualization and smoothing."
/ultralytics/ultralytics/solutions/streamlit_inference.py,configure,"def configure(self):
        """"""Configure the model and load selected classes for inference.""""""
        # Add dropdown menu for model selection
        available_models = [x.replace(""yolo"", ""YOLO"") for x in GITHUB_ASSETS_STEMS if x.startswith(""yolo11"")]
        if self.model_path:  # If user provided the custom model, insert model without suffix as *.pt is added later
            available_models.insert(0, self.model_path.split("".pt"", 1)[0])
        selected_model = self.st.sidebar.selectbox(""Model"", available_models)

        with self.st.spinner(""Model is downloading...""):
            self.model = YOLO(f""{selected_model.lower()}.pt"")  # Load the YOLO model
            class_names = list(self.model.names.values())  # Convert dictionary to list of class names
        self.st.success(""Model loaded successfully!"")

        # Multiselect box with class names and get indices of selected classes
        selected_classes = self.st.sidebar.multiselect(""Classes"", class_names, default=class_names[:3])
        self.selected_ind = [class_names.index(option) for option in selected_classes]

        if not isinstance(self.selected_ind, list):  # Ensure selected_options is a list
            self.selected_ind = list(self.selected_ind)","def configure(self):
        """"""Configure the model and load selected classes for inference.""""""
        # Add dropdown menu for model selection
        available_models = [x.replace(""yolo"", ""YOLO"") for x in GITHUB_ASSETS_STEMS if x.startswith(""yolo11"")]
        if self.model_path:  # If user provided the custom model, insert model without suffix as *.pt is added later
            available_models.insert(0, self.model_path.split("".pt"", 1)[0])
        selected_model = self.st.sidebar.selectbox(""Model"", available_models)

        with self.st.spinner(""Model is downloading...""):
            self.model = YOLO(f""{selected_model.lower()}.pt"")  # Load the YOLO model
            class_names = list(self.model.names.values())  # Convert dictionary to list of class names
        self.st.success(""Model loaded successfully!"")

        # Multiselect box with class names and get indices of selected classes
        selected_classes = self.st.sidebar.multiselect(""Classes"", class_names, default=class_names[:3])
        self.selected_ind = [class_names.index(option) for option in selected_classes]

        if not isinstance(self.selected_ind, list):  # Ensure selected_options is a list
            self.selected_ind = list(self.selected_ind)",Configure the model and load selected classes for inference.,Configure the model and load selected classes for inference.,"def configure(self):
        
        # Add dropdown menu for model selection
        available_models = [x.replace(""yolo"", ""YOLO"") for x in GITHUB_ASSETS_STEMS if x.startswith(""yolo11"")]
        if self.model_path:  # If user provided the custom model, insert model without suffix as *.pt is added later
            available_models.insert(0, self.model_path.split("".pt"", 1)[0])
        selected_model = self.st.sidebar.selectbox(""Model"", available_models)

        with self.st.spinner(""Model is downloading...""):
            self.model = YOLO(f""{selected_model.lower()}.pt"")  # Load the YOLO model
            class_names = list(self.model.names.values())  # Convert dictionary to list of class names
        self.st.success(""Model loaded successfully!"")

        # Multiselect box with class names and get indices of selected classes
        selected_classes = self.st.sidebar.multiselect(""Classes"", class_names, default=class_names[:3])
        self.selected_ind = [class_names.index(option) for option in selected_classes]

        if not isinstance(self.selected_ind, list):  # Ensure selected_options is a list
            self.selected_ind = list(self.selected_ind)",Configure the model and load selected classes for inference.,"def configure ( self ) : # Add dropdown menu for model selection available_models = [ x . replace ( ""yolo"" , ""YOLO"" ) for x in GITHUB_ASSETS_STEMS if x . startswith ( ""yolo11"" ) ] if self . model_path : # If user provided the custom model, insert model without suffix as *.pt is added later available_models . insert ( 0 , self . model_path . split ( "".pt"" , 1 ) [ 0 ] ) selected_model = self . st . sidebar . selectbox ( ""Model"" , available_models ) with self . st . spinner ( ""Model is downloading..."" ) : self . model = YOLO ( f""{selected_model.lower()}.pt"" ) # Load the YOLO model class_names = list ( self . model . names . values ( ) ) # Convert dictionary to list of class names self . st . success ( ""Model loaded successfully!"" ) # Multiselect box with class names and get indices of selected classes selected_classes = self . st . sidebar . multiselect ( ""Classes"" , class_names , default = class_names [ : 3 ] ) self . selected_ind = [ class_names . index ( option ) for option in selected_classes ] if not isinstance ( self . selected_ind , list ) : # Ensure selected_options is a list self . selected_ind = list ( self . selected_ind )",Configure the model and load selected classes for inference.
/ansible/test/sanity/code-smell/package-data.py,check_files,"def check_files(source: str, expected: list[str], actual: list[str]) -> list[str]:
    """"""Verify the expected files exist and no extra files exist.""""""
    missing = set(expected) - set(actual)
    extra = set(actual) - set(expected)

    filter_fuzzy_matches(missing, extra)

    errors = [f'{path}: missing from {source}' for path in sorted(missing)] + [f'{path}: unexpected in {source}' for path in sorted(extra)]

    return errors","def check_files(source: str, expected: list[str], actual: list[str]) -> list[str]:
    """"""Verify the expected files exist and no extra files exist.""""""
    missing = set(expected) - set(actual)
    extra = set(actual) - set(expected)

    filter_fuzzy_matches(missing, extra)

    errors = [f'{path}: missing from {source}' for path in sorted(missing)] + [f'{path}: unexpected in {source}' for path in sorted(extra)]

    return errors",Verify the expected files exist and no extra files exist.,Verify the expected files exist and no extra files exist.,"def check_files(source: str, expected: list[str], actual: list[str]) -> list[str]:
    
    missing = set(expected) - set(actual)
    extra = set(actual) - set(expected)

    filter_fuzzy_matches(missing, extra)

    errors = [f'{path}: missing from {source}' for path in sorted(missing)] + [f'{path}: unexpected in {source}' for path in sorted(extra)]

    return errors",Verify the expected files exist and no extra files exist.,"def check_files ( source : str , expected : list [ str ] , actual : list [ str ] ) -> list [ str ] : missing = set ( expected ) - set ( actual ) extra = set ( actual ) - set ( expected ) filter_fuzzy_matches ( missing , extra ) errors = [ f'{path}: missing from {source}' for path in sorted ( missing ) ] + [ f'{path}: unexpected in {source}' for path in sorted ( extra ) ] return errors",Verify the expected files exist and no extra files exist.
/mitmproxy/examples/contrib/webscanner_helper/mapping.py,response,"def response(self, flow: HTTPFlow) -> None:
        """"""If a response is received, check if we should replace some content.""""""
        try:
            templates = self.mapping_templates[flow]
            res = flow.response
            if res is not None:
                encoding = res.headers.get(""content-encoding"", ""utf-8"")
                content_type = res.headers.get(""content-type"", ""text/html"")

                if ""text/html"" in content_type and encoding == ""utf-8"":
                    content = BeautifulSoup(res.content, MappingAddonConfig.HTML_PARSER)
                    for template in templates:
                        self.apply_template(content, template)
                    res.content = content.encode(encoding)
                else:
                    self.logger.warning(
                        f""Unsupported content type '{content_type}' or content encoding '{encoding}'""
                    )
        except KeyError:
            pass","def response(self, flow: HTTPFlow) -> None:
        """"""If a response is received, check if we should replace some content.""""""
        try:
            templates = self.mapping_templates[flow]
            res = flow.response
            if res is not None:
                encoding = res.headers.get(""content-encoding"", ""utf-8"")
                content_type = res.headers.get(""content-type"", ""text/html"")

                if ""text/html"" in content_type and encoding == ""utf-8"":
                    content = BeautifulSoup(res.content, MappingAddonConfig.HTML_PARSER)
                    for template in templates:
                        self.apply_template(content, template)
                    res.content = content.encode(encoding)
                else:
                    self.logger.warning(
                        f""Unsupported content type '{content_type}' or content encoding '{encoding}'""
                    )
        except KeyError:
            pass","If a response is received, check if we should replace some content.","If a response is received, check if we should replace some content.","def response(self, flow: HTTPFlow) -> None:
        
        try:
            templates = self.mapping_templates[flow]
            res = flow.response
            if res is not None:
                encoding = res.headers.get(""content-encoding"", ""utf-8"")
                content_type = res.headers.get(""content-type"", ""text/html"")

                if ""text/html"" in content_type and encoding == ""utf-8"":
                    content = BeautifulSoup(res.content, MappingAddonConfig.HTML_PARSER)
                    for template in templates:
                        self.apply_template(content, template)
                    res.content = content.encode(encoding)
                else:
                    self.logger.warning(
                        f""Unsupported content type '{content_type}' or content encoding '{encoding}'""
                    )
        except KeyError:
            pass","If a response is received, check if we should replace some content.","def response ( self , flow : HTTPFlow ) -> None : try : templates = self . mapping_templates [ flow ] res = flow . response if res is not None : encoding = res . headers . get ( ""content-encoding"" , ""utf-8"" ) content_type = res . headers . get ( ""content-type"" , ""text/html"" ) if ""text/html"" in content_type and encoding == ""utf-8"" : content = BeautifulSoup ( res . content , MappingAddonConfig . HTML_PARSER ) for template in templates : self . apply_template ( content , template ) res . content = content . encode ( encoding ) else : self . logger . warning ( f""Unsupported content type '{content_type}' or content encoding '{encoding}'"" ) except KeyError : pass","If a response is received, check if we should replace some content."
/ChatTTS/examples/api/openai_api.py,validate_request,"def validate_request(cls, request_data: Dict):
        """"""Filter unsupported request parameters and unify model value to 'tts-1'""""""
        request_data[""model""] = ""tts-1""  # Unify model value
        unsupported_params = set(request_data.keys()) - ALLOWED_PARAMS
        if unsupported_params:
            logger.warning(f""Ignoring unsupported parameters: {unsupported_params}"")
        return {key: request_data[key] for key in ALLOWED_PARAMS if key in request_data}","def validate_request(cls, request_data: Dict):
        """"""Filter unsupported request parameters and unify model value to 'tts-1'""""""
        request_data[""model""] = ""tts-1""  # Unify model value
        unsupported_params = set(request_data.keys()) - ALLOWED_PARAMS
        if unsupported_params:
            logger.warning(f""Ignoring unsupported parameters: {unsupported_params}"")
        return {key: request_data[key] for key in ALLOWED_PARAMS if key in request_data}",Filter unsupported request parameters and unify model value to 'tts-1',Filter unsupported request parameters and unify model value to 'tts-1',"def validate_request(cls, request_data: Dict):
        
        request_data[""model""] = ""tts-1""  # Unify model value
        unsupported_params = set(request_data.keys()) - ALLOWED_PARAMS
        if unsupported_params:
            logger.warning(f""Ignoring unsupported parameters: {unsupported_params}"")
        return {key: request_data[key] for key in ALLOWED_PARAMS if key in request_data}",Filter unsupported request parameters and unify model value to 'tts-1',"def validate_request ( cls , request_data : Dict ) : request_data [ ""model"" ] = ""tts-1"" # Unify model value unsupported_params = set ( request_data . keys ( ) ) - ALLOWED_PARAMS if unsupported_params : logger . warning ( f""Ignoring unsupported parameters: {unsupported_params}"" ) return { key : request_data [ key ] for key in ALLOWED_PARAMS if key in request_data }",Filter unsupported request parameters and unify model value to 'tts-1'
/streamlit/scripts/update_name.py,update_files,"def update_files(project_name: str, files: dict[str, str]) -> None:
    """"""Update files with new project name.""""""
    for filename, regex in files.items():
        file_path = os.path.join(BASE_DIR, filename)
        matched = False
        pattern = re.compile(regex)
        for line in fileinput.input(file_path, inplace=True):
            updated_line = line.rstrip()
            if pattern.match(updated_line):
                updated_line = re.sub(
                    regex, rf""\g<pre_match>{project_name}\g<post_match>"", updated_line
                )
                matched = True
            print(updated_line)
        if not matched:
            raise Exception(f'In file ""{file_path}"", did not find regex ""{regex}""')","def update_files(project_name: str, files: dict[str, str]) -> None:
    """"""Update files with new project name.""""""
    for filename, regex in files.items():
        file_path = os.path.join(BASE_DIR, filename)
        matched = False
        pattern = re.compile(regex)
        for line in fileinput.input(file_path, inplace=True):
            updated_line = line.rstrip()
            if pattern.match(updated_line):
                updated_line = re.sub(
                    regex, rf""\g<pre_match>{project_name}\g<post_match>"", updated_line
                )
                matched = True
            print(updated_line)
        if not matched:
            raise Exception(f'In file ""{file_path}"", did not find regex ""{regex}""')",Update files with new project name.,Update files with new project name.,"def update_files(project_name: str, files: dict[str, str]) -> None:
    
    for filename, regex in files.items():
        file_path = os.path.join(BASE_DIR, filename)
        matched = False
        pattern = re.compile(regex)
        for line in fileinput.input(file_path, inplace=True):
            updated_line = line.rstrip()
            if pattern.match(updated_line):
                updated_line = re.sub(
                    regex, rf""\g<pre_match>{project_name}\g<post_match>"", updated_line
                )
                matched = True
            print(updated_line)
        if not matched:
            raise Exception(f'In file ""{file_path}"", did not find regex ""{regex}""')",Update files with new project name.,"def update_files ( project_name : str , files : dict [ str , str ] ) -> None : for filename , regex in files . items ( ) : file_path = os . path . join ( BASE_DIR , filename ) matched = False pattern = re . compile ( regex ) for line in fileinput . input ( file_path , inplace = True ) : updated_line = line . rstrip ( ) if pattern . match ( updated_line ) : updated_line = re . sub ( regex , rf""\g<pre_match>{project_name}\g<post_match>"" , updated_line ) matched = True print ( updated_line ) if not matched : raise Exception ( f'In file ""{file_path}"", did not find regex ""{regex}""' )",Update files with new project name.
/sentry/src/sentry/runner/commands/config.py,get,"def get(option: str, silent: bool) -> None:
    ""Get a configuration option.""
    from django.conf import settings

    from sentry.options import default_manager as manager
    from sentry.options.manager import UnknownOption

    try:
        key = manager.lookup_key(option)
    except UnknownOption:
        raise click.ClickException(""unknown option: %s"" % option)
    value = manager.get(key.name)
    if silent:
        click.echo(value)
        return

    last_update_channel = manager.get_last_update_channel(option)
    click.echo(f""           type: {key.type.name.upper()}"")
    click.echo(f""    from config: {settings.SENTRY_OPTIONS.get(key.name, '<not set>')}"")
    click.echo(f""        current: {value}"")
    click.echo(
        f"" last update by: {last_update_channel.value if last_update_channel else '<not set>'}""
    )","def get(option: str, silent: bool) -> None:
    ""Get a configuration option.""
    from django.conf import settings

    from sentry.options import default_manager as manager
    from sentry.options.manager import UnknownOption

    try:
        key = manager.lookup_key(option)
    except UnknownOption:
        raise click.ClickException(""unknown option: %s"" % option)
    value = manager.get(key.name)
    if silent:
        click.echo(value)
        return

    last_update_channel = manager.get_last_update_channel(option)
    click.echo(f""           type: {key.type.name.upper()}"")
    click.echo(f""    from config: {settings.SENTRY_OPTIONS.get(key.name, '<not set>')}"")
    click.echo(f""        current: {value}"")
    click.echo(
        f"" last update by: {last_update_channel.value if last_update_channel else '<not set>'}""
    )",Get a configuration option.,Get a configuration option.,"def get(option: str, silent: bool) -> None:
    ""Get a configuration option.""
    from django.conf import settings

    from sentry.options import default_manager as manager
    from sentry.options.manager import UnknownOption

    try:
        key = manager.lookup_key(option)
    except UnknownOption:
        raise click.ClickException(""unknown option: %s"" % option)
    value = manager.get(key.name)
    if silent:
        click.echo(value)
        return

    last_update_channel = manager.get_last_update_channel(option)
    click.echo(f""           type: {key.type.name.upper()}"")
    click.echo(f""    from config: {settings.SENTRY_OPTIONS.get(key.name, '<not set>')}"")
    click.echo(f""        current: {value}"")
    click.echo(
        f"" last update by: {last_update_channel.value if last_update_channel else '<not set>'}""
    )",Get a configuration option.,"def get ( option : str , silent : bool ) -> None : ""Get a configuration option."" from django . conf import settings from sentry . options import default_manager as manager from sentry . options . manager import UnknownOption try : key = manager . lookup_key ( option ) except UnknownOption : raise click . ClickException ( ""unknown option: %s"" % option ) value = manager . get ( key . name ) if silent : click . echo ( value ) return last_update_channel = manager . get_last_update_channel ( option ) click . echo ( f""           type: {key.type.name.upper()}"" ) click . echo ( f""    from config: {settings.SENTRY_OPTIONS.get(key.name, '<not set>')}"" ) click . echo ( f""        current: {value}"" ) click . echo ( f"" last update by: {last_update_channel.value if last_update_channel else '<not set>'}"" )",Get a configuration option.
/black/src/black/numerics.py,format_complex_number,"def format_complex_number(text: str) -> str:
    """"""Formats a complex string like `10j`""""""
    number = text[:-1]
    suffix = text[-1]
    return f""{format_float_or_int_string(number)}{suffix}""","def format_complex_number(text: str) -> str:
    """"""Formats a complex string like `10j`""""""
    number = text[:-1]
    suffix = text[-1]
    return f""{format_float_or_int_string(number)}{suffix}""",Formats a complex string like `10j`,Formats a complex string like `10j`,"def format_complex_number(text: str) -> str:
    
    number = text[:-1]
    suffix = text[-1]
    return f""{format_float_or_int_string(number)}{suffix}""",Formats a complex string like `10j`,"def format_complex_number ( text : str ) -> str : number = text [ : - 1 ] suffix = text [ - 1 ] return f""{format_float_or_int_string(number)}{suffix}""",Formats a complex string like `10j`
/yolov5/classify/train.py,main,"def main(opt):
    """"""Executes YOLOv5 training with given options, handling device setup and DDP mode; includes pre-training checks.""""""
    if RANK in {-1, 0}:
        print_args(vars(opt))
        check_git_status()
        check_requirements(ROOT / ""requirements.txt"")

    # DDP mode
    device = select_device(opt.device, batch_size=opt.batch_size)
    if LOCAL_RANK != -1:
        assert opt.batch_size != -1, ""AutoBatch is coming soon for classification, please pass a valid --batch-size""
        assert opt.batch_size % WORLD_SIZE == 0, f""--batch-size {opt.batch_size} must be multiple of WORLD_SIZE""
        assert torch.cuda.device_count() > LOCAL_RANK, ""insufficient CUDA devices for DDP command""
        torch.cuda.set_device(LOCAL_RANK)
        device = torch.device(""cuda"", LOCAL_RANK)
        dist.init_process_group(backend=""nccl"" if dist.is_nccl_available() else ""gloo"")

    # Parameters
    opt.save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok)  # increment run

    # Train
    train(opt, device)","def main(opt):
    """"""Executes YOLOv5 training with given options, handling device setup and DDP mode; includes pre-training checks.""""""
    if RANK in {-1, 0}:
        print_args(vars(opt))
        check_git_status()
        check_requirements(ROOT / ""requirements.txt"")

    # DDP mode
    device = select_device(opt.device, batch_size=opt.batch_size)
    if LOCAL_RANK != -1:
        assert opt.batch_size != -1, ""AutoBatch is coming soon for classification, please pass a valid --batch-size""
        assert opt.batch_size % WORLD_SIZE == 0, f""--batch-size {opt.batch_size} must be multiple of WORLD_SIZE""
        assert torch.cuda.device_count() > LOCAL_RANK, ""insufficient CUDA devices for DDP command""
        torch.cuda.set_device(LOCAL_RANK)
        device = torch.device(""cuda"", LOCAL_RANK)
        dist.init_process_group(backend=""nccl"" if dist.is_nccl_available() else ""gloo"")

    # Parameters
    opt.save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok)  # increment run

    # Train
    train(opt, device)","Executes YOLOv5 training with given options, handling device setup and DDP mode; includes pre-training checks.","Executes YOLOv5 training with given options, handling device setup and DDP mode; includes pre-training checks.","def main(opt):
    
    if RANK in {-1, 0}:
        print_args(vars(opt))
        check_git_status()
        check_requirements(ROOT / ""requirements.txt"")

    # DDP mode
    device = select_device(opt.device, batch_size=opt.batch_size)
    if LOCAL_RANK != -1:
        assert opt.batch_size != -1, ""AutoBatch is coming soon for classification, please pass a valid --batch-size""
        assert opt.batch_size % WORLD_SIZE == 0, f""--batch-size {opt.batch_size} must be multiple of WORLD_SIZE""
        assert torch.cuda.device_count() > LOCAL_RANK, ""insufficient CUDA devices for DDP command""
        torch.cuda.set_device(LOCAL_RANK)
        device = torch.device(""cuda"", LOCAL_RANK)
        dist.init_process_group(backend=""nccl"" if dist.is_nccl_available() else ""gloo"")

    # Parameters
    opt.save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok)  # increment run

    # Train
    train(opt, device)","Executes YOLOv5 training with given options, handling device setup and DDP mode; includes pre-training checks.","def main ( opt ) : if RANK in { - 1 , 0 } : print_args ( vars ( opt ) ) check_git_status ( ) check_requirements ( ROOT / ""requirements.txt"" ) # DDP mode device = select_device ( opt . device , batch_size = opt . batch_size ) if LOCAL_RANK != - 1 : assert opt . batch_size != - 1 , ""AutoBatch is coming soon for classification, please pass a valid --batch-size"" assert opt . batch_size % WORLD_SIZE == 0 , f""--batch-size {opt.batch_size} must be multiple of WORLD_SIZE"" assert torch . cuda . device_count ( ) > LOCAL_RANK , ""insufficient CUDA devices for DDP command"" torch . cuda . set_device ( LOCAL_RANK ) device = torch . device ( ""cuda"" , LOCAL_RANK ) dist . init_process_group ( backend = ""nccl"" if dist . is_nccl_available ( ) else ""gloo"" ) # Parameters opt . save_dir = increment_path ( Path ( opt . project ) / opt . name , exist_ok = opt . exist_ok ) # increment run # Train train ( opt , device )","Executes YOLOv5 training with given options, handling device setup and DDP mode; includes pre-training checks."
/faceswap/tests/lib/gui/stats/event_reader_test.py,mock_cache,"def mock_cache(self):
        """""" Dummy :class:`_Cache` for testing""""""
        class _CacheMock:
            def __init__(self):
                self.data = {}
                self._loss_labels = []

            def is_cached(self, session_id):
                """""" Dummy is_cached method""""""
                return session_id in self.data

            def cache_data(self, session_id, data, labels,
                           is_live=False):  # pylint:disable=unused-argument
                """""" Dummy cache_data method""""""
                self.data[session_id] = {'data': data, 'labels': labels}

        return _CacheMock()","def mock_cache(self):
        """""" Dummy :class:`_Cache` for testing""""""
        class _CacheMock:
            def __init__(self):
                self.data = {}
                self._loss_labels = []

            def is_cached(self, session_id):
                """""" Dummy is_cached method""""""
                return session_id in self.data

            def cache_data(self, session_id, data, labels,
                           is_live=False):  # pylint:disable=unused-argument
                """""" Dummy cache_data method""""""
                self.data[session_id] = {'data': data, 'labels': labels}

        return _CacheMock()",Dummy :class:`_Cache` for testing,Dummy :class:`_Cache` for testing,"def mock_cache(self):
        
        class _CacheMock:
            def __init__(self):
                self.data = {}
                self._loss_labels = []

            def is_cached(self, session_id):
                
                return session_id in self.data

            def cache_data(self, session_id, data, labels,
                           is_live=False):  # pylint:disable=unused-argument
                
                self.data[session_id] = {'data': data, 'labels': labels}

        return _CacheMock()",Dummy :class:`_Cache` for testing,"def mock_cache ( self ) : class _CacheMock : def __init__ ( self ) : self . data = { } self . _loss_labels = [ ] def is_cached ( self , session_id ) : return session_id in self . data def cache_data ( self , session_id , data , labels , is_live = False ) : # pylint:disable=unused-argument self . data [ session_id ] = { 'data' : data , 'labels' : labels } return _CacheMock ( )",Dummy :class:`_Cache` for testing
/yolov5/utils/loggers/__init__.py,log_model,"def log_model(self, model_path, epoch=0, metadata=None):
        """"""Logs the model to all configured loggers with optional epoch and metadata.""""""
        if metadata is None:
            metadata = {}
        # Log model to all loggers
        if self.wandb:
            art = wandb.Artifact(name=f""run_{wandb.run.id}_model"", type=""model"", metadata=metadata)
            art.add_file(str(model_path))
            wandb.log_artifact(art)
        if self.clearml:
            self.clearml.log_model(model_path=model_path, model_name=model_path.stem)","def log_model(self, model_path, epoch=0, metadata=None):
        """"""Logs the model to all configured loggers with optional epoch and metadata.""""""
        if metadata is None:
            metadata = {}
        # Log model to all loggers
        if self.wandb:
            art = wandb.Artifact(name=f""run_{wandb.run.id}_model"", type=""model"", metadata=metadata)
            art.add_file(str(model_path))
            wandb.log_artifact(art)
        if self.clearml:
            self.clearml.log_model(model_path=model_path, model_name=model_path.stem)",Logs the model to all configured loggers with optional epoch and metadata.,Logs the model to all configured loggers with optional epoch and metadata.,"def log_model(self, model_path, epoch=0, metadata=None):
        
        if metadata is None:
            metadata = {}
        # Log model to all loggers
        if self.wandb:
            art = wandb.Artifact(name=f""run_{wandb.run.id}_model"", type=""model"", metadata=metadata)
            art.add_file(str(model_path))
            wandb.log_artifact(art)
        if self.clearml:
            self.clearml.log_model(model_path=model_path, model_name=model_path.stem)",Logs the model to all configured loggers with optional epoch and metadata.,"def log_model ( self , model_path , epoch = 0 , metadata = None ) : if metadata is None : metadata = { } # Log model to all loggers if self . wandb : art = wandb . Artifact ( name = f""run_{wandb.run.id}_model"" , type = ""model"" , metadata = metadata ) art . add_file ( str ( model_path ) ) wandb . log_artifact ( art ) if self . clearml : self . clearml . log_model ( model_path = model_path , model_name = model_path . stem )",Logs the model to all configured loggers with optional epoch and metadata.
/yolov5/utils/loggers/__init__.py,web_project_name,"def web_project_name(project):
    """"""Converts a local project name to a standardized web project name with optional suffixes.""""""
    if not project.startswith(""runs/train""):
        return project
    suffix = ""-Classify"" if project.endswith(""-cls"") else ""-Segment"" if project.endswith(""-seg"") else """"
    return f""YOLOv5{suffix}""","def web_project_name(project):
    """"""Converts a local project name to a standardized web project name with optional suffixes.""""""
    if not project.startswith(""runs/train""):
        return project
    suffix = ""-Classify"" if project.endswith(""-cls"") else ""-Segment"" if project.endswith(""-seg"") else """"
    return f""YOLOv5{suffix}""",Converts a local project name to a standardized web project name with optional suffixes.,Converts a local project name to a standardized web project name with optional suffixes.,"def web_project_name(project):
    
    if not project.startswith(""runs/train""):
        return project
    suffix = ""-Classify"" if project.endswith(""-cls"") else ""-Segment"" if project.endswith(""-seg"") else """"
    return f""YOLOv5{suffix}""",Converts a local project name to a standardized web project name with optional suffixes.,"def web_project_name ( project ) : if not project . startswith ( ""runs/train"" ) : return project suffix = ""-Classify"" if project . endswith ( ""-cls"" ) else ""-Segment"" if project . endswith ( ""-seg"" ) else """" return f""YOLOv5{suffix}""",Converts a local project name to a standardized web project name with optional suffixes.
/sherlock/tests/sherlock_interactives.py,run_cli,"def run_cli(args:str = """") -> str:
        """"""Pass arguments to Sherlock as a normal user on the command line""""""
        # Adapt for platform differences (Windows likes to be special)
        if platform.system() == ""Windows"":
            command:str = f""py -m sherlock_project {args}""
        else:
            command:str = f""sherlock {args}""

        proc_out:str = """"
        try:
            proc_out = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)
            return proc_out.decode()
        except subprocess.CalledProcessError as e:
            raise InteractivesSubprocessError(e.output.decode())","def run_cli(args:str = """") -> str:
        """"""Pass arguments to Sherlock as a normal user on the command line""""""
        # Adapt for platform differences (Windows likes to be special)
        if platform.system() == ""Windows"":
            command:str = f""py -m sherlock_project {args}""
        else:
            command:str = f""sherlock {args}""

        proc_out:str = """"
        try:
            proc_out = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)
            return proc_out.decode()
        except subprocess.CalledProcessError as e:
            raise InteractivesSubprocessError(e.output.decode())",Pass arguments to Sherlock as a normal user on the command line,Pass arguments to Sherlock as a normal user on the command line,"def run_cli(args:str = """") -> str:
        
        # Adapt for platform differences (Windows likes to be special)
        if platform.system() == ""Windows"":
            command:str = f""py -m sherlock_project {args}""
        else:
            command:str = f""sherlock {args}""

        proc_out:str = """"
        try:
            proc_out = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)
            return proc_out.decode()
        except subprocess.CalledProcessError as e:
            raise InteractivesSubprocessError(e.output.decode())",Pass arguments to Sherlock as a normal user on the command line,"def run_cli ( args : str = """" ) -> str : # Adapt for platform differences (Windows likes to be special) if platform . system ( ) == ""Windows"" : command : str = f""py -m sherlock_project {args}"" else : command : str = f""sherlock {args}"" proc_out : str = """" try : proc_out = subprocess . check_output ( command , shell = True , stderr = subprocess . STDOUT ) return proc_out . decode ( ) except subprocess . CalledProcessError as e : raise InteractivesSubprocessError ( e . output . decode ( ) )",Pass arguments to Sherlock as a normal user on the command line
/yolov5/utils/general.py,check_suffix,"def check_suffix(file=""yolov5s.pt"", suffix=("".pt"",), msg=""""):
    """"""Validates if a file or files have an acceptable suffix, raising an error if not.""""""
    if file and suffix:
        if isinstance(suffix, str):
            suffix = [suffix]
        for f in file if isinstance(file, (list, tuple)) else [file]:
            s = Path(f).suffix.lower()  # file suffix
            if len(s):
                assert s in suffix, f""{msg}{f} acceptable suffix is {suffix}""","def check_suffix(file=""yolov5s.pt"", suffix=("".pt"",), msg=""""):
    """"""Validates if a file or files have an acceptable suffix, raising an error if not.""""""
    if file and suffix:
        if isinstance(suffix, str):
            suffix = [suffix]
        for f in file if isinstance(file, (list, tuple)) else [file]:
            s = Path(f).suffix.lower()  # file suffix
            if len(s):
                assert s in suffix, f""{msg}{f} acceptable suffix is {suffix}""","Validates if a file or files have an acceptable suffix, raising an error if not.","Validates if a file or files have an acceptable suffix, raising an error if not.","def check_suffix(file=""yolov5s.pt"", suffix=("".pt"",), msg=""""):
    
    if file and suffix:
        if isinstance(suffix, str):
            suffix = [suffix]
        for f in file if isinstance(file, (list, tuple)) else [file]:
            s = Path(f).suffix.lower()  # file suffix
            if len(s):
                assert s in suffix, f""{msg}{f} acceptable suffix is {suffix}""","Validates if a file or files have an acceptable suffix, raising an error if not.","def check_suffix ( file = ""yolov5s.pt"" , suffix = ( "".pt"" , ) , msg = """" ) : if file and suffix : if isinstance ( suffix , str ) : suffix = [ suffix ] for f in file if isinstance ( file , ( list , tuple ) ) else [ file ] : s = Path ( f ) . suffix . lower ( ) # file suffix if len ( s ) : assert s in suffix , f""{msg}{f} acceptable suffix is {suffix}""","Validates if a file or files have an acceptable suffix, raising an error if not."
/odoo/odoo/http.py,finalize,"def finalize(self, env):
        """"""
        Finalizes a partial session, should be called on MFA validation
        to convert a partial / pre-session into a logged-in one.
        """"""
        login = self.pop('pre_login')
        uid = self.pop('pre_uid')

        env = env(user=uid)
        user_context = dict(env['res.users'].context_get())

        self.should_rotate = True
        self.update({
            'db': env.registry.db_name,
            'login': login,
            'uid': uid,
            'context': user_context,
            'session_token': env.user._compute_session_token(self.sid),
        })","def finalize(self, env):
        """"""
        Finalizes a partial session, should be called on MFA validation
        to convert a partial / pre-session into a logged-in one.
        """"""
        login = self.pop('pre_login')
        uid = self.pop('pre_uid')

        env = env(user=uid)
        user_context = dict(env['res.users'].context_get())

        self.should_rotate = True
        self.update({
            'db': env.registry.db_name,
            'login': login,
            'uid': uid,
            'context': user_context,
            'session_token': env.user._compute_session_token(self.sid),
        })","Finalizes a partial session, should be called on MFA validation
to convert a partial / pre-session into a logged-in one.","Finalizes a partial session, should be called on MFA validation to convert a partial / pre-session into a logged-in one.","def finalize(self, env):
        
        login = self.pop('pre_login')
        uid = self.pop('pre_uid')

        env = env(user=uid)
        user_context = dict(env['res.users'].context_get())

        self.should_rotate = True
        self.update({
            'db': env.registry.db_name,
            'login': login,
            'uid': uid,
            'context': user_context,
            'session_token': env.user._compute_session_token(self.sid),
        })","Finalizes a partial session, should be called on MFA validation to convert a partial / pre-session into a logged-in one.","def finalize ( self , env ) : login = self . pop ( 'pre_login' ) uid = self . pop ( 'pre_uid' ) env = env ( user = uid ) user_context = dict ( env [ 'res.users' ] . context_get ( ) ) self . should_rotate = True self . update ( { 'db' : env . registry . db_name , 'login' : login , 'uid' : uid , 'context' : user_context , 'session_token' : env . user . _compute_session_token ( self . sid ) , } )","Finalizes a partial session, should be called on MFA validation to convert a partial / pre-session into a logged-in one."
/black/src/black/files.py,find_pyproject_toml,"def find_pyproject_toml(
    path_search_start: tuple[str, ...], stdin_filename: Optional[str] = None
) -> Optional[str]:
    """"""Find the absolute filepath to a pyproject.toml if it exists""""""
    path_project_root, _ = find_project_root(path_search_start, stdin_filename)
    path_pyproject_toml = path_project_root / ""pyproject.toml""
    if path_pyproject_toml.is_file():
        return str(path_pyproject_toml)

    try:
        path_user_pyproject_toml = find_user_pyproject_toml()
        return (
            str(path_user_pyproject_toml)
            if path_user_pyproject_toml.is_file()
            else None
        )
    except (PermissionError, RuntimeError) as e:
        # We do not have access to the user-level config directory, so ignore it.
        err(f""Ignoring user configuration directory due to {e!r}"")
        return None","def find_pyproject_toml(
    path_search_start: tuple[str, ...], stdin_filename: Optional[str] = None
) -> Optional[str]:
    """"""Find the absolute filepath to a pyproject.toml if it exists""""""
    path_project_root, _ = find_project_root(path_search_start, stdin_filename)
    path_pyproject_toml = path_project_root / ""pyproject.toml""
    if path_pyproject_toml.is_file():
        return str(path_pyproject_toml)

    try:
        path_user_pyproject_toml = find_user_pyproject_toml()
        return (
            str(path_user_pyproject_toml)
            if path_user_pyproject_toml.is_file()
            else None
        )
    except (PermissionError, RuntimeError) as e:
        # We do not have access to the user-level config directory, so ignore it.
        err(f""Ignoring user configuration directory due to {e!r}"")
        return None",Find the absolute filepath to a pyproject.toml if it exists,Find the absolute filepath to a pyproject.toml if it exists,"def find_pyproject_toml(
    path_search_start: tuple[str, ...], stdin_filename: Optional[str] = None
) -> Optional[str]:
    
    path_project_root, _ = find_project_root(path_search_start, stdin_filename)
    path_pyproject_toml = path_project_root / ""pyproject.toml""
    if path_pyproject_toml.is_file():
        return str(path_pyproject_toml)

    try:
        path_user_pyproject_toml = find_user_pyproject_toml()
        return (
            str(path_user_pyproject_toml)
            if path_user_pyproject_toml.is_file()
            else None
        )
    except (PermissionError, RuntimeError) as e:
        # We do not have access to the user-level config directory, so ignore it.
        err(f""Ignoring user configuration directory due to {e!r}"")
        return None",Find the absolute filepath to a pyproject.toml if it exists,"def find_pyproject_toml ( path_search_start : tuple [ str , ... ] , stdin_filename : Optional [ str ] = None ) -> Optional [ str ] : path_project_root , _ = find_project_root ( path_search_start , stdin_filename ) path_pyproject_toml = path_project_root / ""pyproject.toml"" if path_pyproject_toml . is_file ( ) : return str ( path_pyproject_toml ) try : path_user_pyproject_toml = find_user_pyproject_toml ( ) return ( str ( path_user_pyproject_toml ) if path_user_pyproject_toml . is_file ( ) else None ) except ( PermissionError , RuntimeError ) as e : # We do not have access to the user-level config directory, so ignore it. err ( f""Ignoring user configuration directory due to {e!r}"" ) return None",Find the absolute filepath to a pyproject.toml if it exists
/ansible/test/sanity/code-smell/package-data.py,main,"def main() -> None:
    """"""Main program entry point.""""""
    complete_file_list = sys.argv[1:] or sys.stdin.read().splitlines()

    python_version = '.'.join(map(str, sys.version_info[:2]))
    python_min = os.environ['ANSIBLE_TEST_MIN_PYTHON']
    python_max = os.environ['ANSIBLE_TEST_MAX_PYTHON']

    if python_version == python_min:
        use_upper_setuptools_version = False
    elif python_version == python_max:
        use_upper_setuptools_version = True
    else:
        raise RuntimeError(f'Python version {python_version} is neither the minimum {python_min} or the maximum {python_max}.')

    errors = check_build(complete_file_list, use_upper_setuptools_version)

    for error in errors:
        print(error)","def main() -> None:
    """"""Main program entry point.""""""
    complete_file_list = sys.argv[1:] or sys.stdin.read().splitlines()

    python_version = '.'.join(map(str, sys.version_info[:2]))
    python_min = os.environ['ANSIBLE_TEST_MIN_PYTHON']
    python_max = os.environ['ANSIBLE_TEST_MAX_PYTHON']

    if python_version == python_min:
        use_upper_setuptools_version = False
    elif python_version == python_max:
        use_upper_setuptools_version = True
    else:
        raise RuntimeError(f'Python version {python_version} is neither the minimum {python_min} or the maximum {python_max}.')

    errors = check_build(complete_file_list, use_upper_setuptools_version)

    for error in errors:
        print(error)",Main program entry point.,Main program entry point.,"def main() -> None:
    
    complete_file_list = sys.argv[1:] or sys.stdin.read().splitlines()

    python_version = '.'.join(map(str, sys.version_info[:2]))
    python_min = os.environ['ANSIBLE_TEST_MIN_PYTHON']
    python_max = os.environ['ANSIBLE_TEST_MAX_PYTHON']

    if python_version == python_min:
        use_upper_setuptools_version = False
    elif python_version == python_max:
        use_upper_setuptools_version = True
    else:
        raise RuntimeError(f'Python version {python_version} is neither the minimum {python_min} or the maximum {python_max}.')

    errors = check_build(complete_file_list, use_upper_setuptools_version)

    for error in errors:
        print(error)",Main program entry point.,"def main ( ) -> None : complete_file_list = sys . argv [ 1 : ] or sys . stdin . read ( ) . splitlines ( ) python_version = '.' . join ( map ( str , sys . version_info [ : 2 ] ) ) python_min = os . environ [ 'ANSIBLE_TEST_MIN_PYTHON' ] python_max = os . environ [ 'ANSIBLE_TEST_MAX_PYTHON' ] if python_version == python_min : use_upper_setuptools_version = False elif python_version == python_max : use_upper_setuptools_version = True else : raise RuntimeError ( f'Python version {python_version} is neither the minimum {python_min} or the maximum {python_max}.' ) errors = check_build ( complete_file_list , use_upper_setuptools_version ) for error in errors : print ( error )",Main program entry point.
/ultralytics/ultralytics/hub/__init__.py,reset_model,"def reset_model(model_id: str = """"):
    """"""Reset a trained model to an untrained state.""""""
    r = requests.post(f""{HUB_API_ROOT}/model-reset"", json={""modelId"": model_id}, headers={""x-api-key"": Auth().api_key})
    if r.status_code == 200:
        LOGGER.info(f""{PREFIX}Model reset successfully"")
        return
    LOGGER.warning(f""{PREFIX}Model reset failure {r.status_code} {r.reason}"")","def reset_model(model_id: str = """"):
    """"""Reset a trained model to an untrained state.""""""
    r = requests.post(f""{HUB_API_ROOT}/model-reset"", json={""modelId"": model_id}, headers={""x-api-key"": Auth().api_key})
    if r.status_code == 200:
        LOGGER.info(f""{PREFIX}Model reset successfully"")
        return
    LOGGER.warning(f""{PREFIX}Model reset failure {r.status_code} {r.reason}"")",Reset a trained model to an untrained state.,Reset a trained model to an untrained state.,"def reset_model(model_id: str = """"):
    
    r = requests.post(f""{HUB_API_ROOT}/model-reset"", json={""modelId"": model_id}, headers={""x-api-key"": Auth().api_key})
    if r.status_code == 200:
        LOGGER.info(f""{PREFIX}Model reset successfully"")
        return
    LOGGER.warning(f""{PREFIX}Model reset failure {r.status_code} {r.reason}"")",Reset a trained model to an untrained state.,"def reset_model ( model_id : str = """" ) : r = requests . post ( f""{HUB_API_ROOT}/model-reset"" , json = { ""modelId"" : model_id } , headers = { ""x-api-key"" : Auth ( ) . api_key } ) if r . status_code == 200 : LOGGER . info ( f""{PREFIX}Model reset successfully"" ) return LOGGER . warning ( f""{PREFIX}Model reset failure {r.status_code} {r.reason}"" )",Reset a trained model to an untrained state.
/faceswap/tools.py,_get_cli_opts,"def _get_cli_opts():
    """""" Optain the subparsers and cli options for available tools """"""
    base_path = os.path.realpath(os.path.dirname(sys.argv[0]))
    tools_dir = os.path.join(base_path, ""tools"")
    for tool_name in sorted(os.listdir(tools_dir)):
        cli_file = os.path.join(tools_dir, tool_name, ""cli.py"")
        if os.path.exists(cli_file):
            mod = ""."".join((""tools"", tool_name, ""cli""))
            module = import_module(mod)
            cliarg_class = getattr(module, f""{tool_name.title()}Args"")
            help_text = getattr(module, ""_HELPTEXT"")
            yield tool_name, help_text, cliarg_class","def _get_cli_opts():
    """""" Optain the subparsers and cli options for available tools """"""
    base_path = os.path.realpath(os.path.dirname(sys.argv[0]))
    tools_dir = os.path.join(base_path, ""tools"")
    for tool_name in sorted(os.listdir(tools_dir)):
        cli_file = os.path.join(tools_dir, tool_name, ""cli.py"")
        if os.path.exists(cli_file):
            mod = ""."".join((""tools"", tool_name, ""cli""))
            module = import_module(mod)
            cliarg_class = getattr(module, f""{tool_name.title()}Args"")
            help_text = getattr(module, ""_HELPTEXT"")
            yield tool_name, help_text, cliarg_class",Optain the subparsers and cli options for available tools,Optain the subparsers and cli options for available tools,"def _get_cli_opts():
    
    base_path = os.path.realpath(os.path.dirname(sys.argv[0]))
    tools_dir = os.path.join(base_path, ""tools"")
    for tool_name in sorted(os.listdir(tools_dir)):
        cli_file = os.path.join(tools_dir, tool_name, ""cli.py"")
        if os.path.exists(cli_file):
            mod = ""."".join((""tools"", tool_name, ""cli""))
            module = import_module(mod)
            cliarg_class = getattr(module, f""{tool_name.title()}Args"")
            help_text = getattr(module, ""_HELPTEXT"")
            yield tool_name, help_text, cliarg_class",Optain the subparsers and cli options for available tools,"def _get_cli_opts ( ) : base_path = os . path . realpath ( os . path . dirname ( sys . argv [ 0 ] ) ) tools_dir = os . path . join ( base_path , ""tools"" ) for tool_name in sorted ( os . listdir ( tools_dir ) ) : cli_file = os . path . join ( tools_dir , tool_name , ""cli.py"" ) if os . path . exists ( cli_file ) : mod = ""."" . join ( ( ""tools"" , tool_name , ""cli"" ) ) module = import_module ( mod ) cliarg_class = getattr ( module , f""{tool_name.title()}Args"" ) help_text = getattr ( module , ""_HELPTEXT"" ) yield tool_name , help_text , cliarg_class",Optain the subparsers and cli options for available tools
/yolov5/utils/__init__.py,__exit__,"def __exit__(self, exc_type, value, traceback):
        """"""Context manager exit method that prints an error message with emojis if an exception occurred, always returns
        True.
        """"""
        if value:
            print(emojis(f""{self.msg}{': ' if self.msg else ''}{value}""))
        return True","def __exit__(self, exc_type, value, traceback):
        """"""Context manager exit method that prints an error message with emojis if an exception occurred, always returns
        True.
        """"""
        if value:
            print(emojis(f""{self.msg}{': ' if self.msg else ''}{value}""))
        return True","Context manager exit method that prints an error message with emojis if an exception occurred, always returns
True.","Context manager exit method that prints an error message with emojis if an exception occurred, always returns","def __exit__(self, exc_type, value, traceback):
        
        if value:
            print(emojis(f""{self.msg}{': ' if self.msg else ''}{value}""))
        return True","Context manager exit method that prints an error message with emojis if an exception occurred, always returns","def __exit__ ( self , exc_type , value , traceback ) : if value : print ( emojis ( f""{self.msg}{': ' if self.msg else ''}{value}"" ) ) return True","Context manager exit method that prints an error message with emojis if an exception occurred, always returns"
/text-generation-webui/extensions/perplexity_colors/script.py,perplexity_color_scale,"def perplexity_color_scale(ppl):
    '''
    Red component only, white for 0 perplexity (sorry if you're not in dark mode)
    '''
    # hue (0.0 = red)
    # saturation (1.0 = red)
    # brightness (0.0 = black, 1.0 = red)
    # scale saturation from white to red the higher the perplexity

    ppl = min(ppl, params['ppl_scale'])  # clip ppl to 0-params['ppl_scale'] for color scaling. 15 should be fine for clipping and scaling
    sat = ppl / params['ppl_scale']
    rv, gv, bv = colorsys.hsv_to_rgb(0.0, sat, 1.0)

    # to hex
    hex_col = f""{int(rv*255):02x}{int(gv*255):02x}{int(bv*255):02x}""
    
    return hex_col","def perplexity_color_scale(ppl):
    '''
    Red component only, white for 0 perplexity (sorry if you're not in dark mode)
    '''
    # hue (0.0 = red)
    # saturation (1.0 = red)
    # brightness (0.0 = black, 1.0 = red)
    # scale saturation from white to red the higher the perplexity

    ppl = min(ppl, params['ppl_scale'])  # clip ppl to 0-params['ppl_scale'] for color scaling. 15 should be fine for clipping and scaling
    sat = ppl / params['ppl_scale']
    rv, gv, bv = colorsys.hsv_to_rgb(0.0, sat, 1.0)

    # to hex
    hex_col = f""{int(rv*255):02x}{int(gv*255):02x}{int(bv*255):02x}""
    
    return hex_col","Red component only, white for 0 perplexity (sorry if you're not in dark mode)","Red component only, white for 0 perplexity (sorry if you're not in dark mode)","def perplexity_color_scale(ppl):
    
    # hue (0.0 = red)
    # saturation (1.0 = red)
    # brightness (0.0 = black, 1.0 = red)
    # scale saturation from white to red the higher the perplexity

    ppl = min(ppl, params['ppl_scale'])  # clip ppl to 0-params['ppl_scale'] for color scaling. 15 should be fine for clipping and scaling
    sat = ppl / params['ppl_scale']
    rv, gv, bv = colorsys.hsv_to_rgb(0.0, sat, 1.0)

    # to hex
    hex_col = f""{int(rv*255):02x}{int(gv*255):02x}{int(bv*255):02x}""
    
    return hex_col","Red component only, white for 0 perplexity (sorry if you're not in dark mode)","def perplexity_color_scale ( ppl ) : # hue (0.0 = red) # saturation (1.0 = red) # brightness (0.0 = black, 1.0 = red) # scale saturation from white to red the higher the perplexity ppl = min ( ppl , params [ 'ppl_scale' ] ) # clip ppl to 0-params['ppl_scale'] for color scaling. 15 should be fine for clipping and scaling sat = ppl / params [ 'ppl_scale' ] rv , gv , bv = colorsys . hsv_to_rgb ( 0.0 , sat , 1.0 ) # to hex hex_col = f""{int(rv*255):02x}{int(gv*255):02x}{int(bv*255):02x}"" return hex_col","Red component only, white for 0 perplexity (sorry if you're not in dark mode)"
/LLaMA-Factory/src/llamafactory/train/dpo/trainer.py,log,"def log(self, logs: dict[str, float], *args, **kwargs) -> None:
        r""""""Log `logs` on the various objects watching training, including stored metrics.""""""
        # logs either has ""loss"" or ""eval_loss""
        train_eval = ""train"" if ""loss"" in logs else ""eval""
        # Add averaged stored metrics to logs
        key_list, metric_list = [], []
        for key, metrics in self._stored_metrics[train_eval].items():
            key_list.append(key)
            metric_list.append(torch.tensor(metrics, dtype=torch.float).to(self.accelerator.device).mean().item())

        del self._stored_metrics[train_eval]
        if len(metric_list) < 10:  # pad to for all reduce
            for i in range(10 - len(metric_list)):
                key_list.append(f""dummy_{i}"")
                metric_list.append(0.0)

        metric_list = torch.tensor(metric_list, dtype=torch.float).to(self.accelerator.device)
        metric_list = self.accelerator.reduce(metric_list, ""mean"").tolist()
        for key, metric in zip(key_list, metric_list):  # add remaining items
            if not key.startswith(""dummy_""):
                logs[key] = metric

        return Trainer.log(self, logs, *args, **kwargs)","def log(self, logs: dict[str, float], *args, **kwargs) -> None:
        r""""""Log `logs` on the various objects watching training, including stored metrics.""""""
        # logs either has ""loss"" or ""eval_loss""
        train_eval = ""train"" if ""loss"" in logs else ""eval""
        # Add averaged stored metrics to logs
        key_list, metric_list = [], []
        for key, metrics in self._stored_metrics[train_eval].items():
            key_list.append(key)
            metric_list.append(torch.tensor(metrics, dtype=torch.float).to(self.accelerator.device).mean().item())

        del self._stored_metrics[train_eval]
        if len(metric_list) < 10:  # pad to for all reduce
            for i in range(10 - len(metric_list)):
                key_list.append(f""dummy_{i}"")
                metric_list.append(0.0)

        metric_list = torch.tensor(metric_list, dtype=torch.float).to(self.accelerator.device)
        metric_list = self.accelerator.reduce(metric_list, ""mean"").tolist()
        for key, metric in zip(key_list, metric_list):  # add remaining items
            if not key.startswith(""dummy_""):
                logs[key] = metric

        return Trainer.log(self, logs, *args, **kwargs)","Log `logs` on the various objects watching training, including stored metrics.","Log `logs` on the various objects watching training, including stored metrics.","def log(self, logs: dict[str, float], *args, **kwargs) -> None:
        
        # logs either has ""loss"" or ""eval_loss""
        train_eval = ""train"" if ""loss"" in logs else ""eval""
        # Add averaged stored metrics to logs
        key_list, metric_list = [], []
        for key, metrics in self._stored_metrics[train_eval].items():
            key_list.append(key)
            metric_list.append(torch.tensor(metrics, dtype=torch.float).to(self.accelerator.device).mean().item())

        del self._stored_metrics[train_eval]
        if len(metric_list) < 10:  # pad to for all reduce
            for i in range(10 - len(metric_list)):
                key_list.append(f""dummy_{i}"")
                metric_list.append(0.0)

        metric_list = torch.tensor(metric_list, dtype=torch.float).to(self.accelerator.device)
        metric_list = self.accelerator.reduce(metric_list, ""mean"").tolist()
        for key, metric in zip(key_list, metric_list):  # add remaining items
            if not key.startswith(""dummy_""):
                logs[key] = metric

        return Trainer.log(self, logs, *args, **kwargs)","Log `logs` on the various objects watching training, including stored metrics.","def log ( self , logs : dict [ str , float ] , * args , ** kwargs ) -> None : # logs either has ""loss"" or ""eval_loss"" train_eval = ""train"" if ""loss"" in logs else ""eval"" # Add averaged stored metrics to logs key_list , metric_list = [ ] , [ ] for key , metrics in self . _stored_metrics [ train_eval ] . items ( ) : key_list . append ( key ) metric_list . append ( torch . tensor ( metrics , dtype = torch . float ) . to ( self . accelerator . device ) . mean ( ) . item ( ) ) del self . _stored_metrics [ train_eval ] if len ( metric_list ) < 10 : # pad to for all reduce for i in range ( 10 - len ( metric_list ) ) : key_list . append ( f""dummy_{i}"" ) metric_list . append ( 0.0 ) metric_list = torch . tensor ( metric_list , dtype = torch . float ) . to ( self . accelerator . device ) metric_list = self . accelerator . reduce ( metric_list , ""mean"" ) . tolist ( ) for key , metric in zip ( key_list , metric_list ) : # add remaining items if not key . startswith ( ""dummy_"" ) : logs [ key ] = metric return Trainer . log ( self , logs , * args , ** kwargs )","Log `logs` on the various objects watching training, including stored metrics."
/localstack/localstack-core/localstack/testing/pytest/marking.py,enforce_single_aws_marker,"def enforce_single_aws_marker(items: List[pytest.Item]):
    """"""Enforce that each test has exactly one aws compatibility marker""""""
    marker_errors = []

    for item in items:
        # we should only concern ourselves with tests in tests/aws/
        if ""tests/aws"" not in item.fspath.dirname:
            continue

        aws_markers = list()
        for mark in item.iter_markers():
            if mark.name.startswith(""aws_""):
                aws_markers.append(mark.name)

        if len(aws_markers) > 1:
            marker_errors.append(f""{item.nodeid}: Too many aws markers specified: {aws_markers}"")
        elif len(aws_markers) == 0:
            marker_errors.append(
                f""{item.nodeid}: Missing aws marker. Specify at least one marker, e.g. @markers.aws.validated""
            )

    if marker_errors:
        raise pytest.UsageError(*marker_errors)","def enforce_single_aws_marker(items: List[pytest.Item]):
    """"""Enforce that each test has exactly one aws compatibility marker""""""
    marker_errors = []

    for item in items:
        # we should only concern ourselves with tests in tests/aws/
        if ""tests/aws"" not in item.fspath.dirname:
            continue

        aws_markers = list()
        for mark in item.iter_markers():
            if mark.name.startswith(""aws_""):
                aws_markers.append(mark.name)

        if len(aws_markers) > 1:
            marker_errors.append(f""{item.nodeid}: Too many aws markers specified: {aws_markers}"")
        elif len(aws_markers) == 0:
            marker_errors.append(
                f""{item.nodeid}: Missing aws marker. Specify at least one marker, e.g. @markers.aws.validated""
            )

    if marker_errors:
        raise pytest.UsageError(*marker_errors)",Enforce that each test has exactly one aws compatibility marker,Enforce that each test has exactly one aws compatibility marker,"def enforce_single_aws_marker(items: List[pytest.Item]):
    
    marker_errors = []

    for item in items:
        # we should only concern ourselves with tests in tests/aws/
        if ""tests/aws"" not in item.fspath.dirname:
            continue

        aws_markers = list()
        for mark in item.iter_markers():
            if mark.name.startswith(""aws_""):
                aws_markers.append(mark.name)

        if len(aws_markers) > 1:
            marker_errors.append(f""{item.nodeid}: Too many aws markers specified: {aws_markers}"")
        elif len(aws_markers) == 0:
            marker_errors.append(
                f""{item.nodeid}: Missing aws marker. Specify at least one marker, e.g. @markers.aws.validated""
            )

    if marker_errors:
        raise pytest.UsageError(*marker_errors)",Enforce that each test has exactly one aws compatibility marker,"def enforce_single_aws_marker ( items : List [ pytest . Item ] ) : marker_errors = [ ] for item in items : # we should only concern ourselves with tests in tests/aws/ if ""tests/aws"" not in item . fspath . dirname : continue aws_markers = list ( ) for mark in item . iter_markers ( ) : if mark . name . startswith ( ""aws_"" ) : aws_markers . append ( mark . name ) if len ( aws_markers ) > 1 : marker_errors . append ( f""{item.nodeid}: Too many aws markers specified: {aws_markers}"" ) elif len ( aws_markers ) == 0 : marker_errors . append ( f""{item.nodeid}: Missing aws marker. Specify at least one marker, e.g. @markers.aws.validated"" ) if marker_errors : raise pytest . UsageError ( * marker_errors )",Enforce that each test has exactly one aws compatibility marker
/markitdown/packages/markitdown/src/markitdown/converters/_youtube_converter.py,_retry_operation,"def _retry_operation(self, operation, retries=3, delay=2):
        """"""Retries the operation if it fails.""""""
        attempt = 0
        while attempt < retries:
            try:
                return operation()  # Attempt the operation
            except Exception as e:
                print(f""Attempt {attempt + 1} failed: {e}"")
                if attempt < retries - 1:
                    time.sleep(delay)  # Wait before retrying
                attempt += 1
        # If all attempts fail, raise the last exception
        raise Exception(f""Operation failed after {retries} attempts."")","def _retry_operation(self, operation, retries=3, delay=2):
        """"""Retries the operation if it fails.""""""
        attempt = 0
        while attempt < retries:
            try:
                return operation()  # Attempt the operation
            except Exception as e:
                print(f""Attempt {attempt + 1} failed: {e}"")
                if attempt < retries - 1:
                    time.sleep(delay)  # Wait before retrying
                attempt += 1
        # If all attempts fail, raise the last exception
        raise Exception(f""Operation failed after {retries} attempts."")",Retries the operation if it fails.,Retries the operation if it fails.,"def _retry_operation(self, operation, retries=3, delay=2):
        
        attempt = 0
        while attempt < retries:
            try:
                return operation()  # Attempt the operation
            except Exception as e:
                print(f""Attempt {attempt + 1} failed: {e}"")
                if attempt < retries - 1:
                    time.sleep(delay)  # Wait before retrying
                attempt += 1
        # If all attempts fail, raise the last exception
        raise Exception(f""Operation failed after {retries} attempts."")",Retries the operation if it fails.,"def _retry_operation ( self , operation , retries = 3 , delay = 2 ) : attempt = 0 while attempt < retries : try : return operation ( ) # Attempt the operation except Exception as e : print ( f""Attempt {attempt + 1} failed: {e}"" ) if attempt < retries - 1 : time . sleep ( delay ) # Wait before retrying attempt += 1 # If all attempts fail, raise the last exception raise Exception ( f""Operation failed after {retries} attempts."" )",Retries the operation if it fails.
/mitmproxy/examples/contrib/jsondump.py,convert_to_strings,"def convert_to_strings(cls, obj):
        """"""
        Recursively convert all list/dict elements of type `bytes` into strings.
        """"""
        if isinstance(obj, dict):
            return {
                cls.convert_to_strings(key): cls.convert_to_strings(value)
                for key, value in obj.items()
            }
        elif isinstance(obj, list) or isinstance(obj, tuple):
            return [cls.convert_to_strings(element) for element in obj]
        elif isinstance(obj, bytes):
            return str(obj)[2:-1]
        return obj","def convert_to_strings(cls, obj):
        """"""
        Recursively convert all list/dict elements of type `bytes` into strings.
        """"""
        if isinstance(obj, dict):
            return {
                cls.convert_to_strings(key): cls.convert_to_strings(value)
                for key, value in obj.items()
            }
        elif isinstance(obj, list) or isinstance(obj, tuple):
            return [cls.convert_to_strings(element) for element in obj]
        elif isinstance(obj, bytes):
            return str(obj)[2:-1]
        return obj",Recursively convert all list/dict elements of type `bytes` into strings.,Recursively convert all list/dict elements of type `bytes` into strings.,"def convert_to_strings(cls, obj):
        
        if isinstance(obj, dict):
            return {
                cls.convert_to_strings(key): cls.convert_to_strings(value)
                for key, value in obj.items()
            }
        elif isinstance(obj, list) or isinstance(obj, tuple):
            return [cls.convert_to_strings(element) for element in obj]
        elif isinstance(obj, bytes):
            return str(obj)[2:-1]
        return obj",Recursively convert all list/dict elements of type `bytes` into strings.,"def convert_to_strings ( cls , obj ) : if isinstance ( obj , dict ) : return { cls . convert_to_strings ( key ) : cls . convert_to_strings ( value ) for key , value in obj . items ( ) } elif isinstance ( obj , list ) or isinstance ( obj , tuple ) : return [ cls . convert_to_strings ( element ) for element in obj ] elif isinstance ( obj , bytes ) : return str ( obj ) [ 2 : - 1 ] return obj",Recursively convert all list/dict elements of type `bytes` into strings.
/sherlock/sherlock_project/sherlock.py,multiple_usernames,"def multiple_usernames(username):
    """"""replace the parameter with with symbols and return a list of usernames""""""
    allUsernames = []
    for i in checksymbols:
        allUsernames.append(username.replace(""{?}"", i))
    return allUsernames","def multiple_usernames(username):
    """"""replace the parameter with with symbols and return a list of usernames""""""
    allUsernames = []
    for i in checksymbols:
        allUsernames.append(username.replace(""{?}"", i))
    return allUsernames",replace the parameter with with symbols and return a list of usernames,replace the parameter with with symbols and return a list of usernames,"def multiple_usernames(username):
    
    allUsernames = []
    for i in checksymbols:
        allUsernames.append(username.replace(""{?}"", i))
    return allUsernames",replace the parameter with with symbols and return a list of usernames,"def multiple_usernames ( username ) : allUsernames = [ ] for i in checksymbols : allUsernames . append ( username . replace ( ""{?}"" , i ) ) return allUsernames",replace the parameter with with symbols and return a list of usernames
/odoo/odoo/models.py,_clean_properties,"def _clean_properties(self) -> None:
        """""" Remove all properties of ``self`` that are no longer in the related definition """"""
        for fname, field in self._fields.items():
            if field.type != 'properties':
                continue
            for record in self:
                old_value = record[fname]
                if not old_value:
                    continue

                definitions = field._get_properties_definition(record)
                all_names = {definition['name'] for definition in definitions}
                new_values = {name: value for name, value in old_value.items() if name in all_names}
                if len(new_values) != len(old_value):
                    record[fname] = new_values","def _clean_properties(self) -> None:
        """""" Remove all properties of ``self`` that are no longer in the related definition """"""
        for fname, field in self._fields.items():
            if field.type != 'properties':
                continue
            for record in self:
                old_value = record[fname]
                if not old_value:
                    continue

                definitions = field._get_properties_definition(record)
                all_names = {definition['name'] for definition in definitions}
                new_values = {name: value for name, value in old_value.items() if name in all_names}
                if len(new_values) != len(old_value):
                    record[fname] = new_values",Remove all properties of ``self`` that are no longer in the related definition,Remove all properties of ``self`` that are no longer in the related definition,"def _clean_properties(self) -> None:
        
        for fname, field in self._fields.items():
            if field.type != 'properties':
                continue
            for record in self:
                old_value = record[fname]
                if not old_value:
                    continue

                definitions = field._get_properties_definition(record)
                all_names = {definition['name'] for definition in definitions}
                new_values = {name: value for name, value in old_value.items() if name in all_names}
                if len(new_values) != len(old_value):
                    record[fname] = new_values",Remove all properties of ``self`` that are no longer in the related definition,"def _clean_properties ( self ) -> None : for fname , field in self . _fields . items ( ) : if field . type != 'properties' : continue for record in self : old_value = record [ fname ] if not old_value : continue definitions = field . _get_properties_definition ( record ) all_names = { definition [ 'name' ] for definition in definitions } new_values = { name : value for name , value in old_value . items ( ) if name in all_names } if len ( new_values ) != len ( old_value ) : record [ fname ] = new_values",Remove all properties of ``self`` that are no longer in the related definition
/sentry/src/sentry/runner/commands/permissions.py,add,"def add(user: str, permission: str) -> None:
    ""Add a permission to a user.""
    from django.db import IntegrityError, transaction

    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)

    try:
        with transaction.atomic(router.db_for_write(UserPermission)):
            UserPermission.objects.create(user=user_inst, permission=permission)
    except IntegrityError:
        click.echo(f""Permission already exists for `{user_inst.username}`"")
    else:
        click.echo(f""Added permission `{permission}` to `{user_inst.username}`"")","def add(user: str, permission: str) -> None:
    ""Add a permission to a user.""
    from django.db import IntegrityError, transaction

    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)

    try:
        with transaction.atomic(router.db_for_write(UserPermission)):
            UserPermission.objects.create(user=user_inst, permission=permission)
    except IntegrityError:
        click.echo(f""Permission already exists for `{user_inst.username}`"")
    else:
        click.echo(f""Added permission `{permission}` to `{user_inst.username}`"")",Add a permission to a user.,Add a permission to a user.,"def add(user: str, permission: str) -> None:
    ""Add a permission to a user.""
    from django.db import IntegrityError, transaction

    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)

    try:
        with transaction.atomic(router.db_for_write(UserPermission)):
            UserPermission.objects.create(user=user_inst, permission=permission)
    except IntegrityError:
        click.echo(f""Permission already exists for `{user_inst.username}`"")
    else:
        click.echo(f""Added permission `{permission}` to `{user_inst.username}`"")",Add a permission to a user.,"def add ( user : str , permission : str ) -> None : ""Add a permission to a user."" from django . db import IntegrityError , transaction from sentry . users . models . userpermission import UserPermission user_inst = user_param_to_user ( user ) try : with transaction . atomic ( router . db_for_write ( UserPermission ) ) : UserPermission . objects . create ( user = user_inst , permission = permission ) except IntegrityError : click . echo ( f""Permission already exists for `{user_inst.username}`"" ) else : click . echo ( f""Added permission `{permission}` to `{user_inst.username}`"" )",Add a permission to a user.
/Fooocus/ldm_patched/k_diffusion/sampling.py,sample_dpm_fast,"def sample_dpm_fast(model, x, sigma_min, sigma_max, n, extra_args=None, callback=None, disable=None, eta=0., s_noise=1., noise_sampler=None):
    """"""DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927.""""""
    if sigma_min <= 0 or sigma_max <= 0:
        raise ValueError('sigma_min and sigma_max must not be 0')
    with tqdm(total=n, disable=disable) as pbar:
        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)
        if callback is not None:
            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})
        return dpm_solver.dpm_solver_fast(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), n, eta, s_noise, noise_sampler)","def sample_dpm_fast(model, x, sigma_min, sigma_max, n, extra_args=None, callback=None, disable=None, eta=0., s_noise=1., noise_sampler=None):
    if sigma_min <= 0 or sigma_max <= 0:
        raise ValueError('sigma_min and sigma_max must not be 0')
    with tqdm(total=n, disable=disable) as pbar:
        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)
        if callback is not None:
            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})
        return dpm_solver.dpm_solver_fast(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), n, eta, s_noise, noise_sampler)",DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927.,DPM-Solver-Fast (fixed step size).,"def sample_dpm_fast(model, x, sigma_min, sigma_max, n, extra_args=None, callback=None, disable=None, eta=0., s_noise=1., noise_sampler=None):
    if sigma_min <= 0 or sigma_max <= 0:
        raise ValueError('sigma_min and sigma_max must not be 0')
    with tqdm(total=n, disable=disable) as pbar:
        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)
        if callback is not None:
            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})
        return dpm_solver.dpm_solver_fast(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), n, eta, s_noise, noise_sampler)",DPM-Solver-Fast (fixed step size).,"def sample_dpm_fast ( model , x , sigma_min , sigma_max , n , extra_args = None , callback = None , disable = None , eta = 0. , s_noise = 1. , noise_sampler = None ) : if sigma_min <= 0 or sigma_max <= 0 : raise ValueError ( 'sigma_min and sigma_max must not be 0' ) with tqdm ( total = n , disable = disable ) as pbar : dpm_solver = DPMSolver ( model , extra_args , eps_callback = pbar . update ) if callback is not None : dpm_solver . info_callback = lambda info : callback ( { 'sigma' : dpm_solver . sigma ( info [ 't' ] ) , 'sigma_hat' : dpm_solver . sigma ( info [ 't_up' ] ) , ** info } ) return dpm_solver . dpm_solver_fast ( x , dpm_solver . t ( torch . tensor ( sigma_max ) ) , dpm_solver . t ( torch . tensor ( sigma_min ) ) , n , eta , s_noise , noise_sampler )",DPM-Solver-Fast (fixed step size).
/gradio/gradio/themes/base.py,to_dict,"def to_dict(self):
        """"""Convert the theme into a python dictionary.""""""
        schema = {""theme"": {}}
        for prop in dir(self):
            if (
                not prop.startswith(""_"")
                or prop.startswith(""_font"")
                or prop in (""_stylesheets"", ""name"")
            ) and isinstance(getattr(self, prop), (list, str)):
                schema[""theme""][prop] = getattr(self, prop)
        return schema","def to_dict(self):
        """"""Convert the theme into a python dictionary.""""""
        schema = {""theme"": {}}
        for prop in dir(self):
            if (
                not prop.startswith(""_"")
                or prop.startswith(""_font"")
                or prop in (""_stylesheets"", ""name"")
            ) and isinstance(getattr(self, prop), (list, str)):
                schema[""theme""][prop] = getattr(self, prop)
        return schema",Convert the theme into a python dictionary.,Convert the theme into a python dictionary.,"def to_dict(self):
        
        schema = {""theme"": {}}
        for prop in dir(self):
            if (
                not prop.startswith(""_"")
                or prop.startswith(""_font"")
                or prop in (""_stylesheets"", ""name"")
            ) and isinstance(getattr(self, prop), (list, str)):
                schema[""theme""][prop] = getattr(self, prop)
        return schema",Convert the theme into a python dictionary.,"def to_dict ( self ) : schema = { ""theme"" : { } } for prop in dir ( self ) : if ( not prop . startswith ( ""_"" ) or prop . startswith ( ""_font"" ) or prop in ( ""_stylesheets"" , ""name"" ) ) and isinstance ( getattr ( self , prop ) , ( list , str ) ) : schema [ ""theme"" ] [ prop ] = getattr ( self , prop ) return schema",Convert the theme into a python dictionary.
/ultralytics/ultralytics/solutions/region_counter.py,__init__,"def __init__(self, **kwargs):
        """"""Initialize the RegionCounter for real-time object counting in user-defined regions.""""""
        super().__init__(**kwargs)
        self.region_template = {
            ""name"": ""Default Region"",
            ""polygon"": None,
            ""counts"": 0,
            ""dragging"": False,
            ""region_color"": (255, 255, 255),
            ""text_color"": (0, 0, 0),
        }
        self.region_counts = {}
        self.counting_regions = []","def __init__(self, **kwargs):
        """"""Initialize the RegionCounter for real-time object counting in user-defined regions.""""""
        super().__init__(**kwargs)
        self.region_template = {
            ""name"": ""Default Region"",
            ""polygon"": None,
            ""counts"": 0,
            ""dragging"": False,
            ""region_color"": (255, 255, 255),
            ""text_color"": (0, 0, 0),
        }
        self.region_counts = {}
        self.counting_regions = []",Initialize the RegionCounter for real-time object counting in user-defined regions.,Initialize the RegionCounter for real-time object counting in user-defined regions.,"def __init__(self, **kwargs):
        
        super().__init__(**kwargs)
        self.region_template = {
            ""name"": ""Default Region"",
            ""polygon"": None,
            ""counts"": 0,
            ""dragging"": False,
            ""region_color"": (255, 255, 255),
            ""text_color"": (0, 0, 0),
        }
        self.region_counts = {}
        self.counting_regions = []",Initialize the RegionCounter for real-time object counting in user-defined regions.,"def __init__ ( self , ** kwargs ) : super ( ) . __init__ ( ** kwargs ) self . region_template = { ""name"" : ""Default Region"" , ""polygon"" : None , ""counts"" : 0 , ""dragging"" : False , ""region_color"" : ( 255 , 255 , 255 ) , ""text_color"" : ( 0 , 0 , 0 ) , } self . region_counts = { } self . counting_regions = [ ]",Initialize the RegionCounter for real-time object counting in user-defined regions.
/black/src/black/files.py,parse_pyproject_toml,"def parse_pyproject_toml(path_config: str) -> dict[str, Any]:
    """"""Parse a pyproject toml file, pulling out relevant parts for Black.

    If parsing fails, will raise a tomllib.TOMLDecodeError.
    """"""
    pyproject_toml = _load_toml(path_config)
    config: dict[str, Any] = pyproject_toml.get(""tool"", {}).get(""black"", {})
    config = {k.replace(""--"", """").replace(""-"", ""_""): v for k, v in config.items()}

    if ""target_version"" not in config:
        inferred_target_version = infer_target_version(pyproject_toml)
        if inferred_target_version is not None:
            config[""target_version""] = [v.name.lower() for v in inferred_target_version]

    return config","def parse_pyproject_toml(path_config: str) -> dict[str, Any]:
    """"""Parse a pyproject toml file, pulling out relevant parts for Black.

    If parsing fails, will raise a tomllib.TOMLDecodeError.
    """"""
    pyproject_toml = _load_toml(path_config)
    config: dict[str, Any] = pyproject_toml.get(""tool"", {}).get(""black"", {})
    config = {k.replace(""--"", """").replace(""-"", ""_""): v for k, v in config.items()}

    if ""target_version"" not in config:
        inferred_target_version = infer_target_version(pyproject_toml)
        if inferred_target_version is not None:
            config[""target_version""] = [v.name.lower() for v in inferred_target_version]

    return config","Parse a pyproject toml file, pulling out relevant parts for Black.

If parsing fails, will raise a tomllib.TOMLDecodeError.","Parse a pyproject toml file, pulling out relevant parts for Black.","def parse_pyproject_toml(path_config: str) -> dict[str, Any]:
    
    pyproject_toml = _load_toml(path_config)
    config: dict[str, Any] = pyproject_toml.get(""tool"", {}).get(""black"", {})
    config = {k.replace(""--"", """").replace(""-"", ""_""): v for k, v in config.items()}

    if ""target_version"" not in config:
        inferred_target_version = infer_target_version(pyproject_toml)
        if inferred_target_version is not None:
            config[""target_version""] = [v.name.lower() for v in inferred_target_version]

    return config","Parse a pyproject toml file, pulling out relevant parts for Black.","def parse_pyproject_toml ( path_config : str ) -> dict [ str , Any ] : pyproject_toml = _load_toml ( path_config ) config : dict [ str , Any ] = pyproject_toml . get ( ""tool"" , { } ) . get ( ""black"" , { } ) config = { k . replace ( ""--"" , """" ) . replace ( ""-"" , ""_"" ) : v for k , v in config . items ( ) } if ""target_version"" not in config : inferred_target_version = infer_target_version ( pyproject_toml ) if inferred_target_version is not None : config [ ""target_version"" ] = [ v . name . lower ( ) for v in inferred_target_version ] return config","Parse a pyproject toml file, pulling out relevant parts for Black."
/odoo/odoo/models.py,_convert_to_record,"def _convert_to_record(self, values):
        """""" Convert the ``values`` dictionary from the cache format to the
        record format.
        """"""
        return {
            name: self._fields[name].convert_to_record(value, self)
            for name, value in values.items()
        }","def _convert_to_record(self, values):
        """""" Convert the ``values`` dictionary from the cache format to the
        record format.
        """"""
        return {
            name: self._fields[name].convert_to_record(value, self)
            for name, value in values.items()
        }","Convert the ``values`` dictionary from the cache format to the
record format.",Convert the ``values`` dictionary from the cache format to the record format.,"def _convert_to_record(self, values):
        
        return {
            name: self._fields[name].convert_to_record(value, self)
            for name, value in values.items()
        }",Convert the ``values`` dictionary from the cache format to the record format.,"def _convert_to_record ( self , values ) : return { name : self . _fields [ name ] . convert_to_record ( value , self ) for name , value in values . items ( ) }",Convert the ``values`` dictionary from the cache format to the record format.
/cpython/Tools/cases_generator/generators_common.py,instruction_size,"def instruction_size(self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: CodeSection,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        """"""Replace the INSTRUCTION_SIZE macro with the size of the current instruction.""""""
        if uop.instruction_size is None:
            raise analysis_error(""The INSTRUCTION_SIZE macro requires uop.instruction_size to be set"", tkn)
        self.out.emit(f"" {uop.instruction_size} "")
        return True","def instruction_size(self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: CodeSection,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        """"""Replace the INSTRUCTION_SIZE macro with the size of the current instruction.""""""
        if uop.instruction_size is None:
            raise analysis_error(""The INSTRUCTION_SIZE macro requires uop.instruction_size to be set"", tkn)
        self.out.emit(f"" {uop.instruction_size} "")
        return True",Replace the INSTRUCTION_SIZE macro with the size of the current instruction.,Replace the INSTRUCTION_SIZE macro with the size of the current instruction.,"def instruction_size(self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: CodeSection,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        
        if uop.instruction_size is None:
            raise analysis_error(""The INSTRUCTION_SIZE macro requires uop.instruction_size to be set"", tkn)
        self.out.emit(f"" {uop.instruction_size} "")
        return True",Replace the INSTRUCTION_SIZE macro with the size of the current instruction.,"def instruction_size ( self , tkn : Token , tkn_iter : TokenIterator , uop : CodeSection , storage : Storage , inst : Instruction | None , ) -> bool : if uop . instruction_size is None : raise analysis_error ( ""The INSTRUCTION_SIZE macro requires uop.instruction_size to be set"" , tkn ) self . out . emit ( f"" {uop.instruction_size} "" ) return True",Replace the INSTRUCTION_SIZE macro with the size of the current instruction.
/llama_index/llama-dev/llama_dev/cli.py,cli,"def cli(ctx, repo_root: str, debug: bool):
    """"""The official CLI for development, testing, and automation in the LlamaIndex monorepo.""""""
    ctx.obj = {
        ""console"": Console(theme=LLAMA_DEV_THEME, soft_wrap=True),
        ""repo_root"": Path(repo_root).resolve(),
        ""debug"": debug,
    }","def cli(ctx, repo_root: str, debug: bool):
    """"""The official CLI for development, testing, and automation in the LlamaIndex monorepo.""""""
    ctx.obj = {
        ""console"": Console(theme=LLAMA_DEV_THEME, soft_wrap=True),
        ""repo_root"": Path(repo_root).resolve(),
        ""debug"": debug,
    }","The official CLI for development, testing, and automation in the LlamaIndex monorepo.","The official CLI for development, testing, and automation in the LlamaIndex monorepo.","def cli(ctx, repo_root: str, debug: bool):
    
    ctx.obj = {
        ""console"": Console(theme=LLAMA_DEV_THEME, soft_wrap=True),
        ""repo_root"": Path(repo_root).resolve(),
        ""debug"": debug,
    }","The official CLI for development, testing, and automation in the LlamaIndex monorepo.","def cli ( ctx , repo_root : str , debug : bool ) : ctx . obj = { ""console"" : Console ( theme = LLAMA_DEV_THEME , soft_wrap = True ) , ""repo_root"" : Path ( repo_root ) . resolve ( ) , ""debug"" : debug , }","The official CLI for development, testing, and automation in the LlamaIndex monorepo."
/yolov5/utils/loggers/comet/__init__.py,download_dataset_artifact,"def download_dataset_artifact(self, artifact_path):
        """"""Downloads a dataset artifact to a specified directory using the experiment's logged artifact.""""""
        logged_artifact = self.experiment.get_artifact(artifact_path)
        artifact_save_dir = str(Path(self.opt.save_dir) / logged_artifact.name)
        logged_artifact.download(artifact_save_dir)

        metadata = logged_artifact.metadata
        data_dict = metadata.copy()
        data_dict[""path""] = artifact_save_dir

        metadata_names = metadata.get(""names"")
        if isinstance(metadata_names, dict):
            data_dict[""names""] = {int(k): v for k, v in metadata.get(""names"").items()}
        elif isinstance(metadata_names, list):
            data_dict[""names""] = {int(k): v for k, v in zip(range(len(metadata_names)), metadata_names)}
        else:
            raise ""Invalid 'names' field in dataset yaml file. Please use a list or dictionary""

        return self.update_data_paths(data_dict)","def download_dataset_artifact(self, artifact_path):
        """"""Downloads a dataset artifact to a specified directory using the experiment's logged artifact.""""""
        logged_artifact = self.experiment.get_artifact(artifact_path)
        artifact_save_dir = str(Path(self.opt.save_dir) / logged_artifact.name)
        logged_artifact.download(artifact_save_dir)

        metadata = logged_artifact.metadata
        data_dict = metadata.copy()
        data_dict[""path""] = artifact_save_dir

        metadata_names = metadata.get(""names"")
        if isinstance(metadata_names, dict):
            data_dict[""names""] = {int(k): v for k, v in metadata.get(""names"").items()}
        elif isinstance(metadata_names, list):
            data_dict[""names""] = {int(k): v for k, v in zip(range(len(metadata_names)), metadata_names)}
        else:
            raise ""Invalid 'names' field in dataset yaml file. Please use a list or dictionary""

        return self.update_data_paths(data_dict)",Downloads a dataset artifact to a specified directory using the experiment's logged artifact.,Downloads a dataset artifact to a specified directory using the experiment's logged artifact.,"def download_dataset_artifact(self, artifact_path):
        
        logged_artifact = self.experiment.get_artifact(artifact_path)
        artifact_save_dir = str(Path(self.opt.save_dir) / logged_artifact.name)
        logged_artifact.download(artifact_save_dir)

        metadata = logged_artifact.metadata
        data_dict = metadata.copy()
        data_dict[""path""] = artifact_save_dir

        metadata_names = metadata.get(""names"")
        if isinstance(metadata_names, dict):
            data_dict[""names""] = {int(k): v for k, v in metadata.get(""names"").items()}
        elif isinstance(metadata_names, list):
            data_dict[""names""] = {int(k): v for k, v in zip(range(len(metadata_names)), metadata_names)}
        else:
            raise ""Invalid 'names' field in dataset yaml file. Please use a list or dictionary""

        return self.update_data_paths(data_dict)",Downloads a dataset artifact to a specified directory using the experiment's logged artifact.,"def download_dataset_artifact ( self , artifact_path ) : logged_artifact = self . experiment . get_artifact ( artifact_path ) artifact_save_dir = str ( Path ( self . opt . save_dir ) / logged_artifact . name ) logged_artifact . download ( artifact_save_dir ) metadata = logged_artifact . metadata data_dict = metadata . copy ( ) data_dict [ ""path"" ] = artifact_save_dir metadata_names = metadata . get ( ""names"" ) if isinstance ( metadata_names , dict ) : data_dict [ ""names"" ] = { int ( k ) : v for k , v in metadata . get ( ""names"" ) . items ( ) } elif isinstance ( metadata_names , list ) : data_dict [ ""names"" ] = { int ( k ) : v for k , v in zip ( range ( len ( metadata_names ) ) , metadata_names ) } else : raise ""Invalid 'names' field in dataset yaml file. Please use a list or dictionary"" return self . update_data_paths ( data_dict )",Downloads a dataset artifact to a specified directory using the experiment's logged artifact.
/mitmproxy/examples/contrib/webscanner_helper/urlinjection.py,response,"def response(self, flow: HTTPFlow):
        """"""Checks if the response matches the filter and such should be injected.
        Injects the URL index if appropriate.
        """"""
        if flow.response is not None:
            if self.flt is not None and self.flt(flow):
                self.injection_gen.inject(self.url_store, flow)
                flow.response.status_code = 200
                flow.response.headers[""content-type""] = ""text/html""
                logger.debug(
                    f""Set status code to 200 and set content to logged ""
                    f""urls. Method: {self.injection_gen}""
                )","def response(self, flow: HTTPFlow):
        """"""Checks if the response matches the filter and such should be injected.
        Injects the URL index if appropriate.
        """"""
        if flow.response is not None:
            if self.flt is not None and self.flt(flow):
                self.injection_gen.inject(self.url_store, flow)
                flow.response.status_code = 200
                flow.response.headers[""content-type""] = ""text/html""
                logger.debug(
                    f""Set status code to 200 and set content to logged ""
                    f""urls. Method: {self.injection_gen}""
                )","Checks if the response matches the filter and such should be injected.
Injects the URL index if appropriate.",Checks if the response matches the filter and such should be injected.,"def response(self, flow: HTTPFlow):
        
        if flow.response is not None:
            if self.flt is not None and self.flt(flow):
                self.injection_gen.inject(self.url_store, flow)
                flow.response.status_code = 200
                flow.response.headers[""content-type""] = ""text/html""
                logger.debug(
                    f""Set status code to 200 and set content to logged ""
                    f""urls. Method: {self.injection_gen}""
                )",Checks if the response matches the filter and such should be injected.,"def response ( self , flow : HTTPFlow ) : if flow . response is not None : if self . flt is not None and self . flt ( flow ) : self . injection_gen . inject ( self . url_store , flow ) flow . response . status_code = 200 flow . response . headers [ ""content-type"" ] = ""text/html"" logger . debug ( f""Set status code to 200 and set content to logged "" f""urls. Method: {self.injection_gen}"" )",Checks if the response matches the filter and such should be injected.
/ray/rllib/connectors/env_to_module/mean_std_filter.py,reset_state,"def reset_state(self) -> None:
        """"""Creates copy of current state and resets accumulated state""""""
        if not self._update_stats:
            raise ValueError(
                f""State of {type(self).__name__} can only be changed when ""
                f""`update_stats` was set to False.""
            )
        self._init_new_filters()","def reset_state(self) -> None:
        """"""Creates copy of current state and resets accumulated state""""""
        if not self._update_stats:
            raise ValueError(
                f""State of {type(self).__name__} can only be changed when ""
                f""`update_stats` was set to False.""
            )
        self._init_new_filters()",Creates copy of current state and resets accumulated state,Creates copy of current state and resets accumulated state,"def reset_state(self) -> None:
        
        if not self._update_stats:
            raise ValueError(
                f""State of {type(self).__name__} can only be changed when ""
                f""`update_stats` was set to False.""
            )
        self._init_new_filters()",Creates copy of current state and resets accumulated state,"def reset_state ( self ) -> None : if not self . _update_stats : raise ValueError ( f""State of {type(self).__name__} can only be changed when "" f""`update_stats` was set to False."" ) self . _init_new_filters ( )",Creates copy of current state and resets accumulated state
/ultralytics/ultralytics/solutions/parking_management.py,__init__,"def __init__(self, **kwargs: Any):
        """"""Initialize the parking management system with a YOLO model and visualization settings.""""""
        super().__init__(**kwargs)

        self.json_file = self.CFG[""json_file""]  # Load parking regions JSON data
        if self.json_file is None:
            LOGGER.warning(""json_file argument missing. Parking region details required."")
            raise ValueError(""❌ Json file path can not be empty"")

        with open(self.json_file) as f:
            self.json = json.load(f)

        self.pr_info = {""Occupancy"": 0, ""Available"": 0}  # Dictionary for parking information

        self.arc = (0, 0, 255)  # Available region color
        self.occ = (0, 255, 0)  # Occupied region color
        self.dc = (255, 0, 189)","def __init__(self, **kwargs: Any):
        """"""Initialize the parking management system with a YOLO model and visualization settings.""""""
        super().__init__(**kwargs)

        self.json_file = self.CFG[""json_file""]  # Load parking regions JSON data
        if self.json_file is None:
            LOGGER.warning(""json_file argument missing. Parking region details required."")
            raise ValueError(""❌ Json file path can not be empty"")

        with open(self.json_file) as f:
            self.json = json.load(f)

        self.pr_info = {""Occupancy"": 0, ""Available"": 0}  # Dictionary for parking information

        self.arc = (0, 0, 255)  # Available region color
        self.occ = (0, 255, 0)  # Occupied region color
        self.dc = (255, 0, 189)",Initialize the parking management system with a YOLO model and visualization settings.,Initialize the parking management system with a YOLO model and visualization settings.,"def __init__(self, **kwargs: Any):
        
        super().__init__(**kwargs)

        self.json_file = self.CFG[""json_file""]  # Load parking regions JSON data
        if self.json_file is None:
            LOGGER.warning(""json_file argument missing. Parking region details required."")
            raise ValueError(""❌ Json file path can not be empty"")

        with open(self.json_file) as f:
            self.json = json.load(f)

        self.pr_info = {""Occupancy"": 0, ""Available"": 0}  # Dictionary for parking information

        self.arc = (0, 0, 255)  # Available region color
        self.occ = (0, 255, 0)  # Occupied region color
        self.dc = (255, 0, 189)",Initialize the parking management system with a YOLO model and visualization settings.,"def __init__ ( self , ** kwargs : Any ) : super ( ) . __init__ ( ** kwargs ) self . json_file = self . CFG [ ""json_file"" ] # Load parking regions JSON data if self . json_file is None : LOGGER . warning ( ""json_file argument missing. Parking region details required."" ) raise ValueError ( ""❌ Json file path can not be empty"" ) with open ( self . json_file ) as f : self . json = json . load ( f ) self . pr_info = { ""Occupancy"" : 0 , ""Available"" : 0 } # Dictionary for parking information self . arc = ( 0 , 0 , 255 ) # Available region color self . occ = ( 0 , 255 , 0 ) # Occupied region color self . dc = ( 255 , 0 , 189 )",Initialize the parking management system with a YOLO model and visualization settings.
/yolov5/models/tf.py,activations,"def activations(act=nn.SiLU):
    """"""Converts PyTorch activations to TensorFlow equivalents, supporting LeakyReLU, Hardswish, and SiLU/Swish.""""""
    if isinstance(act, nn.LeakyReLU):
        return lambda x: keras.activations.relu(x, alpha=0.1)
    elif isinstance(act, nn.Hardswish):
        return lambda x: x * tf.nn.relu6(x + 3) * 0.166666667
    elif isinstance(act, (nn.SiLU, SiLU)):
        return lambda x: keras.activations.swish(x)
    else:
        raise Exception(f""no matching TensorFlow activation found for PyTorch activation {act}"")","def activations(act=nn.SiLU):
    """"""Converts PyTorch activations to TensorFlow equivalents, supporting LeakyReLU, Hardswish, and SiLU/Swish.""""""
    if isinstance(act, nn.LeakyReLU):
        return lambda x: keras.activations.relu(x, alpha=0.1)
    elif isinstance(act, nn.Hardswish):
        return lambda x: x * tf.nn.relu6(x + 3) * 0.166666667
    elif isinstance(act, (nn.SiLU, SiLU)):
        return lambda x: keras.activations.swish(x)
    else:
        raise Exception(f""no matching TensorFlow activation found for PyTorch activation {act}"")","Converts PyTorch activations to TensorFlow equivalents, supporting LeakyReLU, Hardswish, and SiLU/Swish.","Converts PyTorch activations to TensorFlow equivalents, supporting LeakyReLU, Hardswish, and SiLU/Swish.","def activations(act=nn.SiLU):
    
    if isinstance(act, nn.LeakyReLU):
        return lambda x: keras.activations.relu(x, alpha=0.1)
    elif isinstance(act, nn.Hardswish):
        return lambda x: x * tf.nn.relu6(x + 3) * 0.166666667
    elif isinstance(act, (nn.SiLU, SiLU)):
        return lambda x: keras.activations.swish(x)
    else:
        raise Exception(f""no matching TensorFlow activation found for PyTorch activation {act}"")","Converts PyTorch activations to TensorFlow equivalents, supporting LeakyReLU, Hardswish, and SiLU/Swish.","def activations ( act = nn . SiLU ) : if isinstance ( act , nn . LeakyReLU ) : return lambda x : keras . activations . relu ( x , alpha = 0.1 ) elif isinstance ( act , nn . Hardswish ) : return lambda x : x * tf . nn . relu6 ( x + 3 ) * 0.166666667 elif isinstance ( act , ( nn . SiLU , SiLU ) ) : return lambda x : keras . activations . swish ( x ) else : raise Exception ( f""no matching TensorFlow activation found for PyTorch activation {act}"" )","Converts PyTorch activations to TensorFlow equivalents, supporting LeakyReLU, Hardswish, and SiLU/Swish."
/OpenManus/app/llm.py,get_limit_error_message,"def get_limit_error_message(self, input_tokens: int) -> str:
        """"""Generate error message for token limit exceeded""""""
        if (
            self.max_input_tokens is not None
            and (self.total_input_tokens + input_tokens) > self.max_input_tokens
        ):
            return f""Request may exceed input token limit (Current: {self.total_input_tokens}, Needed: {input_tokens}, Max: {self.max_input_tokens})""

        return ""Token limit exceeded""","def get_limit_error_message(self, input_tokens: int) -> str:
        """"""Generate error message for token limit exceeded""""""
        if (
            self.max_input_tokens is not None
            and (self.total_input_tokens + input_tokens) > self.max_input_tokens
        ):
            return f""Request may exceed input token limit (Current: {self.total_input_tokens}, Needed: {input_tokens}, Max: {self.max_input_tokens})""

        return ""Token limit exceeded""",Generate error message for token limit exceeded,Generate error message for token limit exceeded,"def get_limit_error_message(self, input_tokens: int) -> str:
        
        if (
            self.max_input_tokens is not None
            and (self.total_input_tokens + input_tokens) > self.max_input_tokens
        ):
            return f""Request may exceed input token limit (Current: {self.total_input_tokens}, Needed: {input_tokens}, Max: {self.max_input_tokens})""

        return ""Token limit exceeded""",Generate error message for token limit exceeded,"def get_limit_error_message ( self , input_tokens : int ) -> str : if ( self . max_input_tokens is not None and ( self . total_input_tokens + input_tokens ) > self . max_input_tokens ) : return f""Request may exceed input token limit (Current: {self.total_input_tokens}, Needed: {input_tokens}, Max: {self.max_input_tokens})"" return ""Token limit exceeded""",Generate error message for token limit exceeded
/AutoGPT/.github/workflows/scripts/check_actions_status.py,get_environment_variables,"def get_environment_variables() -> Tuple[str, str, str, str, str]:
    """"""Retrieve and return necessary environment variables.""""""
    try:
        with open(os.environ[""GITHUB_EVENT_PATH""]) as f:
            event = json.load(f)

        # Handle both PR and merge group events
        if ""pull_request"" in event:
            sha = event[""pull_request""][""head""][""sha""]
        else:
            sha = os.environ[""GITHUB_SHA""]

        return (
            os.environ[""GITHUB_API_URL""],
            os.environ[""GITHUB_REPOSITORY""],
            sha,
            os.environ[""GITHUB_TOKEN""],
            os.environ[""GITHUB_RUN_ID""],
        )
    except KeyError as e:
        print(f""Error: Missing required environment variable or event data: {e}"")
        sys.exit(1)","def get_environment_variables() -> Tuple[str, str, str, str, str]:
    """"""Retrieve and return necessary environment variables.""""""
    try:
        with open(os.environ[""GITHUB_EVENT_PATH""]) as f:
            event = json.load(f)

        # Handle both PR and merge group events
        if ""pull_request"" in event:
            sha = event[""pull_request""][""head""][""sha""]
        else:
            sha = os.environ[""GITHUB_SHA""]

        return (
            os.environ[""GITHUB_API_URL""],
            os.environ[""GITHUB_REPOSITORY""],
            sha,
            os.environ[""GITHUB_TOKEN""],
            os.environ[""GITHUB_RUN_ID""],
        )
    except KeyError as e:
        print(f""Error: Missing required environment variable or event data: {e}"")
        sys.exit(1)",Retrieve and return necessary environment variables.,Retrieve and return necessary environment variables.,"def get_environment_variables() -> Tuple[str, str, str, str, str]:
    
    try:
        with open(os.environ[""GITHUB_EVENT_PATH""]) as f:
            event = json.load(f)

        # Handle both PR and merge group events
        if ""pull_request"" in event:
            sha = event[""pull_request""][""head""][""sha""]
        else:
            sha = os.environ[""GITHUB_SHA""]

        return (
            os.environ[""GITHUB_API_URL""],
            os.environ[""GITHUB_REPOSITORY""],
            sha,
            os.environ[""GITHUB_TOKEN""],
            os.environ[""GITHUB_RUN_ID""],
        )
    except KeyError as e:
        print(f""Error: Missing required environment variable or event data: {e}"")
        sys.exit(1)",Retrieve and return necessary environment variables.,"def get_environment_variables ( ) -> Tuple [ str , str , str , str , str ] : try : with open ( os . environ [ ""GITHUB_EVENT_PATH"" ] ) as f : event = json . load ( f ) # Handle both PR and merge group events if ""pull_request"" in event : sha = event [ ""pull_request"" ] [ ""head"" ] [ ""sha"" ] else : sha = os . environ [ ""GITHUB_SHA"" ] return ( os . environ [ ""GITHUB_API_URL"" ] , os . environ [ ""GITHUB_REPOSITORY"" ] , sha , os . environ [ ""GITHUB_TOKEN"" ] , os . environ [ ""GITHUB_RUN_ID"" ] , ) except KeyError as e : print ( f""Error: Missing required environment variable or event data: {e}"" ) sys . exit ( 1 )",Retrieve and return necessary environment variables.
/MetaGPT/metagpt/roles/role.py,publish_message,"def publish_message(self, msg):
        """"""If the role belongs to env, then the role's messages will be broadcast to env""""""
        if not msg:
            return
        if MESSAGE_ROUTE_TO_SELF in msg.send_to:
            msg.send_to.add(any_to_str(self))
            msg.send_to.remove(MESSAGE_ROUTE_TO_SELF)
        if not msg.sent_from or msg.sent_from == MESSAGE_ROUTE_TO_SELF:
            msg.sent_from = any_to_str(self)
        if all(to in {any_to_str(self), self.name} for to in msg.send_to):  # Message to myself
            self.put_message(msg)
            return
        if not self.rc.env:
            # If env does not exist, do not publish the message
            return
        if isinstance(msg, AIMessage) and not msg.agent:
            msg.with_agent(self._setting)
        self.rc.env.publish_message(msg)","def publish_message(self, msg):
        """"""If the role belongs to env, then the role's messages will be broadcast to env""""""
        if not msg:
            return
        if MESSAGE_ROUTE_TO_SELF in msg.send_to:
            msg.send_to.add(any_to_str(self))
            msg.send_to.remove(MESSAGE_ROUTE_TO_SELF)
        if not msg.sent_from or msg.sent_from == MESSAGE_ROUTE_TO_SELF:
            msg.sent_from = any_to_str(self)
        if all(to in {any_to_str(self), self.name} for to in msg.send_to):  # Message to myself
            self.put_message(msg)
            return
        if not self.rc.env:
            # If env does not exist, do not publish the message
            return
        if isinstance(msg, AIMessage) and not msg.agent:
            msg.with_agent(self._setting)
        self.rc.env.publish_message(msg)","If the role belongs to env, then the role's messages will be broadcast to env","If the role belongs to env, then the role's messages will be broadcast to env","def publish_message(self, msg):
        
        if not msg:
            return
        if MESSAGE_ROUTE_TO_SELF in msg.send_to:
            msg.send_to.add(any_to_str(self))
            msg.send_to.remove(MESSAGE_ROUTE_TO_SELF)
        if not msg.sent_from or msg.sent_from == MESSAGE_ROUTE_TO_SELF:
            msg.sent_from = any_to_str(self)
        if all(to in {any_to_str(self), self.name} for to in msg.send_to):  # Message to myself
            self.put_message(msg)
            return
        if not self.rc.env:
            # If env does not exist, do not publish the message
            return
        if isinstance(msg, AIMessage) and not msg.agent:
            msg.with_agent(self._setting)
        self.rc.env.publish_message(msg)","If the role belongs to env, then the role's messages will be broadcast to env","def publish_message ( self , msg ) : if not msg : return if MESSAGE_ROUTE_TO_SELF in msg . send_to : msg . send_to . add ( any_to_str ( self ) ) msg . send_to . remove ( MESSAGE_ROUTE_TO_SELF ) if not msg . sent_from or msg . sent_from == MESSAGE_ROUTE_TO_SELF : msg . sent_from = any_to_str ( self ) if all ( to in { any_to_str ( self ) , self . name } for to in msg . send_to ) : # Message to myself self . put_message ( msg ) return if not self . rc . env : # If env does not exist, do not publish the message return if isinstance ( msg , AIMessage ) and not msg . agent : msg . with_agent ( self . _setting ) self . rc . env . publish_message ( msg )","If the role belongs to env, then the role's messages will be broadcast to env"
/ultralytics/ultralytics/data/augment.py,__init__,"def __init__(self, dataset=None, pre_transform=None, p=0.5, mode=""flip"") -> None:
        """"""Initialize CopyPaste object with dataset, pre_transform, and probability of applying MixUp.""""""
        super().__init__(dataset=dataset, pre_transform=pre_transform, p=p)
        assert mode in {""flip"", ""mixup""}, f""Expected `mode` to be `flip` or `mixup`, but got {mode}.""
        self.mode = mode","def __init__(self, dataset=None, pre_transform=None, p=0.5, mode=""flip"") -> None:
        """"""Initialize CopyPaste object with dataset, pre_transform, and probability of applying MixUp.""""""
        super().__init__(dataset=dataset, pre_transform=pre_transform, p=p)
        assert mode in {""flip"", ""mixup""}, f""Expected `mode` to be `flip` or `mixup`, but got {mode}.""
        self.mode = mode","Initialize CopyPaste object with dataset, pre_transform, and probability of applying MixUp.","Initialize CopyPaste object with dataset, pre_transform, and probability of applying MixUp.","def __init__(self, dataset=None, pre_transform=None, p=0.5, mode=""flip"") -> None:
        
        super().__init__(dataset=dataset, pre_transform=pre_transform, p=p)
        assert mode in {""flip"", ""mixup""}, f""Expected `mode` to be `flip` or `mixup`, but got {mode}.""
        self.mode = mode","Initialize CopyPaste object with dataset, pre_transform, and probability of applying MixUp.","def __init__ ( self , dataset = None , pre_transform = None , p = 0.5 , mode = ""flip"" ) -> None : super ( ) . __init__ ( dataset = dataset , pre_transform = pre_transform , p = p ) assert mode in { ""flip"" , ""mixup"" } , f""Expected `mode` to be `flip` or `mixup`, but got {mode}."" self . mode = mode","Initialize CopyPaste object with dataset, pre_transform, and probability of applying MixUp."
/open-interpreter/interpreter/core/utils/temporary_file.py,create_temporary_file,"def create_temporary_file(contents, extension=None, verbose=False):
    """"""
    create a temporary file with the given contents
    """"""

    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(
            mode=""w"", delete=False, suffix=f"".{extension}"" if extension else """"
        ) as f:
            f.write(contents)
            temp_file_name = f.name
            f.close()

        if verbose:
            print(f""Created temporary file {temp_file_name}"")
            print(""---"")

        return temp_file_name

    except Exception as e:
        print(f""Could not create temporary file."")
        print(e)
        print("""")","def create_temporary_file(contents, extension=None, verbose=False):
    """"""
    create a temporary file with the given contents
    """"""

    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(
            mode=""w"", delete=False, suffix=f"".{extension}"" if extension else """"
        ) as f:
            f.write(contents)
            temp_file_name = f.name
            f.close()

        if verbose:
            print(f""Created temporary file {temp_file_name}"")
            print(""---"")

        return temp_file_name

    except Exception as e:
        print(f""Could not create temporary file."")
        print(e)
        print("""")",create a temporary file with the given contents,create a temporary file with the given contents,"def create_temporary_file(contents, extension=None, verbose=False):
    

    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(
            mode=""w"", delete=False, suffix=f"".{extension}"" if extension else """"
        ) as f:
            f.write(contents)
            temp_file_name = f.name
            f.close()

        if verbose:
            print(f""Created temporary file {temp_file_name}"")
            print(""---"")

        return temp_file_name

    except Exception as e:
        print(f""Could not create temporary file."")
        print(e)
        print("""")",create a temporary file with the given contents,"def create_temporary_file ( contents , extension = None , verbose = False ) : try : # Create a temporary file with tempfile . NamedTemporaryFile ( mode = ""w"" , delete = False , suffix = f"".{extension}"" if extension else """" ) as f : f . write ( contents ) temp_file_name = f . name f . close ( ) if verbose : print ( f""Created temporary file {temp_file_name}"" ) print ( ""---"" ) return temp_file_name except Exception as e : print ( f""Could not create temporary file."" ) print ( e ) print ( """" )",create a temporary file with the given contents
/faceswap/plugins/train/model/dfl_sae.py,inter_liae,"def inter_liae(self, side, input_shape):
        """""" DFL SAE LIAE Intermediate Network """"""
        input_ = Input(shape=input_shape)
        lowest_dense_res = self.input_shape[0] // 16
        var_x = input_
        var_x = Dense(self.ae_dims)(var_x)
        var_x = Dense(lowest_dense_res * lowest_dense_res * self.ae_dims * 2)(var_x)
        var_x = Reshape((lowest_dense_res, lowest_dense_res, self.ae_dims * 2))(var_x)
        var_x = UpscaleBlock(self.ae_dims * 2, activation=""leakyrelu"")(var_x)
        return KModel(input_, var_x, name=f""intermediate_{side}"")","def inter_liae(self, side, input_shape):
        """""" DFL SAE LIAE Intermediate Network """"""
        input_ = Input(shape=input_shape)
        var_x = input_
        var_x = Dense(self.ae_dims)(var_x)
        var_x = Dense(lowest_dense_res * lowest_dense_res * self.ae_dims * 2)(var_x)
        var_x = Reshape((lowest_dense_res, lowest_dense_res, self.ae_dims * 2))(var_x)
        var_x = UpscaleBlock(self.ae_dims * 2, activation=""leakyrelu"")(var_x)
        return KModel(input_, var_x, name=f""intermediate_{side}"")",DFL SAE LIAE Intermediate Network,DFL SAE LIAE Intermediate Network,"def inter_liae(self, side, input_shape):
        
        input_ = Input(shape=input_shape)
        var_x = input_
        var_x = Dense(self.ae_dims)(var_x)
        var_x = Dense(lowest_dense_res * lowest_dense_res * self.ae_dims * 2)(var_x)
        var_x = Reshape((lowest_dense_res, lowest_dense_res, self.ae_dims * 2))(var_x)
        var_x = UpscaleBlock(self.ae_dims * 2, activation=""leakyrelu"")(var_x)
        return KModel(input_, var_x, name=f""intermediate_{side}"")",DFL SAE LIAE Intermediate Network,"def inter_liae ( self , side , input_shape ) : input_ = Input ( shape = input_shape ) var_x = input_ var_x = Dense ( self . ae_dims ) ( var_x ) var_x = Dense ( lowest_dense_res * lowest_dense_res * self . ae_dims * 2 ) ( var_x ) var_x = Reshape ( ( lowest_dense_res , lowest_dense_res , self . ae_dims * 2 ) ) ( var_x ) var_x = UpscaleBlock ( self . ae_dims * 2 , activation = ""leakyrelu"" ) ( var_x ) return KModel ( input_ , var_x , name = f""intermediate_{side}"" )",DFL SAE LIAE Intermediate Network
/pandas/pandas/util/_validators.py,validate_ascending,"def validate_ascending(
    ascending: bool | int | Sequence[BoolishT],
) -> bool | int | list[BoolishT]:
    """"""Validate ``ascending`` kwargs for ``sort_index`` method.""""""
    kwargs = {""none_allowed"": False, ""int_allowed"": True}
    if not isinstance(ascending, Sequence):
        return validate_bool_kwarg(ascending, ""ascending"", **kwargs)

    return [validate_bool_kwarg(item, ""ascending"", **kwargs) for item in ascending]","def validate_ascending(
    ascending: bool | int | Sequence[BoolishT],
) -> bool | int | list[BoolishT]:
    """"""Validate ``ascending`` kwargs for ``sort_index`` method.""""""
    kwargs = {""none_allowed"": False, ""int_allowed"": True}
    if not isinstance(ascending, Sequence):
        return validate_bool_kwarg(ascending, ""ascending"", **kwargs)

    return [validate_bool_kwarg(item, ""ascending"", **kwargs) for item in ascending]",Validate ``ascending`` kwargs for ``sort_index`` method.,Validate ``ascending`` kwargs for ``sort_index`` method.,"def validate_ascending(
    ascending: bool | int | Sequence[BoolishT],
) -> bool | int | list[BoolishT]:
    
    kwargs = {""none_allowed"": False, ""int_allowed"": True}
    if not isinstance(ascending, Sequence):
        return validate_bool_kwarg(ascending, ""ascending"", **kwargs)

    return [validate_bool_kwarg(item, ""ascending"", **kwargs) for item in ascending]",Validate ``ascending`` kwargs for ``sort_index`` method.,"def validate_ascending ( ascending : bool | int | Sequence [ BoolishT ] , ) -> bool | int | list [ BoolishT ] : kwargs = { ""none_allowed"" : False , ""int_allowed"" : True } if not isinstance ( ascending , Sequence ) : return validate_bool_kwarg ( ascending , ""ascending"" , ** kwargs ) return [ validate_bool_kwarg ( item , ""ascending"" , ** kwargs ) for item in ascending ]",Validate ``ascending`` kwargs for ``sort_index`` method.
/ultralytics/ultralytics/solutions/parking_management.py,save_to_json,"def save_to_json(self):
        """"""Save the selected parking zone points to a JSON file with scaled coordinates.""""""
        scale_w, scale_h = self.imgw / self.canvas.winfo_width(), self.imgh / self.canvas.winfo_height()
        data = [{""points"": [(int(x * scale_w), int(y * scale_h)) for x, y in box]} for box in self.rg_data]

        from io import StringIO  # Function level import, as it's only required to store coordinates

        write_buffer = StringIO()
        json.dump(data, write_buffer, indent=4)
        with open(""bounding_boxes.json"", ""w"", encoding=""utf-8"") as f:
            f.write(write_buffer.getvalue())
        self.messagebox.showinfo(""Success"", ""Bounding boxes saved to bounding_boxes.json"")","def save_to_json(self):
        """"""Save the selected parking zone points to a JSON file with scaled coordinates.""""""
        scale_w, scale_h = self.imgw / self.canvas.winfo_width(), self.imgh / self.canvas.winfo_height()
        data = [{""points"": [(int(x * scale_w), int(y * scale_h)) for x, y in box]} for box in self.rg_data]

        from io import StringIO  # Function level import, as it's only required to store coordinates

        write_buffer = StringIO()
        json.dump(data, write_buffer, indent=4)
        with open(""bounding_boxes.json"", ""w"", encoding=""utf-8"") as f:
            f.write(write_buffer.getvalue())
        self.messagebox.showinfo(""Success"", ""Bounding boxes saved to bounding_boxes.json"")",Save the selected parking zone points to a JSON file with scaled coordinates.,Save the selected parking zone points to a JSON file with scaled coordinates.,"def save_to_json(self):
        
        scale_w, scale_h = self.imgw / self.canvas.winfo_width(), self.imgh / self.canvas.winfo_height()
        data = [{""points"": [(int(x * scale_w), int(y * scale_h)) for x, y in box]} for box in self.rg_data]

        from io import StringIO  # Function level import, as it's only required to store coordinates

        write_buffer = StringIO()
        json.dump(data, write_buffer, indent=4)
        with open(""bounding_boxes.json"", ""w"", encoding=""utf-8"") as f:
            f.write(write_buffer.getvalue())
        self.messagebox.showinfo(""Success"", ""Bounding boxes saved to bounding_boxes.json"")",Save the selected parking zone points to a JSON file with scaled coordinates.,"def save_to_json ( self ) : scale_w , scale_h = self . imgw / self . canvas . winfo_width ( ) , self . imgh / self . canvas . winfo_height ( ) data = [ { ""points"" : [ ( int ( x * scale_w ) , int ( y * scale_h ) ) for x , y in box ] } for box in self . rg_data ] from io import StringIO # Function level import, as it's only required to store coordinates write_buffer = StringIO ( ) json . dump ( data , write_buffer , indent = 4 ) with open ( ""bounding_boxes.json"" , ""w"" , encoding = ""utf-8"" ) as f : f . write ( write_buffer . getvalue ( ) ) self . messagebox . showinfo ( ""Success"" , ""Bounding boxes saved to bounding_boxes.json"" )",Save the selected parking zone points to a JSON file with scaled coordinates.
/black/src/black/output.py,ipynb_diff,"def ipynb_diff(a: str, b: str, a_name: str, b_name: str) -> str:
    """"""Return a unified diff string between each cell in notebooks `a` and `b`.""""""
    a_nb = json.loads(a)
    b_nb = json.loads(b)
    diff_lines = [
        diff(
            """".join(a_nb[""cells""][cell_number][""source""]) + ""\n"",
            """".join(b_nb[""cells""][cell_number][""source""]) + ""\n"",
            f""{a_name}:cell_{cell_number}"",
            f""{b_name}:cell_{cell_number}"",
        )
        for cell_number, cell in enumerate(a_nb[""cells""])
        if cell[""cell_type""] == ""code""
    ]
    return """".join(diff_lines)","def ipynb_diff(a: str, b: str, a_name: str, b_name: str) -> str:
    """"""Return a unified diff string between each cell in notebooks `a` and `b`.""""""
    a_nb = json.loads(a)
    b_nb = json.loads(b)
    diff_lines = [
        diff(
            """".join(a_nb[""cells""][cell_number][""source""]) + ""\n"",
            """".join(b_nb[""cells""][cell_number][""source""]) + ""\n"",
            f""{a_name}:cell_{cell_number}"",
            f""{b_name}:cell_{cell_number}"",
        )
        for cell_number, cell in enumerate(a_nb[""cells""])
        if cell[""cell_type""] == ""code""
    ]
    return """".join(diff_lines)",Return a unified diff string between each cell in notebooks `a` and `b`.,Return a unified diff string between each cell in notebooks `a` and `b`.,"def ipynb_diff(a: str, b: str, a_name: str, b_name: str) -> str:
    
    a_nb = json.loads(a)
    b_nb = json.loads(b)
    diff_lines = [
        diff(
            """".join(a_nb[""cells""][cell_number][""source""]) + ""\n"",
            """".join(b_nb[""cells""][cell_number][""source""]) + ""\n"",
            f""{a_name}:cell_{cell_number}"",
            f""{b_name}:cell_{cell_number}"",
        )
        for cell_number, cell in enumerate(a_nb[""cells""])
        if cell[""cell_type""] == ""code""
    ]
    return """".join(diff_lines)",Return a unified diff string between each cell in notebooks `a` and `b`.,"def ipynb_diff ( a : str , b : str , a_name : str , b_name : str ) -> str : a_nb = json . loads ( a ) b_nb = json . loads ( b ) diff_lines = [ diff ( """" . join ( a_nb [ ""cells"" ] [ cell_number ] [ ""source"" ] ) + ""\n"" , """" . join ( b_nb [ ""cells"" ] [ cell_number ] [ ""source"" ] ) + ""\n"" , f""{a_name}:cell_{cell_number}"" , f""{b_name}:cell_{cell_number}"" , ) for cell_number , cell in enumerate ( a_nb [ ""cells"" ] ) if cell [ ""cell_type"" ] == ""code"" ] return """" . join ( diff_lines )",Return a unified diff string between each cell in notebooks `a` and `b`.
/pytorch/docs/source/scripts/exportdb/generate_example_rst.py,generate_tag_rst,"def generate_tag_rst(tag_to_modules):
    """"""
    For each tag that shows up in each ExportCase.tag, generate an .rst file
    containing all the examples that have that tag.
    """"""

    for tag, modules_rst in tag_to_modules.items():
        doc_contents = f""{tag}\n{'=' * (len(tag) + 4)}\n""
        full_modules_rst = ""\n\n"".join(modules_rst)
        full_modules_rst = re.sub(
            r""={3,}"", lambda match: ""-"" * len(match.group()), full_modules_rst
        )
        doc_contents += full_modules_rst

        with open(os.path.join(EXPORTDB_SOURCE, f""{tag}.rst""), ""w"") as f:
            f.write(doc_contents)","def generate_tag_rst(tag_to_modules):
    """"""
    For each tag that shows up in each ExportCase.tag, generate an .rst file
    containing all the examples that have that tag.
    """"""

    for tag, modules_rst in tag_to_modules.items():
        doc_contents = f""{tag}\n{'=' * (len(tag) + 4)}\n""
        full_modules_rst = ""\n\n"".join(modules_rst)
        full_modules_rst = re.sub(
            r""={3,}"", lambda match: ""-"" * len(match.group()), full_modules_rst
        )
        doc_contents += full_modules_rst

        with open(os.path.join(EXPORTDB_SOURCE, f""{tag}.rst""), ""w"") as f:
            f.write(doc_contents)","For each tag that shows up in each ExportCase.tag, generate an .rst file
containing all the examples that have that tag.","For each tag that shows up in each ExportCase.tag, generate an .rst file containing all the examples that have that tag.","def generate_tag_rst(tag_to_modules):
    

    for tag, modules_rst in tag_to_modules.items():
        doc_contents = f""{tag}\n{'=' * (len(tag) + 4)}\n""
        full_modules_rst = ""\n\n"".join(modules_rst)
        full_modules_rst = re.sub(
            r""={3,}"", lambda match: ""-"" * len(match.group()), full_modules_rst
        )
        doc_contents += full_modules_rst

        with open(os.path.join(EXPORTDB_SOURCE, f""{tag}.rst""), ""w"") as f:
            f.write(doc_contents)","For each tag that shows up in each ExportCase.tag, generate an .rst file containing all the examples that have that tag.","def generate_tag_rst ( tag_to_modules ) : for tag , modules_rst in tag_to_modules . items ( ) : doc_contents = f""{tag}\n{'=' * (len(tag) + 4)}\n"" full_modules_rst = ""\n\n"" . join ( modules_rst ) full_modules_rst = re . sub ( r""={3,}"" , lambda match : ""-"" * len ( match . group ( ) ) , full_modules_rst ) doc_contents += full_modules_rst with open ( os . path . join ( EXPORTDB_SOURCE , f""{tag}.rst"" ) , ""w"" ) as f : f . write ( doc_contents )","For each tag that shows up in each ExportCase.tag, generate an .rst file containing all the examples that have that tag."
/yolov5/utils/general.py,check_img_size,"def check_img_size(imgsz, s=32, floor=0):
    """"""Adjusts image size to be divisible by stride `s`, supports int or list/tuple input, returns adjusted size.""""""
    if isinstance(imgsz, int):  # integer i.e. img_size=640
        new_size = max(make_divisible(imgsz, int(s)), floor)
    else:  # list i.e. img_size=[640, 480]
        imgsz = list(imgsz)  # convert to list if tuple
        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]
    if new_size != imgsz:
        LOGGER.warning(f""WARNING ⚠️ --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}"")
    return new_size","def check_img_size(imgsz, s=32, floor=0):
    """"""Adjusts image size to be divisible by stride `s`, supports int or list/tuple input, returns adjusted size.""""""
    if isinstance(imgsz, int):  # integer i.e. img_size=640
        new_size = max(make_divisible(imgsz, int(s)), floor)
    else:  # list i.e. img_size=[640, 480]
        imgsz = list(imgsz)  # convert to list if tuple
        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]
    if new_size != imgsz:
        LOGGER.warning(f""WARNING ⚠️ --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}"")
    return new_size","Adjusts image size to be divisible by stride `s`, supports int or list/tuple input, returns adjusted size.","Adjusts image size to be divisible by stride `s`, supports int or list/tuple input, returns adjusted size.","def check_img_size(imgsz, s=32, floor=0):
    
    if isinstance(imgsz, int):  # integer i.e. img_size=640
        new_size = max(make_divisible(imgsz, int(s)), floor)
    else:  # list i.e. img_size=[640, 480]
        imgsz = list(imgsz)  # convert to list if tuple
        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]
    if new_size != imgsz:
        LOGGER.warning(f""WARNING ⚠️ --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}"")
    return new_size","Adjusts image size to be divisible by stride `s`, supports int or list/tuple input, returns adjusted size.","def check_img_size ( imgsz , s = 32 , floor = 0 ) : if isinstance ( imgsz , int ) : # integer i.e. img_size=640 new_size = max ( make_divisible ( imgsz , int ( s ) ) , floor ) else : # list i.e. img_size=[640, 480] imgsz = list ( imgsz ) # convert to list if tuple new_size = [ max ( make_divisible ( x , int ( s ) ) , floor ) for x in imgsz ] if new_size != imgsz : LOGGER . warning ( f""WARNING ⚠️ --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}"" ) return new_size","Adjusts image size to be divisible by stride `s`, supports int or list/tuple input, returns adjusted size."
/yolov5/utils/general.py,intersect_dicts,"def intersect_dicts(da, db, exclude=()):
    """"""Returns intersection of `da` and `db` dicts with matching keys and shapes, excluding `exclude` keys; uses `da`
    values.
    """"""
    return {k: v for k, v in da.items() if k in db and all(x not in k for x in exclude) and v.shape == db[k].shape}","def intersect_dicts(da, db, exclude=()):
    """"""Returns intersection of `da` and `db` dicts with matching keys and shapes, excluding `exclude` keys; uses `da`
    values.
    """"""
    return {k: v for k, v in da.items() if k in db and all(x not in k for x in exclude) and v.shape == db[k].shape}","Returns intersection of `da` and `db` dicts with matching keys and shapes, excluding `exclude` keys; uses `da`
values.","Returns intersection of `da` and `db` dicts with matching keys and shapes, excluding `exclude` keys; uses `da` values.","def intersect_dicts(da, db, exclude=()):
    
    return {k: v for k, v in da.items() if k in db and all(x not in k for x in exclude) and v.shape == db[k].shape}","Returns intersection of `da` and `db` dicts with matching keys and shapes, excluding `exclude` keys; uses `da` values.","def intersect_dicts ( da , db , exclude = ( ) ) : return { k : v for k , v in da . items ( ) if k in db and all ( x not in k for x in exclude ) and v . shape == db [ k ] . shape }","Returns intersection of `da` and `db` dicts with matching keys and shapes, excluding `exclude` keys; uses `da` values."
/cpython/Tools/wasm/wasi/__main__.py,make_build_python,"def make_build_python(context, working_dir):
    """"""Make/build the build Python.""""""
    call([""make"", ""--jobs"", str(cpu_count()), ""all""],
            quiet=context.quiet)

    binary = build_python_path()
    cmd = [binary, ""-c"",
            ""import sys; ""
            ""print(f'{sys.version_info.major}.{sys.version_info.minor}')""]
    version = subprocess.check_output(cmd, encoding=""utf-8"").strip()

    print(f""🎉 {binary} {version}"")","def make_build_python(context, working_dir):
    """"""Make/build the build Python.""""""
    call([""make"", ""--jobs"", str(cpu_count()), ""all""],
            quiet=context.quiet)

    binary = build_python_path()
    cmd = [binary, ""-c"",
            ""import sys; ""
            ""print(f'{sys.version_info.major}.{sys.version_info.minor}')""]
    version = subprocess.check_output(cmd, encoding=""utf-8"").strip()

    print(f""🎉 {binary} {version}"")",Make/build the build Python.,Make/build the build Python.,"def make_build_python(context, working_dir):
    
    call([""make"", ""--jobs"", str(cpu_count()), ""all""],
            quiet=context.quiet)

    binary = build_python_path()
    cmd = [binary, ""-c"",
            ""import sys; ""
            ""print(f'{sys.version_info.major}.{sys.version_info.minor}')""]
    version = subprocess.check_output(cmd, encoding=""utf-8"").strip()

    print(f""🎉 {binary} {version}"")",Make/build the build Python.,"def make_build_python ( context , working_dir ) : call ( [ ""make"" , ""--jobs"" , str ( cpu_count ( ) ) , ""all"" ] , quiet = context . quiet ) binary = build_python_path ( ) cmd = [ binary , ""-c"" , ""import sys; "" ""print(f'{sys.version_info.major}.{sys.version_info.minor}')"" ] version = subprocess . check_output ( cmd , encoding = ""utf-8"" ) . strip ( ) print ( f""🎉 {binary} {version}"" )",Make/build the build Python.
/pandas/web/pandas_web.py,extend_base_template,"def extend_base_template(content: str, base_template: str) -> str:
    """"""
    Wrap document to extend the base template, before it is rendered with
    Jinja2.
    """"""
    result = '{% extends ""' + base_template + '"" %}'
    result += ""{% block body %}""
    result += content
    result += ""{% endblock %}""
    return result","def extend_base_template(content: str, base_template: str) -> str:
    """"""
    Wrap document to extend the base template, before it is rendered with
    Jinja2.
    """"""
    result = '{% extends ""' + base_template + '"" %}'
    result += ""{% block body %}""
    result += content
    result += ""{% endblock %}""
    return result","Wrap document to extend the base template, before it is rendered with
Jinja2.","Wrap document to extend the base template, before it is rendered with","def extend_base_template(content: str, base_template: str) -> str:
    
    result = '{% extends ""' + base_template + '"" %}'
    result += ""{% block body %}""
    result += content
    result += ""{% endblock %}""
    return result","Wrap document to extend the base template, before it is rendered with","def extend_base_template ( content : str , base_template : str ) -> str : result = '{% extends ""' + base_template + '"" %}' result += ""{% block body %}"" result += content result += ""{% endblock %}"" return result","Wrap document to extend the base template, before it is rendered with"
/mitmproxy/examples/contrib/webscanner_helper/mapping.py,apply_template,"def apply_template(
        self, soup: BeautifulSoup, template: dict[str, BeautifulSoup]
    ) -> None:
        """"""Applies the given mapping template to the given soup.""""""
        for css_sel, replace in template.items():
            mapped = soup.select(css_sel)
            if not mapped:
                self.logger.warning(
                    f'Could not find ""{css_sel}"", can not freeze anything.'
                )
            else:
                self.replace(
                    soup,
                    css_sel,
                    BeautifulSoup(replace, features=MappingAddonConfig.HTML_PARSER),
                )","def apply_template(
        self, soup: BeautifulSoup, template: dict[str, BeautifulSoup]
    ) -> None:
        """"""Applies the given mapping template to the given soup.""""""
        for css_sel, replace in template.items():
            mapped = soup.select(css_sel)
            if not mapped:
                self.logger.warning(
                    f'Could not find ""{css_sel}"", can not freeze anything.'
                )
            else:
                self.replace(
                    soup,
                    css_sel,
                    BeautifulSoup(replace, features=MappingAddonConfig.HTML_PARSER),
                )",Applies the given mapping template to the given soup.,Applies the given mapping template to the given soup.,"def apply_template(
        self, soup: BeautifulSoup, template: dict[str, BeautifulSoup]
    ) -> None:
        
        for css_sel, replace in template.items():
            mapped = soup.select(css_sel)
            if not mapped:
                self.logger.warning(
                    f'Could not find ""{css_sel}"", can not freeze anything.'
                )
            else:
                self.replace(
                    soup,
                    css_sel,
                    BeautifulSoup(replace, features=MappingAddonConfig.HTML_PARSER),
                )",Applies the given mapping template to the given soup.,"def apply_template ( self , soup : BeautifulSoup , template : dict [ str , BeautifulSoup ] ) -> None : for css_sel , replace in template . items ( ) : mapped = soup . select ( css_sel ) if not mapped : self . logger . warning ( f'Could not find ""{css_sel}"", can not freeze anything.' ) else : self . replace ( soup , css_sel , BeautifulSoup ( replace , features = MappingAddonConfig . HTML_PARSER ) , )",Applies the given mapping template to the given soup.
/odoo/odoo/api.py,execute_query_dict,"def execute_query_dict(self, query: SQL) -> list[dict]:
        """""" Execute the given query, fetch its results as a list of dicts.
        The method automatically flushes fields in the metadata of the query.
        """"""
        rows = self.execute_query(query)
        if not rows:
            return rows
        description = self.cr.description
        return [
            {column.name: row[index] for index, column in enumerate(description)}
            for row in rows
        ]","def execute_query_dict(self, query: SQL) -> list[dict]:
        """""" Execute the given query, fetch its results as a list of dicts.
        The method automatically flushes fields in the metadata of the query.
        """"""
        rows = self.execute_query(query)
        if not rows:
            return rows
        description = self.cr.description
        return [
            {column.name: row[index] for index, column in enumerate(description)}
            for row in rows
        ]","Execute the given query, fetch its results as a list of dicts.
The method automatically flushes fields in the metadata of the query.","Execute the given query, fetch its results as a list of dicts.","def execute_query_dict(self, query: SQL) -> list[dict]:
        
        rows = self.execute_query(query)
        if not rows:
            return rows
        description = self.cr.description
        return [
            {column.name: row[index] for index, column in enumerate(description)}
            for row in rows
        ]","Execute the given query, fetch its results as a list of dicts.","def execute_query_dict ( self , query : SQL ) -> list [ dict ] : rows = self . execute_query ( query ) if not rows : return rows description = self . cr . description return [ { column . name : row [ index ] for index , column in enumerate ( description ) } for row in rows ]","Execute the given query, fetch its results as a list of dicts."
/wtfpython/irrelevant/notebook_generator.py,generate_markdown_block,"def generate_markdown_block(lines):
    """"""
    Generates a markdown block from a list of lines.
    """"""
    global sequence_num
    result = {
        ""type"": ""markdown"",
        ""sequence_num"": sequence_num,
        ""value"": lines
    }
    sequence_num += 1
    return result","def generate_markdown_block(lines):
    """"""
    Generates a markdown block from a list of lines.
    """"""
    global sequence_num
    result = {
        ""type"": ""markdown"",
        ""sequence_num"": sequence_num,
        ""value"": lines
    }
    sequence_num += 1
    return result",Generates a markdown block from a list of lines.,Generates a markdown block from a list of lines.,"def generate_markdown_block(lines):
    
    global sequence_num
    result = {
        ""type"": ""markdown"",
        ""sequence_num"": sequence_num,
        ""value"": lines
    }
    sequence_num += 1
    return result",Generates a markdown block from a list of lines.,"def generate_markdown_block ( lines ) : global sequence_num result = { ""type"" : ""markdown"" , ""sequence_num"" : sequence_num , ""value"" : lines } sequence_num += 1 return result",Generates a markdown block from a list of lines.
/vllm/tests/compile/piecewise/test_full_cudagraph.py,temporary_environ,"def temporary_environ(env_vars):
    """"""
    Temporarily set environment variables and restore them afterward.
    We have to do this vs monkeypatch because monkeypatch doesn't work
    with ""module"" scoped fixtures.
    """"""
    original_env = {k: os.environ.get(k) for k in env_vars}
    try:
        os.environ.update(env_vars)
        yield
    finally:
        for k, v in original_env.items():
            if v is None:
                os.environ.pop(k, None)
            else:
                os.environ[k] = v","def temporary_environ(env_vars):
    """"""
    Temporarily set environment variables and restore them afterward.
    We have to do this vs monkeypatch because monkeypatch doesn't work
    with ""module"" scoped fixtures.
    """"""
    original_env = {k: os.environ.get(k) for k in env_vars}
    try:
        os.environ.update(env_vars)
        yield
    finally:
        for k, v in original_env.items():
            if v is None:
                os.environ.pop(k, None)
            else:
                os.environ[k] = v","Temporarily set environment variables and restore them afterward.
We have to do this vs monkeypatch because monkeypatch doesn't work
with ""module"" scoped fixtures.",Temporarily set environment variables and restore them afterward.,"def temporary_environ(env_vars):
    
    original_env = {k: os.environ.get(k) for k in env_vars}
    try:
        os.environ.update(env_vars)
        yield
    finally:
        for k, v in original_env.items():
            if v is None:
                os.environ.pop(k, None)
            else:
                os.environ[k] = v",Temporarily set environment variables and restore them afterward.,"def temporary_environ ( env_vars ) : original_env = { k : os . environ . get ( k ) for k in env_vars } try : os . environ . update ( env_vars ) yield finally : for k , v in original_env . items ( ) : if v is None : os . environ . pop ( k , None ) else : os . environ [ k ] = v",Temporarily set environment variables and restore them afterward.
/ultralytics/ultralytics/data/loaders.py,autocast_list,"def autocast_list(source: List[Any]) -> List[Union[Image.Image, np.ndarray]]:
    """"""Merge a list of sources into a list of numpy arrays or PIL images for Ultralytics prediction.""""""
    files = []
    for im in source:
        if isinstance(im, (str, Path)):  # filename or uri
            files.append(Image.open(urllib.request.urlopen(im) if str(im).startswith(""http"") else im))
        elif isinstance(im, (Image.Image, np.ndarray)):  # PIL or np Image
            files.append(im)
        else:
            raise TypeError(
                f""type {type(im).__name__} is not a supported Ultralytics prediction source type. \n""
                f""See https://docs.ultralytics.com/modes/predict for supported source types.""
            )

    return files","def autocast_list(source: List[Any]) -> List[Union[Image.Image, np.ndarray]]:
    """"""Merge a list of sources into a list of numpy arrays or PIL images for Ultralytics prediction.""""""
    files = []
    for im in source:
        if isinstance(im, (str, Path)):  # filename or uri
            files.append(Image.open(urllib.request.urlopen(im) if str(im).startswith(""http"") else im))
        elif isinstance(im, (Image.Image, np.ndarray)):  # PIL or np Image
            files.append(im)
        else:
            raise TypeError(
                f""type {type(im).__name__} is not a supported Ultralytics prediction source type. \n""
            )

    return files",Merge a list of sources into a list of numpy arrays or PIL images for Ultralytics prediction.,Merge a list of sources into a list of numpy arrays or PIL images for Ultralytics prediction.,"def autocast_list(source: List[Any]) -> List[Union[Image.Image, np.ndarray]]:
    
    files = []
    for im in source:
        if isinstance(im, (str, Path)):  # filename or uri
            files.append(Image.open(urllib.request.urlopen(im) if str(im).startswith(""http"") else im))
        elif isinstance(im, (Image.Image, np.ndarray)):  # PIL or np Image
            files.append(im)
        else:
            raise TypeError(
                f""type {type(im).__name__} is not a supported Ultralytics prediction source type. \n""
            )

    return files",Merge a list of sources into a list of numpy arrays or PIL images for Ultralytics prediction.,"def autocast_list ( source : List [ Any ] ) -> List [ Union [ Image . Image , np . ndarray ] ] : files = [ ] for im in source : if isinstance ( im , ( str , Path ) ) : # filename or uri files . append ( Image . open ( urllib . request . urlopen ( im ) if str ( im ) . startswith ( ""http"" ) else im ) ) elif isinstance ( im , ( Image . Image , np . ndarray ) ) : # PIL or np Image files . append ( im ) else : raise TypeError ( f""type {type(im).__name__} is not a supported Ultralytics prediction source type. \n"" ) return files",Merge a list of sources into a list of numpy arrays or PIL images for Ultralytics prediction.
/annotated_deep_learning_paper_implementations/labml_nn/sampling/nucleus.py,__init__,"def __init__(self, p: float, sampler: Sampler):
        """"""
        :param p: is the sum of probabilities of tokens to pick $p$
        :param sampler: is the sampler to use for the selected tokens
        """"""
        self.p = p
        self.sampler = sampler
        # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits
        self.softmax = nn.Softmax(dim=-1)","def __init__(self, p: float, sampler: Sampler):
        """"""
        :param p: is the sum of probabilities of tokens to pick $p$
        :param sampler: is the sampler to use for the selected tokens
        """"""
        self.p = p
        self.sampler = sampler
        # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits
        self.softmax = nn.Softmax(dim=-1)",":param p: is the sum of probabilities of tokens to pick $p$
:param sampler: is the sampler to use for the selected tokens",:param p: is the sum of probabilities of tokens to pick $p$ :param sampler: is the sampler to use for the selected tokens,"def __init__(self, p: float, sampler: Sampler):
        
        self.p = p
        self.sampler = sampler
        # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits
        self.softmax = nn.Softmax(dim=-1)",:param p: is the sum of probabilities of tokens to pick $p$ :param sampler: is the sampler to use for the selected tokens,"def __init__ ( self , p : float , sampler : Sampler ) : self . p = p self . sampler = sampler # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits self . softmax = nn . Softmax ( dim = - 1 )",:param p: is the sum of probabilities of tokens to pick $p$ :param sampler: is the sampler to use for the selected tokens
/ultralytics/ultralytics/solutions/streamlit_inference.py,web_ui,"def web_ui(self):
        """"""Set up the Streamlit web interface with custom HTML elements.""""""
        menu_style_cfg = """"""<style>MainMenu {visibility: hidden;}</style>""""""  # Hide main menu style

        # Main title of streamlit application
        main_title_cfg = """"""<div><h1 style=""color:#FF64DA; text-align:center; font-size:40px; margin-top:-50px;
        font-family: 'Archivo', sans-serif; margin-bottom:20px;"">Ultralytics YOLO Streamlit Application</h1></div>""""""

        # Subtitle of streamlit application
        sub_title_cfg = """"""<div><h4 style=""color:#042AFF; text-align:center; font-family: 'Archivo', sans-serif; 
        margin-top:-15px; margin-bottom:50px;"">Experience real-time object detection on your webcam with the power 
        of Ultralytics YOLO! 🚀</h4></div>""""""

        # Set html page configuration and append custom HTML
        self.st.set_page_config(page_title=""Ultralytics Streamlit App"", layout=""wide"")
        self.st.markdown(menu_style_cfg, unsafe_allow_html=True)
        self.st.markdown(main_title_cfg, unsafe_allow_html=True)
        self.st.markdown(sub_title_cfg, unsafe_allow_html=True)","def web_ui(self):
        """"""Set up the Streamlit web interface with custom HTML elements.""""""
        menu_style_cfg = """"""<style>MainMenu {visibility: hidden;}</style>""""""  # Hide main menu style

        # Main title of streamlit application
        main_title_cfg = """"""<div><h1 style=""color:#FF64DA; text-align:center; font-size:40px; margin-top:-50px;
        font-family: 'Archivo', sans-serif; margin-bottom:20px;"">Ultralytics YOLO Streamlit Application</h1></div>""""""

        # Subtitle of streamlit application
        sub_title_cfg = """"""<div><h4 style=""color:#042AFF; text-align:center; font-family: 'Archivo', sans-serif; 
        margin-top:-15px; margin-bottom:50px;"">Experience real-time object detection on your webcam with the power 
        of Ultralytics YOLO! 🚀</h4></div>""""""

        # Set html page configuration and append custom HTML
        self.st.set_page_config(page_title=""Ultralytics Streamlit App"", layout=""wide"")
        self.st.markdown(menu_style_cfg, unsafe_allow_html=True)
        self.st.markdown(main_title_cfg, unsafe_allow_html=True)
        self.st.markdown(sub_title_cfg, unsafe_allow_html=True)",Set up the Streamlit web interface with custom HTML elements.,Set up the Streamlit web interface with custom HTML elements.,"def web_ui(self):
        
        menu_style_cfg =   # Hide main menu style

        # Main title of streamlit application
        main_title_cfg = 

        # Subtitle of streamlit application
        sub_title_cfg = 

        # Set html page configuration and append custom HTML
        self.st.set_page_config(page_title=""Ultralytics Streamlit App"", layout=""wide"")
        self.st.markdown(menu_style_cfg, unsafe_allow_html=True)
        self.st.markdown(main_title_cfg, unsafe_allow_html=True)
        self.st.markdown(sub_title_cfg, unsafe_allow_html=True)",Set up the Streamlit web interface with custom HTML elements.,"def web_ui ( self ) : menu_style_cfg = # Hide main menu style # Main title of streamlit application main_title_cfg = # Subtitle of streamlit application sub_title_cfg = # Set html page configuration and append custom HTML self . st . set_page_config ( page_title = ""Ultralytics Streamlit App"" , layout = ""wide"" ) self . st . markdown ( menu_style_cfg , unsafe_allow_html = True ) self . st . markdown ( main_title_cfg , unsafe_allow_html = True ) self . st . markdown ( sub_title_cfg , unsafe_allow_html = True )",Set up the Streamlit web interface with custom HTML elements.
/core/script/gen_requirements_all.py,requirements_all_action_output,"def requirements_all_action_output(reqs: dict[str, list[str]], action: str) -> str:
    """"""Generate output for requirements_all_{action}.""""""
    output = [
        f""# Home Assistant Core, full dependency set for {action}\n"",
        GENERATED_MESSAGE,
        ""-r requirements.txt\n"",
    ]
    output.append(generate_action_requirements_list(reqs, action))

    return """".join(output)","def requirements_all_action_output(reqs: dict[str, list[str]], action: str) -> str:
    """"""Generate output for requirements_all_{action}.""""""
    output = [
        f""# Home Assistant Core, full dependency set for {action}\n"",
        GENERATED_MESSAGE,
        ""-r requirements.txt\n"",
    ]
    output.append(generate_action_requirements_list(reqs, action))

    return """".join(output)",Generate output for requirements_all_{action}.,Generate output for requirements_all_{action}.,"def requirements_all_action_output(reqs: dict[str, list[str]], action: str) -> str:
    
    output = [
        f""# Home Assistant Core, full dependency set for {action}\n"",
        GENERATED_MESSAGE,
        ""-r requirements.txt\n"",
    ]
    output.append(generate_action_requirements_list(reqs, action))

    return """".join(output)",Generate output for requirements_all_{action}.,"def requirements_all_action_output ( reqs : dict [ str , list [ str ] ] , action : str ) -> str : output = [ f""# Home Assistant Core, full dependency set for {action}\n"" , GENERATED_MESSAGE , ""-r requirements.txt\n"" , ] output . append ( generate_action_requirements_list ( reqs , action ) ) return """" . join ( output )",Generate output for requirements_all_{action}.
/pandas/pandas/util/_validators.py,_check_for_invalid_keys,"def _check_for_invalid_keys(fname, kwargs, compat_args) -> None:
    """"""
    Checks whether 'kwargs' contains any keys that are not
    in 'compat_args' and raises a TypeError if there is one.
    """"""
    # set(dict) --> set of the dictionary's keys
    diff = set(kwargs) - set(compat_args)

    if diff:
        bad_arg = next(iter(diff))
        raise TypeError(f""{fname}() got an unexpected keyword argument '{bad_arg}'"")","def _check_for_invalid_keys(fname, kwargs, compat_args) -> None:
    """"""
    Checks whether 'kwargs' contains any keys that are not
    in 'compat_args' and raises a TypeError if there is one.
    """"""
    # set(dict) --> set of the dictionary's keys
    diff = set(kwargs) - set(compat_args)

    if diff:
        bad_arg = next(iter(diff))
        raise TypeError(f""{fname}() got an unexpected keyword argument '{bad_arg}'"")","Checks whether 'kwargs' contains any keys that are not
in 'compat_args' and raises a TypeError if there is one.",Checks whether 'kwargs' contains any keys that are not in 'compat_args' and raises a TypeError if there is one.,"def _check_for_invalid_keys(fname, kwargs, compat_args) -> None:
    
    # set(dict) --> set of the dictionary's keys
    diff = set(kwargs) - set(compat_args)

    if diff:
        bad_arg = next(iter(diff))
        raise TypeError(f""{fname}() got an unexpected keyword argument '{bad_arg}'"")",Checks whether 'kwargs' contains any keys that are not in 'compat_args' and raises a TypeError if there is one.,"def _check_for_invalid_keys ( fname , kwargs , compat_args ) -> None : # set(dict) --> set of the dictionary's keys diff = set ( kwargs ) - set ( compat_args ) if diff : bad_arg = next ( iter ( diff ) ) raise TypeError ( f""{fname}() got an unexpected keyword argument '{bad_arg}'"" )",Checks whether 'kwargs' contains any keys that are not in 'compat_args' and raises a TypeError if there is one.
/yolov5/utils/downloads.py,curl_download,"def curl_download(url, filename, *, silent: bool = False) -> bool:
    """"""Download a file from a url to a filename using curl.""""""
    silent_option = ""sS"" if silent else """"  # silent
    proc = subprocess.run(
        [
            ""curl"",
            ""-#"",
            f""-{silent_option}L"",
            url,
            ""--output"",
            filename,
            ""--retry"",
            ""9"",
            ""-C"",
            ""-"",
        ]
    )
    return proc.returncode == 0","def curl_download(url, filename, *, silent: bool = False) -> bool:
    """"""Download a file from a url to a filename using curl.""""""
    silent_option = ""sS"" if silent else """"  # silent
    proc = subprocess.run(
        [
            ""curl"",
            ""-#"",
            f""-{silent_option}L"",
            url,
            ""--output"",
            filename,
            ""--retry"",
            ""9"",
            ""-C"",
            ""-"",
        ]
    )
    return proc.returncode == 0",Download a file from a url to a filename using curl.,Download a file from a url to a filename using curl.,"def curl_download(url, filename, *, silent: bool = False) -> bool:
    
    silent_option = ""sS"" if silent else """"  # silent
    proc = subprocess.run(
        [
            ""curl"",
            ""-#"",
            f""-{silent_option}L"",
            url,
            ""--output"",
            filename,
            ""--retry"",
            ""9"",
            ""-C"",
            ""-"",
        ]
    )
    return proc.returncode == 0",Download a file from a url to a filename using curl.,"def curl_download ( url , filename , * , silent : bool = False ) -> bool : silent_option = ""sS"" if silent else """" # silent proc = subprocess . run ( [ ""curl"" , ""-#"" , f""-{silent_option}L"" , url , ""--output"" , filename , ""--retry"" , ""9"" , ""-C"" , ""-"" , ] ) return proc . returncode == 0",Download a file from a url to a filename using curl.
/yolov5/utils/plots.py,plot_targets_txt,"def plot_targets_txt():
    """"""
    Plots histograms of object detection targets from 'targets.txt', saving the figure as 'targets.jpg'.

    Example: from utils.plots import *; plot_targets_txt()
    """"""
    x = np.loadtxt(""targets.txt"", dtype=np.float32).T
    s = [""x targets"", ""y targets"", ""width targets"", ""height targets""]
    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)
    ax = ax.ravel()
    for i in range(4):
        ax[i].hist(x[i], bins=100, label=f""{x[i].mean():.3g} +/- {x[i].std():.3g}"")
        ax[i].legend()
        ax[i].set_title(s[i])
    plt.savefig(""targets.jpg"", dpi=200)","def plot_targets_txt():
    """"""
    Plots histograms of object detection targets from 'targets.txt', saving the figure as 'targets.jpg'.

    Example: from utils.plots import *; plot_targets_txt()
    """"""
    x = np.loadtxt(""targets.txt"", dtype=np.float32).T
    s = [""x targets"", ""y targets"", ""width targets"", ""height targets""]
    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)
    ax = ax.ravel()
    for i in range(4):
        ax[i].hist(x[i], bins=100, label=f""{x[i].mean():.3g} +/- {x[i].std():.3g}"")
        ax[i].legend()
        ax[i].set_title(s[i])
    plt.savefig(""targets.jpg"", dpi=200)","Plots histograms of object detection targets from 'targets.txt', saving the figure as 'targets.jpg'.

Example: from utils.plots import *; plot_targets_txt()","Plots histograms of object detection targets from 'targets.txt', saving the figure as 'targets.jpg'.","def plot_targets_txt():
    
    x = np.loadtxt(""targets.txt"", dtype=np.float32).T
    s = [""x targets"", ""y targets"", ""width targets"", ""height targets""]
    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)
    ax = ax.ravel()
    for i in range(4):
        ax[i].hist(x[i], bins=100, label=f""{x[i].mean():.3g} +/- {x[i].std():.3g}"")
        ax[i].legend()
        ax[i].set_title(s[i])
    plt.savefig(""targets.jpg"", dpi=200)","Plots histograms of object detection targets from 'targets.txt', saving the figure as 'targets.jpg'.","def plot_targets_txt ( ) : x = np . loadtxt ( ""targets.txt"" , dtype = np . float32 ) . T s = [ ""x targets"" , ""y targets"" , ""width targets"" , ""height targets"" ] fig , ax = plt . subplots ( 2 , 2 , figsize = ( 8 , 8 ) , tight_layout = True ) ax = ax . ravel ( ) for i in range ( 4 ) : ax [ i ] . hist ( x [ i ] , bins = 100 , label = f""{x[i].mean():.3g} +/- {x[i].std():.3g}"" ) ax [ i ] . legend ( ) ax [ i ] . set_title ( s [ i ] ) plt . savefig ( ""targets.jpg"" , dpi = 200 )","Plots histograms of object detection targets from 'targets.txt', saving the figure as 'targets.jpg'."
/transformers/src/transformers/modeling_tf_utils.py,_convert_head_mask_to_5d,"def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):
        """"""-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]""""""
        if head_mask.shape.rank == 1:
            head_mask = head_mask[None, None, :, None, None]
            head_mask = tf.repeat(head_mask, repeats=num_hidden_layers, axis=0)
        elif head_mask.shape.rank == 2:
            head_mask = head_mask[:, None, :, None, None]
        assert head_mask.shape.rank == 5, f""head_mask.dim != 5, instead {head_mask.dim()}""
        head_mask = tf.cast(head_mask, tf.float32)  # switch to float if need + fp16 compatibility
        return head_mask","def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):
        """"""-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]""""""
        if head_mask.shape.rank == 1:
            head_mask = head_mask[None, None, :, None, None]
            head_mask = tf.repeat(head_mask, repeats=num_hidden_layers, axis=0)
        elif head_mask.shape.rank == 2:
            head_mask = head_mask[:, None, :, None, None]
        assert head_mask.shape.rank == 5, f""head_mask.dim != 5, instead {head_mask.dim()}""
        head_mask = tf.cast(head_mask, tf.float32)  # switch to float if need + fp16 compatibility
        return head_mask",-> [num_hidden_layers x batch x num_heads x seq_length x seq_length],> [num_hidden_layers x batch x num_heads x seq_length x seq_length],"def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):
        
        if head_mask.shape.rank == 1:
            head_mask = head_mask[None, None, :, None, None]
            head_mask = tf.repeat(head_mask, repeats=num_hidden_layers, axis=0)
        elif head_mask.shape.rank == 2:
            head_mask = head_mask[:, None, :, None, None]
        assert head_mask.shape.rank == 5, f""head_mask.dim != 5, instead {head_mask.dim()}""
        head_mask = tf.cast(head_mask, tf.float32)  # switch to float if need + fp16 compatibility
        return head_mask",> [num_hidden_layers x batch x num_heads x seq_length x seq_length],"def _convert_head_mask_to_5d ( self , head_mask , num_hidden_layers ) : if head_mask . shape . rank == 1 : head_mask = head_mask [ None , None , : , None , None ] head_mask = tf . repeat ( head_mask , repeats = num_hidden_layers , axis = 0 ) elif head_mask . shape . rank == 2 : head_mask = head_mask [ : , None , : , None , None ] assert head_mask . shape . rank == 5 , f""head_mask.dim != 5, instead {head_mask.dim()}"" head_mask = tf . cast ( head_mask , tf . float32 ) # switch to float if need + fp16 compatibility return head_mask",> [num_hidden_layers x batch x num_heads x seq_length x seq_length]
/cpython/Tools/wasm/emscripten/__main__.py,make_build_python,"def make_build_python(context, working_dir):
    """"""Make/build the build Python.""""""
    call([""make"", ""--jobs"", str(cpu_count()), ""all""], quiet=context.quiet)

    binary = build_python_path()
    cmd = [
        binary,
        ""-c"",
        ""import sys; "" ""print(f'{sys.version_info.major}.{sys.version_info.minor}')"",
    ]
    version = subprocess.check_output(cmd, encoding=""utf-8"").strip()

    print(f""🎉 {binary} {version}"")","def make_build_python(context, working_dir):
    """"""Make/build the build Python.""""""
    call([""make"", ""--jobs"", str(cpu_count()), ""all""], quiet=context.quiet)

    binary = build_python_path()
    cmd = [
        binary,
        ""-c"",
        ""import sys; "" ""print(f'{sys.version_info.major}.{sys.version_info.minor}')"",
    ]
    version = subprocess.check_output(cmd, encoding=""utf-8"").strip()

    print(f""🎉 {binary} {version}"")",Make/build the build Python.,Make/build the build Python.,"def make_build_python(context, working_dir):
    
    call([""make"", ""--jobs"", str(cpu_count()), ""all""], quiet=context.quiet)

    binary = build_python_path()
    cmd = [
        binary,
        ""-c"",
        ""import sys; "" ""print(f'{sys.version_info.major}.{sys.version_info.minor}')"",
    ]
    version = subprocess.check_output(cmd, encoding=""utf-8"").strip()

    print(f""🎉 {binary} {version}"")",Make/build the build Python.,"def make_build_python ( context , working_dir ) : call ( [ ""make"" , ""--jobs"" , str ( cpu_count ( ) ) , ""all"" ] , quiet = context . quiet ) binary = build_python_path ( ) cmd = [ binary , ""-c"" , ""import sys; "" ""print(f'{sys.version_info.major}.{sys.version_info.minor}')"" , ] version = subprocess . check_output ( cmd , encoding = ""utf-8"" ) . strip ( ) print ( f""🎉 {binary} {version}"" )",Make/build the build Python.
/cpython/Tools/ssl/make_ssl_data.py,gen_error_codes,"def gen_error_codes(args):
    """"""Generate error code table for error reasons.""""""
    yield ""/* generated from args.reasons */""
    yield ""static struct py_ssl_error_code error_codes[] = {""
    for reason, libname, errname, num in args.reasons:
        yield f""  #ifdef {reason}""
        yield f'    {{""{errname}"", ERR_LIB_{libname}, {reason}}},'
        yield ""  #else""
        yield f'    {{""{errname}"", {args.lib2errnum[libname]}, {num}}},'
        yield ""  #endif""
    yield ""    {NULL, 0, 0}  /* sentinel */""
    yield ""};""","def gen_error_codes(args):
    """"""Generate error code table for error reasons.""""""
    yield ""/* generated from args.reasons */""
    yield ""static struct py_ssl_error_code error_codes[] = {""
    for reason, libname, errname, num in args.reasons:
        yield f""  #ifdef {reason}""
        yield f'    {{""{errname}"", ERR_LIB_{libname}, {reason}}},'
        yield ""  #else""
        yield f'    {{""{errname}"", {args.lib2errnum[libname]}, {num}}},'
        yield ""  #endif""
    yield ""    {NULL, 0, 0}  /* sentinel */""
    yield ""};""",Generate error code table for error reasons.,Generate error code table for error reasons.,"def gen_error_codes(args):
    
    yield ""/* generated from args.reasons */""
    yield ""static struct py_ssl_error_code error_codes[] = {""
    for reason, libname, errname, num in args.reasons:
        yield f""  #ifdef {reason}""
        yield f'    {{""{errname}"", ERR_LIB_{libname}, {reason}}},'
        yield ""  #else""
        yield f'    {{""{errname}"", {args.lib2errnum[libname]}, {num}}},'
        yield ""  #endif""
    yield ""    {NULL, 0, 0}  /* sentinel */""
    yield ""};""",Generate error code table for error reasons.,"def gen_error_codes ( args ) : yield ""/* generated from args.reasons */"" yield ""static struct py_ssl_error_code error_codes[] = {"" for reason , libname , errname , num in args . reasons : yield f""  #ifdef {reason}"" yield f'    {{""{errname}"", ERR_LIB_{libname}, {reason}}},' yield ""  #else"" yield f'    {{""{errname}"", {args.lib2errnum[libname]}, {num}}},' yield ""  #endif"" yield ""    {NULL, 0, 0}  /* sentinel */"" yield ""};""",Generate error code table for error reasons.
/black/src/blib2to3/pgen2/conv.py,finish_off,"def finish_off(self):
        """"""Create additional useful structures.  (Internal).""""""
        self.keywords = {}  # map from keyword strings to arc labels
        self.tokens = {}  # map from numeric token values to arc labels
        for ilabel, (type, value) in enumerate(self.labels):
            if type == token.NAME and value is not None:
                self.keywords[value] = ilabel
            elif value is None:
                self.tokens[type] = ilabel","def finish_off(self):
        """"""Create additional useful structures.  (Internal).""""""
        self.keywords = {}  # map from keyword strings to arc labels
        self.tokens = {}  # map from numeric token values to arc labels
        for ilabel, (type, value) in enumerate(self.labels):
            if type == token.NAME and value is not None:
                self.keywords[value] = ilabel
            elif value is None:
                self.tokens[type] = ilabel",Create additional useful structures.  (Internal).,Create additional useful structures.,"def finish_off(self):
        
        self.keywords = {}  # map from keyword strings to arc labels
        self.tokens = {}  # map from numeric token values to arc labels
        for ilabel, (type, value) in enumerate(self.labels):
            if type == token.NAME and value is not None:
                self.keywords[value] = ilabel
            elif value is None:
                self.tokens[type] = ilabel",Create additional useful structures.,"def finish_off ( self ) : self . keywords = { } # map from keyword strings to arc labels self . tokens = { } # map from numeric token values to arc labels for ilabel , ( type , value ) in enumerate ( self . labels ) : if type == token . NAME and value is not None : self . keywords [ value ] = ilabel elif value is None : self . tokens [ type ] = ilabel",Create additional useful structures.
/ansible/test/units/ansible_test/test_diff.py,get_parsed_diff,"def get_parsed_diff(name: str) -> list[FileDiff]:
    """"""Parse and return the named git diff.""""""
    cache = pathlib.Path(__file__).parent / 'diff' / f'{name}.diff'
    content = cache.read_text()
    lines = content.splitlines()

    assert lines

    # noinspection PyProtectedMember
    from ansible_test._internal.diff import parse_diff

    diffs = parse_diff(lines)

    assert diffs

    for item in diffs:
        assert item.headers
        assert item.is_complete

        item.old.format_lines()
        item.new.format_lines()

        for line_range in item.old.ranges:
            assert line_range[1] >= line_range[0] > 0

        for line_range in item.new.ranges:
            assert line_range[1] >= line_range[0] > 0

    return diffs","def get_parsed_diff(name: str) -> list[FileDiff]:
    """"""Parse and return the named git diff.""""""
    cache = pathlib.Path(__file__).parent / 'diff' / f'{name}.diff'
    content = cache.read_text()
    lines = content.splitlines()

    assert lines

    # noinspection PyProtectedMember
    from ansible_test._internal.diff import parse_diff

    diffs = parse_diff(lines)

    assert diffs

    for item in diffs:
        assert item.headers
        assert item.is_complete

        item.old.format_lines()
        item.new.format_lines()

        for line_range in item.old.ranges:
            assert line_range[1] >= line_range[0] > 0

        for line_range in item.new.ranges:
            assert line_range[1] >= line_range[0] > 0

    return diffs",Parse and return the named git diff.,Parse and return the named git diff.,"def get_parsed_diff(name: str) -> list[FileDiff]:
    
    cache = pathlib.Path(__file__).parent / 'diff' / f'{name}.diff'
    content = cache.read_text()
    lines = content.splitlines()

    assert lines

    # noinspection PyProtectedMember
    from ansible_test._internal.diff import parse_diff

    diffs = parse_diff(lines)

    assert diffs

    for item in diffs:
        assert item.headers
        assert item.is_complete

        item.old.format_lines()
        item.new.format_lines()

        for line_range in item.old.ranges:
            assert line_range[1] >= line_range[0] > 0

        for line_range in item.new.ranges:
            assert line_range[1] >= line_range[0] > 0

    return diffs",Parse and return the named git diff.,def get_parsed_diff ( name : str ) -> list [ FileDiff ] : cache = pathlib . Path ( __file__ ) . parent / 'diff' / f'{name}.diff' content = cache . read_text ( ) lines = content . splitlines ( ) assert lines # noinspection PyProtectedMember from ansible_test . _internal . diff import parse_diff diffs = parse_diff ( lines ) assert diffs for item in diffs : assert item . headers assert item . is_complete item . old . format_lines ( ) item . new . format_lines ( ) for line_range in item . old . ranges : assert line_range [ 1 ] >= line_range [ 0 ] > 0 for line_range in item . new . ranges : assert line_range [ 1 ] >= line_range [ 0 ] > 0 return diffs,Parse and return the named git diff.
/LLaMA-Factory/src/llamafactory/data/data_utils.py,get_dataset_module,"def get_dataset_module(dataset: Union[""Dataset"", ""DatasetDict""]) -> ""DatasetModule"":
    r""""""Convert dataset or dataset dict to dataset module.""""""
    dataset_module: DatasetModule = {}
    if isinstance(dataset, DatasetDict):  # dataset dict
        if ""train"" in dataset:
            dataset_module[""train_dataset""] = dataset[""train""]

        if ""validation"" in dataset:
            dataset_module[""eval_dataset""] = dataset[""validation""]
        else:
            eval_dataset = {}
            for key in dataset.keys():
                if key.startswith(""validation_""):
                    eval_dataset[key[len(""validation_"") :]] = dataset[key]

            if len(eval_dataset):
                dataset_module[""eval_dataset""] = eval_dataset

    else:  # single dataset
        dataset_module[""train_dataset""] = dataset

    return dataset_module","def get_dataset_module(dataset: Union[""Dataset"", ""DatasetDict""]) -> ""DatasetModule"":
    r""""""Convert dataset or dataset dict to dataset module.""""""
    dataset_module: DatasetModule = {}
    if isinstance(dataset, DatasetDict):  # dataset dict
        if ""train"" in dataset:
            dataset_module[""train_dataset""] = dataset[""train""]

        if ""validation"" in dataset:
            dataset_module[""eval_dataset""] = dataset[""validation""]
        else:
            eval_dataset = {}
            for key in dataset.keys():
                if key.startswith(""validation_""):
                    eval_dataset[key[len(""validation_"") :]] = dataset[key]

            if len(eval_dataset):
                dataset_module[""eval_dataset""] = eval_dataset

    else:  # single dataset
        dataset_module[""train_dataset""] = dataset

    return dataset_module",Convert dataset or dataset dict to dataset module.,Convert dataset or dataset dict to dataset module.,"def get_dataset_module(dataset: Union[""Dataset"", ""DatasetDict""]) -> ""DatasetModule"":
    
    dataset_module: DatasetModule = {}
    if isinstance(dataset, DatasetDict):  # dataset dict
        if ""train"" in dataset:
            dataset_module[""train_dataset""] = dataset[""train""]

        if ""validation"" in dataset:
            dataset_module[""eval_dataset""] = dataset[""validation""]
        else:
            eval_dataset = {}
            for key in dataset.keys():
                if key.startswith(""validation_""):
                    eval_dataset[key[len(""validation_"") :]] = dataset[key]

            if len(eval_dataset):
                dataset_module[""eval_dataset""] = eval_dataset

    else:  # single dataset
        dataset_module[""train_dataset""] = dataset

    return dataset_module",Convert dataset or dataset dict to dataset module.,"def get_dataset_module ( dataset : Union [ ""Dataset"" , ""DatasetDict"" ] ) -> ""DatasetModule"" : dataset_module : DatasetModule = { } if isinstance ( dataset , DatasetDict ) : # dataset dict if ""train"" in dataset : dataset_module [ ""train_dataset"" ] = dataset [ ""train"" ] if ""validation"" in dataset : dataset_module [ ""eval_dataset"" ] = dataset [ ""validation"" ] else : eval_dataset = { } for key in dataset . keys ( ) : if key . startswith ( ""validation_"" ) : eval_dataset [ key [ len ( ""validation_"" ) : ] ] = dataset [ key ] if len ( eval_dataset ) : dataset_module [ ""eval_dataset"" ] = eval_dataset else : # single dataset dataset_module [ ""train_dataset"" ] = dataset return dataset_module",Convert dataset or dataset dict to dataset module.
/odoo/odoo/tests/form.py,add,"def add(self, record):
        """""" Adds ``record`` to the field, the record must already exist.

        The addition will only be finalized when the parent record is saved.
        """"""
        self._assert_editable()
        parent = self._form
        comodel_name = self._field_info['relation']
        assert isinstance(record, BaseModel) and record._name == comodel_name, \
            f""trying to assign a {record._name!r} object to a {comodel_name!r} field""

        if record.id not in self._field_value:
            self._field_value.add(record.id, {'id': record.id})
            parent._perform_onchange(self._field)","def add(self, record):
        """""" Adds ``record`` to the field, the record must already exist.

        The addition will only be finalized when the parent record is saved.
        """"""
        self._assert_editable()
        parent = self._form
        comodel_name = self._field_info['relation']
        assert isinstance(record, BaseModel) and record._name == comodel_name, \
            f""trying to assign a {record._name!r} object to a {comodel_name!r} field""

        if record.id not in self._field_value:
            self._field_value.add(record.id, {'id': record.id})
            parent._perform_onchange(self._field)","Adds ``record`` to the field, the record must already exist.

The addition will only be finalized when the parent record is saved.","Adds ``record`` to the field, the record must already exist.","def add(self, record):
        
        self._assert_editable()
        parent = self._form
        comodel_name = self._field_info['relation']
        assert isinstance(record, BaseModel) and record._name == comodel_name, \
            f""trying to assign a {record._name!r} object to a {comodel_name!r} field""

        if record.id not in self._field_value:
            self._field_value.add(record.id, {'id': record.id})
            parent._perform_onchange(self._field)","Adds ``record`` to the field, the record must already exist.","def add ( self , record ) : self . _assert_editable ( ) parent = self . _form comodel_name = self . _field_info [ 'relation' ] assert isinstance ( record , BaseModel ) and record . _name == comodel_name , f""trying to assign a {record._name!r} object to a {comodel_name!r} field"" if record . id not in self . _field_value : self . _field_value . add ( record . id , { 'id' : record . id } ) parent . _perform_onchange ( self . _field )","Adds ``record`` to the field, the record must already exist."
/OpenManus/app/sandbox/core/manager.py,start_cleanup_task,"def start_cleanup_task(self) -> None:
        """"""Starts automatic cleanup task.""""""

        async def cleanup_loop():
            while not self._is_shutting_down:
                try:
                    await self._cleanup_idle_sandboxes()
                except Exception as e:
                    logger.error(f""Error in cleanup loop: {e}"")
                await asyncio.sleep(self.cleanup_interval)

        self._cleanup_task = asyncio.create_task(cleanup_loop())","def start_cleanup_task(self) -> None:
        """"""Starts automatic cleanup task.""""""

        async def cleanup_loop():
            while not self._is_shutting_down:
                try:
                    await self._cleanup_idle_sandboxes()
                except Exception as e:
                    logger.error(f""Error in cleanup loop: {e}"")
                await asyncio.sleep(self.cleanup_interval)

        self._cleanup_task = asyncio.create_task(cleanup_loop())",Starts automatic cleanup task.,Starts automatic cleanup task.,"def start_cleanup_task(self) -> None:
        

        async def cleanup_loop():
            while not self._is_shutting_down:
                try:
                    await self._cleanup_idle_sandboxes()
                except Exception as e:
                    logger.error(f""Error in cleanup loop: {e}"")
                await asyncio.sleep(self.cleanup_interval)

        self._cleanup_task = asyncio.create_task(cleanup_loop())",Starts automatic cleanup task.,"def start_cleanup_task ( self ) -> None : async def cleanup_loop ( ) : while not self . _is_shutting_down : try : await self . _cleanup_idle_sandboxes ( ) except Exception as e : logger . error ( f""Error in cleanup loop: {e}"" ) await asyncio . sleep ( self . cleanup_interval ) self . _cleanup_task = asyncio . create_task ( cleanup_loop ( ) )",Starts automatic cleanup task.
/markitdown/packages/markitdown/src/markitdown/_uri_utils.py,file_uri_to_path,"def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    """"""Convert a file URI to a local file path""""""
    parsed = urlparse(file_uri)
    if parsed.scheme != ""file"":
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path","def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    """"""Convert a file URI to a local file path""""""
    parsed = urlparse(file_uri)
    if parsed.scheme != ""file"":
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path",Convert a file URI to a local file path,Convert a file URI to a local file path,"def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    
    parsed = urlparse(file_uri)
    if parsed.scheme != ""file"":
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path",Convert a file URI to a local file path,"def file_uri_to_path ( file_uri : str ) -> Tuple [ str | None , str ] : parsed = urlparse ( file_uri ) if parsed . scheme != ""file"" : raise ValueError ( f""Not a file URL: {file_uri}"" ) netloc = parsed . netloc if parsed . netloc else None path = os . path . abspath ( url2pathname ( parsed . path ) ) return netloc , path",Convert a file URI to a local file path
/black/src/black/nodes.py,is_type_ignore_comment,"def is_type_ignore_comment(leaf: Leaf) -> bool:
    """"""Return True if the given leaf is a type comment with ignore annotation.""""""
    t = leaf.type
    v = leaf.value
    return t in {token.COMMENT, STANDALONE_COMMENT} and is_type_ignore_comment_string(v)","def is_type_ignore_comment(leaf: Leaf) -> bool:
    """"""Return True if the given leaf is a type comment with ignore annotation.""""""
    t = leaf.type
    v = leaf.value
    return t in {token.COMMENT, STANDALONE_COMMENT} and is_type_ignore_comment_string(v)",Return True if the given leaf is a type comment with ignore annotation.,Return True if the given leaf is a type comment with ignore annotation.,"def is_type_ignore_comment(leaf: Leaf) -> bool:
    
    t = leaf.type
    v = leaf.value
    return t in {token.COMMENT, STANDALONE_COMMENT} and is_type_ignore_comment_string(v)",Return True if the given leaf is a type comment with ignore annotation.,"def is_type_ignore_comment ( leaf : Leaf ) -> bool : t = leaf . type v = leaf . value return t in { token . COMMENT , STANDALONE_COMMENT } and is_type_ignore_comment_string ( v )",Return True if the given leaf is a type comment with ignore annotation.
/sentry/src/sentry/runner/commands/backup.py,decrypt,"def decrypt(
    dest: IO[bytes], decrypt_with: IO[bytes], decrypt_with_gcp_kms: IO[bytes], src: IO[bytes]
) -> None:
    """"""
    Decrypt an encrypted tarball export into an unencrypted JSON file.
    """"""

    # Decrypt the tarball, if the user has indicated that this is one by using either of the
    # `--decrypt...` flags.
    decryptor = get_decryptor_from_flags(decrypt_with, decrypt_with_gcp_kms)
    if decryptor is None:
        raise click.UsageError(
            """"""You must specify one of `--decrypt-with` or `--decrypt-with-gcp-kms`.""""""
        )

    try:
        decrypted = decrypt_encrypted_tarball(src, decryptor)
    except DecryptionError as e:
        click.echo(f""Invalid tarball: {str(e)}"", err=True)
    else:
        with dest:
            dest.write(decrypted)","def decrypt(
    dest: IO[bytes], decrypt_with: IO[bytes], decrypt_with_gcp_kms: IO[bytes], src: IO[bytes]
) -> None:
    """"""
    Decrypt an encrypted tarball export into an unencrypted JSON file.
    """"""

    # Decrypt the tarball, if the user has indicated that this is one by using either of the
    # `--decrypt...` flags.
    decryptor = get_decryptor_from_flags(decrypt_with, decrypt_with_gcp_kms)
    if decryptor is None:
        raise click.UsageError(
            """"""You must specify one of `--decrypt-with` or `--decrypt-with-gcp-kms`.""""""
        )

    try:
        decrypted = decrypt_encrypted_tarball(src, decryptor)
    except DecryptionError as e:
        click.echo(f""Invalid tarball: {str(e)}"", err=True)
    else:
        with dest:
            dest.write(decrypted)",Decrypt an encrypted tarball export into an unencrypted JSON file.,Decrypt an encrypted tarball export into an unencrypted JSON file.,"def decrypt(
    dest: IO[bytes], decrypt_with: IO[bytes], decrypt_with_gcp_kms: IO[bytes], src: IO[bytes]
) -> None:
    

    # Decrypt the tarball, if the user has indicated that this is one by using either of the
    # `--decrypt...` flags.
    decryptor = get_decryptor_from_flags(decrypt_with, decrypt_with_gcp_kms)
    if decryptor is None:
        raise click.UsageError(
            
        )

    try:
        decrypted = decrypt_encrypted_tarball(src, decryptor)
    except DecryptionError as e:
        click.echo(f""Invalid tarball: {str(e)}"", err=True)
    else:
        with dest:
            dest.write(decrypted)",Decrypt an encrypted tarball export into an unencrypted JSON file.,"def decrypt ( dest : IO [ bytes ] , decrypt_with : IO [ bytes ] , decrypt_with_gcp_kms : IO [ bytes ] , src : IO [ bytes ] ) -> None : # Decrypt the tarball, if the user has indicated that this is one by using either of the # `--decrypt...` flags. decryptor = get_decryptor_from_flags ( decrypt_with , decrypt_with_gcp_kms ) if decryptor is None : raise click . UsageError ( ) try : decrypted = decrypt_encrypted_tarball ( src , decryptor ) except DecryptionError as e : click . echo ( f""Invalid tarball: {str(e)}"" , err = True ) else : with dest : dest . write ( decrypted )",Decrypt an encrypted tarball export into an unencrypted JSON file.
/sentry/src/sentry/runner/commands/init.py,_generate_settings,"def _generate_settings(dev: bool = False) -> tuple[str, str]:
    """"""
    This command is run when ``default_path`` doesn't exist, or ``init`` is
    run and returns a string representing the default data to put into their
    settings file.
    """"""
    context = {
        ""secret_key"": generate_secret_key(),
        ""debug_flag"": dev,
        ""mail.backend"": ""console"" if dev else ""smtp"",
    }

    py = _load_config_template(DEFAULT_SETTINGS_OVERRIDE, ""default"") % context
    yaml = _load_config_template(DEFAULT_SETTINGS_CONF, ""default"") % context
    return py, yaml","def _generate_settings(dev: bool = False) -> tuple[str, str]:
    """"""
    This command is run when ``default_path`` doesn't exist, or ``init`` is
    run and returns a string representing the default data to put into their
    settings file.
    """"""
    context = {
        ""secret_key"": generate_secret_key(),
        ""debug_flag"": dev,
        ""mail.backend"": ""console"" if dev else ""smtp"",
    }

    py = _load_config_template(DEFAULT_SETTINGS_OVERRIDE, ""default"") % context
    yaml = _load_config_template(DEFAULT_SETTINGS_CONF, ""default"") % context
    return py, yaml","This command is run when ``default_path`` doesn't exist, or ``init`` is
run and returns a string representing the default data to put into their
settings file.","This command is run when ``default_path`` doesn't exist, or ``init`` is run and returns a string representing the default data to put into their settings file.","def _generate_settings(dev: bool = False) -> tuple[str, str]:
    
    context = {
        ""secret_key"": generate_secret_key(),
        ""debug_flag"": dev,
        ""mail.backend"": ""console"" if dev else ""smtp"",
    }

    py = _load_config_template(DEFAULT_SETTINGS_OVERRIDE, ""default"") % context
    yaml = _load_config_template(DEFAULT_SETTINGS_CONF, ""default"") % context
    return py, yaml","This command is run when ``default_path`` doesn't exist, or ``init`` is run and returns a string representing the default data to put into their settings file.","def _generate_settings ( dev : bool = False ) -> tuple [ str , str ] : context = { ""secret_key"" : generate_secret_key ( ) , ""debug_flag"" : dev , ""mail.backend"" : ""console"" if dev else ""smtp"" , } py = _load_config_template ( DEFAULT_SETTINGS_OVERRIDE , ""default"" ) % context yaml = _load_config_template ( DEFAULT_SETTINGS_CONF , ""default"" ) % context return py , yaml","This command is run when ``default_path`` doesn't exist, or ``init`` is run and returns a string representing the default data to put into their settings file."
/chatglm-6b/ptuning/trainer.py,_prepare_inputs,"def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:
        """"""
        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and
        handling potential state.
        """"""
        inputs = self._prepare_input(inputs)
        if len(inputs) == 0:
            raise ValueError(
                ""The batch received was empty, your model won't be able to train on it. Double-check that your ""
                f""training dataset contains keys expected by the model: {','.join(self._signature_columns)}.""
            )
        if self.args.past_index >= 0 and self._past is not None:
            inputs[""mems""] = self._past

        return inputs","def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:
        """"""
        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and
        handling potential state.
        """"""
        inputs = self._prepare_input(inputs)
        if len(inputs) == 0:
            raise ValueError(
                ""The batch received was empty, your model won't be able to train on it. Double-check that your ""
                f""training dataset contains keys expected by the model: {','.join(self._signature_columns)}.""
            )
        if self.args.past_index >= 0 and self._past is not None:
            inputs[""mems""] = self._past

        return inputs","Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and
handling potential state.","Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and handling potential state.","def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:
        
        inputs = self._prepare_input(inputs)
        if len(inputs) == 0:
            raise ValueError(
                ""The batch received was empty, your model won't be able to train on it. Double-check that your ""
                f""training dataset contains keys expected by the model: {','.join(self._signature_columns)}.""
            )
        if self.args.past_index >= 0 and self._past is not None:
            inputs[""mems""] = self._past

        return inputs","Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and handling potential state.","def _prepare_inputs ( self , inputs : Dict [ str , Union [ torch . Tensor , Any ] ] ) -> Dict [ str , Union [ torch . Tensor , Any ] ] : inputs = self . _prepare_input ( inputs ) if len ( inputs ) == 0 : raise ValueError ( ""The batch received was empty, your model won't be able to train on it. Double-check that your "" f""training dataset contains keys expected by the model: {','.join(self._signature_columns)}."" ) if self . args . past_index >= 0 and self . _past is not None : inputs [ ""mems"" ] = self . _past return inputs","Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and handling potential state."
/ultralytics/ultralytics/data/loaders.py,__next__,"def __next__(self) -> Tuple[List[str], List[np.ndarray], List[str]]:
        """"""Capture and return the next screenshot as a numpy array using the mss library.""""""
        im0 = np.asarray(self.sct.grab(self.monitor))[:, :, :3]  # BGRA to BGR
        im0 = cv2.cvtColor(im0, cv2.COLOR_BGR2GRAY)[..., None] if self.cv2_flag == cv2.IMREAD_GRAYSCALE else im0
        s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: ""

        self.frame += 1
        return [str(self.screen)], [im0], [s]","def __next__(self) -> Tuple[List[str], List[np.ndarray], List[str]]:
        """"""Capture and return the next screenshot as a numpy array using the mss library.""""""
        im0 = np.asarray(self.sct.grab(self.monitor))[:, :, :3]  # BGRA to BGR
        im0 = cv2.cvtColor(im0, cv2.COLOR_BGR2GRAY)[..., None] if self.cv2_flag == cv2.IMREAD_GRAYSCALE else im0
        s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: ""

        self.frame += 1
        return [str(self.screen)], [im0], [s]",Capture and return the next screenshot as a numpy array using the mss library.,Capture and return the next screenshot as a numpy array using the mss library.,"def __next__(self) -> Tuple[List[str], List[np.ndarray], List[str]]:
        
        im0 = np.asarray(self.sct.grab(self.monitor))[:, :, :3]  # BGRA to BGR
        im0 = cv2.cvtColor(im0, cv2.COLOR_BGR2GRAY)[..., None] if self.cv2_flag == cv2.IMREAD_GRAYSCALE else im0
        s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: ""

        self.frame += 1
        return [str(self.screen)], [im0], [s]",Capture and return the next screenshot as a numpy array using the mss library.,"def __next__ ( self ) -> Tuple [ List [ str ] , List [ np . ndarray ] , List [ str ] ] : im0 = np . asarray ( self . sct . grab ( self . monitor ) ) [ : , : , : 3 ] # BGRA to BGR im0 = cv2 . cvtColor ( im0 , cv2 . COLOR_BGR2GRAY ) [ ... , None ] if self . cv2_flag == cv2 . IMREAD_GRAYSCALE else im0 s = f""screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: "" self . frame += 1 return [ str ( self . screen ) ] , [ im0 ] , [ s ]",Capture and return the next screenshot as a numpy array using the mss library.
/ansible/packaging/release.py,get_git_state,"def get_git_state(version: Version, allow_stale: bool) -> GitState:
    """"""Return information about the current state of the git repository.""""""
    remotes = get_remotes()
    upstream_branch = get_upstream_branch(version)

    git(""fetch"", remotes.upstream.name, upstream_branch)

    upstream_ref = f""{remotes.upstream.name}/{upstream_branch}""
    upstream_commit = get_commit(upstream_ref)

    commit = get_commit()

    if commit != upstream_commit:
        with suppress_when(allow_stale):
            raise ApplicationError(f""The current commit ({commit}) does not match {upstream_ref} ({upstream_commit})."")

    branch = git(""branch"", ""--show-current"", capture_output=True).stdout.strip() or None

    state = GitState(
        remotes=remotes,
        branch=branch,
        commit=commit,
    )

    return state","def get_git_state(version: Version, allow_stale: bool) -> GitState:
    """"""Return information about the current state of the git repository.""""""
    remotes = get_remotes()
    upstream_branch = get_upstream_branch(version)

    git(""fetch"", remotes.upstream.name, upstream_branch)

    upstream_ref = f""{remotes.upstream.name}/{upstream_branch}""
    upstream_commit = get_commit(upstream_ref)

    commit = get_commit()

    if commit != upstream_commit:
        with suppress_when(allow_stale):
            raise ApplicationError(f""The current commit ({commit}) does not match {upstream_ref} ({upstream_commit})."")

    branch = git(""branch"", ""--show-current"", capture_output=True).stdout.strip() or None

    state = GitState(
        remotes=remotes,
        branch=branch,
        commit=commit,
    )

    return state",Return information about the current state of the git repository.,Return information about the current state of the git repository.,"def get_git_state(version: Version, allow_stale: bool) -> GitState:
    
    remotes = get_remotes()
    upstream_branch = get_upstream_branch(version)

    git(""fetch"", remotes.upstream.name, upstream_branch)

    upstream_ref = f""{remotes.upstream.name}/{upstream_branch}""
    upstream_commit = get_commit(upstream_ref)

    commit = get_commit()

    if commit != upstream_commit:
        with suppress_when(allow_stale):
            raise ApplicationError(f""The current commit ({commit}) does not match {upstream_ref} ({upstream_commit})."")

    branch = git(""branch"", ""--show-current"", capture_output=True).stdout.strip() or None

    state = GitState(
        remotes=remotes,
        branch=branch,
        commit=commit,
    )

    return state",Return information about the current state of the git repository.,"def get_git_state ( version : Version , allow_stale : bool ) -> GitState : remotes = get_remotes ( ) upstream_branch = get_upstream_branch ( version ) git ( ""fetch"" , remotes . upstream . name , upstream_branch ) upstream_ref = f""{remotes.upstream.name}/{upstream_branch}"" upstream_commit = get_commit ( upstream_ref ) commit = get_commit ( ) if commit != upstream_commit : with suppress_when ( allow_stale ) : raise ApplicationError ( f""The current commit ({commit}) does not match {upstream_ref} ({upstream_commit})."" ) branch = git ( ""branch"" , ""--show-current"" , capture_output = True ) . stdout . strip ( ) or None state = GitState ( remotes = remotes , branch = branch , commit = commit , ) return state",Return information about the current state of the git repository.
/LLaMA-Factory/src/llamafactory/data/mm_plugin.py,register_mm_plugin,"def register_mm_plugin(name: str, plugin_class: type[""BasePlugin""]) -> None:
    r""""""Register a multimodal plugin.""""""
    if name in PLUGINS:
        raise ValueError(f""Multimodal plugin {name} already exists."")

    PLUGINS[name] = plugin_class","def register_mm_plugin(name: str, plugin_class: type[""BasePlugin""]) -> None:
    r""""""Register a multimodal plugin.""""""
    if name in PLUGINS:
        raise ValueError(f""Multimodal plugin {name} already exists."")

    PLUGINS[name] = plugin_class",Register a multimodal plugin.,Register a multimodal plugin.,"def register_mm_plugin(name: str, plugin_class: type[""BasePlugin""]) -> None:
    
    if name in PLUGINS:
        raise ValueError(f""Multimodal plugin {name} already exists."")

    PLUGINS[name] = plugin_class",Register a multimodal plugin.,"def register_mm_plugin ( name : str , plugin_class : type [ ""BasePlugin"" ] ) -> None : if name in PLUGINS : raise ValueError ( f""Multimodal plugin {name} already exists."" ) PLUGINS [ name ] = plugin_class",Register a multimodal plugin.
/text-generation-webui/extensions/openai/script.py,find_available_port,"def find_available_port(starting_port):
    """"""Try the starting port, then find an available one if it's taken.""""""
    try:
        # Try to create a socket with the starting port
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', starting_port))
            return starting_port
    except OSError:
        # Port is already in use, so find a new one
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', 0))  # Bind to port 0 to get an available port
            new_port = s.getsockname()[1]
            logger.warning(f""Port {starting_port} is already in use. Using port {new_port} instead."")
            return new_port","def find_available_port(starting_port):
    """"""Try the starting port, then find an available one if it's taken.""""""
    try:
        # Try to create a socket with the starting port
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', starting_port))
            return starting_port
    except OSError:
        # Port is already in use, so find a new one
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', 0))  # Bind to port 0 to get an available port
            new_port = s.getsockname()[1]
            logger.warning(f""Port {starting_port} is already in use. Using port {new_port} instead."")
            return new_port","Try the starting port, then find an available one if it's taken.","Try the starting port, then find an available one if it's taken.","def find_available_port(starting_port):
    
    try:
        # Try to create a socket with the starting port
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', starting_port))
            return starting_port
    except OSError:
        # Port is already in use, so find a new one
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', 0))  # Bind to port 0 to get an available port
            new_port = s.getsockname()[1]
            logger.warning(f""Port {starting_port} is already in use. Using port {new_port} instead."")
            return new_port","Try the starting port, then find an available one if it's taken.","def find_available_port ( starting_port ) : try : # Try to create a socket with the starting port with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind ( ( '' , starting_port ) ) return starting_port except OSError : # Port is already in use, so find a new one with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind ( ( '' , 0 ) ) # Bind to port 0 to get an available port new_port = s . getsockname ( ) [ 1 ] logger . warning ( f""Port {starting_port} is already in use. Using port {new_port} instead."" ) return new_port","Try the starting port, then find an available one if it's taken."
/markitdown/packages/markitdown/src/markitdown/converter_utils/docx/math/omml.py,do_r,"def do_r(self, elm):
        """"""
        Get text from 'r' element,And try convert them to latex symbols
        @todo text style support , (sty)
        @todo \text (latex pure text support)
        """"""
        _str = []
        for s in elm.findtext(""./{0}t"".format(OMML_NS)):
            # s = s if isinstance(s,unicode) else unicode(s,'utf-8')
            _str.append(self._t_dict.get(s, s))
        return escape_latex(BLANK.join(_str))","def do_r(self, elm):
        """"""
        Get text from 'r' element,And try convert them to latex symbols
        @todo text style support , (sty)
        @todo \text (latex pure text support)
        """"""
        _str = []
        for s in elm.findtext(""./{0}t"".format(OMML_NS)):
            # s = s if isinstance(s,unicode) else unicode(s,'utf-8')
            _str.append(self._t_dict.get(s, s))
        return escape_latex(BLANK.join(_str))","Get text from 'r' element,And try convert them to latex symbols
@todo text style support , (sty)
@todo   ext (latex pure text support)","Get text from 'r' element,And try convert them to latex symbols","def do_r(self, elm):
        
        _str = []
        for s in elm.findtext(""./{0}t"".format(OMML_NS)):
            # s = s if isinstance(s,unicode) else unicode(s,'utf-8')
            _str.append(self._t_dict.get(s, s))
        return escape_latex(BLANK.join(_str))","Get text from 'r' element,And try convert them to latex symbols","def do_r ( self , elm ) : _str = [ ] for s in elm . findtext ( ""./{0}t"" . format ( OMML_NS ) ) : # s = s if isinstance(s,unicode) else unicode(s,'utf-8') _str . append ( self . _t_dict . get ( s , s ) ) return escape_latex ( BLANK . join ( _str ) )","Get text from 'r' element,And try convert them to latex symbols"
/llama_index/docs/docs/examples/finetuning/embeddings/eval_utils.py,display_results,"def display_results(names, results_arr):
    """"""Display results from evaluate.""""""

    hit_rates = []
    mrrs = []
    for name, results in zip(names, results_arr):
        results_df = pd.DataFrame(results)
        hit_rate = results_df[""is_hit""].mean()
        mrr = results_df[""mrr""].mean()
        hit_rates.append(hit_rate)
        mrrs.append(mrr)

    final_df = pd.DataFrame(
        {""retrievers"": names, ""hit_rate"": hit_rates, ""mrr"": mrrs}
    )
    display(final_df)","def display_results(names, results_arr):
    """"""Display results from evaluate.""""""

    hit_rates = []
    mrrs = []
    for name, results in zip(names, results_arr):
        results_df = pd.DataFrame(results)
        hit_rate = results_df[""is_hit""].mean()
        mrr = results_df[""mrr""].mean()
        hit_rates.append(hit_rate)
        mrrs.append(mrr)

    final_df = pd.DataFrame(
        {""retrievers"": names, ""hit_rate"": hit_rates, ""mrr"": mrrs}
    )
    display(final_df)",Display results from evaluate.,Display results from evaluate.,"def display_results(names, results_arr):
    

    hit_rates = []
    mrrs = []
    for name, results in zip(names, results_arr):
        results_df = pd.DataFrame(results)
        hit_rate = results_df[""is_hit""].mean()
        mrr = results_df[""mrr""].mean()
        hit_rates.append(hit_rate)
        mrrs.append(mrr)

    final_df = pd.DataFrame(
        {""retrievers"": names, ""hit_rate"": hit_rates, ""mrr"": mrrs}
    )
    display(final_df)",Display results from evaluate.,"def display_results ( names , results_arr ) : hit_rates = [ ] mrrs = [ ] for name , results in zip ( names , results_arr ) : results_df = pd . DataFrame ( results ) hit_rate = results_df [ ""is_hit"" ] . mean ( ) mrr = results_df [ ""mrr"" ] . mean ( ) hit_rates . append ( hit_rate ) mrrs . append ( mrr ) final_df = pd . DataFrame ( { ""retrievers"" : names , ""hit_rate"" : hit_rates , ""mrr"" : mrrs } ) display ( final_df )",Display results from evaluate.
/odoo/odoo/tests/form.py,__setitem__,"def __setitem__(self, field_name, value):
        """""" Set the given field to the given value, and proceed with the expected onchanges. """"""
        field_info = self._view['fields'].get(field_name)
        assert field_info is not None, f""{field_name!r} was not found in the view""
        assert field_info['type'] != 'one2many', ""Can't set an one2many field directly, use its proxy instead""
        assert not self._get_modifier(field_name, 'readonly'), f""can't write on readonly field {field_name!r}""
        assert not self._get_modifier(field_name, 'invisible'), f""can't write on invisible field {field_name!r}""

        if field_info['type'] == 'many2many':
            return M2MProxy(self, field_name).set(value)

        if field_info['type'] == 'many2one':
            assert isinstance(value, BaseModel) and value._name == field_info['relation']
            value = value.id

        self._values[field_name] = value
        self._perform_onchange(field_name)","def __setitem__(self, field_name, value):
        """""" Set the given field to the given value, and proceed with the expected onchanges. """"""
        field_info = self._view['fields'].get(field_name)
        assert field_info is not None, f""{field_name!r} was not found in the view""
        assert field_info['type'] != 'one2many', ""Can't set an one2many field directly, use its proxy instead""
        assert not self._get_modifier(field_name, 'readonly'), f""can't write on readonly field {field_name!r}""
        assert not self._get_modifier(field_name, 'invisible'), f""can't write on invisible field {field_name!r}""

        if field_info['type'] == 'many2many':
            return M2MProxy(self, field_name).set(value)

        if field_info['type'] == 'many2one':
            assert isinstance(value, BaseModel) and value._name == field_info['relation']
            value = value.id

        self._values[field_name] = value
        self._perform_onchange(field_name)","Set the given field to the given value, and proceed with the expected onchanges.","Set the given field to the given value, and proceed with the expected onchanges.","def __setitem__(self, field_name, value):
        
        field_info = self._view['fields'].get(field_name)
        assert field_info is not None, f""{field_name!r} was not found in the view""
        assert field_info['type'] != 'one2many', ""Can't set an one2many field directly, use its proxy instead""
        assert not self._get_modifier(field_name, 'readonly'), f""can't write on readonly field {field_name!r}""
        assert not self._get_modifier(field_name, 'invisible'), f""can't write on invisible field {field_name!r}""

        if field_info['type'] == 'many2many':
            return M2MProxy(self, field_name).set(value)

        if field_info['type'] == 'many2one':
            assert isinstance(value, BaseModel) and value._name == field_info['relation']
            value = value.id

        self._values[field_name] = value
        self._perform_onchange(field_name)","Set the given field to the given value, and proceed with the expected onchanges.","def __setitem__ ( self , field_name , value ) : field_info = self . _view [ 'fields' ] . get ( field_name ) assert field_info is not None , f""{field_name!r} was not found in the view"" assert field_info [ 'type' ] != 'one2many' , ""Can't set an one2many field directly, use its proxy instead"" assert not self . _get_modifier ( field_name , 'readonly' ) , f""can't write on readonly field {field_name!r}"" assert not self . _get_modifier ( field_name , 'invisible' ) , f""can't write on invisible field {field_name!r}"" if field_info [ 'type' ] == 'many2many' : return M2MProxy ( self , field_name ) . set ( value ) if field_info [ 'type' ] == 'many2one' : assert isinstance ( value , BaseModel ) and value . _name == field_info [ 'relation' ] value = value . id self . _values [ field_name ] = value self . _perform_onchange ( field_name )","Set the given field to the given value, and proceed with the expected onchanges."
/text-generation-webui/extensions/perplexity_colors/script.py,probability_perplexity_color_scale,"def probability_perplexity_color_scale(prob, max_prob, ppl):
    '''
    Green-yellow-red for relative probability compared to maximum for the current token, and blue component for perplexity
    '''
    hue = prob/max_prob * 0.33
    rv, gv, _ = colorsys.hsv_to_rgb(hue, 1.0, 1.0)
    
    ppl = min(ppl, params['ppl_scale'])  # clip ppl to 0-params['ppl_scale'] for color scaling. 15 should be fine for clipping and scaling
    bv = ppl / params['ppl_scale']
    
    # to hex
    hex_col = f""{int(rv*255):02x}{int(gv*255):02x}{int(bv*255):02x}""
    
    return hex_col","def probability_perplexity_color_scale(prob, max_prob, ppl):
    '''
    Green-yellow-red for relative probability compared to maximum for the current token, and blue component for perplexity
    '''
    hue = prob/max_prob * 0.33
    rv, gv, _ = colorsys.hsv_to_rgb(hue, 1.0, 1.0)
    
    ppl = min(ppl, params['ppl_scale'])  # clip ppl to 0-params['ppl_scale'] for color scaling. 15 should be fine for clipping and scaling
    bv = ppl / params['ppl_scale']
    
    # to hex
    hex_col = f""{int(rv*255):02x}{int(gv*255):02x}{int(bv*255):02x}""
    
    return hex_col","Green-yellow-red for relative probability compared to maximum for the current token, and blue component for perplexity","Green-yellow-red for relative probability compared to maximum for the current token, and blue component for perplexity","def probability_perplexity_color_scale(prob, max_prob, ppl):
    
    hue = prob/max_prob * 0.33
    rv, gv, _ = colorsys.hsv_to_rgb(hue, 1.0, 1.0)
    
    ppl = min(ppl, params['ppl_scale'])  # clip ppl to 0-params['ppl_scale'] for color scaling. 15 should be fine for clipping and scaling
    bv = ppl / params['ppl_scale']
    
    # to hex
    hex_col = f""{int(rv*255):02x}{int(gv*255):02x}{int(bv*255):02x}""
    
    return hex_col","Green-yellow-red for relative probability compared to maximum for the current token, and blue component for perplexity","def probability_perplexity_color_scale ( prob , max_prob , ppl ) : hue = prob / max_prob * 0.33 rv , gv , _ = colorsys . hsv_to_rgb ( hue , 1.0 , 1.0 ) ppl = min ( ppl , params [ 'ppl_scale' ] ) # clip ppl to 0-params['ppl_scale'] for color scaling. 15 should be fine for clipping and scaling bv = ppl / params [ 'ppl_scale' ] # to hex hex_col = f""{int(rv*255):02x}{int(gv*255):02x}{int(bv*255):02x}"" return hex_col","Green-yellow-red for relative probability compared to maximum for the current token, and blue component for perplexity"
/yolov5/utils/loggers/__init__.py,on_val_end,"def on_val_end(self, nt, tp, fp, p, r, f1, ap, ap50, ap_class, confusion_matrix):
        """"""Logs validation results to WandB or ClearML at the end of the validation process.""""""
        if self.wandb or self.clearml:
            files = sorted(self.save_dir.glob(""val*.jpg""))
        if self.wandb:
            self.wandb.log({""Validation"": [wandb.Image(str(f), caption=f.name) for f in files]})
        if self.clearml:
            self.clearml.log_debug_samples(files, title=""Validation"")

        if self.comet_logger:
            self.comet_logger.on_val_end(nt, tp, fp, p, r, f1, ap, ap50, ap_class, confusion_matrix)","def on_val_end(self, nt, tp, fp, p, r, f1, ap, ap50, ap_class, confusion_matrix):
        """"""Logs validation results to WandB or ClearML at the end of the validation process.""""""
        if self.wandb or self.clearml:
            files = sorted(self.save_dir.glob(""val*.jpg""))
        if self.wandb:
            self.wandb.log({""Validation"": [wandb.Image(str(f), caption=f.name) for f in files]})
        if self.clearml:
            self.clearml.log_debug_samples(files, title=""Validation"")

        if self.comet_logger:
            self.comet_logger.on_val_end(nt, tp, fp, p, r, f1, ap, ap50, ap_class, confusion_matrix)",Logs validation results to WandB or ClearML at the end of the validation process.,Logs validation results to WandB or ClearML at the end of the validation process.,"def on_val_end(self, nt, tp, fp, p, r, f1, ap, ap50, ap_class, confusion_matrix):
        
        if self.wandb or self.clearml:
            files = sorted(self.save_dir.glob(""val*.jpg""))
        if self.wandb:
            self.wandb.log({""Validation"": [wandb.Image(str(f), caption=f.name) for f in files]})
        if self.clearml:
            self.clearml.log_debug_samples(files, title=""Validation"")

        if self.comet_logger:
            self.comet_logger.on_val_end(nt, tp, fp, p, r, f1, ap, ap50, ap_class, confusion_matrix)",Logs validation results to WandB or ClearML at the end of the validation process.,"def on_val_end ( self , nt , tp , fp , p , r , f1 , ap , ap50 , ap_class , confusion_matrix ) : if self . wandb or self . clearml : files = sorted ( self . save_dir . glob ( ""val*.jpg"" ) ) if self . wandb : self . wandb . log ( { ""Validation"" : [ wandb . Image ( str ( f ) , caption = f . name ) for f in files ] } ) if self . clearml : self . clearml . log_debug_samples ( files , title = ""Validation"" ) if self . comet_logger : self . comet_logger . on_val_end ( nt , tp , fp , p , r , f1 , ap , ap50 , ap_class , confusion_matrix )",Logs validation results to WandB or ClearML at the end of the validation process.
/yolov5/utils/triton.py,_create_inputs,"def _create_inputs(self, *args, **kwargs):
        """"""Creates input tensors from args or kwargs, not both; raises error if none or both are provided.""""""
        args_len, kwargs_len = len(args), len(kwargs)
        if not args_len and not kwargs_len:
            raise RuntimeError(""No inputs provided."")
        if args_len and kwargs_len:
            raise RuntimeError(""Cannot specify args and kwargs at the same time"")

        placeholders = self._create_input_placeholders_fn()
        if args_len:
            if args_len != len(placeholders):
                raise RuntimeError(f""Expected {len(placeholders)} inputs, got {args_len}."")
            for input, value in zip(placeholders, args):
                input.set_data_from_numpy(value.cpu().numpy())
        else:
            for input in placeholders:
                value = kwargs[input.name]
                input.set_data_from_numpy(value.cpu().numpy())
        return placeholders","def _create_inputs(self, *args, **kwargs):
        """"""Creates input tensors from args or kwargs, not both; raises error if none or both are provided.""""""
        args_len, kwargs_len = len(args), len(kwargs)
        if not args_len and not kwargs_len:
            raise RuntimeError(""No inputs provided."")
        if args_len and kwargs_len:
            raise RuntimeError(""Cannot specify args and kwargs at the same time"")

        placeholders = self._create_input_placeholders_fn()
        if args_len:
            if args_len != len(placeholders):
                raise RuntimeError(f""Expected {len(placeholders)} inputs, got {args_len}."")
            for input, value in zip(placeholders, args):
                input.set_data_from_numpy(value.cpu().numpy())
        else:
            for input in placeholders:
                value = kwargs[input.name]
                input.set_data_from_numpy(value.cpu().numpy())
        return placeholders","Creates input tensors from args or kwargs, not both; raises error if none or both are provided.","Creates input tensors from args or kwargs, not both; raises error if none or both are provided.","def _create_inputs(self, *args, **kwargs):
        
        args_len, kwargs_len = len(args), len(kwargs)
        if not args_len and not kwargs_len:
            raise RuntimeError(""No inputs provided."")
        if args_len and kwargs_len:
            raise RuntimeError(""Cannot specify args and kwargs at the same time"")

        placeholders = self._create_input_placeholders_fn()
        if args_len:
            if args_len != len(placeholders):
                raise RuntimeError(f""Expected {len(placeholders)} inputs, got {args_len}."")
            for input, value in zip(placeholders, args):
                input.set_data_from_numpy(value.cpu().numpy())
        else:
            for input in placeholders:
                value = kwargs[input.name]
                input.set_data_from_numpy(value.cpu().numpy())
        return placeholders","Creates input tensors from args or kwargs, not both; raises error if none or both are provided.","def _create_inputs ( self , * args , ** kwargs ) : args_len , kwargs_len = len ( args ) , len ( kwargs ) if not args_len and not kwargs_len : raise RuntimeError ( ""No inputs provided."" ) if args_len and kwargs_len : raise RuntimeError ( ""Cannot specify args and kwargs at the same time"" ) placeholders = self . _create_input_placeholders_fn ( ) if args_len : if args_len != len ( placeholders ) : raise RuntimeError ( f""Expected {len(placeholders)} inputs, got {args_len}."" ) for input , value in zip ( placeholders , args ) : input . set_data_from_numpy ( value . cpu ( ) . numpy ( ) ) else : for input in placeholders : value = kwargs [ input . name ] input . set_data_from_numpy ( value . cpu ( ) . numpy ( ) ) return placeholders","Creates input tensors from args or kwargs, not both; raises error if none or both are provided."
/unsloth/unsloth/models/_utils.py,get_model_param_count,"def get_model_param_count(model, trainable_only = False):
    """"""
    Calculate model's total param count. If trainable_only is True then count only those requiring grads
    """"""
    if is_deepspeed_zero3_enabled():
        def numel(p):
            return p.ds_numel if hasattr(p, ""ds_numel"") else p.numel()
    else:
        def numel(p):
            return p.numel()
    s = sum(numel(p) for p in model.parameters() if not trainable_only or p.requires_grad)
    if (not trainable_only) and \
        hasattr(model, ""config"") and \
        hasattr(model.config, ""quantization_config""):

        billions = re.findall(r""([0-9]{1,})(?:b|B)"", model.config.name_or_path)
        if len(billions) != 0:
            billions = int(billions[0])
            s = 1_000_000_000 * billions
    pass
    return s","def get_model_param_count(model, trainable_only = False):
    """"""
    Calculate model's total param count. If trainable_only is True then count only those requiring grads
    """"""
    if is_deepspeed_zero3_enabled():
        def numel(p):
            return p.ds_numel if hasattr(p, ""ds_numel"") else p.numel()
    else:
        def numel(p):
            return p.numel()
    s = sum(numel(p) for p in model.parameters() if not trainable_only or p.requires_grad)
    if (not trainable_only) and \
        hasattr(model, ""config"") and \
        hasattr(model.config, ""quantization_config""):

        billions = re.findall(r""([0-9]{1,})(?:b|B)"", model.config.name_or_path)
        if len(billions) != 0:
            billions = int(billions[0])
            s = 1_000_000_000 * billions
    pass
    return s",Calculate model's total param count. If trainable_only is True then count only those requiring grads,Calculate model's total param count.,"def get_model_param_count(model, trainable_only = False):
    
    if is_deepspeed_zero3_enabled():
        def numel(p):
            return p.ds_numel if hasattr(p, ""ds_numel"") else p.numel()
    else:
        def numel(p):
            return p.numel()
    s = sum(numel(p) for p in model.parameters() if not trainable_only or p.requires_grad)
    if (not trainable_only) and \
        hasattr(model, ""config"") and \
        hasattr(model.config, ""quantization_config""):

        billions = re.findall(r""([0-9]{1,})(?:b|B)"", model.config.name_or_path)
        if len(billions) != 0:
            billions = int(billions[0])
            s = 1_000_000_000 * billions
    pass
    return s",Calculate model's total param count.,"def get_model_param_count ( model , trainable_only = False ) : if is_deepspeed_zero3_enabled ( ) : def numel ( p ) : return p . ds_numel if hasattr ( p , ""ds_numel"" ) else p . numel ( ) else : def numel ( p ) : return p . numel ( ) s = sum ( numel ( p ) for p in model . parameters ( ) if not trainable_only or p . requires_grad ) if ( not trainable_only ) and hasattr ( model , ""config"" ) and hasattr ( model . config , ""quantization_config"" ) : billions = re . findall ( r""([0-9]{1,})(?:b|B)"" , model . config . name_or_path ) if len ( billions ) != 0 : billions = int ( billions [ 0 ] ) s = 1_000_000_000 * billions pass return s",Calculate model's total param count.
/OpenBB/openbb_platform/extensions/tests/utils/helpers.py,execute_docstring_examples,"def execute_docstring_examples(module_name: str, path: str) -> List[str]:
    """"""Execute the docstring examples of a module.""""""
    errors = []
    module = importlib.import_module(f""openbb.package.{module_name}"")
    doc_tests = doctest.DocTestFinder().find(module)

    for dt in doc_tests:
        code = """".join([ex.source for ex in dt.examples])
        try:
            print(f""* Executing example from {path}"")  # noqa: T201
            exec(code)  # pylint: disable=exec-used  # noqa: S102
        except Exception as e:
            error = (
                f""{'_'*136}\nPath: {path}\nCode:\n{code}\nError: {str(e)}\n{'_'*136}""
            )
            print(error)  # noqa: T201
            errors.append(error)

    return errors","def execute_docstring_examples(module_name: str, path: str) -> List[str]:
    """"""Execute the docstring examples of a module.""""""
    errors = []
    module = importlib.import_module(f""openbb.package.{module_name}"")
    doc_tests = doctest.DocTestFinder().find(module)

    for dt in doc_tests:
        code = """".join([ex.source for ex in dt.examples])
        try:
            print(f""* Executing example from {path}"")  # noqa: T201
            exec(code)  # pylint: disable=exec-used  # noqa: S102
        except Exception as e:
            error = (
                f""{'_'*136}\nPath: {path}\nCode:\n{code}\nError: {str(e)}\n{'_'*136}""
            )
            print(error)  # noqa: T201
            errors.append(error)

    return errors",Execute the docstring examples of a module.,Execute the docstring examples of a module.,"def execute_docstring_examples(module_name: str, path: str) -> List[str]:
    
    errors = []
    module = importlib.import_module(f""openbb.package.{module_name}"")
    doc_tests = doctest.DocTestFinder().find(module)

    for dt in doc_tests:
        code = """".join([ex.source for ex in dt.examples])
        try:
            print(f""* Executing example from {path}"")  # noqa: T201
            exec(code)  # pylint: disable=exec-used  # noqa: S102
        except Exception as e:
            error = (
                f""{'_'*136}\nPath: {path}\nCode:\n{code}\nError: {str(e)}\n{'_'*136}""
            )
            print(error)  # noqa: T201
            errors.append(error)

    return errors",Execute the docstring examples of a module.,"def execute_docstring_examples ( module_name : str , path : str ) -> List [ str ] : errors = [ ] module = importlib . import_module ( f""openbb.package.{module_name}"" ) doc_tests = doctest . DocTestFinder ( ) . find ( module ) for dt in doc_tests : code = """" . join ( [ ex . source for ex in dt . examples ] ) try : print ( f""* Executing example from {path}"" ) # noqa: T201 exec ( code ) # pylint: disable=exec-used  # noqa: S102 except Exception as e : error = ( f""{'_'*136}\nPath: {path}\nCode:\n{code}\nError: {str(e)}\n{'_'*136}"" ) print ( error ) # noqa: T201 errors . append ( error ) return errors",Execute the docstring examples of a module.
/Fooocus/ldm_patched/k_diffusion/utils.py,download_file,"def download_file(path, url, digest=None):
    """"""Downloads a file if it does not exist, optionally checking its SHA-256 hash.""""""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    if not path.exists():
        with urllib.request.urlopen(url) as response, open(path, 'wb') as f:
            shutil.copyfileobj(response, f)
    if digest is not None:
        file_digest = hashlib.sha256(open(path, 'rb').read()).hexdigest()
        if digest != file_digest:
            raise OSError(f'hash of {path} (url: {url}) failed to validate')
    return path","def download_file(path, url, digest=None):
    """"""Downloads a file if it does not exist, optionally checking its SHA-256 hash.""""""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    if not path.exists():
        with urllib.request.urlopen(url) as response, open(path, 'wb') as f:
            shutil.copyfileobj(response, f)
    if digest is not None:
        file_digest = hashlib.sha256(open(path, 'rb').read()).hexdigest()
        if digest != file_digest:
            raise OSError(f'hash of {path} (url: {url}) failed to validate')
    return path","Downloads a file if it does not exist, optionally checking its SHA-256 hash.","Downloads a file if it does not exist, optionally checking its SHA-256 hash.","def download_file(path, url, digest=None):
    
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    if not path.exists():
        with urllib.request.urlopen(url) as response, open(path, 'wb') as f:
            shutil.copyfileobj(response, f)
    if digest is not None:
        file_digest = hashlib.sha256(open(path, 'rb').read()).hexdigest()
        if digest != file_digest:
            raise OSError(f'hash of {path} (url: {url}) failed to validate')
    return path","Downloads a file if it does not exist, optionally checking its SHA-256 hash.","def download_file ( path , url , digest = None ) : path = Path ( path ) path . parent . mkdir ( parents = True , exist_ok = True ) if not path . exists ( ) : with urllib . request . urlopen ( url ) as response , open ( path , 'wb' ) as f : shutil . copyfileobj ( response , f ) if digest is not None : file_digest = hashlib . sha256 ( open ( path , 'rb' ) . read ( ) ) . hexdigest ( ) if digest != file_digest : raise OSError ( f'hash of {path} (url: {url}) failed to validate' ) return path","Downloads a file if it does not exist, optionally checking its SHA-256 hash."
/yt-dlp/yt_dlp/networking/common.py,_get_handlers,"def _get_handlers(self, request: Request) -> list[RequestHandler]:
        """"""Sorts handlers by preference, given a request""""""
        preferences = {
            rh: sum(pref(rh, request) for pref in self.preferences)
            for rh in self.handlers.values()
        }
        self._print_verbose('Handler preferences for this request: {}'.format(', '.join(
            f'{rh.RH_NAME}={pref}' for rh, pref in preferences.items())))
        return sorted(self.handlers.values(), key=preferences.get, reverse=True)","def _get_handlers(self, request: Request) -> list[RequestHandler]:
        """"""Sorts handlers by preference, given a request""""""
        preferences = {
            rh: sum(pref(rh, request) for pref in self.preferences)
            for rh in self.handlers.values()
        }
        self._print_verbose('Handler preferences for this request: {}'.format(', '.join(
            f'{rh.RH_NAME}={pref}' for rh, pref in preferences.items())))
        return sorted(self.handlers.values(), key=preferences.get, reverse=True)","Sorts handlers by preference, given a request","Sorts handlers by preference, given a request","def _get_handlers(self, request: Request) -> list[RequestHandler]:
        
        preferences = {
            rh: sum(pref(rh, request) for pref in self.preferences)
            for rh in self.handlers.values()
        }
        self._print_verbose('Handler preferences for this request: {}'.format(', '.join(
            f'{rh.RH_NAME}={pref}' for rh, pref in preferences.items())))
        return sorted(self.handlers.values(), key=preferences.get, reverse=True)","Sorts handlers by preference, given a request","def _get_handlers ( self , request : Request ) -> list [ RequestHandler ] : preferences = { rh : sum ( pref ( rh , request ) for pref in self . preferences ) for rh in self . handlers . values ( ) } self . _print_verbose ( 'Handler preferences for this request: {}' . format ( ', ' . join ( f'{rh.RH_NAME}={pref}' for rh , pref in preferences . items ( ) ) ) ) return sorted ( self . handlers . values ( ) , key = preferences . get , reverse = True )","Sorts handlers by preference, given a request"
/OpenBB/openbb_platform/extensions/derivatives/openbb_derivatives/derivatives_views.py,derivatives_futures_historical,"def derivatives_futures_historical(  # noqa: PLR0912
        **kwargs,
    ) -> Tuple[""OpenBBFigure"", Dict[str, Any]]:
        """"""Get Derivatives Price Historical Chart.""""""
        # pylint: disable=import-outside-toplevel
        from openbb_charting.charts.price_historical import price_historical

        kwargs.update({""candles"": False, ""same_axis"": False})

        return price_historical(**kwargs)","def derivatives_futures_historical(  # noqa: PLR0912
        **kwargs,
    ) -> Tuple[""OpenBBFigure"", Dict[str, Any]]:
        """"""Get Derivatives Price Historical Chart.""""""
        # pylint: disable=import-outside-toplevel
        from openbb_charting.charts.price_historical import price_historical

        kwargs.update({""candles"": False, ""same_axis"": False})

        return price_historical(**kwargs)",Get Derivatives Price Historical Chart.,Get Derivatives Price Historical Chart.,"def derivatives_futures_historical(  # noqa: PLR0912
        **kwargs,
    ) -> Tuple[""OpenBBFigure"", Dict[str, Any]]:
        
        # pylint: disable=import-outside-toplevel
        from openbb_charting.charts.price_historical import price_historical

        kwargs.update({""candles"": False, ""same_axis"": False})

        return price_historical(**kwargs)",Get Derivatives Price Historical Chart.,"def derivatives_futures_historical ( # noqa: PLR0912 ** kwargs , ) -> Tuple [ ""OpenBBFigure"" , Dict [ str , Any ] ] : # pylint: disable=import-outside-toplevel from openbb_charting . charts . price_historical import price_historical kwargs . update ( { ""candles"" : False , ""same_axis"" : False } ) return price_historical ( ** kwargs )",Get Derivatives Price Historical Chart.
/streamlit/scripts/update_e2e_snapshots.py,get_artifacts,"def get_artifacts(
    owner: str, repo: str, run_id: int, token: str
) -> list[dict[str, Any]]:
    """"""Get the artifacts for a given workflow run ID.""""""
    url = f""https://api.github.com/repos/{owner}/{repo}/actions/runs/{run_id}/artifacts""
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""token {token}"",
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(
            f""Error getting artifacts: {response.status_code} {response.text}""
        )
    data = response.json()
    return cast(""list[dict[str, Any]]"", data.get(""artifacts"", []))","def get_artifacts(
    owner: str, repo: str, run_id: int, token: str
) -> list[dict[str, Any]]:
    """"""Get the artifacts for a given workflow run ID.""""""
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""token {token}"",
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(
            f""Error getting artifacts: {response.status_code} {response.text}""
        )
    data = response.json()
    return cast(""list[dict[str, Any]]"", data.get(""artifacts"", []))",Get the artifacts for a given workflow run ID.,Get the artifacts for a given workflow run ID.,"def get_artifacts(
    owner: str, repo: str, run_id: int, token: str
) -> list[dict[str, Any]]:
    
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""token {token}"",
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(
            f""Error getting artifacts: {response.status_code} {response.text}""
        )
    data = response.json()
    return cast(""list[dict[str, Any]]"", data.get(""artifacts"", []))",Get the artifacts for a given workflow run ID.,"def get_artifacts ( owner : str , repo : str , run_id : int , token : str ) -> list [ dict [ str , Any ] ] : headers = { ""Accept"" : ""application/vnd.github.v3+json"" , ""Authorization"" : f""token {token}"" , } response = requests . get ( url , headers = headers ) if response . status_code != 200 : raise Exception ( f""Error getting artifacts: {response.status_code} {response.text}"" ) data = response . json ( ) return cast ( ""list[dict[str, Any]]"" , data . get ( ""artifacts"" , [ ] ) )",Get the artifacts for a given workflow run ID.
/cpython/Tools/cases_generator/opcode_metadata_generator.py,generate_is_pseudo,"def generate_is_pseudo(analysis: Analysis, out: CWriter) -> None:
    """"""Write the IS_PSEUDO_INSTR macro""""""
    out.emit(""\n\n#define IS_PSEUDO_INSTR(OP)  ( \\\n"")
    for op in analysis.pseudos:
        out.emit(f""((OP) == {op}) || \\\n"")
    out.emit(""0"")
    out.emit("")\n\n"")","def generate_is_pseudo(analysis: Analysis, out: CWriter) -> None:
    """"""Write the IS_PSEUDO_INSTR macro""""""
    out.emit(""\n\n#define IS_PSEUDO_INSTR(OP)  ( \\\n"")
    for op in analysis.pseudos:
        out.emit(f""((OP) == {op}) || \\\n"")
    out.emit(""0"")
    out.emit("")\n\n"")",Write the IS_PSEUDO_INSTR macro,Write the IS_PSEUDO_INSTR macro,"def generate_is_pseudo(analysis: Analysis, out: CWriter) -> None:
    
    out.emit(""\n\n#define IS_PSEUDO_INSTR(OP)  ( \\\n"")
    for op in analysis.pseudos:
        out.emit(f""((OP) == {op}) || \\\n"")
    out.emit(""0"")
    out.emit("")\n\n"")",Write the IS_PSEUDO_INSTR macro,"def generate_is_pseudo ( analysis : Analysis , out : CWriter ) -> None : out . emit ( ""\n\n#define IS_PSEUDO_INSTR(OP)  ( \\\n"" ) for op in analysis . pseudos : out . emit ( f""((OP) == {op}) || \\\n"" ) out . emit ( ""0"" ) out . emit ( "")\n\n"" )",Write the IS_PSEUDO_INSTR macro
/cpython/Tools/build/generate_sbom.py,download_with_retries,"def download_with_retries(download_location: str,
                          max_retries: int = 7,
                          base_delay: float = 2.25,
                          max_jitter: float = 1.0) -> typing.Any:
    """"""Download a file with exponential backoff retry.""""""
    for attempt in range(max_retries + 1):
        try:
            resp = urllib.request.urlopen(download_location)
        except urllib.error.URLError as ex:
            if attempt == max_retries:
                msg = f""Download from {download_location} failed.""
                raise OSError(msg) from ex
            time.sleep(base_delay**attempt + random.uniform(0, max_jitter))
        else:
            return resp","def download_with_retries(download_location: str,
                          max_retries: int = 7,
                          base_delay: float = 2.25,
                          max_jitter: float = 1.0) -> typing.Any:
    """"""Download a file with exponential backoff retry.""""""
    for attempt in range(max_retries + 1):
        try:
            resp = urllib.request.urlopen(download_location)
        except urllib.error.URLError as ex:
            if attempt == max_retries:
                msg = f""Download from {download_location} failed.""
                raise OSError(msg) from ex
            time.sleep(base_delay**attempt + random.uniform(0, max_jitter))
        else:
            return resp",Download a file with exponential backoff retry.,Download a file with exponential backoff retry.,"def download_with_retries(download_location: str,
                          max_retries: int = 7,
                          base_delay: float = 2.25,
                          max_jitter: float = 1.0) -> typing.Any:
    
    for attempt in range(max_retries + 1):
        try:
            resp = urllib.request.urlopen(download_location)
        except urllib.error.URLError as ex:
            if attempt == max_retries:
                msg = f""Download from {download_location} failed.""
                raise OSError(msg) from ex
            time.sleep(base_delay**attempt + random.uniform(0, max_jitter))
        else:
            return resp",Download a file with exponential backoff retry.,"def download_with_retries ( download_location : str , max_retries : int = 7 , base_delay : float = 2.25 , max_jitter : float = 1.0 ) -> typing . Any : for attempt in range ( max_retries + 1 ) : try : resp = urllib . request . urlopen ( download_location ) except urllib . error . URLError as ex : if attempt == max_retries : msg = f""Download from {download_location} failed."" raise OSError ( msg ) from ex time . sleep ( base_delay ** attempt + random . uniform ( 0 , max_jitter ) ) else : return resp",Download a file with exponential backoff retry.
/black/src/black/report.py,done,"def done(self, src: Path, changed: Changed) -> None:
        """"""Increment the counter for successful reformatting. Write out a message.""""""
        if changed is Changed.YES:
            reformatted = ""would reformat"" if self.check or self.diff else ""reformatted""
            if self.verbose or not self.quiet:
                out(f""{reformatted} {src}"")
            self.change_count += 1
        else:
            if self.verbose:
                if changed is Changed.NO:
                    msg = f""{src} already well formatted, good job.""
                else:
                    msg = f""{src} wasn't modified on disk since last run.""
                out(msg, bold=False)
            self.same_count += 1","def done(self, src: Path, changed: Changed) -> None:
        """"""Increment the counter for successful reformatting. Write out a message.""""""
        if changed is Changed.YES:
            reformatted = ""would reformat"" if self.check or self.diff else ""reformatted""
            if self.verbose or not self.quiet:
                out(f""{reformatted} {src}"")
            self.change_count += 1
        else:
            if self.verbose:
                if changed is Changed.NO:
                    msg = f""{src} already well formatted, good job.""
                else:
                    msg = f""{src} wasn't modified on disk since last run.""
                out(msg, bold=False)
            self.same_count += 1",Increment the counter for successful reformatting. Write out a message.,Increment the counter for successful reformatting.,"def done(self, src: Path, changed: Changed) -> None:
        
        if changed is Changed.YES:
            reformatted = ""would reformat"" if self.check or self.diff else ""reformatted""
            if self.verbose or not self.quiet:
                out(f""{reformatted} {src}"")
            self.change_count += 1
        else:
            if self.verbose:
                if changed is Changed.NO:
                    msg = f""{src} already well formatted, good job.""
                else:
                    msg = f""{src} wasn't modified on disk since last run.""
                out(msg, bold=False)
            self.same_count += 1",Increment the counter for successful reformatting.,"def done ( self , src : Path , changed : Changed ) -> None : if changed is Changed . YES : reformatted = ""would reformat"" if self . check or self . diff else ""reformatted"" if self . verbose or not self . quiet : out ( f""{reformatted} {src}"" ) self . change_count += 1 else : if self . verbose : if changed is Changed . NO : msg = f""{src} already well formatted, good job."" else : msg = f""{src} wasn't modified on disk since last run."" out ( msg , bold = False ) self . same_count += 1",Increment the counter for successful reformatting.
/ultralytics/ultralytics/data/loaders.py,_new_video,"def _new_video(self, path: str):
        """"""Create a new video capture object for the given path and initialize video-related attributes.""""""
        self.frame = 0
        self.cap = cv2.VideoCapture(path)
        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))
        if not self.cap.isOpened():
            raise FileNotFoundError(f""Failed to open video {path}"")
        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)","def _new_video(self, path: str):
        """"""Create a new video capture object for the given path and initialize video-related attributes.""""""
        self.frame = 0
        self.cap = cv2.VideoCapture(path)
        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))
        if not self.cap.isOpened():
            raise FileNotFoundError(f""Failed to open video {path}"")
        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)",Create a new video capture object for the given path and initialize video-related attributes.,Create a new video capture object for the given path and initialize video-related attributes.,"def _new_video(self, path: str):
        
        self.frame = 0
        self.cap = cv2.VideoCapture(path)
        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))
        if not self.cap.isOpened():
            raise FileNotFoundError(f""Failed to open video {path}"")
        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)",Create a new video capture object for the given path and initialize video-related attributes.,"def _new_video ( self , path : str ) : self . frame = 0 self . cap = cv2 . VideoCapture ( path ) self . fps = int ( self . cap . get ( cv2 . CAP_PROP_FPS ) ) if not self . cap . isOpened ( ) : raise FileNotFoundError ( f""Failed to open video {path}"" ) self . frames = int ( self . cap . get ( cv2 . CAP_PROP_FRAME_COUNT ) / self . vid_stride )",Create a new video capture object for the given path and initialize video-related attributes.
/mitmproxy/examples/contrib/webscanner_helper/mapping.py,replace,"def replace(
        self, soup: BeautifulSoup, css_sel: str, replace: BeautifulSoup
    ) -> None:
        """"""Replaces the content of soup that matches the css selector with the given replace content.""""""
        for content in soup.select(css_sel):
            self.logger.debug(f'replace ""{content}"" with ""{replace}""')
            content.replace_with(copy.copy(replace))","def replace(
        self, soup: BeautifulSoup, css_sel: str, replace: BeautifulSoup
    ) -> None:
        """"""Replaces the content of soup that matches the css selector with the given replace content.""""""
        for content in soup.select(css_sel):
            self.logger.debug(f'replace ""{content}"" with ""{replace}""')
            content.replace_with(copy.copy(replace))",Replaces the content of soup that matches the css selector with the given replace content.,Replaces the content of soup that matches the css selector with the given replace content.,"def replace(
        self, soup: BeautifulSoup, css_sel: str, replace: BeautifulSoup
    ) -> None:
        
        for content in soup.select(css_sel):
            self.logger.debug(f'replace ""{content}"" with ""{replace}""')
            content.replace_with(copy.copy(replace))",Replaces the content of soup that matches the css selector with the given replace content.,"def replace ( self , soup : BeautifulSoup , css_sel : str , replace : BeautifulSoup ) -> None : for content in soup . select ( css_sel ) : self . logger . debug ( f'replace ""{content}"" with ""{replace}""' ) content . replace_with ( copy . copy ( replace ) )",Replaces the content of soup that matches the css selector with the given replace content.
/MetaGPT/metagpt/roles/role.py,_get_prefix,"def _get_prefix(self):
        """"""Get the role prefix""""""
        if self.desc:
            return self.desc

        prefix = PREFIX_TEMPLATE.format(**{""profile"": self.profile, ""name"": self.name, ""goal"": self.goal})

        if self.constraints:
            prefix += CONSTRAINT_TEMPLATE.format(**{""constraints"": self.constraints})

        if self.rc.env and self.rc.env.desc:
            all_roles = self.rc.env.role_names()
            other_role_names = "", "".join([r for r in all_roles if r != self.name])
            env_desc = f""You are in {self.rc.env.desc} with roles({other_role_names}).""
            prefix += env_desc
        return prefix","def _get_prefix(self):
        """"""Get the role prefix""""""
        if self.desc:
            return self.desc

        prefix = PREFIX_TEMPLATE.format(**{""profile"": self.profile, ""name"": self.name, ""goal"": self.goal})

        if self.constraints:
            prefix += CONSTRAINT_TEMPLATE.format(**{""constraints"": self.constraints})

        if self.rc.env and self.rc.env.desc:
            all_roles = self.rc.env.role_names()
            other_role_names = "", "".join([r for r in all_roles if r != self.name])
            env_desc = f""You are in {self.rc.env.desc} with roles({other_role_names}).""
            prefix += env_desc
        return prefix",Get the role prefix,Get the role prefix,"def _get_prefix(self):
        
        if self.desc:
            return self.desc

        prefix = PREFIX_TEMPLATE.format(**{""profile"": self.profile, ""name"": self.name, ""goal"": self.goal})

        if self.constraints:
            prefix += CONSTRAINT_TEMPLATE.format(**{""constraints"": self.constraints})

        if self.rc.env and self.rc.env.desc:
            all_roles = self.rc.env.role_names()
            other_role_names = "", "".join([r for r in all_roles if r != self.name])
            env_desc = f""You are in {self.rc.env.desc} with roles({other_role_names}).""
            prefix += env_desc
        return prefix",Get the role prefix,"def _get_prefix ( self ) : if self . desc : return self . desc prefix = PREFIX_TEMPLATE . format ( ** { ""profile"" : self . profile , ""name"" : self . name , ""goal"" : self . goal } ) if self . constraints : prefix += CONSTRAINT_TEMPLATE . format ( ** { ""constraints"" : self . constraints } ) if self . rc . env and self . rc . env . desc : all_roles = self . rc . env . role_names ( ) other_role_names = "", "" . join ( [ r for r in all_roles if r != self . name ] ) env_desc = f""You are in {self.rc.env.desc} with roles({other_role_names})."" prefix += env_desc return prefix",Get the role prefix
/yt-dlp/yt_dlp/networking/impersonate.py,_resolve_target,"def _resolve_target(self, target: ImpersonateTarget | None):
        """"""Resolve a target to a supported target.""""""
        if target is None:
            return
        for supported_target in self.supported_targets:
            if target in supported_target:
                if self.verbose:
                    self._logger.stdout(
                        f'{self.RH_NAME}: resolved impersonate target {target} to {supported_target}')
                return supported_target","def _resolve_target(self, target: ImpersonateTarget | None):
        """"""Resolve a target to a supported target.""""""
        if target is None:
            return
        for supported_target in self.supported_targets:
            if target in supported_target:
                if self.verbose:
                    self._logger.stdout(
                        f'{self.RH_NAME}: resolved impersonate target {target} to {supported_target}')
                return supported_target",Resolve a target to a supported target.,Resolve a target to a supported target.,"def _resolve_target(self, target: ImpersonateTarget | None):
        
        if target is None:
            return
        for supported_target in self.supported_targets:
            if target in supported_target:
                if self.verbose:
                    self._logger.stdout(
                        f'{self.RH_NAME}: resolved impersonate target {target} to {supported_target}')
                return supported_target",Resolve a target to a supported target.,"def _resolve_target ( self , target : ImpersonateTarget | None ) : if target is None : return for supported_target in self . supported_targets : if target in supported_target : if self . verbose : self . _logger . stdout ( f'{self.RH_NAME}: resolved impersonate target {target} to {supported_target}' ) return supported_target",Resolve a target to a supported target.
/faceswap/plugins/train/model/dfl_sae.py,build_model,"def build_model(self, inputs):
        """""" Build the DFL-SAE Model """"""
        encoder = getattr(self, f""encoder_{self.architecture}"")()
        enc_output_shape = encoder.output_shape[1:]
        encoder_a = encoder(inputs[0])
        encoder_b = encoder(inputs[1])

        if self.architecture == ""liae"":
            inter_both = self.inter_liae(""both"", enc_output_shape)
            int_output_shape = (np.array(inter_both.output_shape[1:]) * (1, 1, 2)).tolist()

            inter_a = Concatenate()([inter_both(encoder_a), inter_both(encoder_a)])
            inter_b = Concatenate()([self.inter_liae(""b"", enc_output_shape)(encoder_b),
                                     inter_both(encoder_b)])

            decoder = self.decoder(""both"", int_output_shape)
            outputs = [decoder(inter_a), decoder(inter_b)]
        else:
            outputs = [self.decoder(""a"", enc_output_shape)(encoder_a),
                       self.decoder(""b"", enc_output_shape)(encoder_b)]
        autoencoder = KModel(inputs, outputs, name=self.model_name)
        return autoencoder","def build_model(self, inputs):
        """""" Build the DFL-SAE Model """"""
        encoder = getattr(self, f""encoder_{self.architecture}"")()
        enc_output_shape = encoder.output_shape[1:]
        encoder_a = encoder(inputs[0])
        encoder_b = encoder(inputs[1])

        if self.architecture == ""liae"":
            inter_both = self.inter_liae(""both"", enc_output_shape)
            int_output_shape = (np.array(inter_both.output_shape[1:]) * (1, 1, 2)).tolist()

            inter_a = Concatenate()([inter_both(encoder_a), inter_both(encoder_a)])
            inter_b = Concatenate()([self.inter_liae(""b"", enc_output_shape)(encoder_b),
                                     inter_both(encoder_b)])

            decoder = self.decoder(""both"", int_output_shape)
            outputs = [decoder(inter_a), decoder(inter_b)]
        else:
            outputs = [self.decoder(""a"", enc_output_shape)(encoder_a),
                       self.decoder(""b"", enc_output_shape)(encoder_b)]
        autoencoder = KModel(inputs, outputs, name=self.model_name)
        return autoencoder",Build the DFL-SAE Model,Build the DFL-SAE Model,"def build_model(self, inputs):
        
        encoder = getattr(self, f""encoder_{self.architecture}"")()
        enc_output_shape = encoder.output_shape[1:]
        encoder_a = encoder(inputs[0])
        encoder_b = encoder(inputs[1])

        if self.architecture == ""liae"":
            inter_both = self.inter_liae(""both"", enc_output_shape)
            int_output_shape = (np.array(inter_both.output_shape[1:]) * (1, 1, 2)).tolist()

            inter_a = Concatenate()([inter_both(encoder_a), inter_both(encoder_a)])
            inter_b = Concatenate()([self.inter_liae(""b"", enc_output_shape)(encoder_b),
                                     inter_both(encoder_b)])

            decoder = self.decoder(""both"", int_output_shape)
            outputs = [decoder(inter_a), decoder(inter_b)]
        else:
            outputs = [self.decoder(""a"", enc_output_shape)(encoder_a),
                       self.decoder(""b"", enc_output_shape)(encoder_b)]
        autoencoder = KModel(inputs, outputs, name=self.model_name)
        return autoencoder",Build the DFL-SAE Model,"def build_model ( self , inputs ) : encoder = getattr ( self , f""encoder_{self.architecture}"" ) ( ) enc_output_shape = encoder . output_shape [ 1 : ] encoder_a = encoder ( inputs [ 0 ] ) encoder_b = encoder ( inputs [ 1 ] ) if self . architecture == ""liae"" : inter_both = self . inter_liae ( ""both"" , enc_output_shape ) int_output_shape = ( np . array ( inter_both . output_shape [ 1 : ] ) * ( 1 , 1 , 2 ) ) . tolist ( ) inter_a = Concatenate ( ) ( [ inter_both ( encoder_a ) , inter_both ( encoder_a ) ] ) inter_b = Concatenate ( ) ( [ self . inter_liae ( ""b"" , enc_output_shape ) ( encoder_b ) , inter_both ( encoder_b ) ] ) decoder = self . decoder ( ""both"" , int_output_shape ) outputs = [ decoder ( inter_a ) , decoder ( inter_b ) ] else : outputs = [ self . decoder ( ""a"" , enc_output_shape ) ( encoder_a ) , self . decoder ( ""b"" , enc_output_shape ) ( encoder_b ) ] autoencoder = KModel ( inputs , outputs , name = self . model_name ) return autoencoder",Build the DFL-SAE Model
/odoo/odoo/tests/form.py,_get_eval_context,"def _get_eval_context(self, values=None):
        """""" Return the context dict to eval something. """"""
        context = {
            'id': self._record.id,
            'active_id': self._record.id,
            'active_ids': self._record.ids,
            'active_model': self._record._name,
            'current_date': date.today().strftime(""%Y-%m-%d""),
            **self._env.context,
        }
        if values is None:
            values = self._get_all_values()
        return {
            **context,
            'context': context,
            **values,
        }","def _get_eval_context(self, values=None):
        """""" Return the context dict to eval something. """"""
        context = {
            'id': self._record.id,
            'active_id': self._record.id,
            'active_ids': self._record.ids,
            'active_model': self._record._name,
            'current_date': date.today().strftime(""%Y-%m-%d""),
            **self._env.context,
        }
        if values is None:
            values = self._get_all_values()
        return {
            **context,
            'context': context,
            **values,
        }",Return the context dict to eval something.,Return the context dict to eval something.,"def _get_eval_context(self, values=None):
        
        context = {
            'id': self._record.id,
            'active_id': self._record.id,
            'active_ids': self._record.ids,
            'active_model': self._record._name,
            'current_date': date.today().strftime(""%Y-%m-%d""),
            **self._env.context,
        }
        if values is None:
            values = self._get_all_values()
        return {
            **context,
            'context': context,
            **values,
        }",Return the context dict to eval something.,"def _get_eval_context ( self , values = None ) : context = { 'id' : self . _record . id , 'active_id' : self . _record . id , 'active_ids' : self . _record . ids , 'active_model' : self . _record . _name , 'current_date' : date . today ( ) . strftime ( ""%Y-%m-%d"" ) , ** self . _env . context , } if values is None : values = self . _get_all_values ( ) return { ** context , 'context' : context , ** values , }",Return the context dict to eval something.
/core/script/gen_requirements_all.py,gather_requirements_from_manifests,"def gather_requirements_from_manifests(
    errors: list[str], reqs: dict[str, list[str]]
) -> None:
    """"""Gather all of the requirements from manifests.""""""
    config = _get_hassfest_config()
    integrations = Integration.load_dir(config.core_integrations_path, config)
    for domain in sorted(integrations):
        integration = integrations[domain]

        if integration.disabled:
            continue

        process_requirements(
            errors, integration.requirements, f""homeassistant.components.{domain}"", reqs
        )","def gather_requirements_from_manifests(
    errors: list[str], reqs: dict[str, list[str]]
) -> None:
    """"""Gather all of the requirements from manifests.""""""
    config = _get_hassfest_config()
    integrations = Integration.load_dir(config.core_integrations_path, config)
    for domain in sorted(integrations):
        integration = integrations[domain]

        if integration.disabled:
            continue

        process_requirements(
            errors, integration.requirements, f""homeassistant.components.{domain}"", reqs
        )",Gather all of the requirements from manifests.,Gather all of the requirements from manifests.,"def gather_requirements_from_manifests(
    errors: list[str], reqs: dict[str, list[str]]
) -> None:
    
    config = _get_hassfest_config()
    integrations = Integration.load_dir(config.core_integrations_path, config)
    for domain in sorted(integrations):
        integration = integrations[domain]

        if integration.disabled:
            continue

        process_requirements(
            errors, integration.requirements, f""homeassistant.components.{domain}"", reqs
        )",Gather all of the requirements from manifests.,"def gather_requirements_from_manifests ( errors : list [ str ] , reqs : dict [ str , list [ str ] ] ) -> None : config = _get_hassfest_config ( ) integrations = Integration . load_dir ( config . core_integrations_path , config ) for domain in sorted ( integrations ) : integration = integrations [ domain ] if integration . disabled : continue process_requirements ( errors , integration . requirements , f""homeassistant.components.{domain}"" , reqs )",Gather all of the requirements from manifests.
/crawl4ai/docs/examples/dispatcher_example.py,create_performance_table,"def create_performance_table(results):
    """"""Creates a rich table showing performance results""""""
    table = Table(title=""Crawler Strategy Performance Comparison"")
    table.add_column(""Strategy"", style=""cyan"")
    table.add_column(""URLs Crawled"", justify=""right"", style=""green"")
    table.add_column(""Time (seconds)"", justify=""right"", style=""yellow"")
    table.add_column(""URLs/second"", justify=""right"", style=""magenta"")

    sorted_results = sorted(results.items(), key=lambda x: x[1][1])

    for strategy, (urls_crawled, duration) in sorted_results:
        urls_per_second = urls_crawled / duration
        table.add_row(
            strategy, str(urls_crawled), f""{duration:.2f}"", f""{urls_per_second:.2f}""
        )

    return table","def create_performance_table(results):
    """"""Creates a rich table showing performance results""""""
    table = Table(title=""Crawler Strategy Performance Comparison"")
    table.add_column(""Strategy"", style=""cyan"")
    table.add_column(""URLs Crawled"", justify=""right"", style=""green"")
    table.add_column(""Time (seconds)"", justify=""right"", style=""yellow"")
    table.add_column(""URLs/second"", justify=""right"", style=""magenta"")

    sorted_results = sorted(results.items(), key=lambda x: x[1][1])

    for strategy, (urls_crawled, duration) in sorted_results:
        urls_per_second = urls_crawled / duration
        table.add_row(
            strategy, str(urls_crawled), f""{duration:.2f}"", f""{urls_per_second:.2f}""
        )

    return table",Creates a rich table showing performance results,Creates a rich table showing performance results,"def create_performance_table(results):
    
    table = Table(title=""Crawler Strategy Performance Comparison"")
    table.add_column(""Strategy"", style=""cyan"")
    table.add_column(""URLs Crawled"", justify=""right"", style=""green"")
    table.add_column(""Time (seconds)"", justify=""right"", style=""yellow"")
    table.add_column(""URLs/second"", justify=""right"", style=""magenta"")

    sorted_results = sorted(results.items(), key=lambda x: x[1][1])

    for strategy, (urls_crawled, duration) in sorted_results:
        urls_per_second = urls_crawled / duration
        table.add_row(
            strategy, str(urls_crawled), f""{duration:.2f}"", f""{urls_per_second:.2f}""
        )

    return table",Creates a rich table showing performance results,"def create_performance_table ( results ) : table = Table ( title = ""Crawler Strategy Performance Comparison"" ) table . add_column ( ""Strategy"" , style = ""cyan"" ) table . add_column ( ""URLs Crawled"" , justify = ""right"" , style = ""green"" ) table . add_column ( ""Time (seconds)"" , justify = ""right"" , style = ""yellow"" ) table . add_column ( ""URLs/second"" , justify = ""right"" , style = ""magenta"" ) sorted_results = sorted ( results . items ( ) , key = lambda x : x [ 1 ] [ 1 ] ) for strategy , ( urls_crawled , duration ) in sorted_results : urls_per_second = urls_crawled / duration table . add_row ( strategy , str ( urls_crawled ) , f""{duration:.2f}"" , f""{urls_per_second:.2f}"" ) return table",Creates a rich table showing performance results
/OpenBB/openbb_platform/extensions/technical/openbb_technical/relative_rotation.py,_get_type_name,"def _get_type_name(t):
    """"""Get the type name of a type hint.""""""
    if hasattr(t, ""__origin__""):
        if hasattr(t.__origin__, ""__name__""):
            return f""{t.__origin__.__name__}[{', '.join([_get_type_name(arg) for arg in t.__args__])}]""
        if hasattr(t.__origin__, ""_name""):
            return f""{t.__origin__._name}[{', '.join([_get_type_name(arg) for arg in t.__args__])}]""
    if isinstance(t, str):
        return t
    if hasattr(t, ""__name__""):
        return t.__name__
    if hasattr(t, ""_name""):
        return t._name
    return str(t)","def _get_type_name(t):
    """"""Get the type name of a type hint.""""""
    if hasattr(t, ""__origin__""):
        if hasattr(t.__origin__, ""__name__""):
            return f""{t.__origin__.__name__}[{', '.join([_get_type_name(arg) for arg in t.__args__])}]""
        if hasattr(t.__origin__, ""_name""):
            return f""{t.__origin__._name}[{', '.join([_get_type_name(arg) for arg in t.__args__])}]""
    if isinstance(t, str):
        return t
    if hasattr(t, ""__name__""):
        return t.__name__
    if hasattr(t, ""_name""):
        return t._name
    return str(t)",Get the type name of a type hint.,Get the type name of a type hint.,"def _get_type_name(t):
    
    if hasattr(t, ""__origin__""):
        if hasattr(t.__origin__, ""__name__""):
            return f""{t.__origin__.__name__}[{', '.join([_get_type_name(arg) for arg in t.__args__])}]""
        if hasattr(t.__origin__, ""_name""):
            return f""{t.__origin__._name}[{', '.join([_get_type_name(arg) for arg in t.__args__])}]""
    if isinstance(t, str):
        return t
    if hasattr(t, ""__name__""):
        return t.__name__
    if hasattr(t, ""_name""):
        return t._name
    return str(t)",Get the type name of a type hint.,"def _get_type_name ( t ) : if hasattr ( t , ""__origin__"" ) : if hasattr ( t . __origin__ , ""__name__"" ) : return f""{t.__origin__.__name__}[{', '.join([_get_type_name(arg) for arg in t.__args__])}]"" if hasattr ( t . __origin__ , ""_name"" ) : return f""{t.__origin__._name}[{', '.join([_get_type_name(arg) for arg in t.__args__])}]"" if isinstance ( t , str ) : return t if hasattr ( t , ""__name__"" ) : return t . __name__ if hasattr ( t , ""_name"" ) : return t . _name return str ( t )",Get the type name of a type hint.
/faceswap/plugins/train/model/dlight.py,_legacy_mapping,"def _legacy_mapping(self):
        """""" The mapping of legacy separate model names to single model names """"""
        decoder_b = ""decoder_b"" if self.details > 0 else ""decoder_b_fast""
        return {f""{self.name}_encoder.h5"": ""encoder"",
                f""{self.name}_decoder_A.h5"": ""decoder_a"",
                f""{self.name}_decoder_B.h5"": decoder_b}","def _legacy_mapping(self):
        """""" The mapping of legacy separate model names to single model names """"""
        decoder_b = ""decoder_b"" if self.details > 0 else ""decoder_b_fast""
        return {f""{self.name}_encoder.h5"": ""encoder"",
                f""{self.name}_decoder_A.h5"": ""decoder_a"",
                f""{self.name}_decoder_B.h5"": decoder_b}",The mapping of legacy separate model names to single model names,The mapping of legacy separate model names to single model names,"def _legacy_mapping(self):
        
        decoder_b = ""decoder_b"" if self.details > 0 else ""decoder_b_fast""
        return {f""{self.name}_encoder.h5"": ""encoder"",
                f""{self.name}_decoder_A.h5"": ""decoder_a"",
                f""{self.name}_decoder_B.h5"": decoder_b}",The mapping of legacy separate model names to single model names,"def _legacy_mapping ( self ) : decoder_b = ""decoder_b"" if self . details > 0 else ""decoder_b_fast"" return { f""{self.name}_encoder.h5"" : ""encoder"" , f""{self.name}_decoder_A.h5"" : ""decoder_a"" , f""{self.name}_decoder_B.h5"" : decoder_b }",The mapping of legacy separate model names to single model names
/LLaMA-Factory/src/llamafactory/train/trainer_utils.py,nested_detach,"def nested_detach(
    tensors: Union[""torch.Tensor"", list[""torch.Tensor""], tuple[""torch.Tensor""], dict[str, ""torch.Tensor""]],
    clone: bool = False,
):
    r""""""Detach `tensors` (even if it's a nested list/tuple/dict of tensors).""""""
    if isinstance(tensors, (list, tuple)):
        return type(tensors)(nested_detach(t, clone=clone) for t in tensors)
    elif isinstance(tensors, Mapping):
        return type(tensors)({k: nested_detach(t, clone=clone) for k, t in tensors.items()})

    if isinstance(tensors, torch.Tensor):
        if clone:
            return tensors.detach().clone()
        else:
            return tensors.detach()
    else:
        return tensors","def nested_detach(
    tensors: Union[""torch.Tensor"", list[""torch.Tensor""], tuple[""torch.Tensor""], dict[str, ""torch.Tensor""]],
    clone: bool = False,
):
    r""""""Detach `tensors` (even if it's a nested list/tuple/dict of tensors).""""""
    if isinstance(tensors, (list, tuple)):
        return type(tensors)(nested_detach(t, clone=clone) for t in tensors)
    elif isinstance(tensors, Mapping):
        return type(tensors)({k: nested_detach(t, clone=clone) for k, t in tensors.items()})

    if isinstance(tensors, torch.Tensor):
        if clone:
            return tensors.detach().clone()
        else:
            return tensors.detach()
    else:
        return tensors",Detach `tensors` (even if it's a nested list/tuple/dict of tensors).,Detach `tensors` (even if it's a nested list/tuple/dict of tensors).,"def nested_detach(
    tensors: Union[""torch.Tensor"", list[""torch.Tensor""], tuple[""torch.Tensor""], dict[str, ""torch.Tensor""]],
    clone: bool = False,
):
    
    if isinstance(tensors, (list, tuple)):
        return type(tensors)(nested_detach(t, clone=clone) for t in tensors)
    elif isinstance(tensors, Mapping):
        return type(tensors)({k: nested_detach(t, clone=clone) for k, t in tensors.items()})

    if isinstance(tensors, torch.Tensor):
        if clone:
            return tensors.detach().clone()
        else:
            return tensors.detach()
    else:
        return tensors",Detach `tensors` (even if it's a nested list/tuple/dict of tensors).,"def nested_detach ( tensors : Union [ ""torch.Tensor"" , list [ ""torch.Tensor"" ] , tuple [ ""torch.Tensor"" ] , dict [ str , ""torch.Tensor"" ] ] , clone : bool = False , ) : if isinstance ( tensors , ( list , tuple ) ) : return type ( tensors ) ( nested_detach ( t , clone = clone ) for t in tensors ) elif isinstance ( tensors , Mapping ) : return type ( tensors ) ( { k : nested_detach ( t , clone = clone ) for k , t in tensors . items ( ) } ) if isinstance ( tensors , torch . Tensor ) : if clone : return tensors . detach ( ) . clone ( ) else : return tensors . detach ( ) else : return tensors",Detach `tensors` (even if it's a nested list/tuple/dict of tensors).
/black/src/black/numerics.py,format_scientific_notation,"def format_scientific_notation(text: str) -> str:
    """"""Formats a numeric string utilizing scientific notation""""""
    before, after = text.split(""e"")
    sign = """"
    if after.startswith(""-""):
        after = after[1:]
        sign = ""-""
    elif after.startswith(""+""):
        after = after[1:]
    before = format_float_or_int_string(before)
    return f""{before}e{sign}{after}""","def format_scientific_notation(text: str) -> str:
    """"""Formats a numeric string utilizing scientific notation""""""
    before, after = text.split(""e"")
    sign = """"
    if after.startswith(""-""):
        after = after[1:]
        sign = ""-""
    elif after.startswith(""+""):
        after = after[1:]
    before = format_float_or_int_string(before)
    return f""{before}e{sign}{after}""",Formats a numeric string utilizing scientific notation,Formats a numeric string utilizing scientific notation,"def format_scientific_notation(text: str) -> str:
    
    before, after = text.split(""e"")
    sign = """"
    if after.startswith(""-""):
        after = after[1:]
        sign = ""-""
    elif after.startswith(""+""):
        after = after[1:]
    before = format_float_or_int_string(before)
    return f""{before}e{sign}{after}""",Formats a numeric string utilizing scientific notation,"def format_scientific_notation ( text : str ) -> str : before , after = text . split ( ""e"" ) sign = """" if after . startswith ( ""-"" ) : after = after [ 1 : ] sign = ""-"" elif after . startswith ( ""+"" ) : after = after [ 1 : ] before = format_float_or_int_string ( before ) return f""{before}e{sign}{after}""",Formats a numeric string utilizing scientific notation
/odoo/odoo/fields.py,_get_stored_translations,"def _get_stored_translations(self, record):
        """"""
        : return: {'en_US': 'value_en_US', 'fr_FR': 'French'}
        """"""
        # assert (self.translate and self.store and record)
        record.flush_recordset([self.name])
        cr = record.env.cr
        cr.execute(SQL(
            ""SELECT %s FROM %s WHERE id = %s"",
            SQL.identifier(self.name),
            SQL.identifier(record._table),
            record.id,
        ))
        res = cr.fetchone()
        return res[0] if res else None","def _get_stored_translations(self, record):
        """"""
        : return: {'en_US': 'value_en_US', 'fr_FR': 'French'}
        """"""
        # assert (self.translate and self.store and record)
        record.flush_recordset([self.name])
        cr = record.env.cr
        cr.execute(SQL(
            ""SELECT %s FROM %s WHERE id = %s"",
            SQL.identifier(self.name),
            SQL.identifier(record._table),
            record.id,
        ))
        res = cr.fetchone()
        return res[0] if res else None",": return: {'en_US': 'value_en_US', 'fr_FR': 'French'}",": return: {'en_US': 'value_en_US', 'fr_FR': 'French'}","def _get_stored_translations(self, record):
        
        # assert (self.translate and self.store and record)
        record.flush_recordset([self.name])
        cr = record.env.cr
        cr.execute(SQL(
            ""SELECT %s FROM %s WHERE id = %s"",
            SQL.identifier(self.name),
            SQL.identifier(record._table),
            record.id,
        ))
        res = cr.fetchone()
        return res[0] if res else None",": return: {'en_US': 'value_en_US', 'fr_FR': 'French'}","def _get_stored_translations ( self , record ) : # assert (self.translate and self.store and record) record . flush_recordset ( [ self . name ] ) cr = record . env . cr cr . execute ( SQL ( ""SELECT %s FROM %s WHERE id = %s"" , SQL . identifier ( self . name ) , SQL . identifier ( record . _table ) , record . id , ) ) res = cr . fetchone ( ) return res [ 0 ] if res else None",": return: {'en_US': 'value_en_US', 'fr_FR': 'French'}"
/OpenBB/openbb_platform/extensions/tests/utils/helpers.py,get_packages_info,"def get_packages_info() -> Dict[str, str]:
    """"""Get the paths and names of all the static packages.""""""
    paths_and_names: Dict[str, str] = {}
    package_paths = glob.glob(""openbb_platform/openbb/package/*.py"")
    for path in package_paths:
        name = os.path.basename(path).split(""."")[0]
        paths_and_names[path] = name

    paths_and_names = {
        path: name for path, name in paths_and_names.items() if not name.startswith(""_"")
    }
    return paths_and_names","def get_packages_info() -> Dict[str, str]:
    """"""Get the paths and names of all the static packages.""""""
    paths_and_names: Dict[str, str] = {}
    package_paths = glob.glob(""openbb_platform/openbb/package/*.py"")
    for path in package_paths:
        name = os.path.basename(path).split(""."")[0]
        paths_and_names[path] = name

    paths_and_names = {
        path: name for path, name in paths_and_names.items() if not name.startswith(""_"")
    }
    return paths_and_names",Get the paths and names of all the static packages.,Get the paths and names of all the static packages.,"def get_packages_info() -> Dict[str, str]:
    
    paths_and_names: Dict[str, str] = {}
    package_paths = glob.glob(""openbb_platform/openbb/package/*.py"")
    for path in package_paths:
        name = os.path.basename(path).split(""."")[0]
        paths_and_names[path] = name

    paths_and_names = {
        path: name for path, name in paths_and_names.items() if not name.startswith(""_"")
    }
    return paths_and_names",Get the paths and names of all the static packages.,"def get_packages_info ( ) -> Dict [ str , str ] : paths_and_names : Dict [ str , str ] = { } package_paths = glob . glob ( ""openbb_platform/openbb/package/*.py"" ) for path in package_paths : name = os . path . basename ( path ) . split ( ""."" ) [ 0 ] paths_and_names [ path ] = name paths_and_names = { path : name for path , name in paths_and_names . items ( ) if not name . startswith ( ""_"" ) } return paths_and_names",Get the paths and names of all the static packages.
/black/src/black/numerics.py,format_float_or_int_string,"def format_float_or_int_string(text: str) -> str:
    """"""Formats a float string like ""1.0"".""""""
    if ""."" not in text:
        return text

    before, after = text.split(""."")
    return f""{before or 0}.{after or 0}""","def format_float_or_int_string(text: str) -> str:
    """"""Formats a float string like ""1.0"".""""""
    if ""."" not in text:
        return text

    before, after = text.split(""."")
    return f""{before or 0}.{after or 0}""","Formats a float string like ""1.0"".","Formats a float string like ""1.0"".","def format_float_or_int_string(text: str) -> str:
    
    if ""."" not in text:
        return text

    before, after = text.split(""."")
    return f""{before or 0}.{after or 0}""","Formats a float string like ""1.0"".","def format_float_or_int_string ( text : str ) -> str : if ""."" not in text : return text before , after = text . split ( ""."" ) return f""{before or 0}.{after or 0}""","Formats a float string like ""1.0""."
/yolov5/utils/dataloaders.py,flatten_recursive,"def flatten_recursive(path=DATASETS_DIR / ""coco128""):
    """"""Flattens a directory by copying all files from subdirectories to a new top-level directory, preserving
    filenames.
    """"""
    new_path = Path(f""{str(path)}_flat"")
    if os.path.exists(new_path):
        shutil.rmtree(new_path)  # delete output folder
    os.makedirs(new_path)  # make new output folder
    for file in tqdm(glob.glob(f""{str(Path(path))}/**/*.*"", recursive=True)):
        shutil.copyfile(file, new_path / Path(file).name)","def flatten_recursive(path=DATASETS_DIR / ""coco128""):
    """"""Flattens a directory by copying all files from subdirectories to a new top-level directory, preserving
    filenames.
    """"""
    new_path = Path(f""{str(path)}_flat"")
    if os.path.exists(new_path):
        shutil.rmtree(new_path)  # delete output folder
    os.makedirs(new_path)  # make new output folder
    for file in tqdm(glob.glob(f""{str(Path(path))}/**/*.*"", recursive=True)):
        shutil.copyfile(file, new_path / Path(file).name)","Flattens a directory by copying all files from subdirectories to a new top-level directory, preserving
filenames.","Flattens a directory by copying all files from subdirectories to a new top-level directory, preserving filenames.","def flatten_recursive(path=DATASETS_DIR / ""coco128""):
    
    new_path = Path(f""{str(path)}_flat"")
    if os.path.exists(new_path):
        shutil.rmtree(new_path)  # delete output folder
    os.makedirs(new_path)  # make new output folder
    for file in tqdm(glob.glob(f""{str(Path(path))}/**/*.*"", recursive=True)):
        shutil.copyfile(file, new_path / Path(file).name)","Flattens a directory by copying all files from subdirectories to a new top-level directory, preserving filenames.","def flatten_recursive ( path = DATASETS_DIR / ""coco128"" ) : new_path = Path ( f""{str(path)}_flat"" ) if os . path . exists ( new_path ) : shutil . rmtree ( new_path ) # delete output folder os . makedirs ( new_path ) # make new output folder for file in tqdm ( glob . glob ( f""{str(Path(path))}/**/*.*"" , recursive = True ) ) : shutil . copyfile ( file , new_path / Path ( file ) . name )","Flattens a directory by copying all files from subdirectories to a new top-level directory, preserving filenames."
/sentry/src/sentry/runner/commands/spans.py,write_hashes,"def write_hashes(filename: str) -> None:
    """"""
    Runs span hash grouping on event data in the supplied filename using the
    default grouping strategy. Write the results to a copy of the file.
    Filename should be a path to a JSON event data file.
    """"""

    [head, tail] = os.path.split(filename)
    new_filename = f""{head}/hashed-{tail}""

    shutil.copy(filename, new_filename)

    with open(filename) as in_file:
        data = json.loads(in_file.read())

    click.echo(f""Event ID: {data['event_id']}"")
    click.echo(""Writing span hashes"")

    config = load_span_grouping_config({""id"": DEFAULT_CONFIG_ID})
    results = config.execute_strategy(data)

    with open(new_filename, ""w"") as out_file:
        results.write_to_event(data)
        out_file.write(json.dumps(data))

    click.echo(""Done"")
    click.echo(""\n"")","def write_hashes(filename: str) -> None:
    """"""
    Runs span hash grouping on event data in the supplied filename using the
    default grouping strategy. Write the results to a copy of the file.
    Filename should be a path to a JSON event data file.
    """"""

    [head, tail] = os.path.split(filename)
    new_filename = f""{head}/hashed-{tail}""

    shutil.copy(filename, new_filename)

    with open(filename) as in_file:
        data = json.loads(in_file.read())

    click.echo(f""Event ID: {data['event_id']}"")
    click.echo(""Writing span hashes"")

    config = load_span_grouping_config({""id"": DEFAULT_CONFIG_ID})
    results = config.execute_strategy(data)

    with open(new_filename, ""w"") as out_file:
        results.write_to_event(data)
        out_file.write(json.dumps(data))

    click.echo(""Done"")
    click.echo(""\n"")","Runs span hash grouping on event data in the supplied filename using the
default grouping strategy. Write the results to a copy of the file.
Filename should be a path to a JSON event data file.",Runs span hash grouping on event data in the supplied filename using the default grouping strategy.,"def write_hashes(filename: str) -> None:
    

    [head, tail] = os.path.split(filename)
    new_filename = f""{head}/hashed-{tail}""

    shutil.copy(filename, new_filename)

    with open(filename) as in_file:
        data = json.loads(in_file.read())

    click.echo(f""Event ID: {data['event_id']}"")
    click.echo(""Writing span hashes"")

    config = load_span_grouping_config({""id"": DEFAULT_CONFIG_ID})
    results = config.execute_strategy(data)

    with open(new_filename, ""w"") as out_file:
        results.write_to_event(data)
        out_file.write(json.dumps(data))

    click.echo(""Done"")
    click.echo(""\n"")",Runs span hash grouping on event data in the supplied filename using the default grouping strategy.,"def write_hashes ( filename : str ) -> None : [ head , tail ] = os . path . split ( filename ) new_filename = f""{head}/hashed-{tail}"" shutil . copy ( filename , new_filename ) with open ( filename ) as in_file : data = json . loads ( in_file . read ( ) ) click . echo ( f""Event ID: {data['event_id']}"" ) click . echo ( ""Writing span hashes"" ) config = load_span_grouping_config ( { ""id"" : DEFAULT_CONFIG_ID } ) results = config . execute_strategy ( data ) with open ( new_filename , ""w"" ) as out_file : results . write_to_event ( data ) out_file . write ( json . dumps ( data ) ) click . echo ( ""Done"" ) click . echo ( ""\n"" )",Runs span hash grouping on event data in the supplied filename using the default grouping strategy.
/yolov5/utils/__init__.py,join_threads,"def join_threads(verbose=False):
    """"""
    Joins all daemon threads, optionally printing their names if verbose is True.

    Example: atexit.register(lambda: join_threads())
    """"""
    main_thread = threading.current_thread()
    for t in threading.enumerate():
        if t is not main_thread:
            if verbose:
                print(f""Joining thread {t.name}"")
            t.join()","def join_threads(verbose=False):
    """"""
    Joins all daemon threads, optionally printing their names if verbose is True.

    Example: atexit.register(lambda: join_threads())
    """"""
    main_thread = threading.current_thread()
    for t in threading.enumerate():
        if t is not main_thread:
            if verbose:
                print(f""Joining thread {t.name}"")
            t.join()","Joins all daemon threads, optionally printing their names if verbose is True.

Example: atexit.register(lambda: join_threads())","Joins all daemon threads, optionally printing their names if verbose is True.","def join_threads(verbose=False):
    
    main_thread = threading.current_thread()
    for t in threading.enumerate():
        if t is not main_thread:
            if verbose:
                print(f""Joining thread {t.name}"")
            t.join()","Joins all daemon threads, optionally printing their names if verbose is True.","def join_threads ( verbose = False ) : main_thread = threading . current_thread ( ) for t in threading . enumerate ( ) : if t is not main_thread : if verbose : print ( f""Joining thread {t.name}"" ) t . join ( )","Joins all daemon threads, optionally printing their names if verbose is True."
/private-gpt/private_gpt/utils/eta.py,eta,"def eta(iterator: list[Any]) -> Any:
    """"""Report an ETA after 30s and every 60s thereafter.""""""
    total = len(iterator)
    _eta = ETA(total)
    _eta.needReport(30)
    for processed, data in enumerate(iterator, start=1):
        yield data
        _eta.update(processed)
        if _eta.needReport(60):
            logger.info(f""{processed}/{total} - ETA {_eta.human_time()}"")","def eta(iterator: list[Any]) -> Any:
    """"""Report an ETA after 30s and every 60s thereafter.""""""
    total = len(iterator)
    _eta = ETA(total)
    _eta.needReport(30)
    for processed, data in enumerate(iterator, start=1):
        yield data
        _eta.update(processed)
        if _eta.needReport(60):
            logger.info(f""{processed}/{total} - ETA {_eta.human_time()}"")",Report an ETA after 30s and every 60s thereafter.,Report an ETA after 30s and every 60s thereafter.,"def eta(iterator: list[Any]) -> Any:
    
    total = len(iterator)
    _eta = ETA(total)
    _eta.needReport(30)
    for processed, data in enumerate(iterator, start=1):
        yield data
        _eta.update(processed)
        if _eta.needReport(60):
            logger.info(f""{processed}/{total} - ETA {_eta.human_time()}"")",Report an ETA after 30s and every 60s thereafter.,"def eta ( iterator : list [ Any ] ) -> Any : total = len ( iterator ) _eta = ETA ( total ) _eta . needReport ( 30 ) for processed , data in enumerate ( iterator , start = 1 ) : yield data _eta . update ( processed ) if _eta . needReport ( 60 ) : logger . info ( f""{processed}/{total} - ETA {_eta.human_time()}"" )",Report an ETA after 30s and every 60s thereafter.
/faceswap/setup.py,get_required_packages,"def get_required_packages(self) -> None:
        """""" Load the requirements from the backend specific requirements list """"""
        req_files = [""_requirements_base.txt"", f""requirements_{self._env.backend}.txt""]
        pypath = os.path.dirname(os.path.realpath(__file__))
        requirements = []
        for req_file in req_files:
            requirements_file = os.path.join(pypath, ""requirements"", req_file)
            with open(requirements_file, encoding=""utf8"") as req:
                for package in req.readlines():
                    package = package.strip()
                    if package and (not package.startswith((""#"", ""-r""))):
                        requirements.append(package)

        self._required_packages = self._format_requirements(requirements)
        logger.debug(self._required_packages)","def get_required_packages(self) -> None:
        """""" Load the requirements from the backend specific requirements list """"""
        req_files = [""_requirements_base.txt"", f""requirements_{self._env.backend}.txt""]
        pypath = os.path.dirname(os.path.realpath(__file__))
        requirements = []
        for req_file in req_files:
            requirements_file = os.path.join(pypath, ""requirements"", req_file)
            with open(requirements_file, encoding=""utf8"") as req:
                for package in req.readlines():
                    package = package.strip()
                    if package and (not package.startswith((""#"", ""-r""))):
                        requirements.append(package)

        self._required_packages = self._format_requirements(requirements)
        logger.debug(self._required_packages)",Load the requirements from the backend specific requirements list,Load the requirements from the backend specific requirements list,"def get_required_packages(self) -> None:
        
        req_files = [""_requirements_base.txt"", f""requirements_{self._env.backend}.txt""]
        pypath = os.path.dirname(os.path.realpath(__file__))
        requirements = []
        for req_file in req_files:
            requirements_file = os.path.join(pypath, ""requirements"", req_file)
            with open(requirements_file, encoding=""utf8"") as req:
                for package in req.readlines():
                    package = package.strip()
                    if package and (not package.startswith((""#"", ""-r""))):
                        requirements.append(package)

        self._required_packages = self._format_requirements(requirements)
        logger.debug(self._required_packages)",Load the requirements from the backend specific requirements list,"def get_required_packages ( self ) -> None : req_files = [ ""_requirements_base.txt"" , f""requirements_{self._env.backend}.txt"" ] pypath = os . path . dirname ( os . path . realpath ( __file__ ) ) requirements = [ ] for req_file in req_files : requirements_file = os . path . join ( pypath , ""requirements"" , req_file ) with open ( requirements_file , encoding = ""utf8"" ) as req : for package in req . readlines ( ) : package = package . strip ( ) if package and ( not package . startswith ( ( ""#"" , ""-r"" ) ) ) : requirements . append ( package ) self . _required_packages = self . _format_requirements ( requirements ) logger . debug ( self . _required_packages )",Load the requirements from the backend specific requirements list
/odoo/odoo/models.py,_traverse_related_sql,"def _traverse_related_sql(self, alias: str, field: Field, query: Query):
        """""" Traverse the related `field` and add needed join to the `query`. """"""
        assert field.related and not field.store
        if not (self.env.su or field.compute_sudo or field.inherited):
            raise ValueError(f'Cannot convert {field} to SQL because it is not a sudoed related or inherited field')

        model = self.sudo(self.env.su or field.compute_sudo)
        *path_fnames, last_fname = field.related.split('.')
        for path_fname in path_fnames:
            path_field = model._fields[path_fname]
            if path_field.type != 'many2one':
                raise ValueError(f'Cannot convert {field} (related={field.related}) to SQL because {path_fname} is not a Many2one')

            comodel = model.env[path_field.comodel_name]
            coalias = query.make_alias(alias, path_fname)
            query.add_join('LEFT JOIN', coalias, comodel._table, SQL(
                ""%s = %s"",
                model._field_to_sql(alias, path_fname, query),
                SQL.identifier(coalias, 'id'),
            ))
            model, alias = comodel, coalias

        return model, model._fields[last_fname], alias","def _traverse_related_sql(self, alias: str, field: Field, query: Query):
        """""" Traverse the related `field` and add needed join to the `query`. """"""
        assert field.related and not field.store
        if not (self.env.su or field.compute_sudo or field.inherited):
            raise ValueError(f'Cannot convert {field} to SQL because it is not a sudoed related or inherited field')

        model = self.sudo(self.env.su or field.compute_sudo)
        *path_fnames, last_fname = field.related.split('.')
        for path_fname in path_fnames:
            path_field = model._fields[path_fname]
            if path_field.type != 'many2one':
                raise ValueError(f'Cannot convert {field} (related={field.related}) to SQL because {path_fname} is not a Many2one')

            comodel = model.env[path_field.comodel_name]
            coalias = query.make_alias(alias, path_fname)
            query.add_join('LEFT JOIN', coalias, comodel._table, SQL(
                ""%s = %s"",
                model._field_to_sql(alias, path_fname, query),
                SQL.identifier(coalias, 'id'),
            ))
            model, alias = comodel, coalias

        return model, model._fields[last_fname], alias",Traverse the related `field` and add needed join to the `query`.,Traverse the related `field` and add needed join to the `query`.,"def _traverse_related_sql(self, alias: str, field: Field, query: Query):
        
        assert field.related and not field.store
        if not (self.env.su or field.compute_sudo or field.inherited):
            raise ValueError(f'Cannot convert {field} to SQL because it is not a sudoed related or inherited field')

        model = self.sudo(self.env.su or field.compute_sudo)
        *path_fnames, last_fname = field.related.split('.')
        for path_fname in path_fnames:
            path_field = model._fields[path_fname]
            if path_field.type != 'many2one':
                raise ValueError(f'Cannot convert {field} (related={field.related}) to SQL because {path_fname} is not a Many2one')

            comodel = model.env[path_field.comodel_name]
            coalias = query.make_alias(alias, path_fname)
            query.add_join('LEFT JOIN', coalias, comodel._table, SQL(
                ""%s = %s"",
                model._field_to_sql(alias, path_fname, query),
                SQL.identifier(coalias, 'id'),
            ))
            model, alias = comodel, coalias

        return model, model._fields[last_fname], alias",Traverse the related `field` and add needed join to the `query`.,"def _traverse_related_sql ( self , alias : str , field : Field , query : Query ) : assert field . related and not field . store if not ( self . env . su or field . compute_sudo or field . inherited ) : raise ValueError ( f'Cannot convert {field} to SQL because it is not a sudoed related or inherited field' ) model = self . sudo ( self . env . su or field . compute_sudo ) * path_fnames , last_fname = field . related . split ( '.' ) for path_fname in path_fnames : path_field = model . _fields [ path_fname ] if path_field . type != 'many2one' : raise ValueError ( f'Cannot convert {field} (related={field.related}) to SQL because {path_fname} is not a Many2one' ) comodel = model . env [ path_field . comodel_name ] coalias = query . make_alias ( alias , path_fname ) query . add_join ( 'LEFT JOIN' , coalias , comodel . _table , SQL ( ""%s = %s"" , model . _field_to_sql ( alias , path_fname , query ) , SQL . identifier ( coalias , 'id' ) , ) ) model , alias = comodel , coalias return model , model . _fields [ last_fname ] , alias",Traverse the related `field` and add needed join to the `query`.
/yolov5/utils/dataloaders.py,_find_yaml,"def _find_yaml(dir):
        """"""Finds and returns the path to a single '.yaml' file in the specified directory, preferring files that match
        the directory name.
        """"""
        files = list(dir.glob(""*.yaml"")) or list(dir.rglob(""*.yaml""))  # try root level first and then recursive
        assert files, f""No *.yaml file found in {dir}""
        if len(files) > 1:
            files = [f for f in files if f.stem == dir.stem]  # prefer *.yaml files that match dir name
            assert files, f""Multiple *.yaml files found in {dir}, only 1 *.yaml file allowed""
        assert len(files) == 1, f""Multiple *.yaml files found: {files}, only 1 *.yaml file allowed in {dir}""
        return files[0]","def _find_yaml(dir):
        """"""Finds and returns the path to a single '.yaml' file in the specified directory, preferring files that match
        the directory name.
        """"""
        files = list(dir.glob(""*.yaml"")) or list(dir.rglob(""*.yaml""))  # try root level first and then recursive
        assert files, f""No *.yaml file found in {dir}""
        if len(files) > 1:
            files = [f for f in files if f.stem == dir.stem]  # prefer *.yaml files that match dir name
            assert files, f""Multiple *.yaml files found in {dir}, only 1 *.yaml file allowed""
        assert len(files) == 1, f""Multiple *.yaml files found: {files}, only 1 *.yaml file allowed in {dir}""
        return files[0]","Finds and returns the path to a single '.yaml' file in the specified directory, preferring files that match
the directory name.","Finds and returns the path to a single '.yaml' file in the specified directory, preferring files that match the directory name.","def _find_yaml(dir):
        
        files = list(dir.glob(""*.yaml"")) or list(dir.rglob(""*.yaml""))  # try root level first and then recursive
        assert files, f""No *.yaml file found in {dir}""
        if len(files) > 1:
            files = [f for f in files if f.stem == dir.stem]  # prefer *.yaml files that match dir name
            assert files, f""Multiple *.yaml files found in {dir}, only 1 *.yaml file allowed""
        assert len(files) == 1, f""Multiple *.yaml files found: {files}, only 1 *.yaml file allowed in {dir}""
        return files[0]","Finds and returns the path to a single '.yaml' file in the specified directory, preferring files that match the directory name.","def _find_yaml ( dir ) : files = list ( dir . glob ( ""*.yaml"" ) ) or list ( dir . rglob ( ""*.yaml"" ) ) # try root level first and then recursive assert files , f""No *.yaml file found in {dir}"" if len ( files ) > 1 : files = [ f for f in files if f . stem == dir . stem ] # prefer *.yaml files that match dir name assert files , f""Multiple *.yaml files found in {dir}, only 1 *.yaml file allowed"" assert len ( files ) == 1 , f""Multiple *.yaml files found: {files}, only 1 *.yaml file allowed in {dir}"" return files [ 0 ]","Finds and returns the path to a single '.yaml' file in the specified directory, preferring files that match the directory name."
/ansible/packaging/release.py,get_remote,"def get_remote(name: str, push: bool) -> Remote:
    """"""Return details about the specified remote.""""""
    remote_url = git(""remote"", ""get-url"", *([""--push""] if push else []), name, capture_output=True).stdout.strip()
    remote_match = re.search(r""[@/]github[.]com[:/](?P<user>[^/]+)/(?P<repo>[^.]+)(?:[.]git)?$"", remote_url)

    if not remote_match:
        raise RuntimeError(f""Unable to identify the user and repo in the '{name}' remote: {remote_url}"")

    remote = Remote(
        name=name,
        user=remote_match.group(""user""),
        repo=remote_match.group(""repo""),
    )

    return remote","def get_remote(name: str, push: bool) -> Remote:
    """"""Return details about the specified remote.""""""
    remote_url = git(""remote"", ""get-url"", *([""--push""] if push else []), name, capture_output=True).stdout.strip()
    remote_match = re.search(r""[@/]github[.]com[:/](?P<user>[^/]+)/(?P<repo>[^.]+)(?:[.]git)?$"", remote_url)

    if not remote_match:
        raise RuntimeError(f""Unable to identify the user and repo in the '{name}' remote: {remote_url}"")

    remote = Remote(
        name=name,
        user=remote_match.group(""user""),
        repo=remote_match.group(""repo""),
    )

    return remote",Return details about the specified remote.,Return details about the specified remote.,"def get_remote(name: str, push: bool) -> Remote:
    
    remote_url = git(""remote"", ""get-url"", *([""--push""] if push else []), name, capture_output=True).stdout.strip()
    remote_match = re.search(r""[@/]github[.]com[:/](?P<user>[^/]+)/(?P<repo>[^.]+)(?:[.]git)?$"", remote_url)

    if not remote_match:
        raise RuntimeError(f""Unable to identify the user and repo in the '{name}' remote: {remote_url}"")

    remote = Remote(
        name=name,
        user=remote_match.group(""user""),
        repo=remote_match.group(""repo""),
    )

    return remote",Return details about the specified remote.,"def get_remote ( name : str , push : bool ) -> Remote : remote_url = git ( ""remote"" , ""get-url"" , * ( [ ""--push"" ] if push else [ ] ) , name , capture_output = True ) . stdout . strip ( ) remote_match = re . search ( r""[@/]github[.]com[:/](?P<user>[^/]+)/(?P<repo>[^.]+)(?:[.]git)?$"" , remote_url ) if not remote_match : raise RuntimeError ( f""Unable to identify the user and repo in the '{name}' remote: {remote_url}"" ) remote = Remote ( name = name , user = remote_match . group ( ""user"" ) , repo = remote_match . group ( ""repo"" ) , ) return remote",Return details about the specified remote.
/yolov5/utils/general.py,check_font,"def check_font(font=FONT, progress=False):
    """"""Ensures specified font exists or downloads it from Ultralytics assets, optionally displaying progress.""""""
    font = Path(font)
    file = CONFIG_DIR / font.name
    if not font.exists() and not file.exists():
        url = f""https://github.com/ultralytics/assets/releases/download/v0.0.0/{font.name}""
        LOGGER.info(f""Downloading {url} to {file}..."")
        torch.hub.download_url_to_file(url, str(file), progress=progress)","def check_font(font=FONT, progress=False):
    """"""Ensures specified font exists or downloads it from Ultralytics assets, optionally displaying progress.""""""
    font = Path(font)
    file = CONFIG_DIR / font.name
    if not font.exists() and not file.exists():
        LOGGER.info(f""Downloading {url} to {file}..."")
        torch.hub.download_url_to_file(url, str(file), progress=progress)","Ensures specified font exists or downloads it from Ultralytics assets, optionally displaying progress.","Ensures specified font exists or downloads it from Ultralytics assets, optionally displaying progress.","def check_font(font=FONT, progress=False):
    
    font = Path(font)
    file = CONFIG_DIR / font.name
    if not font.exists() and not file.exists():
        LOGGER.info(f""Downloading {url} to {file}..."")
        torch.hub.download_url_to_file(url, str(file), progress=progress)","Ensures specified font exists or downloads it from Ultralytics assets, optionally displaying progress.","def check_font ( font = FONT , progress = False ) : font = Path ( font ) file = CONFIG_DIR / font . name if not font . exists ( ) and not file . exists ( ) : LOGGER . info ( f""Downloading {url} to {file}..."" ) torch . hub . download_url_to_file ( url , str ( file ) , progress = progress )","Ensures specified font exists or downloads it from Ultralytics assets, optionally displaying progress."
/odoo/odoo/models.py,__eq__,"def __eq__(self, other):
        """""" Test whether two recordsets are equivalent (up to reordering). """"""
        try:
            return self._name == other._name and set(self._ids) == set(other._ids)
        except AttributeError:
            if other:
                warnings.warn(f""unsupported operand type(s) for \""==\"": '{self._name}()' == '{other!r}'"", stacklevel=2)
        return NotImplemented","def __eq__(self, other):
        """""" Test whether two recordsets are equivalent (up to reordering). """"""
        try:
            return self._name == other._name and set(self._ids) == set(other._ids)
        except AttributeError:
            if other:
                warnings.warn(f""unsupported operand type(s) for \""==\"": '{self._name}()' == '{other!r}'"", stacklevel=2)
        return NotImplemented",Test whether two recordsets are equivalent (up to reordering).,Test whether two recordsets are equivalent (up to reordering).,"def __eq__(self, other):
        
        try:
            return self._name == other._name and set(self._ids) == set(other._ids)
        except AttributeError:
            if other:
                warnings.warn(f""unsupported operand type(s) for \""==\"": '{self._name}()' == '{other!r}'"", stacklevel=2)
        return NotImplemented",Test whether two recordsets are equivalent (up to reordering).,"def __eq__ ( self , other ) : try : return self . _name == other . _name and set ( self . _ids ) == set ( other . _ids ) except AttributeError : if other : warnings . warn ( f""unsupported operand type(s) for \""==\"": '{self._name}()' == '{other!r}'"" , stacklevel = 2 ) return NotImplemented",Test whether two recordsets are equivalent (up to reordering).
/core/script/util.py,valid_integration,"def valid_integration(integration):
    """"""Test if it's a valid integration.""""""
    if not (COMPONENT_DIR / integration).exists():
        raise argparse.ArgumentTypeError(
            f""The integration {integration} does not exist.""
        )

    return integration","def valid_integration(integration):
    """"""Test if it's a valid integration.""""""
    if not (COMPONENT_DIR / integration).exists():
        raise argparse.ArgumentTypeError(
            f""The integration {integration} does not exist.""
        )

    return integration",Test if it's a valid integration.,Test if it's a valid integration.,"def valid_integration(integration):
    
    if not (COMPONENT_DIR / integration).exists():
        raise argparse.ArgumentTypeError(
            f""The integration {integration} does not exist.""
        )

    return integration",Test if it's a valid integration.,"def valid_integration ( integration ) : if not ( COMPONENT_DIR / integration ) . exists ( ) : raise argparse . ArgumentTypeError ( f""The integration {integration} does not exist."" ) return integration",Test if it's a valid integration.
/pandas/pandas/util/_print_versions.py,_get_sys_info,"def _get_sys_info() -> dict[str, JSONSerializable]:
    """"""
    Returns system information as a JSON serializable dictionary.
    """"""
    uname_result = platform.uname()
    language_code, encoding = locale.getlocale()
    return {
        ""commit"": _get_commit_hash(),
        ""python"": platform.python_version(),
        ""python-bits"": struct.calcsize(""P"") * 8,
        ""OS"": uname_result.system,
        ""OS-release"": uname_result.release,
        ""Version"": uname_result.version,
        ""machine"": uname_result.machine,
        ""processor"": uname_result.processor,
        ""byteorder"": sys.byteorder,
        ""LC_ALL"": os.environ.get(""LC_ALL""),
        ""LANG"": os.environ.get(""LANG""),
        ""LOCALE"": {""language-code"": language_code, ""encoding"": encoding},
    }","def _get_sys_info() -> dict[str, JSONSerializable]:
    """"""
    Returns system information as a JSON serializable dictionary.
    """"""
    uname_result = platform.uname()
    language_code, encoding = locale.getlocale()
    return {
        ""commit"": _get_commit_hash(),
        ""python"": platform.python_version(),
        ""python-bits"": struct.calcsize(""P"") * 8,
        ""OS"": uname_result.system,
        ""OS-release"": uname_result.release,
        ""Version"": uname_result.version,
        ""machine"": uname_result.machine,
        ""processor"": uname_result.processor,
        ""byteorder"": sys.byteorder,
        ""LC_ALL"": os.environ.get(""LC_ALL""),
        ""LANG"": os.environ.get(""LANG""),
        ""LOCALE"": {""language-code"": language_code, ""encoding"": encoding},
    }",Returns system information as a JSON serializable dictionary.,Returns system information as a JSON serializable dictionary.,"def _get_sys_info() -> dict[str, JSONSerializable]:
    
    uname_result = platform.uname()
    language_code, encoding = locale.getlocale()
    return {
        ""commit"": _get_commit_hash(),
        ""python"": platform.python_version(),
        ""python-bits"": struct.calcsize(""P"") * 8,
        ""OS"": uname_result.system,
        ""OS-release"": uname_result.release,
        ""Version"": uname_result.version,
        ""machine"": uname_result.machine,
        ""processor"": uname_result.processor,
        ""byteorder"": sys.byteorder,
        ""LC_ALL"": os.environ.get(""LC_ALL""),
        ""LANG"": os.environ.get(""LANG""),
        ""LOCALE"": {""language-code"": language_code, ""encoding"": encoding},
    }",Returns system information as a JSON serializable dictionary.,"def _get_sys_info ( ) -> dict [ str , JSONSerializable ] : uname_result = platform . uname ( ) language_code , encoding = locale . getlocale ( ) return { ""commit"" : _get_commit_hash ( ) , ""python"" : platform . python_version ( ) , ""python-bits"" : struct . calcsize ( ""P"" ) * 8 , ""OS"" : uname_result . system , ""OS-release"" : uname_result . release , ""Version"" : uname_result . version , ""machine"" : uname_result . machine , ""processor"" : uname_result . processor , ""byteorder"" : sys . byteorder , ""LC_ALL"" : os . environ . get ( ""LC_ALL"" ) , ""LANG"" : os . environ . get ( ""LANG"" ) , ""LOCALE"" : { ""language-code"" : language_code , ""encoding"" : encoding } , }",Returns system information as a JSON serializable dictionary.
/cpython/Tools/build/stable_abi.py,check_private_names,"def check_private_names(manifest):
    """"""Ensure limited API doesn't contain private names

    Names prefixed by an underscore are private by definition.
    """"""
    for name, item in manifest.contents.items():
        if name.startswith('_') and not item.abi_only:
            raise ValueError(
                f'`{name}` is private (underscore-prefixed) and should be '
                'removed from the stable ABI list or marked `abi_only`')","def check_private_names(manifest):
    """"""Ensure limited API doesn't contain private names

    Names prefixed by an underscore are private by definition.
    """"""
    for name, item in manifest.contents.items():
        if name.startswith('_') and not item.abi_only:
            raise ValueError(
                f'`{name}` is private (underscore-prefixed) and should be '
                'removed from the stable ABI list or marked `abi_only`')","Ensure limited API doesn't contain private names

Names prefixed by an underscore are private by definition.",Ensure limited API doesn't contain private names,"def check_private_names(manifest):
    
    for name, item in manifest.contents.items():
        if name.startswith('_') and not item.abi_only:
            raise ValueError(
                f'`{name}` is private (underscore-prefixed) and should be '
                'removed from the stable ABI list or marked `abi_only`')",Ensure limited API doesn't contain private names,"def check_private_names ( manifest ) : for name , item in manifest . contents . items ( ) : if name . startswith ( '_' ) and not item . abi_only : raise ValueError ( f'`{name}` is private (underscore-prefixed) and should be ' 'removed from the stable ABI list or marked `abi_only`' )",Ensure limited API doesn't contain private names
/LLaMA-Factory/src/llamafactory/data/mm_plugin.py,_regularize_videos,"def _regularize_videos(self, videos: list[""VideoInput""], **kwargs) -> dict[str, list[list[""ImageObject""]]]:
        r""""""Regularizes videos to avoid error. Including reading, resizing and converting.""""""
        results = []
        for video in videos:
            container = av.open(video, ""r"")
            video_stream = next(stream for stream in container.streams if stream.type == ""video"")
            sample_indices = self._get_video_sample_indices(video_stream, **kwargs)
            frames: list[ImageObject] = []
            container.seek(0)
            for frame_idx, frame in enumerate(container.decode(video_stream)):
                if frame_idx in sample_indices:
                    frames.append(frame.to_image())

            frames = self._regularize_images(frames, **kwargs)[""images""]
            results.append(frames)

        return {""videos"": results}","def _regularize_videos(self, videos: list[""VideoInput""], **kwargs) -> dict[str, list[list[""ImageObject""]]]:
        r""""""Regularizes videos to avoid error. Including reading, resizing and converting.""""""
        results = []
        for video in videos:
            container = av.open(video, ""r"")
            video_stream = next(stream for stream in container.streams if stream.type == ""video"")
            sample_indices = self._get_video_sample_indices(video_stream, **kwargs)
            frames: list[ImageObject] = []
            container.seek(0)
            for frame_idx, frame in enumerate(container.decode(video_stream)):
                if frame_idx in sample_indices:
                    frames.append(frame.to_image())

            frames = self._regularize_images(frames, **kwargs)[""images""]
            results.append(frames)

        return {""videos"": results}","Regularizes videos to avoid error. Including reading, resizing and converting.",Regularizes videos to avoid error.,"def _regularize_videos(self, videos: list[""VideoInput""], **kwargs) -> dict[str, list[list[""ImageObject""]]]:
        
        results = []
        for video in videos:
            container = av.open(video, ""r"")
            video_stream = next(stream for stream in container.streams if stream.type == ""video"")
            sample_indices = self._get_video_sample_indices(video_stream, **kwargs)
            frames: list[ImageObject] = []
            container.seek(0)
            for frame_idx, frame in enumerate(container.decode(video_stream)):
                if frame_idx in sample_indices:
                    frames.append(frame.to_image())

            frames = self._regularize_images(frames, **kwargs)[""images""]
            results.append(frames)

        return {""videos"": results}",Regularizes videos to avoid error.,"def _regularize_videos ( self , videos : list [ ""VideoInput"" ] , ** kwargs ) -> dict [ str , list [ list [ ""ImageObject"" ] ] ] : results = [ ] for video in videos : container = av . open ( video , ""r"" ) video_stream = next ( stream for stream in container . streams if stream . type == ""video"" ) sample_indices = self . _get_video_sample_indices ( video_stream , ** kwargs ) frames : list [ ImageObject ] = [ ] container . seek ( 0 ) for frame_idx , frame in enumerate ( container . decode ( video_stream ) ) : if frame_idx in sample_indices : frames . append ( frame . to_image ( ) ) frames = self . _regularize_images ( frames , ** kwargs ) [ ""images"" ] results . append ( frames ) return { ""videos"" : results }",Regularizes videos to avoid error.
/ansible/packaging/release.py,post_version,"def post_version() -> None:
    """"""Set the post release version.""""""
    current_version = get_ansible_version()
    requested_version = get_ansible_version(f""{current_version}.post0"", mode=VersionMode.REQUIRE_POST)

    set_ansible_version(current_version, requested_version)","def post_version() -> None:
    """"""Set the post release version.""""""
    current_version = get_ansible_version()
    requested_version = get_ansible_version(f""{current_version}.post0"", mode=VersionMode.REQUIRE_POST)

    set_ansible_version(current_version, requested_version)",Set the post release version.,Set the post release version.,"def post_version() -> None:
    
    current_version = get_ansible_version()
    requested_version = get_ansible_version(f""{current_version}.post0"", mode=VersionMode.REQUIRE_POST)

    set_ansible_version(current_version, requested_version)",Set the post release version.,"def post_version ( ) -> None : current_version = get_ansible_version ( ) requested_version = get_ansible_version ( f""{current_version}.post0"" , mode = VersionMode . REQUIRE_POST ) set_ansible_version ( current_version , requested_version )",Set the post release version.
/core/script/gen_requirements_all.py,gather_recursive_requirements,"def gather_recursive_requirements(
    domain: str, seen: set[str] | None = None
) -> set[str]:
    """"""Recursively gather requirements from a module.""""""
    if seen is None:
        seen = set()

    seen.add(domain)
    integration = Integration(
        Path(f""homeassistant/components/{domain}""), _get_hassfest_config()
    )
    integration.load_manifest()
    reqs = {x for x in integration.requirements if x not in CONSTRAINT_BASE}
    for dep_domain in integration.dependencies:
        reqs.update(gather_recursive_requirements(dep_domain, seen))
    return reqs","def gather_recursive_requirements(
    domain: str, seen: set[str] | None = None
) -> set[str]:
    """"""Recursively gather requirements from a module.""""""
    if seen is None:
        seen = set()

    seen.add(domain)
    integration = Integration(
        Path(f""homeassistant/components/{domain}""), _get_hassfest_config()
    )
    integration.load_manifest()
    reqs = {x for x in integration.requirements if x not in CONSTRAINT_BASE}
    for dep_domain in integration.dependencies:
        reqs.update(gather_recursive_requirements(dep_domain, seen))
    return reqs",Recursively gather requirements from a module.,Recursively gather requirements from a module.,"def gather_recursive_requirements(
    domain: str, seen: set[str] | None = None
) -> set[str]:
    
    if seen is None:
        seen = set()

    seen.add(domain)
    integration = Integration(
        Path(f""homeassistant/components/{domain}""), _get_hassfest_config()
    )
    integration.load_manifest()
    reqs = {x for x in integration.requirements if x not in CONSTRAINT_BASE}
    for dep_domain in integration.dependencies:
        reqs.update(gather_recursive_requirements(dep_domain, seen))
    return reqs",Recursively gather requirements from a module.,"def gather_recursive_requirements ( domain : str , seen : set [ str ] | None = None ) -> set [ str ] : if seen is None : seen = set ( ) seen . add ( domain ) integration = Integration ( Path ( f""homeassistant/components/{domain}"" ) , _get_hassfest_config ( ) ) integration . load_manifest ( ) reqs = { x for x in integration . requirements if x not in CONSTRAINT_BASE } for dep_domain in integration . dependencies : reqs . update ( gather_recursive_requirements ( dep_domain , seen ) ) return reqs",Recursively gather requirements from a module.
/you-get/src/you_get/extractors/dailymotion.py,dailymotion_download,"def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads Dailymotion videos by URL.
    """"""

    html = get_content(rebuilt_url(url))
    info = json.loads(match1(html, r'qualities"":({.+?}),""'))
    title = match1(html, r'""video_title""\s*:\s*""([^""]+)""') or \
            match1(html, r'""title""\s*:\s*""([^""]+)""')
    title = unicodize(title)

    for quality in ['1080','720','480','380','240','144','auto']:
        try:
            real_url = info[quality][1][""url""]
            if real_url:
                break
        except KeyError:
            pass

    mime, ext, size = url_info(real_url)

    print_info(site_info, title, mime, size)
    if not info_only:
        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)","def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads Dailymotion videos by URL.
    """"""

    html = get_content(rebuilt_url(url))
    info = json.loads(match1(html, r'qualities"":({.+?}),""'))
    title = match1(html, r'""video_title""\s*:\s*""([^""]+)""') or \
            match1(html, r'""title""\s*:\s*""([^""]+)""')
    title = unicodize(title)

    for quality in ['1080','720','480','380','240','144','auto']:
        try:
            real_url = info[quality][1][""url""]
            if real_url:
                break
        except KeyError:
            pass

    mime, ext, size = url_info(real_url)

    print_info(site_info, title, mime, size)
    if not info_only:
        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",Downloads Dailymotion videos by URL.,Downloads Dailymotion videos by URL.,"def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    

    html = get_content(rebuilt_url(url))
    info = json.loads(match1(html, r'qualities"":({.+?}),""'))
    title = match1(html, r'""video_title""\s*:\s*""([^""]+)""') or \
            match1(html, r'""title""\s*:\s*""([^""]+)""')
    title = unicodize(title)

    for quality in ['1080','720','480','380','240','144','auto']:
        try:
            real_url = info[quality][1][""url""]
            if real_url:
                break
        except KeyError:
            pass

    mime, ext, size = url_info(real_url)

    print_info(site_info, title, mime, size)
    if not info_only:
        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",Downloads Dailymotion videos by URL.,"def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities"":({.+?}),""' ) ) title = match1 ( html , r'""video_title""\s*:\s*""([^""]+)""' ) or match1 ( html , r'""title""\s*:\s*""([^""]+)""' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ ""url"" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )",Downloads Dailymotion videos by URL.
/ansible/test/sanity/code-smell/package-data.py,build,"def build(source_dir: str, tmp_dir: str) -> tuple[pathlib.Path, pathlib.Path]:
    """"""Create a sdist and wheel.""""""
    create = subprocess.run(  # pylint: disable=subprocess-run-check
        [sys.executable, '-m', 'build', '--outdir', tmp_dir],
        stdin=subprocess.DEVNULL,
        capture_output=True,
        text=True,
        cwd=source_dir,
    )

    if create.returncode != 0:
        raise RuntimeError(f'build failed:\n{create.stderr}\n{create.stdout}')

    tmp_dir_files = list(pathlib.Path(tmp_dir).iterdir())

    if len(tmp_dir_files) != 2:
        raise RuntimeError(f'build resulted in {len(tmp_dir_files)} items instead of 2')

    sdist_path = [path for path in tmp_dir_files if path.suffix == '.gz'][0]
    wheel_path = [path for path in tmp_dir_files if path.suffix == '.whl'][0]

    return sdist_path, wheel_path","def build(source_dir: str, tmp_dir: str) -> tuple[pathlib.Path, pathlib.Path]:
    """"""Create a sdist and wheel.""""""
    create = subprocess.run(  # pylint: disable=subprocess-run-check
        [sys.executable, '-m', 'build', '--outdir', tmp_dir],
        stdin=subprocess.DEVNULL,
        capture_output=True,
        text=True,
        cwd=source_dir,
    )

    if create.returncode != 0:
        raise RuntimeError(f'build failed:\n{create.stderr}\n{create.stdout}')

    tmp_dir_files = list(pathlib.Path(tmp_dir).iterdir())

    if len(tmp_dir_files) != 2:
        raise RuntimeError(f'build resulted in {len(tmp_dir_files)} items instead of 2')

    sdist_path = [path for path in tmp_dir_files if path.suffix == '.gz'][0]
    wheel_path = [path for path in tmp_dir_files if path.suffix == '.whl'][0]

    return sdist_path, wheel_path",Create a sdist and wheel.,Create a sdist and wheel.,"def build(source_dir: str, tmp_dir: str) -> tuple[pathlib.Path, pathlib.Path]:
    
    create = subprocess.run(  # pylint: disable=subprocess-run-check
        [sys.executable, '-m', 'build', '--outdir', tmp_dir],
        stdin=subprocess.DEVNULL,
        capture_output=True,
        text=True,
        cwd=source_dir,
    )

    if create.returncode != 0:
        raise RuntimeError(f'build failed:\n{create.stderr}\n{create.stdout}')

    tmp_dir_files = list(pathlib.Path(tmp_dir).iterdir())

    if len(tmp_dir_files) != 2:
        raise RuntimeError(f'build resulted in {len(tmp_dir_files)} items instead of 2')

    sdist_path = [path for path in tmp_dir_files if path.suffix == '.gz'][0]
    wheel_path = [path for path in tmp_dir_files if path.suffix == '.whl'][0]

    return sdist_path, wheel_path",Create a sdist and wheel.,"def build ( source_dir : str , tmp_dir : str ) -> tuple [ pathlib . Path , pathlib . Path ] : create = subprocess . run ( # pylint: disable=subprocess-run-check [ sys . executable , '-m' , 'build' , '--outdir' , tmp_dir ] , stdin = subprocess . DEVNULL , capture_output = True , text = True , cwd = source_dir , ) if create . returncode != 0 : raise RuntimeError ( f'build failed:\n{create.stderr}\n{create.stdout}' ) tmp_dir_files = list ( pathlib . Path ( tmp_dir ) . iterdir ( ) ) if len ( tmp_dir_files ) != 2 : raise RuntimeError ( f'build resulted in {len(tmp_dir_files)} items instead of 2' ) sdist_path = [ path for path in tmp_dir_files if path . suffix == '.gz' ] [ 0 ] wheel_path = [ path for path in tmp_dir_files if path . suffix == '.whl' ] [ 0 ] return sdist_path , wheel_path",Create a sdist and wheel.
/ansible/test/sanity/code-smell/package-data.py,filter_fuzzy_matches,"def filter_fuzzy_matches(missing: set[str], extra: set[str]) -> None:
    """"""
    Removes entries from `missing` and `extra` that share a common basename that also appears in `fuzzy_match_basenames`.
    Accounts for variable placement of non-runtime files by different versions of setuptools.
    """"""
    if not (missing or extra):
        return

    # calculate a set of basenames that appear in both missing and extra that are also marked as possibly needing fuzzy matching
    corresponding_fuzzy_basenames = {os.path.basename(p) for p in missing}.intersection(os.path.basename(p) for p in extra).intersection(fuzzy_match_basenames)

    # filter successfully fuzzy-matched entries from missing and extra
    missing.difference_update({p for p in missing if os.path.basename(p) in corresponding_fuzzy_basenames})
    extra.difference_update({p for p in extra if os.path.basename(p) in corresponding_fuzzy_basenames})","def filter_fuzzy_matches(missing: set[str], extra: set[str]) -> None:
    """"""
    Removes entries from `missing` and `extra` that share a common basename that also appears in `fuzzy_match_basenames`.
    Accounts for variable placement of non-runtime files by different versions of setuptools.
    """"""
    if not (missing or extra):
        return

    # calculate a set of basenames that appear in both missing and extra that are also marked as possibly needing fuzzy matching
    corresponding_fuzzy_basenames = {os.path.basename(p) for p in missing}.intersection(os.path.basename(p) for p in extra).intersection(fuzzy_match_basenames)

    # filter successfully fuzzy-matched entries from missing and extra
    missing.difference_update({p for p in missing if os.path.basename(p) in corresponding_fuzzy_basenames})
    extra.difference_update({p for p in extra if os.path.basename(p) in corresponding_fuzzy_basenames})","Removes entries from `missing` and `extra` that share a common basename that also appears in `fuzzy_match_basenames`.
Accounts for variable placement of non-runtime files by different versions of setuptools.",Removes entries from `missing` and `extra` that share a common basename that also appears in `fuzzy_match_basenames`.,"def filter_fuzzy_matches(missing: set[str], extra: set[str]) -> None:
    
    if not (missing or extra):
        return

    # calculate a set of basenames that appear in both missing and extra that are also marked as possibly needing fuzzy matching
    corresponding_fuzzy_basenames = {os.path.basename(p) for p in missing}.intersection(os.path.basename(p) for p in extra).intersection(fuzzy_match_basenames)

    # filter successfully fuzzy-matched entries from missing and extra
    missing.difference_update({p for p in missing if os.path.basename(p) in corresponding_fuzzy_basenames})
    extra.difference_update({p for p in extra if os.path.basename(p) in corresponding_fuzzy_basenames})",Removes entries from `missing` and `extra` that share a common basename that also appears in `fuzzy_match_basenames`.,"def filter_fuzzy_matches ( missing : set [ str ] , extra : set [ str ] ) -> None : if not ( missing or extra ) : return # calculate a set of basenames that appear in both missing and extra that are also marked as possibly needing fuzzy matching corresponding_fuzzy_basenames = { os . path . basename ( p ) for p in missing } . intersection ( os . path . basename ( p ) for p in extra ) . intersection ( fuzzy_match_basenames ) # filter successfully fuzzy-matched entries from missing and extra missing . difference_update ( { p for p in missing if os . path . basename ( p ) in corresponding_fuzzy_basenames } ) extra . difference_update ( { p for p in extra if os . path . basename ( p ) in corresponding_fuzzy_basenames } )",Removes entries from `missing` and `extra` that share a common basename that also appears in `fuzzy_match_basenames`.
/sentry/src/sentry/runner/commands/performance.py,timeit,"def timeit(filename: str, detector_class: str, n: int) -> None:
    """"""
    Runs timing on performance problem detection on event data in the supplied
    filename and report results.
    """"""

    click.echo(f""Running timeit {n} times on {detector_class}"")

    import timeit

    from sentry.utils.performance_issues import performance_detection

    settings = performance_detection.get_detection_settings()

    with open(filename) as file:
        data = json.loads(file.read())

    detector = performance_detection.__dict__[detector_class](settings, data)

    def detect() -> None:
        performance_detection.run_detector_on_data(detector, data)

    result = timeit.timeit(stmt=detect, number=n)
    click.echo(f""Average runtime: {result * 1000 / n} ms"")","def timeit(filename: str, detector_class: str, n: int) -> None:
    """"""
    Runs timing on performance problem detection on event data in the supplied
    filename and report results.
    """"""

    click.echo(f""Running timeit {n} times on {detector_class}"")

    import timeit

    from sentry.utils.performance_issues import performance_detection

    settings = performance_detection.get_detection_settings()

    with open(filename) as file:
        data = json.loads(file.read())

    detector = performance_detection.__dict__[detector_class](settings, data)

    def detect() -> None:
        performance_detection.run_detector_on_data(detector, data)

    result = timeit.timeit(stmt=detect, number=n)
    click.echo(f""Average runtime: {result * 1000 / n} ms"")","Runs timing on performance problem detection on event data in the supplied
filename and report results.",Runs timing on performance problem detection on event data in the supplied filename and report results.,"def timeit(filename: str, detector_class: str, n: int) -> None:
    

    click.echo(f""Running timeit {n} times on {detector_class}"")

    import timeit

    from sentry.utils.performance_issues import performance_detection

    settings = performance_detection.get_detection_settings()

    with open(filename) as file:
        data = json.loads(file.read())

    detector = performance_detection.__dict__[detector_class](settings, data)

    def detect() -> None:
        performance_detection.run_detector_on_data(detector, data)

    result = timeit.timeit(stmt=detect, number=n)
    click.echo(f""Average runtime: {result * 1000 / n} ms"")",Runs timing on performance problem detection on event data in the supplied filename and report results.,"def timeit ( filename : str , detector_class : str , n : int ) -> None : click . echo ( f""Running timeit {n} times on {detector_class}"" ) import timeit from sentry . utils . performance_issues import performance_detection settings = performance_detection . get_detection_settings ( ) with open ( filename ) as file : data = json . loads ( file . read ( ) ) detector = performance_detection . __dict__ [ detector_class ] ( settings , data ) def detect ( ) -> None : performance_detection . run_detector_on_data ( detector , data ) result = timeit . timeit ( stmt = detect , number = n ) click . echo ( f""Average runtime: {result * 1000 / n} ms"" )",Runs timing on performance problem detection on event data in the supplied filename and report results.
/ultralytics/ultralytics/data/utils.py,process_images,"def process_images(self) -> Path:
        """"""Compress images for Ultralytics HUB.""""""
        from ultralytics.data import YOLODataset  # ClassificationDataset

        self.im_dir.mkdir(parents=True, exist_ok=True)  # makes dataset-hub/images/
        for split in ""train"", ""val"", ""test"":
            if self.data.get(split) is None:
                continue
            dataset = YOLODataset(img_path=self.data[split], data=self.data)
            with ThreadPool(NUM_THREADS) as pool:
                for _ in TQDM(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f""{split} images""):
                    pass
        LOGGER.info(f""Done. All images saved to {self.im_dir}"")
        return self.im_dir","def process_images(self) -> Path:
        """"""Compress images for Ultralytics HUB.""""""
        from ultralytics.data import YOLODataset  # ClassificationDataset

        self.im_dir.mkdir(parents=True, exist_ok=True)  # makes dataset-hub/images/
        for split in ""train"", ""val"", ""test"":
            if self.data.get(split) is None:
                continue
            dataset = YOLODataset(img_path=self.data[split], data=self.data)
            with ThreadPool(NUM_THREADS) as pool:
                for _ in TQDM(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f""{split} images""):
                    pass
        LOGGER.info(f""Done. All images saved to {self.im_dir}"")
        return self.im_dir",Compress images for Ultralytics HUB.,Compress images for Ultralytics HUB.,"def process_images(self) -> Path:
        
        from ultralytics.data import YOLODataset  # ClassificationDataset

        self.im_dir.mkdir(parents=True, exist_ok=True)  # makes dataset-hub/images/
        for split in ""train"", ""val"", ""test"":
            if self.data.get(split) is None:
                continue
            dataset = YOLODataset(img_path=self.data[split], data=self.data)
            with ThreadPool(NUM_THREADS) as pool:
                for _ in TQDM(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f""{split} images""):
                    pass
        LOGGER.info(f""Done. All images saved to {self.im_dir}"")
        return self.im_dir",Compress images for Ultralytics HUB.,"def process_images ( self ) -> Path : from ultralytics . data import YOLODataset # ClassificationDataset self . im_dir . mkdir ( parents = True , exist_ok = True ) # makes dataset-hub/images/ for split in ""train"" , ""val"" , ""test"" : if self . data . get ( split ) is None : continue dataset = YOLODataset ( img_path = self . data [ split ] , data = self . data ) with ThreadPool ( NUM_THREADS ) as pool : for _ in TQDM ( pool . imap ( self . _hub_ops , dataset . im_files ) , total = len ( dataset ) , desc = f""{split} images"" ) : pass LOGGER . info ( f""Done. All images saved to {self.im_dir}"" ) return self . im_dir",Compress images for Ultralytics HUB.
/ansible/packaging/cli-doc/build.py,build_man,"def build_man(output_dir: pathlib.Path, template_file: pathlib.Path) -> None:
    """"""Build man pages for ansible-core CLI programs.""""""
    if not template_file.resolve().is_relative_to(SCRIPT_DIR):
        warnings.warn(""Custom templates are intended for debugging purposes only. The data model may change in future releases without notice."")

    import docutils.core
    import docutils.writers.manpage

    output_dir.mkdir(exist_ok=True, parents=True)

    for cli_name, source in generate_rst(template_file).items():
        with io.StringIO(source) as source_file:
            docutils.core.publish_file(
                source=source_file,
                destination_path=output_dir / f'{cli_name}.1',
                writer=docutils.writers.manpage.Writer(),
            )","def build_man(output_dir: pathlib.Path, template_file: pathlib.Path) -> None:
    """"""Build man pages for ansible-core CLI programs.""""""
    if not template_file.resolve().is_relative_to(SCRIPT_DIR):
        warnings.warn(""Custom templates are intended for debugging purposes only. The data model may change in future releases without notice."")

    import docutils.core
    import docutils.writers.manpage

    output_dir.mkdir(exist_ok=True, parents=True)

    for cli_name, source in generate_rst(template_file).items():
        with io.StringIO(source) as source_file:
            docutils.core.publish_file(
                source=source_file,
                destination_path=output_dir / f'{cli_name}.1',
                writer=docutils.writers.manpage.Writer(),
            )",Build man pages for ansible-core CLI programs.,Build man pages for ansible-core CLI programs.,"def build_man(output_dir: pathlib.Path, template_file: pathlib.Path) -> None:
    
    if not template_file.resolve().is_relative_to(SCRIPT_DIR):
        warnings.warn(""Custom templates are intended for debugging purposes only. The data model may change in future releases without notice."")

    import docutils.core
    import docutils.writers.manpage

    output_dir.mkdir(exist_ok=True, parents=True)

    for cli_name, source in generate_rst(template_file).items():
        with io.StringIO(source) as source_file:
            docutils.core.publish_file(
                source=source_file,
                destination_path=output_dir / f'{cli_name}.1',
                writer=docutils.writers.manpage.Writer(),
            )",Build man pages for ansible-core CLI programs.,"def build_man ( output_dir : pathlib . Path , template_file : pathlib . Path ) -> None : if not template_file . resolve ( ) . is_relative_to ( SCRIPT_DIR ) : warnings . warn ( ""Custom templates are intended for debugging purposes only. The data model may change in future releases without notice."" ) import docutils . core import docutils . writers . manpage output_dir . mkdir ( exist_ok = True , parents = True ) for cli_name , source in generate_rst ( template_file ) . items ( ) : with io . StringIO ( source ) as source_file : docutils . core . publish_file ( source = source_file , destination_path = output_dir / f'{cli_name}.1' , writer = docutils . writers . manpage . Writer ( ) , )",Build man pages for ansible-core CLI programs.
/OpenManus/app/logger.py,define_log_level,"def define_log_level(print_level=""INFO"", logfile_level=""DEBUG"", name: str = None):
    """"""Adjust the log level to above level""""""
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime(""%Y%m%d%H%M%S"")
    log_name = (
        f""{name}_{formatted_date}"" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()
    _logger.add(sys.stderr, level=print_level)
    _logger.add(PROJECT_ROOT / f""logs/{log_name}.log"", level=logfile_level)
    return _logger","def define_log_level(print_level=""INFO"", logfile_level=""DEBUG"", name: str = None):
    """"""Adjust the log level to above level""""""
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime(""%Y%m%d%H%M%S"")
    log_name = (
        f""{name}_{formatted_date}"" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()
    _logger.add(sys.stderr, level=print_level)
    _logger.add(PROJECT_ROOT / f""logs/{log_name}.log"", level=logfile_level)
    return _logger",Adjust the log level to above level,Adjust the log level to above level,"def define_log_level(print_level=""INFO"", logfile_level=""DEBUG"", name: str = None):
    
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime(""%Y%m%d%H%M%S"")
    log_name = (
        f""{name}_{formatted_date}"" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()
    _logger.add(sys.stderr, level=print_level)
    _logger.add(PROJECT_ROOT / f""logs/{log_name}.log"", level=logfile_level)
    return _logger",Adjust the log level to above level,"def define_log_level ( print_level = ""INFO"" , logfile_level = ""DEBUG"" , name : str = None ) : global _print_level _print_level = print_level current_date = datetime . now ( ) formatted_date = current_date . strftime ( ""%Y%m%d%H%M%S"" ) log_name = ( f""{name}_{formatted_date}"" if name else formatted_date ) # name a log with prefix name _logger . remove ( ) _logger . add ( sys . stderr , level = print_level ) _logger . add ( PROJECT_ROOT / f""logs/{log_name}.log"" , level = logfile_level ) return _logger",Adjust the log level to above level
/cpython/Tools/wasm/wasi/__main__.py,clean_contents,"def clean_contents(context):
    """"""Delete all files created by this script.""""""
    if CROSS_BUILD_DIR.exists():
        print(f""🧹 Deleting {CROSS_BUILD_DIR} ..."")
        shutil.rmtree(CROSS_BUILD_DIR)

    if LOCAL_SETUP.exists():
        with LOCAL_SETUP.open(""rb"") as file:
            if file.read(len(LOCAL_SETUP_MARKER)) == LOCAL_SETUP_MARKER:
                print(f""🧹 Deleting generated {LOCAL_SETUP} ..."")","def clean_contents(context):
    """"""Delete all files created by this script.""""""
    if CROSS_BUILD_DIR.exists():
        print(f""🧹 Deleting {CROSS_BUILD_DIR} ..."")
        shutil.rmtree(CROSS_BUILD_DIR)

    if LOCAL_SETUP.exists():
        with LOCAL_SETUP.open(""rb"") as file:
            if file.read(len(LOCAL_SETUP_MARKER)) == LOCAL_SETUP_MARKER:
                print(f""🧹 Deleting generated {LOCAL_SETUP} ..."")",Delete all files created by this script.,Delete all files created by this script.,"def clean_contents(context):
    
    if CROSS_BUILD_DIR.exists():
        print(f""🧹 Deleting {CROSS_BUILD_DIR} ..."")
        shutil.rmtree(CROSS_BUILD_DIR)

    if LOCAL_SETUP.exists():
        with LOCAL_SETUP.open(""rb"") as file:
            if file.read(len(LOCAL_SETUP_MARKER)) == LOCAL_SETUP_MARKER:
                print(f""🧹 Deleting generated {LOCAL_SETUP} ..."")",Delete all files created by this script.,"def clean_contents ( context ) : if CROSS_BUILD_DIR . exists ( ) : print ( f""🧹 Deleting {CROSS_BUILD_DIR} ..."" ) shutil . rmtree ( CROSS_BUILD_DIR ) if LOCAL_SETUP . exists ( ) : with LOCAL_SETUP . open ( ""rb"" ) as file : if file . read ( len ( LOCAL_SETUP_MARKER ) ) == LOCAL_SETUP_MARKER : print ( f""🧹 Deleting generated {LOCAL_SETUP} ..."" )",Delete all files created by this script.
/sentry/src/sentry/runner/commands/backup.py,print_elapsed_time,"def print_elapsed_time(kind: str, interval_ms: int, done_event: Event, printer: Printer) -> None:
    """"""
    Prints an update every `interval_ms` seconds. Intended to be run on a separate thread. When that
    thread is done with its work, it should `done_event.set()` to indicate to this thread to finish
    as well.
    """"""
    start_time = time()
    last_print_time = start_time
    check_interval = 1  # Check every second if we should exit

    while not done_event.is_set():
        current_time = time()
        diff_ms = (current_time - last_print_time) * 1000
        if diff_ms >= interval_ms:
            printer.echo(f""{kind}: {(current_time - start_time):.2f} seconds elapsed."")
            last_print_time = current_time
        sleep(check_interval)","def print_elapsed_time(kind: str, interval_ms: int, done_event: Event, printer: Printer) -> None:
    """"""
    Prints an update every `interval_ms` seconds. Intended to be run on a separate thread. When that
    thread is done with its work, it should `done_event.set()` to indicate to this thread to finish
    as well.
    """"""
    start_time = time()
    last_print_time = start_time
    check_interval = 1  # Check every second if we should exit

    while not done_event.is_set():
        current_time = time()
        diff_ms = (current_time - last_print_time) * 1000
        if diff_ms >= interval_ms:
            printer.echo(f""{kind}: {(current_time - start_time):.2f} seconds elapsed."")
            last_print_time = current_time
        sleep(check_interval)","Prints an update every `interval_ms` seconds. Intended to be run on a separate thread. When that
thread is done with its work, it should `done_event.set()` to indicate to this thread to finish
as well.",Prints an update every `interval_ms` seconds.,"def print_elapsed_time(kind: str, interval_ms: int, done_event: Event, printer: Printer) -> None:
    
    start_time = time()
    last_print_time = start_time
    check_interval = 1  # Check every second if we should exit

    while not done_event.is_set():
        current_time = time()
        diff_ms = (current_time - last_print_time) * 1000
        if diff_ms >= interval_ms:
            printer.echo(f""{kind}: {(current_time - start_time):.2f} seconds elapsed."")
            last_print_time = current_time
        sleep(check_interval)",Prints an update every `interval_ms` seconds.,"def print_elapsed_time ( kind : str , interval_ms : int , done_event : Event , printer : Printer ) -> None : start_time = time ( ) last_print_time = start_time check_interval = 1 # Check every second if we should exit while not done_event . is_set ( ) : current_time = time ( ) diff_ms = ( current_time - last_print_time ) * 1000 if diff_ms >= interval_ms : printer . echo ( f""{kind}: {(current_time - start_time):.2f} seconds elapsed."" ) last_print_time = current_time sleep ( check_interval )",Prints an update every `interval_ms` seconds.
/cpython/Tools/build/stable_abi.py,parse_manifest,"def parse_manifest(file):
    """"""Parse the given file (iterable of lines) to a Manifest""""""

    manifest = Manifest()

    data = tomllib.load(file)

    for kind, itemclass in itemclasses.items():
        for name, item_data in data[kind].items():
            try:
                item = itemclass(name=name, kind=kind, **item_data)
                manifest.add(item)
            except BaseException as exc:
                exc.add_note(f'in {kind} {name}')
                raise

    return manifest","def parse_manifest(file):
    """"""Parse the given file (iterable of lines) to a Manifest""""""

    manifest = Manifest()

    data = tomllib.load(file)

    for kind, itemclass in itemclasses.items():
        for name, item_data in data[kind].items():
            try:
                item = itemclass(name=name, kind=kind, **item_data)
                manifest.add(item)
            except BaseException as exc:
                exc.add_note(f'in {kind} {name}')
                raise

    return manifest",Parse the given file (iterable of lines) to a Manifest,Parse the given file (iterable of lines) to a Manifest,"def parse_manifest(file):
    

    manifest = Manifest()

    data = tomllib.load(file)

    for kind, itemclass in itemclasses.items():
        for name, item_data in data[kind].items():
            try:
                item = itemclass(name=name, kind=kind, **item_data)
                manifest.add(item)
            except BaseException as exc:
                exc.add_note(f'in {kind} {name}')
                raise

    return manifest",Parse the given file (iterable of lines) to a Manifest,"def parse_manifest ( file ) : manifest = Manifest ( ) data = tomllib . load ( file ) for kind , itemclass in itemclasses . items ( ) : for name , item_data in data [ kind ] . items ( ) : try : item = itemclass ( name = name , kind = kind , ** item_data ) manifest . add ( item ) except BaseException as exc : exc . add_note ( f'in {kind} {name}' ) raise return manifest",Parse the given file (iterable of lines) to a Manifest
/pandas/web/pandas_web.py,get_context,"def get_context(config_fname: str, **kwargs):
    """"""
    Load the config yaml as the base context, and enrich it with the
    information added by the context preprocessors defined in the file.
    """"""
    with open(config_fname, encoding=""utf-8"") as f:
        context = yaml.safe_load(f)

    context[""source_path""] = os.path.dirname(config_fname)
    context.update(kwargs)

    preprocessors = (
        get_callable(context_prep)
        for context_prep in context[""main""][""context_preprocessors""]
    )
    for preprocessor in preprocessors:
        context = preprocessor(context)
        msg = f""{preprocessor.__name__} is missing the return statement""
        assert context is not None, msg

    return context","def get_context(config_fname: str, **kwargs):
    """"""
    Load the config yaml as the base context, and enrich it with the
    information added by the context preprocessors defined in the file.
    """"""
    with open(config_fname, encoding=""utf-8"") as f:
        context = yaml.safe_load(f)

    context[""source_path""] = os.path.dirname(config_fname)
    context.update(kwargs)

    preprocessors = (
        get_callable(context_prep)
        for context_prep in context[""main""][""context_preprocessors""]
    )
    for preprocessor in preprocessors:
        context = preprocessor(context)
        msg = f""{preprocessor.__name__} is missing the return statement""
        assert context is not None, msg

    return context","Load the config yaml as the base context, and enrich it with the
information added by the context preprocessors defined in the file.","Load the config yaml as the base context, and enrich it with the information added by the context preprocessors defined in the file.","def get_context(config_fname: str, **kwargs):
    
    with open(config_fname, encoding=""utf-8"") as f:
        context = yaml.safe_load(f)

    context[""source_path""] = os.path.dirname(config_fname)
    context.update(kwargs)

    preprocessors = (
        get_callable(context_prep)
        for context_prep in context[""main""][""context_preprocessors""]
    )
    for preprocessor in preprocessors:
        context = preprocessor(context)
        msg = f""{preprocessor.__name__} is missing the return statement""
        assert context is not None, msg

    return context","Load the config yaml as the base context, and enrich it with the information added by the context preprocessors defined in the file.","def get_context ( config_fname : str , ** kwargs ) : with open ( config_fname , encoding = ""utf-8"" ) as f : context = yaml . safe_load ( f ) context [ ""source_path"" ] = os . path . dirname ( config_fname ) context . update ( kwargs ) preprocessors = ( get_callable ( context_prep ) for context_prep in context [ ""main"" ] [ ""context_preprocessors"" ] ) for preprocessor in preprocessors : context = preprocessor ( context ) msg = f""{preprocessor.__name__} is missing the return statement"" assert context is not None , msg return context","Load the config yaml as the base context, and enrich it with the information added by the context preprocessors defined in the file."
/AutoGPT/classic/original_autogpt/autogpt/agents/agent_manager.py,get_agent_dir,"def get_agent_dir(self, agent_id: str) -> Path:
        """"""Return the directory of the agent with the given ID.""""""
        assert len(agent_id) > 0
        agent_dir: Path | None = None
        if self.file_manager.exists(agent_id):
            agent_dir = self.file_manager.root / agent_id
        else:
            raise FileNotFoundError(f""No agent with ID '{agent_id}'"")
        return agent_dir","def get_agent_dir(self, agent_id: str) -> Path:
        """"""Return the directory of the agent with the given ID.""""""
        assert len(agent_id) > 0
        agent_dir: Path | None = None
        if self.file_manager.exists(agent_id):
            agent_dir = self.file_manager.root / agent_id
        else:
            raise FileNotFoundError(f""No agent with ID '{agent_id}'"")
        return agent_dir",Return the directory of the agent with the given ID.,Return the directory of the agent with the given ID.,"def get_agent_dir(self, agent_id: str) -> Path:
        
        assert len(agent_id) > 0
        agent_dir: Path | None = None
        if self.file_manager.exists(agent_id):
            agent_dir = self.file_manager.root / agent_id
        else:
            raise FileNotFoundError(f""No agent with ID '{agent_id}'"")
        return agent_dir",Return the directory of the agent with the given ID.,"def get_agent_dir ( self , agent_id : str ) -> Path : assert len ( agent_id ) > 0 agent_dir : Path | None = None if self . file_manager . exists ( agent_id ) : agent_dir = self . file_manager . root / agent_id else : raise FileNotFoundError ( f""No agent with ID '{agent_id}'"" ) return agent_dir",Return the directory of the agent with the given ID.
/ultralytics/ultralytics/data/converter.py,convert_label,"def convert_label(image_name: str, image_width: int, image_height: int, orig_label_dir: Path, save_dir: Path):
        """"""Convert a single image's DOTA annotation to YOLO OBB format and save it to a specified directory.""""""
        orig_label_path = orig_label_dir / f""{image_name}.txt""
        save_path = save_dir / f""{image_name}.txt""

        with orig_label_path.open(""r"") as f, save_path.open(""w"") as g:
            lines = f.readlines()
            for line in lines:
                parts = line.strip().split()
                if len(parts) < 9:
                    continue
                class_name = parts[8]
                class_idx = class_mapping[class_name]
                coords = [float(p) for p in parts[:8]]
                normalized_coords = [
                    coords[i] / image_width if i % 2 == 0 else coords[i] / image_height for i in range(8)
                ]
                formatted_coords = [f""{coord:.6g}"" for coord in normalized_coords]
                g.write(f""{class_idx} {' '.join(formatted_coords)}\n"")","def convert_label(image_name: str, image_width: int, image_height: int, orig_label_dir: Path, save_dir: Path):
        """"""Convert a single image's DOTA annotation to YOLO OBB format and save it to a specified directory.""""""
        orig_label_path = orig_label_dir / f""{image_name}.txt""
        save_path = save_dir / f""{image_name}.txt""

        with orig_label_path.open(""r"") as f, save_path.open(""w"") as g:
            lines = f.readlines()
            for line in lines:
                parts = line.strip().split()
                if len(parts) < 9:
                    continue
                class_name = parts[8]
                class_idx = class_mapping[class_name]
                coords = [float(p) for p in parts[:8]]
                normalized_coords = [
                    coords[i] / image_width if i % 2 == 0 else coords[i] / image_height for i in range(8)
                ]
                formatted_coords = [f""{coord:.6g}"" for coord in normalized_coords]
                g.write(f""{class_idx} {' '.join(formatted_coords)}\n"")",Convert a single image's DOTA annotation to YOLO OBB format and save it to a specified directory.,Convert a single image's DOTA annotation to YOLO OBB format and save it to a specified directory.,"def convert_label(image_name: str, image_width: int, image_height: int, orig_label_dir: Path, save_dir: Path):
        
        orig_label_path = orig_label_dir / f""{image_name}.txt""
        save_path = save_dir / f""{image_name}.txt""

        with orig_label_path.open(""r"") as f, save_path.open(""w"") as g:
            lines = f.readlines()
            for line in lines:
                parts = line.strip().split()
                if len(parts) < 9:
                    continue
                class_name = parts[8]
                class_idx = class_mapping[class_name]
                coords = [float(p) for p in parts[:8]]
                normalized_coords = [
                    coords[i] / image_width if i % 2 == 0 else coords[i] / image_height for i in range(8)
                ]
                formatted_coords = [f""{coord:.6g}"" for coord in normalized_coords]
                g.write(f""{class_idx} {' '.join(formatted_coords)}\n"")",Convert a single image's DOTA annotation to YOLO OBB format and save it to a specified directory.,"def convert_label ( image_name : str , image_width : int , image_height : int , orig_label_dir : Path , save_dir : Path ) : orig_label_path = orig_label_dir / f""{image_name}.txt"" save_path = save_dir / f""{image_name}.txt"" with orig_label_path . open ( ""r"" ) as f , save_path . open ( ""w"" ) as g : lines = f . readlines ( ) for line in lines : parts = line . strip ( ) . split ( ) if len ( parts ) < 9 : continue class_name = parts [ 8 ] class_idx = class_mapping [ class_name ] coords = [ float ( p ) for p in parts [ : 8 ] ] normalized_coords = [ coords [ i ] / image_width if i % 2 == 0 else coords [ i ] / image_height for i in range ( 8 ) ] formatted_coords = [ f""{coord:.6g}"" for coord in normalized_coords ] g . write ( f""{class_idx} {' '.join(formatted_coords)}\n"" )",Convert a single image's DOTA annotation to YOLO OBB format and save it to a specified directory.
/llama_index/llama-dev/llama_dev/utils.py,get_changed_files,"def get_changed_files(repo_root: Path, base_ref: str = ""main"") -> list[Path]:
    """"""Use git to get the list of files changed compared to the base branch.""""""
    try:
        cmd = [""git"", ""diff"", ""--name-only"", f""{base_ref}...HEAD""]
        result = subprocess.run(cmd, cwd=repo_root, text=True, capture_output=True)
        if result.returncode != 0:
            raise RuntimeError(f""Git command failed: {result.stderr}"")

        return [repo_root / Path(f) for f in result.stdout.splitlines() if f.strip()]
    except Exception as e:
        print(f""Exception occurred: {e!s}"")
        raise","def get_changed_files(repo_root: Path, base_ref: str = ""main"") -> list[Path]:
    """"""Use git to get the list of files changed compared to the base branch.""""""
    try:
        cmd = [""git"", ""diff"", ""--name-only"", f""{base_ref}...HEAD""]
        result = subprocess.run(cmd, cwd=repo_root, text=True, capture_output=True)
        if result.returncode != 0:
            raise RuntimeError(f""Git command failed: {result.stderr}"")

        return [repo_root / Path(f) for f in result.stdout.splitlines() if f.strip()]
    except Exception as e:
        print(f""Exception occurred: {e!s}"")
        raise",Use git to get the list of files changed compared to the base branch.,Use git to get the list of files changed compared to the base branch.,"def get_changed_files(repo_root: Path, base_ref: str = ""main"") -> list[Path]:
    
    try:
        cmd = [""git"", ""diff"", ""--name-only"", f""{base_ref}...HEAD""]
        result = subprocess.run(cmd, cwd=repo_root, text=True, capture_output=True)
        if result.returncode != 0:
            raise RuntimeError(f""Git command failed: {result.stderr}"")

        return [repo_root / Path(f) for f in result.stdout.splitlines() if f.strip()]
    except Exception as e:
        print(f""Exception occurred: {e!s}"")
        raise",Use git to get the list of files changed compared to the base branch.,"def get_changed_files ( repo_root : Path , base_ref : str = ""main"" ) -> list [ Path ] : try : cmd = [ ""git"" , ""diff"" , ""--name-only"" , f""{base_ref}...HEAD"" ] result = subprocess . run ( cmd , cwd = repo_root , text = True , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( f""Git command failed: {result.stderr}"" ) return [ repo_root / Path ( f ) for f in result . stdout . splitlines ( ) if f . strip ( ) ] except Exception as e : print ( f""Exception occurred: {e!s}"" ) raise",Use git to get the list of files changed compared to the base branch.
/black/src/black/handle_ipynb_magics.py,create_token,"def create_token(n_chars: int) -> str:
    """"""Create a randomly generated token that is n_chars characters long.""""""
    assert n_chars > 0
    n_bytes = max(n_chars // 2 - 1, 1)
    token = secrets.token_hex(n_bytes)
    if len(token) + 3 > n_chars:
        token = token[:-1]
    # We use a bytestring so that the string does not get interpreted
    # as a docstring.
    return f'b""{token}""'","def create_token(n_chars: int) -> str:
    """"""Create a randomly generated token that is n_chars characters long.""""""
    assert n_chars > 0
    token = secrets.token_hex(n_bytes)
    if len(token) + 3 > n_chars:
        token = token[:-1]
    # We use a bytestring so that the string does not get interpreted
    # as a docstring.
    return f'b""{token}""'",Create a randomly generated token that is n_chars characters long.,Create a randomly generated token that is n_chars characters long.,"def create_token(n_chars: int) -> str:
    
    assert n_chars > 0
    token = secrets.token_hex(n_bytes)
    if len(token) + 3 > n_chars:
        token = token[:-1]
    # We use a bytestring so that the string does not get interpreted
    # as a docstring.
    return f'b""{token}""'",Create a randomly generated token that is n_chars characters long.,"def create_token ( n_chars : int ) -> str : assert n_chars > 0 token = secrets . token_hex ( n_bytes ) if len ( token ) + 3 > n_chars : token = token [ : - 1 ] # We use a bytestring so that the string does not get interpreted # as a docstring. return f'b""{token}""'",Create a randomly generated token that is n_chars characters long.
/OpenBB/openbb_platform/extensions/tests/utils/integration_tests_generator.py,write_charting_extension_integration_tests,"def write_charting_extension_integration_tests():
    """"""Write charting extension integration tests.""""""
    import openbb_charting  # pylint: disable=import-outside-toplevel

    functions = Charting.functions()

    charting_ext_path = Path(openbb_charting.__file__).parent.parent
    test_file = charting_ext_path / ""integration"" / ""test_charting_python.py""

    extra = """"""assert result.chart.content
    assert isinstance(result.chart.fig, OpenBBFigure)""""""

    for func in functions:
        write_to_file_w_template(
            test_file=test_file,
            params_list=[{""chart"": ""True""}],
            full_command=func.replace(""_"", "".""),
            test_name=f""chart_{func}"",  # TODO: fix the name of the charting library
            extra=extra,
        )","def write_charting_extension_integration_tests():
    """"""Write charting extension integration tests.""""""
    import openbb_charting  # pylint: disable=import-outside-toplevel

    functions = Charting.functions()

    charting_ext_path = Path(openbb_charting.__file__).parent.parent
    test_file = charting_ext_path / ""integration"" / ""test_charting_python.py""

    extra = """"""assert result.chart.content
    assert isinstance(result.chart.fig, OpenBBFigure)""""""

    for func in functions:
        write_to_file_w_template(
            test_file=test_file,
            params_list=[{""chart"": ""True""}],
            full_command=func.replace(""_"", "".""),
            test_name=f""chart_{func}"",  # TODO: fix the name of the charting library
            extra=extra,
        )",Write charting extension integration tests.,Write charting extension integration tests.,"def write_charting_extension_integration_tests():
    
    import openbb_charting  # pylint: disable=import-outside-toplevel

    functions = Charting.functions()

    charting_ext_path = Path(openbb_charting.__file__).parent.parent
    test_file = charting_ext_path / ""integration"" / ""test_charting_python.py""

    extra = 

    for func in functions:
        write_to_file_w_template(
            test_file=test_file,
            params_list=[{""chart"": ""True""}],
            full_command=func.replace(""_"", "".""),
            test_name=f""chart_{func}"",  # TODO: fix the name of the charting library
            extra=extra,
        )",Write charting extension integration tests.,"def write_charting_extension_integration_tests ( ) : import openbb_charting # pylint: disable=import-outside-toplevel functions = Charting . functions ( ) charting_ext_path = Path ( openbb_charting . __file__ ) . parent . parent test_file = charting_ext_path / ""integration"" / ""test_charting_python.py"" extra = for func in functions : write_to_file_w_template ( test_file = test_file , params_list = [ { ""chart"" : ""True"" } ] , full_command = func . replace ( ""_"" , ""."" ) , test_name = f""chart_{func}"" , # TODO: fix the name of the charting library extra = extra , )",Write charting extension integration tests.
/OpenBB/openbb_platform/extensions/tests/utils/integration_tests_api_generator.py,write_charting_extension_integration_tests,"def write_charting_extension_integration_tests():
    """"""Write the charting extension integration tests.""""""
    import openbb_charting  # pylint: disable=import-outside-toplevel

    functions = Charting.functions()

    charting_ext_path = Path(openbb_charting.__file__).parent.parent
    test_file = charting_ext_path / ""integration"" / ""test_charting_api.py""

    for function in functions:
        route = ""/"" + function.replace(""_"", ""/"")
        if not test_exists(route=function, path=str(test_file)):
            write_test_w_template(
                http_method=""post"",
                params_list=[{""chart"": True}],
                route=route,
                path=str(test_file),
                chart=True,
            )","def write_charting_extension_integration_tests():
    """"""Write the charting extension integration tests.""""""
    import openbb_charting  # pylint: disable=import-outside-toplevel

    functions = Charting.functions()

    charting_ext_path = Path(openbb_charting.__file__).parent.parent
    test_file = charting_ext_path / ""integration"" / ""test_charting_api.py""

    for function in functions:
        route = ""/"" + function.replace(""_"", ""/"")
        if not test_exists(route=function, path=str(test_file)):
            write_test_w_template(
                http_method=""post"",
                params_list=[{""chart"": True}],
                route=route,
                path=str(test_file),
                chart=True,
            )",Write the charting extension integration tests.,Write the charting extension integration tests.,"def write_charting_extension_integration_tests():
    
    import openbb_charting  # pylint: disable=import-outside-toplevel

    functions = Charting.functions()

    charting_ext_path = Path(openbb_charting.__file__).parent.parent
    test_file = charting_ext_path / ""integration"" / ""test_charting_api.py""

    for function in functions:
        route = ""/"" + function.replace(""_"", ""/"")
        if not test_exists(route=function, path=str(test_file)):
            write_test_w_template(
                http_method=""post"",
                params_list=[{""chart"": True}],
                route=route,
                path=str(test_file),
                chart=True,
            )",Write the charting extension integration tests.,"def write_charting_extension_integration_tests ( ) : import openbb_charting # pylint: disable=import-outside-toplevel functions = Charting . functions ( ) charting_ext_path = Path ( openbb_charting . __file__ ) . parent . parent test_file = charting_ext_path / ""integration"" / ""test_charting_api.py"" for function in functions : route = ""/"" + function . replace ( ""_"" , ""/"" ) if not test_exists ( route = function , path = str ( test_file ) ) : write_test_w_template ( http_method = ""post"" , params_list = [ { ""chart"" : True } ] , route = route , path = str ( test_file ) , chart = True , )",Write the charting extension integration tests.
/yolov5/utils/loggers/comet/hpo.py,run,"def run(parameters, opt):
    """"""Executes YOLOv5 training with given hyperparameters and options, setting up device and training directories.""""""
    hyp_dict = {k: v for k, v in parameters.items() if k not in [""epochs"", ""batch_size""]}

    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok or opt.evolve))
    opt.batch_size = parameters.get(""batch_size"")
    opt.epochs = parameters.get(""epochs"")

    device = select_device(opt.device, batch_size=opt.batch_size)
    train(hyp_dict, opt, device, callbacks=Callbacks())","def run(parameters, opt):
    """"""Executes YOLOv5 training with given hyperparameters and options, setting up device and training directories.""""""
    hyp_dict = {k: v for k, v in parameters.items() if k not in [""epochs"", ""batch_size""]}

    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok or opt.evolve))
    opt.batch_size = parameters.get(""batch_size"")
    opt.epochs = parameters.get(""epochs"")

    device = select_device(opt.device, batch_size=opt.batch_size)
    train(hyp_dict, opt, device, callbacks=Callbacks())","Executes YOLOv5 training with given hyperparameters and options, setting up device and training directories.","Executes YOLOv5 training with given hyperparameters and options, setting up device and training directories.","def run(parameters, opt):
    
    hyp_dict = {k: v for k, v in parameters.items() if k not in [""epochs"", ""batch_size""]}

    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok or opt.evolve))
    opt.batch_size = parameters.get(""batch_size"")
    opt.epochs = parameters.get(""epochs"")

    device = select_device(opt.device, batch_size=opt.batch_size)
    train(hyp_dict, opt, device, callbacks=Callbacks())","Executes YOLOv5 training with given hyperparameters and options, setting up device and training directories.","def run ( parameters , opt ) : hyp_dict = { k : v for k , v in parameters . items ( ) if k not in [ ""epochs"" , ""batch_size"" ] } opt . save_dir = str ( increment_path ( Path ( opt . project ) / opt . name , exist_ok = opt . exist_ok or opt . evolve ) ) opt . batch_size = parameters . get ( ""batch_size"" ) opt . epochs = parameters . get ( ""epochs"" ) device = select_device ( opt . device , batch_size = opt . batch_size ) train ( hyp_dict , opt , device , callbacks = Callbacks ( ) )","Executes YOLOv5 training with given hyperparameters and options, setting up device and training directories."
/MinerU/tests/test_cli/lib/pre_clean.py,convert_htmltale_to_md,"def convert_htmltale_to_md(content):
    """"""
    convert html table to markdown table
    """"""
    tables = re.findall(r'<table>(.*?)</table>', content, re.DOTALL)  
    placeholders = []
    for table in tables:
        placeholder = f""<!-- TABLE_PLACEHOLDER_{len(placeholders)} -->""  
        content = content.replace(f""<table>{table}</table>"", placeholder)  
        try:
            convert_table = htmltabletomd.convert_table(table)
        except:
            convert_table = table
        placeholders.append((placeholder,convert_table)) 
    new_content = content  
    for placeholder, md_table in placeholders:  
        new_content = new_content.replace(placeholder, md_table)  
        # 写入文件  
    return new_content","def convert_htmltale_to_md(content):
    """"""
    convert html table to markdown table
    """"""
    tables = re.findall(r'<table>(.*?)</table>', content, re.DOTALL)  
    placeholders = []
    for table in tables:
        placeholder = f""<!-- TABLE_PLACEHOLDER_{len(placeholders)} -->""  
        content = content.replace(f""<table>{table}</table>"", placeholder)  
        try:
            convert_table = htmltabletomd.convert_table(table)
        except:
            convert_table = table
        placeholders.append((placeholder,convert_table)) 
    new_content = content  
    for placeholder, md_table in placeholders:  
        new_content = new_content.replace(placeholder, md_table)  
        # 写入文件  
    return new_content",convert html table to markdown table,convert html table to markdown table,"def convert_htmltale_to_md(content):
    
    tables = re.findall(r'<table>(.*?)</table>', content, re.DOTALL)  
    placeholders = []
    for table in tables:
        placeholder = f""<!-- TABLE_PLACEHOLDER_{len(placeholders)} -->""  
        content = content.replace(f""<table>{table}</table>"", placeholder)  
        try:
            convert_table = htmltabletomd.convert_table(table)
        except:
            convert_table = table
        placeholders.append((placeholder,convert_table)) 
    new_content = content  
    for placeholder, md_table in placeholders:  
        new_content = new_content.replace(placeholder, md_table)  
        # 写入文件  
    return new_content",convert html table to markdown table,"def convert_htmltale_to_md ( content ) : tables = re . findall ( r'<table>(.*?)</table>' , content , re . DOTALL ) placeholders = [ ] for table in tables : placeholder = f""<!-- TABLE_PLACEHOLDER_{len(placeholders)} -->"" content = content . replace ( f""<table>{table}</table>"" , placeholder ) try : convert_table = htmltabletomd . convert_table ( table ) except : convert_table = table placeholders . append ( ( placeholder , convert_table ) ) new_content = content for placeholder , md_table in placeholders : new_content = new_content . replace ( placeholder , md_table ) # 写入文件   return new_content",convert html table to markdown table
/LLaMA-Factory/src/llamafactory/train/ppo/ppo_utils.py,replace_model,"def replace_model(model: ""AutoModelForCausalLMWithValueHead"", target: Literal[""default"", ""reward""]) -> None:
    r""""""Replace the default/reward modules in the model. The model is already unwrapped.""""""
    v_head_layer = model.v_head.summary
    if is_deepspeed_zero3_enabled():
        import deepspeed  # type: ignore

        params = [v_head_layer.weight, v_head_layer.bias]
        context_maybe_zero3 = deepspeed.zero.GatheredParameters(params, modifier_rank=0)
    else:
        context_maybe_zero3 = nullcontext()

    model.pretrained_model.set_adapter(target)  # set the LoRA adapter to be active
    with context_maybe_zero3:
        if target == ""reward"":  # save default head temporarily
            setattr(model, ""default_head_weight"", v_head_layer.weight.data.detach().clone())
            setattr(model, ""default_head_bias"", v_head_layer.bias.data.detach().clone())

        device = v_head_layer.weight.device
        v_head_layer.weight.data = model.get_buffer(f""{target}_head_weight"").detach().clone().to(device)
        v_head_layer.bias.data = model.get_buffer(f""{target}_head_bias"").detach().clone().to(device)","def replace_model(model: ""AutoModelForCausalLMWithValueHead"", target: Literal[""default"", ""reward""]) -> None:
    r""""""Replace the default/reward modules in the model. The model is already unwrapped.""""""
    v_head_layer = model.v_head.summary
    if is_deepspeed_zero3_enabled():
        import deepspeed  # type: ignore

        params = [v_head_layer.weight, v_head_layer.bias]
        context_maybe_zero3 = deepspeed.zero.GatheredParameters(params, modifier_rank=0)
    else:
        context_maybe_zero3 = nullcontext()

    model.pretrained_model.set_adapter(target)  # set the LoRA adapter to be active
    with context_maybe_zero3:
        if target == ""reward"":  # save default head temporarily
            setattr(model, ""default_head_weight"", v_head_layer.weight.data.detach().clone())
            setattr(model, ""default_head_bias"", v_head_layer.bias.data.detach().clone())

        device = v_head_layer.weight.device
        v_head_layer.weight.data = model.get_buffer(f""{target}_head_weight"").detach().clone().to(device)
        v_head_layer.bias.data = model.get_buffer(f""{target}_head_bias"").detach().clone().to(device)",Replace the default/reward modules in the model. The model is already unwrapped.,Replace the default/reward modules in the model.,"def replace_model(model: ""AutoModelForCausalLMWithValueHead"", target: Literal[""default"", ""reward""]) -> None:
    
    v_head_layer = model.v_head.summary
    if is_deepspeed_zero3_enabled():
        import deepspeed  # type: ignore

        params = [v_head_layer.weight, v_head_layer.bias]
        context_maybe_zero3 = deepspeed.zero.GatheredParameters(params, modifier_rank=0)
    else:
        context_maybe_zero3 = nullcontext()

    model.pretrained_model.set_adapter(target)  # set the LoRA adapter to be active
    with context_maybe_zero3:
        if target == ""reward"":  # save default head temporarily
            setattr(model, ""default_head_weight"", v_head_layer.weight.data.detach().clone())
            setattr(model, ""default_head_bias"", v_head_layer.bias.data.detach().clone())

        device = v_head_layer.weight.device
        v_head_layer.weight.data = model.get_buffer(f""{target}_head_weight"").detach().clone().to(device)
        v_head_layer.bias.data = model.get_buffer(f""{target}_head_bias"").detach().clone().to(device)",Replace the default/reward modules in the model.,"def replace_model ( model : ""AutoModelForCausalLMWithValueHead"" , target : Literal [ ""default"" , ""reward"" ] ) -> None : v_head_layer = model . v_head . summary if is_deepspeed_zero3_enabled ( ) : import deepspeed # type: ignore params = [ v_head_layer . weight , v_head_layer . bias ] context_maybe_zero3 = deepspeed . zero . GatheredParameters ( params , modifier_rank = 0 ) else : context_maybe_zero3 = nullcontext ( ) model . pretrained_model . set_adapter ( target ) # set the LoRA adapter to be active with context_maybe_zero3 : if target == ""reward"" : # save default head temporarily setattr ( model , ""default_head_weight"" , v_head_layer . weight . data . detach ( ) . clone ( ) ) setattr ( model , ""default_head_bias"" , v_head_layer . bias . data . detach ( ) . clone ( ) ) device = v_head_layer . weight . device v_head_layer . weight . data = model . get_buffer ( f""{target}_head_weight"" ) . detach ( ) . clone ( ) . to ( device ) v_head_layer . bias . data = model . get_buffer ( f""{target}_head_bias"" ) . detach ( ) . clone ( ) . to ( device )",Replace the default/reward modules in the model.
/yt-dlp/yt_dlp/options.py,_match_long_opt,"def _match_long_opt(self, opt):
        """"""Improve ambiguous argument resolution by comparing option objects instead of argument strings""""""
        try:
            return super()._match_long_opt(opt)
        except optparse.AmbiguousOptionError as e:
            if len({self._long_opt[p] for p in e.possibilities}) == 1:
                return e.possibilities[0]
            raise","def _match_long_opt(self, opt):
        """"""Improve ambiguous argument resolution by comparing option objects instead of argument strings""""""
        try:
            return super()._match_long_opt(opt)
        except optparse.AmbiguousOptionError as e:
            if len({self._long_opt[p] for p in e.possibilities}) == 1:
                return e.possibilities[0]
            raise",Improve ambiguous argument resolution by comparing option objects instead of argument strings,Improve ambiguous argument resolution by comparing option objects instead of argument strings,"def _match_long_opt(self, opt):
        
        try:
            return super()._match_long_opt(opt)
        except optparse.AmbiguousOptionError as e:
            if len({self._long_opt[p] for p in e.possibilities}) == 1:
                return e.possibilities[0]
            raise",Improve ambiguous argument resolution by comparing option objects instead of argument strings,"def _match_long_opt ( self , opt ) : try : return super ( ) . _match_long_opt ( opt ) except optparse . AmbiguousOptionError as e : if len ( { self . _long_opt [ p ] for p in e . possibilities } ) == 1 : return e . possibilities [ 0 ] raise",Improve ambiguous argument resolution by comparing option objects instead of argument strings
/yolov5/utils/loggers/comet/__init__.py,update_data_paths,"def update_data_paths(self, data_dict):
        """"""Updates data paths in the dataset dictionary, defaulting 'path' to an empty string if not present.""""""
        path = data_dict.get(""path"", """")

        for split in [""train"", ""val"", ""test""]:
            if data_dict.get(split):
                split_path = data_dict.get(split)
                data_dict[split] = (
                    f""{path}/{split_path}"" if isinstance(split, str) else [f""{path}/{x}"" for x in split_path]
                )

        return data_dict","def update_data_paths(self, data_dict):
        """"""Updates data paths in the dataset dictionary, defaulting 'path' to an empty string if not present.""""""
        path = data_dict.get(""path"", """")

        for split in [""train"", ""val"", ""test""]:
            if data_dict.get(split):
                split_path = data_dict.get(split)
                data_dict[split] = (
                    f""{path}/{split_path}"" if isinstance(split, str) else [f""{path}/{x}"" for x in split_path]
                )

        return data_dict","Updates data paths in the dataset dictionary, defaulting 'path' to an empty string if not present.","Updates data paths in the dataset dictionary, defaulting 'path' to an empty string if not present.","def update_data_paths(self, data_dict):
        
        path = data_dict.get(""path"", """")

        for split in [""train"", ""val"", ""test""]:
            if data_dict.get(split):
                split_path = data_dict.get(split)
                data_dict[split] = (
                    f""{path}/{split_path}"" if isinstance(split, str) else [f""{path}/{x}"" for x in split_path]
                )

        return data_dict","Updates data paths in the dataset dictionary, defaulting 'path' to an empty string if not present.","def update_data_paths ( self , data_dict ) : path = data_dict . get ( ""path"" , """" ) for split in [ ""train"" , ""val"" , ""test"" ] : if data_dict . get ( split ) : split_path = data_dict . get ( split ) data_dict [ split ] = ( f""{path}/{split_path}"" if isinstance ( split , str ) else [ f""{path}/{x}"" for x in split_path ] ) return data_dict","Updates data paths in the dataset dictionary, defaulting 'path' to an empty string if not present."
/localstack/localstack-core/localstack/config.py,defaults,"def defaults() -> ""Directories"":
        """"""Returns Localstack directory paths based on the localstack filesystem hierarchy.""""""
        return Directories(
            static_libs=""/usr/lib/localstack"",
            var_libs=f""{DEFAULT_VOLUME_DIR}/lib"",
            cache=f""{DEFAULT_VOLUME_DIR}/cache"",
            tmp=os.path.join(tempfile.gettempdir(), ""localstack""),
            mounted_tmp=f""{DEFAULT_VOLUME_DIR}/tmp"",
            functions=f""{DEFAULT_VOLUME_DIR}/tmp"",  # FIXME: remove - this was misconceived
            data=f""{DEFAULT_VOLUME_DIR}/state"",
            logs=f""{DEFAULT_VOLUME_DIR}/logs"",
            config=""/etc/localstack/conf.d"",  # for future use
            init=""/etc/localstack/init"",
        )","def defaults() -> ""Directories"":
        """"""Returns Localstack directory paths based on the localstack filesystem hierarchy.""""""
        return Directories(
            static_libs=""/usr/lib/localstack"",
            var_libs=f""{DEFAULT_VOLUME_DIR}/lib"",
            cache=f""{DEFAULT_VOLUME_DIR}/cache"",
            tmp=os.path.join(tempfile.gettempdir(), ""localstack""),
            mounted_tmp=f""{DEFAULT_VOLUME_DIR}/tmp"",
            functions=f""{DEFAULT_VOLUME_DIR}/tmp"",  # FIXME: remove - this was misconceived
            data=f""{DEFAULT_VOLUME_DIR}/state"",
            logs=f""{DEFAULT_VOLUME_DIR}/logs"",
            config=""/etc/localstack/conf.d"",  # for future use
            init=""/etc/localstack/init"",
        )",Returns Localstack directory paths based on the localstack filesystem hierarchy.,Returns Localstack directory paths based on the localstack filesystem hierarchy.,"def defaults() -> ""Directories"":
        
        return Directories(
            static_libs=""/usr/lib/localstack"",
            var_libs=f""{DEFAULT_VOLUME_DIR}/lib"",
            cache=f""{DEFAULT_VOLUME_DIR}/cache"",
            tmp=os.path.join(tempfile.gettempdir(), ""localstack""),
            mounted_tmp=f""{DEFAULT_VOLUME_DIR}/tmp"",
            functions=f""{DEFAULT_VOLUME_DIR}/tmp"",  # FIXME: remove - this was misconceived
            data=f""{DEFAULT_VOLUME_DIR}/state"",
            logs=f""{DEFAULT_VOLUME_DIR}/logs"",
            config=""/etc/localstack/conf.d"",  # for future use
            init=""/etc/localstack/init"",
        )",Returns Localstack directory paths based on the localstack filesystem hierarchy.,"def defaults ( ) -> ""Directories"" : return Directories ( static_libs = ""/usr/lib/localstack"" , var_libs = f""{DEFAULT_VOLUME_DIR}/lib"" , cache = f""{DEFAULT_VOLUME_DIR}/cache"" , tmp = os . path . join ( tempfile . gettempdir ( ) , ""localstack"" ) , mounted_tmp = f""{DEFAULT_VOLUME_DIR}/tmp"" , functions = f""{DEFAULT_VOLUME_DIR}/tmp"" , # FIXME: remove - this was misconceived data = f""{DEFAULT_VOLUME_DIR}/state"" , logs = f""{DEFAULT_VOLUME_DIR}/logs"" , config = ""/etc/localstack/conf.d"" , # for future use init = ""/etc/localstack/init"" , )",Returns Localstack directory paths based on the localstack filesystem hierarchy.
/yolov5/utils/loggers/__init__.py,on_pretrain_routine_end,"def on_pretrain_routine_end(self, labels, names):
        """"""Callback that runs at the end of pre-training routine, logging label plots if enabled.""""""
        if self.plots:
            plot_labels(labels, names, self.save_dir)
            paths = self.save_dir.glob(""*labels*.jpg"")  # training labels
            if self.wandb:
                self.wandb.log({""Labels"": [wandb.Image(str(x), caption=x.name) for x in paths]})
            if self.comet_logger:
                self.comet_logger.on_pretrain_routine_end(paths)
            if self.clearml:
                for path in paths:
                    self.clearml.log_plot(title=path.stem, plot_path=path)","def on_pretrain_routine_end(self, labels, names):
        """"""Callback that runs at the end of pre-training routine, logging label plots if enabled.""""""
        if self.plots:
            plot_labels(labels, names, self.save_dir)
            paths = self.save_dir.glob(""*labels*.jpg"")  # training labels
            if self.wandb:
                self.wandb.log({""Labels"": [wandb.Image(str(x), caption=x.name) for x in paths]})
            if self.comet_logger:
                self.comet_logger.on_pretrain_routine_end(paths)
            if self.clearml:
                for path in paths:
                    self.clearml.log_plot(title=path.stem, plot_path=path)","Callback that runs at the end of pre-training routine, logging label plots if enabled.","Callback that runs at the end of pre-training routine, logging label plots if enabled.","def on_pretrain_routine_end(self, labels, names):
        
        if self.plots:
            plot_labels(labels, names, self.save_dir)
            paths = self.save_dir.glob(""*labels*.jpg"")  # training labels
            if self.wandb:
                self.wandb.log({""Labels"": [wandb.Image(str(x), caption=x.name) for x in paths]})
            if self.comet_logger:
                self.comet_logger.on_pretrain_routine_end(paths)
            if self.clearml:
                for path in paths:
                    self.clearml.log_plot(title=path.stem, plot_path=path)","Callback that runs at the end of pre-training routine, logging label plots if enabled.","def on_pretrain_routine_end ( self , labels , names ) : if self . plots : plot_labels ( labels , names , self . save_dir ) paths = self . save_dir . glob ( ""*labels*.jpg"" ) # training labels if self . wandb : self . wandb . log ( { ""Labels"" : [ wandb . Image ( str ( x ) , caption = x . name ) for x in paths ] } ) if self . comet_logger : self . comet_logger . on_pretrain_routine_end ( paths ) if self . clearml : for path in paths : self . clearml . log_plot ( title = path . stem , plot_path = path )","Callback that runs at the end of pre-training routine, logging label plots if enabled."
/Fooocus/ldm_patched/k_diffusion/sampling.py,sample_dpm_adaptive,"def sample_dpm_adaptive(model, x, sigma_min, sigma_max, extra_args=None, callback=None, disable=None, order=3, rtol=0.05, atol=0.0078, h_init=0.05, pcoeff=0., icoeff=1., dcoeff=0., accept_safety=0.81, eta=0., s_noise=1., noise_sampler=None, return_info=False):
    """"""DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927.""""""
    if sigma_min <= 0 or sigma_max <= 0:
        raise ValueError('sigma_min and sigma_max must not be 0')
    with tqdm(disable=disable) as pbar:
        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)
        if callback is not None:
            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})
        x, info = dpm_solver.dpm_solver_adaptive(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), order, rtol, atol, h_init, pcoeff, icoeff, dcoeff, accept_safety, eta, s_noise, noise_sampler)
    if return_info:
        return x, info
    return x","def sample_dpm_adaptive(model, x, sigma_min, sigma_max, extra_args=None, callback=None, disable=None, order=3, rtol=0.05, atol=0.0078, h_init=0.05, pcoeff=0., icoeff=1., dcoeff=0., accept_safety=0.81, eta=0., s_noise=1., noise_sampler=None, return_info=False):
    if sigma_min <= 0 or sigma_max <= 0:
        raise ValueError('sigma_min and sigma_max must not be 0')
    with tqdm(disable=disable) as pbar:
        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)
        if callback is not None:
            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})
        x, info = dpm_solver.dpm_solver_adaptive(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), order, rtol, atol, h_init, pcoeff, icoeff, dcoeff, accept_safety, eta, s_noise, noise_sampler)
    if return_info:
        return x, info
    return x",DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927.,DPM-Solver-12 and 23 (adaptive step size).,"def sample_dpm_adaptive(model, x, sigma_min, sigma_max, extra_args=None, callback=None, disable=None, order=3, rtol=0.05, atol=0.0078, h_init=0.05, pcoeff=0., icoeff=1., dcoeff=0., accept_safety=0.81, eta=0., s_noise=1., noise_sampler=None, return_info=False):
    if sigma_min <= 0 or sigma_max <= 0:
        raise ValueError('sigma_min and sigma_max must not be 0')
    with tqdm(disable=disable) as pbar:
        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)
        if callback is not None:
            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})
        x, info = dpm_solver.dpm_solver_adaptive(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), order, rtol, atol, h_init, pcoeff, icoeff, dcoeff, accept_safety, eta, s_noise, noise_sampler)
    if return_info:
        return x, info
    return x",DPM-Solver-12 and 23 (adaptive step size).,"def sample_dpm_adaptive ( model , x , sigma_min , sigma_max , extra_args = None , callback = None , disable = None , order = 3 , rtol = 0.05 , atol = 0.0078 , h_init = 0.05 , pcoeff = 0. , icoeff = 1. , dcoeff = 0. , accept_safety = 0.81 , eta = 0. , s_noise = 1. , noise_sampler = None , return_info = False ) : if sigma_min <= 0 or sigma_max <= 0 : raise ValueError ( 'sigma_min and sigma_max must not be 0' ) with tqdm ( disable = disable ) as pbar : dpm_solver = DPMSolver ( model , extra_args , eps_callback = pbar . update ) if callback is not None : dpm_solver . info_callback = lambda info : callback ( { 'sigma' : dpm_solver . sigma ( info [ 't' ] ) , 'sigma_hat' : dpm_solver . sigma ( info [ 't_up' ] ) , ** info } ) x , info = dpm_solver . dpm_solver_adaptive ( x , dpm_solver . t ( torch . tensor ( sigma_max ) ) , dpm_solver . t ( torch . tensor ( sigma_min ) ) , order , rtol , atol , h_init , pcoeff , icoeff , dcoeff , accept_safety , eta , s_noise , noise_sampler ) if return_info : return x , info return x",DPM-Solver-12 and 23 (adaptive step size).
/markitdown/packages/markitdown/src/markitdown/_markitdown.py,enable_plugins,"def enable_plugins(self, **kwargs) -> None:
        """"""
        Enable and register converters provided by plugins.
        Plugins are disabled by default.
        This method should only be called once, if plugins were initially disabled.
        """"""
        if not self._plugins_enabled:
            # Load plugins
            plugins = _load_plugins()
            assert plugins is not None
            for plugin in plugins:
                try:
                    plugin.register_converters(self, **kwargs)
                except Exception:
                    tb = traceback.format_exc()
                    warn(f""Plugin '{plugin}' failed to register converters:\n{tb}"")
            self._plugins_enabled = True
        else:
            warn(""Plugins converters are already enabled."", RuntimeWarning)","def enable_plugins(self, **kwargs) -> None:
        """"""
        Enable and register converters provided by plugins.
        Plugins are disabled by default.
        This method should only be called once, if plugins were initially disabled.
        """"""
        if not self._plugins_enabled:
            # Load plugins
            plugins = _load_plugins()
            assert plugins is not None
            for plugin in plugins:
                try:
                    plugin.register_converters(self, **kwargs)
                except Exception:
                    tb = traceback.format_exc()
                    warn(f""Plugin '{plugin}' failed to register converters:\n{tb}"")
            self._plugins_enabled = True
        else:
            warn(""Plugins converters are already enabled."", RuntimeWarning)","Enable and register converters provided by plugins.
Plugins are disabled by default.
This method should only be called once, if plugins were initially disabled.",Enable and register converters provided by plugins.,"def enable_plugins(self, **kwargs) -> None:
        
        if not self._plugins_enabled:
            # Load plugins
            plugins = _load_plugins()
            assert plugins is not None
            for plugin in plugins:
                try:
                    plugin.register_converters(self, **kwargs)
                except Exception:
                    tb = traceback.format_exc()
                    warn(f""Plugin '{plugin}' failed to register converters:\n{tb}"")
            self._plugins_enabled = True
        else:
            warn(""Plugins converters are already enabled."", RuntimeWarning)",Enable and register converters provided by plugins.,"def enable_plugins ( self , ** kwargs ) -> None : if not self . _plugins_enabled : # Load plugins plugins = _load_plugins ( ) assert plugins is not None for plugin in plugins : try : plugin . register_converters ( self , ** kwargs ) except Exception : tb = traceback . format_exc ( ) warn ( f""Plugin '{plugin}' failed to register converters:\n{tb}"" ) self . _plugins_enabled = True else : warn ( ""Plugins converters are already enabled."" , RuntimeWarning )",Enable and register converters provided by plugins.
/youtube-dl/youtube_dl/jsinterp.py,assertion,"def assertion(cndn, msg):
                """""" assert, but without risk of getting optimized out """"""
                if not cndn:
                    memb = member
                    raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)","def assertion(cndn, msg):
                """""" assert, but without risk of getting optimized out """"""
                if not cndn:
                    memb = member
                    raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)","assert, but without risk of getting optimized out","assert, but without risk of getting optimized out","def assertion(cndn, msg):
                
                if not cndn:
                    memb = member
                    raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)","assert, but without risk of getting optimized out","def assertion ( cndn , msg ) : if not cndn : memb = member raise self . Exception ( '{memb} {msg}' . format ( ** locals ( ) ) , expr = expr )","assert, but without risk of getting optimized out"
/AutoGPT/classic/original_autogpt/autogpt/app/utils.py,set_env_config_value,"def set_env_config_value(key: str, value: str) -> None:
    """"""Sets the specified env variable and updates it in .env as well""""""
    os.environ[key] = value

    with ENV_FILE_PATH.open(""r+"") as file:
        lines = file.readlines()
        file.seek(0)
        key_already_in_file = False
        for line in lines:
            if re.match(rf""^(?:# )?{key}=.*$"", line):
                file.write(f""{key}={value}\n"")
                key_already_in_file = True
            else:
                file.write(line)

        if not key_already_in_file:
            file.write(f""{key}={value}\n"")

        file.truncate()","def set_env_config_value(key: str, value: str) -> None:
    """"""Sets the specified env variable and updates it in .env as well""""""
    os.environ[key] = value

    with ENV_FILE_PATH.open(""r+"") as file:
        lines = file.readlines()
        file.seek(0)
        key_already_in_file = False
        for line in lines:
            if re.match(rf""^(?:# )?{key}=.*$"", line):
                file.write(f""{key}={value}\n"")
                key_already_in_file = True
            else:
                file.write(line)

        if not key_already_in_file:
            file.write(f""{key}={value}\n"")

        file.truncate()",Sets the specified env variable and updates it in .env as well,Sets the specified env variable and updates it in .env as well,"def set_env_config_value(key: str, value: str) -> None:
    
    os.environ[key] = value

    with ENV_FILE_PATH.open(""r+"") as file:
        lines = file.readlines()
        file.seek(0)
        key_already_in_file = False
        for line in lines:
            if re.match(rf""^(?:# )?{key}=.*$"", line):
                file.write(f""{key}={value}\n"")
                key_already_in_file = True
            else:
                file.write(line)

        if not key_already_in_file:
            file.write(f""{key}={value}\n"")

        file.truncate()",Sets the specified env variable and updates it in .env as well,"def set_env_config_value ( key : str , value : str ) -> None : os . environ [ key ] = value with ENV_FILE_PATH . open ( ""r+"" ) as file : lines = file . readlines ( ) file . seek ( 0 ) key_already_in_file = False for line in lines : if re . match ( rf""^(?:# )?{key}=.*$"" , line ) : file . write ( f""{key}={value}\n"" ) key_already_in_file = True else : file . write ( line ) if not key_already_in_file : file . write ( f""{key}={value}\n"" ) file . truncate ( )",Sets the specified env variable and updates it in .env as well
/yolov5/utils/general.py,yaml_save,"def yaml_save(file=""data.yaml"", data=None):
    """"""Safely saves `data` to a YAML file specified by `file`, converting `Path` objects to strings; `data` is a
    dictionary.
    """"""
    if data is None:
        data = {}
    with open(file, ""w"") as f:
        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)","def yaml_save(file=""data.yaml"", data=None):
    """"""Safely saves `data` to a YAML file specified by `file`, converting `Path` objects to strings; `data` is a
    dictionary.
    """"""
    if data is None:
        data = {}
    with open(file, ""w"") as f:
        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)","Safely saves `data` to a YAML file specified by `file`, converting `Path` objects to strings; `data` is a
dictionary.","Safely saves `data` to a YAML file specified by `file`, converting `Path` objects to strings; `data` is a dictionary.","def yaml_save(file=""data.yaml"", data=None):
    
    if data is None:
        data = {}
    with open(file, ""w"") as f:
        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)","Safely saves `data` to a YAML file specified by `file`, converting `Path` objects to strings; `data` is a dictionary.","def yaml_save ( file = ""data.yaml"" , data = None ) : if data is None : data = { } with open ( file , ""w"" ) as f : yaml . safe_dump ( { k : str ( v ) if isinstance ( v , Path ) else v for k , v in data . items ( ) } , f , sort_keys = False )","Safely saves `data` to a YAML file specified by `file`, converting `Path` objects to strings; `data` is a dictionary."
/OpenManus/app/config.py,load_server_config,"def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        """"""Load MCP server configuration from JSON file""""""
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if not config_file:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig(
                        type=server_config[""type""],
                        url=server_config.get(""url""),
                        command=server_config.get(""command""),
                        args=server_config.get(""args"", []),
                    )
                return servers
        except Exception as e:
            raise ValueError(f""Failed to load MCP server config: {e}"")","def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        """"""Load MCP server configuration from JSON file""""""
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if not config_file:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig(
                        type=server_config[""type""],
                        url=server_config.get(""url""),
                        command=server_config.get(""command""),
                        args=server_config.get(""args"", []),
                    )
                return servers
        except Exception as e:
            raise ValueError(f""Failed to load MCP server config: {e}"")",Load MCP server configuration from JSON file,Load MCP server configuration from JSON file,"def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if not config_file:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig(
                        type=server_config[""type""],
                        url=server_config.get(""url""),
                        command=server_config.get(""command""),
                        args=server_config.get(""args"", []),
                    )
                return servers
        except Exception as e:
            raise ValueError(f""Failed to load MCP server config: {e}"")",Load MCP server configuration from JSON file,"def load_server_config ( cls ) -> Dict [ str , MCPServerConfig ] : config_path = PROJECT_ROOT / ""config"" / ""mcp.json"" try : config_file = config_path if config_path . exists ( ) else None if not config_file : return { } with config_file . open ( ) as f : data = json . load ( f ) servers = { } for server_id , server_config in data . get ( ""mcpServers"" , { } ) . items ( ) : servers [ server_id ] = MCPServerConfig ( type = server_config [ ""type"" ] , url = server_config . get ( ""url"" ) , command = server_config . get ( ""command"" ) , args = server_config . get ( ""args"" , [ ] ) , ) return servers except Exception as e : raise ValueError ( f""Failed to load MCP server config: {e}"" )",Load MCP server configuration from JSON file
/cheat.sh/lib/routing.py,get_topics_list,"def get_topics_list(self, skip_dirs=False, skip_internal=False):
        """"""
        List of topics returned on /:list
        """"""

        if self._cached_topics_list:
            return self._cached_topics_list

        skip = ['fosdem']
        if skip_dirs:
            skip.append(""cheat.sheets dir"")
        if skip_internal:
            skip.append(""internal"")
        sources_to_merge = [x for x in self._adapter if x not in skip]

        answer = {}
        for key in sources_to_merge:
            answer.update({name:key for name in self._topic_list[key]})
        answer = sorted(set(answer.keys()))

        self._cached_topics_list = answer
        return answer","def get_topics_list(self, skip_dirs=False, skip_internal=False):
        """"""
        List of topics returned on /:list
        """"""

        if self._cached_topics_list:
            return self._cached_topics_list

        skip = ['fosdem']
        if skip_dirs:
            skip.append(""cheat.sheets dir"")
        if skip_internal:
            skip.append(""internal"")
        sources_to_merge = [x for x in self._adapter if x not in skip]

        answer = {}
        for key in sources_to_merge:
            answer.update({name:key for name in self._topic_list[key]})
        answer = sorted(set(answer.keys()))

        self._cached_topics_list = answer
        return answer",List of topics returned on /:list,List of topics returned on /:list,"def get_topics_list(self, skip_dirs=False, skip_internal=False):
        

        if self._cached_topics_list:
            return self._cached_topics_list

        skip = ['fosdem']
        if skip_dirs:
            skip.append(""cheat.sheets dir"")
        if skip_internal:
            skip.append(""internal"")
        sources_to_merge = [x for x in self._adapter if x not in skip]

        answer = {}
        for key in sources_to_merge:
            answer.update({name:key for name in self._topic_list[key]})
        answer = sorted(set(answer.keys()))

        self._cached_topics_list = answer
        return answer",List of topics returned on /:list,"def get_topics_list ( self , skip_dirs = False , skip_internal = False ) : if self . _cached_topics_list : return self . _cached_topics_list skip = [ 'fosdem' ] if skip_dirs : skip . append ( ""cheat.sheets dir"" ) if skip_internal : skip . append ( ""internal"" ) sources_to_merge = [ x for x in self . _adapter if x not in skip ] answer = { } for key in sources_to_merge : answer . update ( { name : key for name in self . _topic_list [ key ] } ) answer = sorted ( set ( answer . keys ( ) ) ) self . _cached_topics_list = answer return answer",List of topics returned on /:list
/ultralytics/ultralytics/solutions/object_counter.py,__init__,"def __init__(self, **kwargs):
        """"""Initialize the ObjectCounter class for real-time object counting in video streams.""""""
        super().__init__(**kwargs)

        self.in_count = 0  # Counter for objects moving inward
        self.out_count = 0  # Counter for objects moving outward
        self.counted_ids = []  # List of IDs of objects that have been counted
        self.classwise_counts = defaultdict(lambda: {""IN"": 0, ""OUT"": 0})  # Dictionary for counts, categorized by class
        self.region_initialized = False  # Flag indicating whether the region has been initialized

        self.show_in = self.CFG[""show_in""]
        self.show_out = self.CFG[""show_out""]
        self.margin = self.line_width * 2","def __init__(self, **kwargs):
        """"""Initialize the ObjectCounter class for real-time object counting in video streams.""""""
        super().__init__(**kwargs)

        self.in_count = 0  # Counter for objects moving inward
        self.out_count = 0  # Counter for objects moving outward
        self.counted_ids = []  # List of IDs of objects that have been counted
        self.classwise_counts = defaultdict(lambda: {""IN"": 0, ""OUT"": 0})  # Dictionary for counts, categorized by class
        self.region_initialized = False  # Flag indicating whether the region has been initialized

        self.show_in = self.CFG[""show_in""]
        self.show_out = self.CFG[""show_out""]
        self.margin = self.line_width * 2",Initialize the ObjectCounter class for real-time object counting in video streams.,Initialize the ObjectCounter class for real-time object counting in video streams.,"def __init__(self, **kwargs):
        
        super().__init__(**kwargs)

        self.in_count = 0  # Counter for objects moving inward
        self.out_count = 0  # Counter for objects moving outward
        self.counted_ids = []  # List of IDs of objects that have been counted
        self.classwise_counts = defaultdict(lambda: {""IN"": 0, ""OUT"": 0})  # Dictionary for counts, categorized by class
        self.region_initialized = False  # Flag indicating whether the region has been initialized

        self.show_in = self.CFG[""show_in""]
        self.show_out = self.CFG[""show_out""]
        self.margin = self.line_width * 2",Initialize the ObjectCounter class for real-time object counting in video streams.,"def __init__ ( self , ** kwargs ) : super ( ) . __init__ ( ** kwargs ) self . in_count = 0 # Counter for objects moving inward self . out_count = 0 # Counter for objects moving outward self . counted_ids = [ ] # List of IDs of objects that have been counted self . classwise_counts = defaultdict ( lambda : { ""IN"" : 0 , ""OUT"" : 0 } ) # Dictionary for counts, categorized by class self . region_initialized = False # Flag indicating whether the region has been initialized self . show_in = self . CFG [ ""show_in"" ] self . show_out = self . CFG [ ""show_out"" ] self . margin = self . line_width * 2",Initialize the ObjectCounter class for real-time object counting in video streams.
/ultralytics/ultralytics/hub/session.py,_should_retry,"def _should_retry(status_code: int) -> bool:
        """"""Determine if a request should be retried based on the HTTP status code.""""""
        retry_codes = {
            HTTPStatus.REQUEST_TIMEOUT,
            HTTPStatus.BAD_GATEWAY,
            HTTPStatus.GATEWAY_TIMEOUT,
        }
        return status_code in retry_codes","def _should_retry(status_code: int) -> bool:
        """"""Determine if a request should be retried based on the HTTP status code.""""""
        retry_codes = {
            HTTPStatus.REQUEST_TIMEOUT,
            HTTPStatus.BAD_GATEWAY,
            HTTPStatus.GATEWAY_TIMEOUT,
        }
        return status_code in retry_codes",Determine if a request should be retried based on the HTTP status code.,Determine if a request should be retried based on the HTTP status code.,"def _should_retry(status_code: int) -> bool:
        
        retry_codes = {
            HTTPStatus.REQUEST_TIMEOUT,
            HTTPStatus.BAD_GATEWAY,
            HTTPStatus.GATEWAY_TIMEOUT,
        }
        return status_code in retry_codes",Determine if a request should be retried based on the HTTP status code.,"def _should_retry ( status_code : int ) -> bool : retry_codes = { HTTPStatus . REQUEST_TIMEOUT , HTTPStatus . BAD_GATEWAY , HTTPStatus . GATEWAY_TIMEOUT , } return status_code in retry_codes",Determine if a request should be retried based on the HTTP status code.
/MetaGPT/metagpt/schema.py,load,"def load(data) -> ""MessageQueue"":
        """"""Convert the json string to the `MessageQueue` object.""""""
        queue = MessageQueue()
        try:
            lst = json.loads(data)
            for i in lst:
                msg = Message.load(i)
                queue.push(msg)
        except JSONDecodeError as e:
            logger.warning(f""JSON load failed: {data}, error:{e}"")

        return queue","def load(data) -> ""MessageQueue"":
        """"""Convert the json string to the `MessageQueue` object.""""""
        queue = MessageQueue()
        try:
            lst = json.loads(data)
            for i in lst:
                msg = Message.load(i)
                queue.push(msg)
        except JSONDecodeError as e:
            logger.warning(f""JSON load failed: {data}, error:{e}"")

        return queue",Convert the json string to the `MessageQueue` object.,Convert the json string to the `MessageQueue` object.,"def load(data) -> ""MessageQueue"":
        
        queue = MessageQueue()
        try:
            lst = json.loads(data)
            for i in lst:
                msg = Message.load(i)
                queue.push(msg)
        except JSONDecodeError as e:
            logger.warning(f""JSON load failed: {data}, error:{e}"")

        return queue",Convert the json string to the `MessageQueue` object.,"def load ( data ) -> ""MessageQueue"" : queue = MessageQueue ( ) try : lst = json . loads ( data ) for i in lst : msg = Message . load ( i ) queue . push ( msg ) except JSONDecodeError as e : logger . warning ( f""JSON load failed: {data}, error:{e}"" ) return queue",Convert the json string to the `MessageQueue` object.
/yolov5/utils/autoanchor.py,print_results,"def print_results(k, verbose=True):
        """"""Sorts and logs kmeans-evolved anchor metrics and best possible recall values for YOLOv5 anchor evaluation.""""""
        k = k[np.argsort(k.prod(1))]  # sort small to large
        x, best = metric(k, wh0)
        bpr, aat = (best > thr).float().mean(), (x > thr).float().mean() * n  # best possible recall, anch > thr
        s = (
            f""{PREFIX}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr\n""
            f""{PREFIX}n={n}, img_size={img_size}, metric_all={x.mean():.3f}/{best.mean():.3f}-mean/best, ""
            f""past_thr={x[x > thr].mean():.3f}-mean: ""
        )
        for x in k:
            s += ""%i,%i, "" % (round(x[0]), round(x[1]))
        if verbose:
            LOGGER.info(s[:-2])
        return k","def print_results(k, verbose=True):
        """"""Sorts and logs kmeans-evolved anchor metrics and best possible recall values for YOLOv5 anchor evaluation.""""""
        k = k[np.argsort(k.prod(1))]  # sort small to large
        x, best = metric(k, wh0)
        bpr, aat = (best > thr).float().mean(), (x > thr).float().mean() * n  # best possible recall, anch > thr
        s = (
            f""{PREFIX}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr\n""
            f""{PREFIX}n={n}, img_size={img_size}, metric_all={x.mean():.3f}/{best.mean():.3f}-mean/best, ""
            f""past_thr={x[x > thr].mean():.3f}-mean: ""
        )
        for x in k:
            s += ""%i,%i, "" % (round(x[0]), round(x[1]))
        if verbose:
            LOGGER.info(s[:-2])
        return k",Sorts and logs kmeans-evolved anchor metrics and best possible recall values for YOLOv5 anchor evaluation.,Sorts and logs kmeans-evolved anchor metrics and best possible recall values for YOLOv5 anchor evaluation.,"def print_results(k, verbose=True):
        
        k = k[np.argsort(k.prod(1))]  # sort small to large
        x, best = metric(k, wh0)
        bpr, aat = (best > thr).float().mean(), (x > thr).float().mean() * n  # best possible recall, anch > thr
        s = (
            f""{PREFIX}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr\n""
            f""{PREFIX}n={n}, img_size={img_size}, metric_all={x.mean():.3f}/{best.mean():.3f}-mean/best, ""
            f""past_thr={x[x > thr].mean():.3f}-mean: ""
        )
        for x in k:
            s += ""%i,%i, "" % (round(x[0]), round(x[1]))
        if verbose:
            LOGGER.info(s[:-2])
        return k",Sorts and logs kmeans-evolved anchor metrics and best possible recall values for YOLOv5 anchor evaluation.,"def print_results ( k , verbose = True ) : k = k [ np . argsort ( k . prod ( 1 ) ) ] # sort small to large x , best = metric ( k , wh0 ) bpr , aat = ( best > thr ) . float ( ) . mean ( ) , ( x > thr ) . float ( ) . mean ( ) * n # best possible recall, anch > thr s = ( f""{PREFIX}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr\n"" f""{PREFIX}n={n}, img_size={img_size}, metric_all={x.mean():.3f}/{best.mean():.3f}-mean/best, "" f""past_thr={x[x > thr].mean():.3f}-mean: "" ) for x in k : s += ""%i,%i, "" % ( round ( x [ 0 ] ) , round ( x [ 1 ] ) ) if verbose : LOGGER . info ( s [ : - 2 ] ) return k",Sorts and logs kmeans-evolved anchor metrics and best possible recall values for YOLOv5 anchor evaluation.
/yolov5/models/tf.py,__init__,"def __init__(self, cfg=""yolov5s.yaml"", ch=3, nc=None, model=None, imgsz=(640, 640)):
        """"""Initializes TF YOLOv5 model with specified configuration, channels, classes, model instance, and input
        size.
        """"""
        super().__init__()
        if isinstance(cfg, dict):
            self.yaml = cfg  # model dict
        else:  # is *.yaml
            import yaml  # for torch hub

            self.yaml_file = Path(cfg).name
            with open(cfg) as f:
                self.yaml = yaml.load(f, Loader=yaml.FullLoader)  # model dict

        # Define model
        if nc and nc != self.yaml[""nc""]:
            LOGGER.info(f""Overriding {cfg} nc={self.yaml['nc']} with nc={nc}"")
            self.yaml[""nc""] = nc  # override yaml value
        self.model, self.savelist = parse_model(deepcopy(self.yaml), ch=[ch], model=model, imgsz=imgsz)","def __init__(self, cfg=""yolov5s.yaml"", ch=3, nc=None, model=None, imgsz=(640, 640)):
        """"""Initializes TF YOLOv5 model with specified configuration, channels, classes, model instance, and input
        size.
        """"""
        super().__init__()
        if isinstance(cfg, dict):
            self.yaml = cfg  # model dict
        else:  # is *.yaml
            import yaml  # for torch hub

            self.yaml_file = Path(cfg).name
            with open(cfg) as f:
                self.yaml = yaml.load(f, Loader=yaml.FullLoader)  # model dict

        # Define model
        if nc and nc != self.yaml[""nc""]:
            LOGGER.info(f""Overriding {cfg} nc={self.yaml['nc']} with nc={nc}"")
            self.yaml[""nc""] = nc  # override yaml value
        self.model, self.savelist = parse_model(deepcopy(self.yaml), ch=[ch], model=model, imgsz=imgsz)","Initializes TF YOLOv5 model with specified configuration, channels, classes, model instance, and input
size.","Initializes TF YOLOv5 model with specified configuration, channels, classes, model instance, and input size.","def __init__(self, cfg=""yolov5s.yaml"", ch=3, nc=None, model=None, imgsz=(640, 640)):
        
        super().__init__()
        if isinstance(cfg, dict):
            self.yaml = cfg  # model dict
        else:  # is *.yaml
            import yaml  # for torch hub

            self.yaml_file = Path(cfg).name
            with open(cfg) as f:
                self.yaml = yaml.load(f, Loader=yaml.FullLoader)  # model dict

        # Define model
        if nc and nc != self.yaml[""nc""]:
            LOGGER.info(f""Overriding {cfg} nc={self.yaml['nc']} with nc={nc}"")
            self.yaml[""nc""] = nc  # override yaml value
        self.model, self.savelist = parse_model(deepcopy(self.yaml), ch=[ch], model=model, imgsz=imgsz)","Initializes TF YOLOv5 model with specified configuration, channels, classes, model instance, and input size.","def __init__ ( self , cfg = ""yolov5s.yaml"" , ch = 3 , nc = None , model = None , imgsz = ( 640 , 640 ) ) : super ( ) . __init__ ( ) if isinstance ( cfg , dict ) : self . yaml = cfg # model dict else : # is *.yaml import yaml # for torch hub self . yaml_file = Path ( cfg ) . name with open ( cfg ) as f : self . yaml = yaml . load ( f , Loader = yaml . FullLoader ) # model dict # Define model if nc and nc != self . yaml [ ""nc"" ] : LOGGER . info ( f""Overriding {cfg} nc={self.yaml['nc']} with nc={nc}"" ) self . yaml [ ""nc"" ] = nc # override yaml value self . model , self . savelist = parse_model ( deepcopy ( self . yaml ) , ch = [ ch ] , model = model , imgsz = imgsz )","Initializes TF YOLOv5 model with specified configuration, channels, classes, model instance, and input size."
/yolov5/utils/general.py,check_imshow,"def check_imshow(warn=False):
    """"""Checks environment support for image display; warns on failure if `warn=True`.""""""
    try:
        assert not is_jupyter()
        assert not is_docker()
        cv2.imshow(""test"", np.zeros((1, 1, 3)))
        cv2.waitKey(1)
        cv2.destroyAllWindows()
        cv2.waitKey(1)
        return True
    except Exception as e:
        if warn:
            LOGGER.warning(f""WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n{e}"")
        return False","def check_imshow(warn=False):
    """"""Checks environment support for image display; warns on failure if `warn=True`.""""""
    try:
        assert not is_jupyter()
        assert not is_docker()
        cv2.imshow(""test"", np.zeros((1, 1, 3)))
        cv2.waitKey(1)
        cv2.destroyAllWindows()
        cv2.waitKey(1)
        return True
    except Exception as e:
        if warn:
            LOGGER.warning(f""WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n{e}"")
        return False",Checks environment support for image display; warns on failure if `warn=True`.,Checks environment support for image display; warns on failure if `warn=True`.,"def check_imshow(warn=False):
    
    try:
        assert not is_jupyter()
        assert not is_docker()
        cv2.imshow(""test"", np.zeros((1, 1, 3)))
        cv2.waitKey(1)
        cv2.destroyAllWindows()
        cv2.waitKey(1)
        return True
    except Exception as e:
        if warn:
            LOGGER.warning(f""WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n{e}"")
        return False",Checks environment support for image display; warns on failure if `warn=True`.,"def check_imshow ( warn = False ) : try : assert not is_jupyter ( ) assert not is_docker ( ) cv2 . imshow ( ""test"" , np . zeros ( ( 1 , 1 , 3 ) ) ) cv2 . waitKey ( 1 ) cv2 . destroyAllWindows ( ) cv2 . waitKey ( 1 ) return True except Exception as e : if warn : LOGGER . warning ( f""WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n{e}"" ) return False",Checks environment support for image display; warns on failure if `warn=True`.
/odoo/odoo/models.py,action_unarchive,"def action_unarchive(self):
        """"""Sets :attr:`active` to ``True`` on a recordset, by calling
        :meth:`toggle_active` on its currently inactive records.
        """"""
        assert self._active_name, f""No 'active' field on model {self._name}""
        return self.filtered(lambda record: not record[self._active_name]).toggle_active()","def action_unarchive(self):
        """"""Sets :attr:`active` to ``True`` on a recordset, by calling
        :meth:`toggle_active` on its currently inactive records.
        """"""
        assert self._active_name, f""No 'active' field on model {self._name}""
        return self.filtered(lambda record: not record[self._active_name]).toggle_active()","Sets :attr:`active` to ``True`` on a recordset, by calling
:meth:`toggle_active` on its currently inactive records.","Sets :attr:`active` to ``True`` on a recordset, by calling :meth:`toggle_active` on its currently inactive records.","def action_unarchive(self):
        
        assert self._active_name, f""No 'active' field on model {self._name}""
        return self.filtered(lambda record: not record[self._active_name]).toggle_active()","Sets :attr:`active` to ``True`` on a recordset, by calling :meth:`toggle_active` on its currently inactive records.","def action_unarchive ( self ) : assert self . _active_name , f""No 'active' field on model {self._name}"" return self . filtered ( lambda record : not record [ self . _active_name ] ) . toggle_active ( )","Sets :attr:`active` to ``True`` on a recordset, by calling :meth:`toggle_active` on its currently inactive records."
/cpython/Tools/wasm/emscripten/__main__.py,configure_build_python,"def configure_build_python(context, working_dir):
    """"""Configure the build/host Python.""""""
    if LOCAL_SETUP.exists():
        print(f""👍 {LOCAL_SETUP} exists ..."")
    else:
        print(f""📝 Touching {LOCAL_SETUP} ..."")
        LOCAL_SETUP.write_bytes(LOCAL_SETUP_MARKER)

    configure = [os.path.relpath(CHECKOUT / ""configure"", working_dir)]
    if context.args:
        configure.extend(context.args)

    call(configure, quiet=context.quiet)","def configure_build_python(context, working_dir):
    """"""Configure the build/host Python.""""""
    if LOCAL_SETUP.exists():
        print(f""👍 {LOCAL_SETUP} exists ..."")
    else:
        print(f""📝 Touching {LOCAL_SETUP} ..."")
        LOCAL_SETUP.write_bytes(LOCAL_SETUP_MARKER)

    configure = [os.path.relpath(CHECKOUT / ""configure"", working_dir)]
    if context.args:
        configure.extend(context.args)

    call(configure, quiet=context.quiet)",Configure the build/host Python.,Configure the build/host Python.,"def configure_build_python(context, working_dir):
    
    if LOCAL_SETUP.exists():
        print(f""👍 {LOCAL_SETUP} exists ..."")
    else:
        print(f""📝 Touching {LOCAL_SETUP} ..."")
        LOCAL_SETUP.write_bytes(LOCAL_SETUP_MARKER)

    configure = [os.path.relpath(CHECKOUT / ""configure"", working_dir)]
    if context.args:
        configure.extend(context.args)

    call(configure, quiet=context.quiet)",Configure the build/host Python.,"def configure_build_python ( context , working_dir ) : if LOCAL_SETUP . exists ( ) : print ( f""👍 {LOCAL_SETUP} exists ..."" ) else : print ( f""📝 Touching {LOCAL_SETUP} ..."" ) LOCAL_SETUP . write_bytes ( LOCAL_SETUP_MARKER ) configure = [ os . path . relpath ( CHECKOUT / ""configure"" , working_dir ) ] if context . args : configure . extend ( context . args ) call ( configure , quiet = context . quiet )",Configure the build/host Python.
/yolov5/utils/augmentations.py,classify_transforms,"def classify_transforms(size=224):
    """"""Applies a series of transformations including center crop, ToTensor, and normalization for classification.""""""
    assert isinstance(size, int), f""ERROR: classify_transforms size {size} must be integer, not (list, tuple)""
    # T.Compose([T.ToTensor(), T.Resize(size), T.CenterCrop(size), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])
    return T.Compose([CenterCrop(size), ToTensor(), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])","def classify_transforms(size=224):
    """"""Applies a series of transformations including center crop, ToTensor, and normalization for classification.""""""
    assert isinstance(size, int), f""ERROR: classify_transforms size {size} must be integer, not (list, tuple)""
    # T.Compose([T.ToTensor(), T.Resize(size), T.CenterCrop(size), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])
    return T.Compose([CenterCrop(size), ToTensor(), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])","Applies a series of transformations including center crop, ToTensor, and normalization for classification.","Applies a series of transformations including center crop, ToTensor, and normalization for classification.","def classify_transforms(size=224):
    
    assert isinstance(size, int), f""ERROR: classify_transforms size {size} must be integer, not (list, tuple)""
    # T.Compose([T.ToTensor(), T.Resize(size), T.CenterCrop(size), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])
    return T.Compose([CenterCrop(size), ToTensor(), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])","Applies a series of transformations including center crop, ToTensor, and normalization for classification.","def classify_transforms ( size = 224 ) : assert isinstance ( size , int ) , f""ERROR: classify_transforms size {size} must be integer, not (list, tuple)"" # T.Compose([T.ToTensor(), T.Resize(size), T.CenterCrop(size), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)]) return T . Compose ( [ CenterCrop ( size ) , ToTensor ( ) , T . Normalize ( IMAGENET_MEAN , IMAGENET_STD ) ] )","Applies a series of transformations including center crop, ToTensor, and normalization for classification."
/flask/src/flask/cli.py,show_server_banner,"def show_server_banner(debug: bool, app_import_path: str | None) -> None:
    """"""Show extra startup messages the first time the server is run,
    ignoring the reloader.
    """"""
    if is_running_from_reloader():
        return

    if app_import_path is not None:
        click.echo(f"" * Serving Flask app '{app_import_path}'"")

    if debug is not None:
        click.echo(f"" * Debug mode: {'on' if debug else 'off'}"")","def show_server_banner(debug: bool, app_import_path: str | None) -> None:
    """"""Show extra startup messages the first time the server is run,
    ignoring the reloader.
    """"""
    if is_running_from_reloader():
        return

    if app_import_path is not None:
        click.echo(f"" * Serving Flask app '{app_import_path}'"")

    if debug is not None:
        click.echo(f"" * Debug mode: {'on' if debug else 'off'}"")","Show extra startup messages the first time the server is run,
ignoring the reloader.","Show extra startup messages the first time the server is run, ignoring the reloader.","def show_server_banner(debug: bool, app_import_path: str | None) -> None:
    
    if is_running_from_reloader():
        return

    if app_import_path is not None:
        click.echo(f"" * Serving Flask app '{app_import_path}'"")

    if debug is not None:
        click.echo(f"" * Debug mode: {'on' if debug else 'off'}"")","Show extra startup messages the first time the server is run, ignoring the reloader.","def show_server_banner ( debug : bool , app_import_path : str | None ) -> None : if is_running_from_reloader ( ) : return if app_import_path is not None : click . echo ( f"" * Serving Flask app '{app_import_path}'"" ) if debug is not None : click . echo ( f"" * Debug mode: {'on' if debug else 'off'}"" )","Show extra startup messages the first time the server is run, ignoring the reloader."
/text-generation-webui/extensions/Training_PRO/script.py,clean_path,"def clean_path(base_path: str, path: str):
    """"""Strips unusual symbols and forcibly builds a path as relative to the intended directory.""""""
    path = path.replace('\\', '/').replace('..', '_')
    if base_path is None:
        return path

    return f'{Path(base_path).absolute()}/{path}'","def clean_path(base_path: str, path: str):
    """"""Strips unusual symbols and forcibly builds a path as relative to the intended directory.""""""
    path = path.replace('\\', '/').replace('..', '_')
    if base_path is None:
        return path

    return f'{Path(base_path).absolute()}/{path}'",Strips unusual symbols and forcibly builds a path as relative to the intended directory.,Strips unusual symbols and forcibly builds a path as relative to the intended directory.,"def clean_path(base_path: str, path: str):
    
    path = path.replace('\\', '/').replace('..', '_')
    if base_path is None:
        return path

    return f'{Path(base_path).absolute()}/{path}'",Strips unusual symbols and forcibly builds a path as relative to the intended directory.,"def clean_path ( base_path : str , path : str ) : path = path . replace ( '\\' , '/' ) . replace ( '..' , '_' ) if base_path is None : return path return f'{Path(base_path).absolute()}/{path}'",Strips unusual symbols and forcibly builds a path as relative to the intended directory.
/localstack/localstack-core/localstack/testing/pytest/fixtures.py,setup_sender_email_address,"def setup_sender_email_address(ses_verify_identity):
    """"""
    If the test is running against AWS then assume the email address passed is already
    verified, and passes the given email address through. Otherwise, it generates one random
    email address and verify them.
    """"""

    def inner(sender_email_address: Optional[str] = None) -> str:
        if is_aws_cloud():
            if sender_email_address is None:
                raise ValueError(
                    ""sender_email_address must be specified to run this test against AWS""
                )
        else:
            # overwrite the given parameters with localstack specific ones
            sender_email_address = f""sender-{short_uid()}@example.com""
            ses_verify_identity(sender_email_address)

        return sender_email_address

    return inner","def setup_sender_email_address(ses_verify_identity):
    """"""
    If the test is running against AWS then assume the email address passed is already
    verified, and passes the given email address through. Otherwise, it generates one random
    email address and verify them.
    """"""

    def inner(sender_email_address: Optional[str] = None) -> str:
        if is_aws_cloud():
            if sender_email_address is None:
                raise ValueError(
                    ""sender_email_address must be specified to run this test against AWS""
                )
        else:
            # overwrite the given parameters with localstack specific ones
            sender_email_address = f""sender-{short_uid()}@example.com""
            ses_verify_identity(sender_email_address)

        return sender_email_address

    return inner","If the test is running against AWS then assume the email address passed is already
verified, and passes the given email address through. Otherwise, it generates one random
email address and verify them.","If the test is running against AWS then assume the email address passed is already verified, and passes the given email address through.","def setup_sender_email_address(ses_verify_identity):
    

    def inner(sender_email_address: Optional[str] = None) -> str:
        if is_aws_cloud():
            if sender_email_address is None:
                raise ValueError(
                    ""sender_email_address must be specified to run this test against AWS""
                )
        else:
            # overwrite the given parameters with localstack specific ones
            sender_email_address = f""sender-{short_uid()}@example.com""
            ses_verify_identity(sender_email_address)

        return sender_email_address

    return inner","If the test is running against AWS then assume the email address passed is already verified, and passes the given email address through.","def setup_sender_email_address ( ses_verify_identity ) : def inner ( sender_email_address : Optional [ str ] = None ) -> str : if is_aws_cloud ( ) : if sender_email_address is None : raise ValueError ( ""sender_email_address must be specified to run this test against AWS"" ) else : # overwrite the given parameters with localstack specific ones sender_email_address = f""sender-{short_uid()}@example.com"" ses_verify_identity ( sender_email_address ) return sender_email_address return inner","If the test is running against AWS then assume the email address passed is already verified, and passes the given email address through."
/cpython/Tools/build/stable_abi.py,dump,"def dump(self):
        """"""Yield lines to recreate the manifest file (sans comments/newlines)""""""
        for item in self.contents.values():
            fields = dataclasses.fields(item)
            yield f""[{item.kind}.{item.name}]""
            for field in fields:
                if field.name in {'name', 'value', 'kind'}:
                    continue
                value = getattr(item, field.name)
                if value == field.default:
                    pass
                elif value is True:
                    yield f""    {field.name} = true""
                elif value:
                    yield f""    {field.name} = {value!r}""","def dump(self):
        """"""Yield lines to recreate the manifest file (sans comments/newlines)""""""
        for item in self.contents.values():
            fields = dataclasses.fields(item)
            yield f""[{item.kind}.{item.name}]""
            for field in fields:
                if field.name in {'name', 'value', 'kind'}:
                    continue
                value = getattr(item, field.name)
                if value == field.default:
                    pass
                elif value is True:
                    yield f""    {field.name} = true""
                elif value:
                    yield f""    {field.name} = {value!r}""",Yield lines to recreate the manifest file (sans comments/newlines),Yield lines to recreate the manifest file (sans comments/newlines),"def dump(self):
        
        for item in self.contents.values():
            fields = dataclasses.fields(item)
            yield f""[{item.kind}.{item.name}]""
            for field in fields:
                if field.name in {'name', 'value', 'kind'}:
                    continue
                value = getattr(item, field.name)
                if value == field.default:
                    pass
                elif value is True:
                    yield f""    {field.name} = true""
                elif value:
                    yield f""    {field.name} = {value!r}""",Yield lines to recreate the manifest file (sans comments/newlines),"def dump ( self ) : for item in self . contents . values ( ) : fields = dataclasses . fields ( item ) yield f""[{item.kind}.{item.name}]"" for field in fields : if field . name in { 'name' , 'value' , 'kind' } : continue value = getattr ( item , field . name ) if value == field . default : pass elif value is True : yield f""    {field.name} = true"" elif value : yield f""    {field.name} = {value!r}""",Yield lines to recreate the manifest file (sans comments/newlines)
/devops-exercises/coding/python/binary_search.py,main,"def main():
    """"""
    Executes the binary search algorithm with a randomly generated list.
    Time Complexity: O(log n)
    """"""
    rand_num_li = generate_random_list()
    target = random.randint(1, 50)
    index = find_target_in_list(target, rand_num_li)
    print(f""List: {rand_num_li}\nTarget: {target}\nIndex: {index}"")","def main():
    """"""
    Executes the binary search algorithm with a randomly generated list.
    Time Complexity: O(log n)
    """"""
    rand_num_li = generate_random_list()
    target = random.randint(1, 50)
    index = find_target_in_list(target, rand_num_li)
    print(f""List: {rand_num_li}\nTarget: {target}\nIndex: {index}"")","Executes the binary search algorithm with a randomly generated list.
Time Complexity: O(log n)",Executes the binary search algorithm with a randomly generated list.,"def main():
    
    rand_num_li = generate_random_list()
    target = random.randint(1, 50)
    index = find_target_in_list(target, rand_num_li)
    print(f""List: {rand_num_li}\nTarget: {target}\nIndex: {index}"")",Executes the binary search algorithm with a randomly generated list.,"def main ( ) : rand_num_li = generate_random_list ( ) target = random . randint ( 1 , 50 ) index = find_target_in_list ( target , rand_num_li ) print ( f""List: {rand_num_li}\nTarget: {target}\nIndex: {index}"" )",Executes the binary search algorithm with a randomly generated list.
/ansible/packaging/cli-doc/build.py,build_rst,"def build_rst(output_dir: pathlib.Path, template_file: pathlib.Path) -> None:
    """"""Build RST documentation for ansible-core CLI programs.""""""
    if not template_file.resolve().is_relative_to(SCRIPT_DIR):
        warnings.warn(""Custom templates are intended for debugging purposes only. The data model may change in future releases without notice."")

    output_dir.mkdir(exist_ok=True, parents=True)

    for cli_name, source in generate_rst(template_file).items():
        (output_dir / f'{cli_name}.rst').write_text(source)","def build_rst(output_dir: pathlib.Path, template_file: pathlib.Path) -> None:
    """"""Build RST documentation for ansible-core CLI programs.""""""
    if not template_file.resolve().is_relative_to(SCRIPT_DIR):
        warnings.warn(""Custom templates are intended for debugging purposes only. The data model may change in future releases without notice."")

    output_dir.mkdir(exist_ok=True, parents=True)

    for cli_name, source in generate_rst(template_file).items():
        (output_dir / f'{cli_name}.rst').write_text(source)",Build RST documentation for ansible-core CLI programs.,Build RST documentation for ansible-core CLI programs.,"def build_rst(output_dir: pathlib.Path, template_file: pathlib.Path) -> None:
    
    if not template_file.resolve().is_relative_to(SCRIPT_DIR):
        warnings.warn(""Custom templates are intended for debugging purposes only. The data model may change in future releases without notice."")

    output_dir.mkdir(exist_ok=True, parents=True)

    for cli_name, source in generate_rst(template_file).items():
        (output_dir / f'{cli_name}.rst').write_text(source)",Build RST documentation for ansible-core CLI programs.,"def build_rst ( output_dir : pathlib . Path , template_file : pathlib . Path ) -> None : if not template_file . resolve ( ) . is_relative_to ( SCRIPT_DIR ) : warnings . warn ( ""Custom templates are intended for debugging purposes only. The data model may change in future releases without notice."" ) output_dir . mkdir ( exist_ok = True , parents = True ) for cli_name , source in generate_rst ( template_file ) . items ( ) : ( output_dir / f'{cli_name}.rst' ) . write_text ( source )",Build RST documentation for ansible-core CLI programs.
/yolov5/utils/general.py,user_config_dir,"def user_config_dir(dir=""Ultralytics"", env_var=""YOLOV5_CONFIG_DIR""):
    """"""Returns user configuration directory path, preferring environment variable `YOLOV5_CONFIG_DIR` if set, else OS-
    specific.
    """"""
    if env := os.getenv(env_var):
        path = Path(env)  # use environment variable
    else:
        cfg = {""Windows"": ""AppData/Roaming"", ""Linux"": "".config"", ""Darwin"": ""Library/Application Support""}  # 3 OS dirs
        path = Path.home() / cfg.get(platform.system(), """")  # OS-specific config dir
        path = (path if is_writeable(path) else Path(""/tmp"")) / dir  # GCP and AWS lambda fix, only /tmp is writeable
    path.mkdir(exist_ok=True)  # make if required
    return path","def user_config_dir(dir=""Ultralytics"", env_var=""YOLOV5_CONFIG_DIR""):
    """"""Returns user configuration directory path, preferring environment variable `YOLOV5_CONFIG_DIR` if set, else OS-
    specific.
    """"""
    if env := os.getenv(env_var):
        path = Path(env)  # use environment variable
    else:
        cfg = {""Windows"": ""AppData/Roaming"", ""Linux"": "".config"", ""Darwin"": ""Library/Application Support""}  # 3 OS dirs
        path = Path.home() / cfg.get(platform.system(), """")  # OS-specific config dir
        path = (path if is_writeable(path) else Path(""/tmp"")) / dir  # GCP and AWS lambda fix, only /tmp is writeable
    path.mkdir(exist_ok=True)  # make if required
    return path","Returns user configuration directory path, preferring environment variable `YOLOV5_CONFIG_DIR` if set, else OS-
specific.","Returns user configuration directory path, preferring environment variable `YOLOV5_CONFIG_DIR` if set, else OS- specific.","def user_config_dir(dir=""Ultralytics"", env_var=""YOLOV5_CONFIG_DIR""):
    
    if env := os.getenv(env_var):
        path = Path(env)  # use environment variable
    else:
        cfg = {""Windows"": ""AppData/Roaming"", ""Linux"": "".config"", ""Darwin"": ""Library/Application Support""}  # 3 OS dirs
        path = Path.home() / cfg.get(platform.system(), """")  # OS-specific config dir
        path = (path if is_writeable(path) else Path(""/tmp"")) / dir  # GCP and AWS lambda fix, only /tmp is writeable
    path.mkdir(exist_ok=True)  # make if required
    return path","Returns user configuration directory path, preferring environment variable `YOLOV5_CONFIG_DIR` if set, else OS- specific.","def user_config_dir ( dir = ""Ultralytics"" , env_var = ""YOLOV5_CONFIG_DIR"" ) : if env := os . getenv ( env_var ) : path = Path ( env ) # use environment variable else : cfg = { ""Windows"" : ""AppData/Roaming"" , ""Linux"" : "".config"" , ""Darwin"" : ""Library/Application Support"" } # 3 OS dirs path = Path . home ( ) / cfg . get ( platform . system ( ) , """" ) # OS-specific config dir path = ( path if is_writeable ( path ) else Path ( ""/tmp"" ) ) / dir # GCP and AWS lambda fix, only /tmp is writeable path . mkdir ( exist_ok = True ) # make if required return path","Returns user configuration directory path, preferring environment variable `YOLOV5_CONFIG_DIR` if set, else OS- specific."
/open-interpreter/interpreter/core/utils/temporary_file.py,cleanup_temporary_file,"def cleanup_temporary_file(temp_file_name, verbose=False):
    """"""
    clean up temporary file
    """"""

    try:
        # clean up temporary file
        os.remove(temp_file_name)

        if verbose:
            print(f""Cleaning up temporary file {temp_file_name}"")
            print(""---"")

    except Exception as e:
        print(f""Could not clean up temporary file."")
        print(e)
        print("""")","def cleanup_temporary_file(temp_file_name, verbose=False):
    """"""
    clean up temporary file
    """"""

    try:
        # clean up temporary file
        os.remove(temp_file_name)

        if verbose:
            print(f""Cleaning up temporary file {temp_file_name}"")
            print(""---"")

    except Exception as e:
        print(f""Could not clean up temporary file."")
        print(e)
        print("""")",clean up temporary file,clean up temporary file,"def cleanup_temporary_file(temp_file_name, verbose=False):
    

    try:
        # clean up temporary file
        os.remove(temp_file_name)

        if verbose:
            print(f""Cleaning up temporary file {temp_file_name}"")
            print(""---"")

    except Exception as e:
        print(f""Could not clean up temporary file."")
        print(e)
        print("""")",clean up temporary file,"def cleanup_temporary_file ( temp_file_name , verbose = False ) : try : # clean up temporary file os . remove ( temp_file_name ) if verbose : print ( f""Cleaning up temporary file {temp_file_name}"" ) print ( ""---"" ) except Exception as e : print ( f""Could not clean up temporary file."" ) print ( e ) print ( """" )",clean up temporary file
/AutoGPT/classic/original_autogpt/autogpt/agents/agent_manager.py,load_agent_state,"def load_agent_state(self, agent_id: str) -> AgentSettings:
        """"""Load the state of the agent with the given ID.""""""
        state_file_path = Path(agent_id) / ""state.json""
        if not self.file_manager.exists(state_file_path):
            raise FileNotFoundError(f""Agent with ID '{agent_id}' has no state.json"")

        state = self.file_manager.read_file(state_file_path)
        return AgentSettings.parse_raw(state)","def load_agent_state(self, agent_id: str) -> AgentSettings:
        """"""Load the state of the agent with the given ID.""""""
        state_file_path = Path(agent_id) / ""state.json""
        if not self.file_manager.exists(state_file_path):
            raise FileNotFoundError(f""Agent with ID '{agent_id}' has no state.json"")

        state = self.file_manager.read_file(state_file_path)
        return AgentSettings.parse_raw(state)",Load the state of the agent with the given ID.,Load the state of the agent with the given ID.,"def load_agent_state(self, agent_id: str) -> AgentSettings:
        
        state_file_path = Path(agent_id) / ""state.json""
        if not self.file_manager.exists(state_file_path):
            raise FileNotFoundError(f""Agent with ID '{agent_id}' has no state.json"")

        state = self.file_manager.read_file(state_file_path)
        return AgentSettings.parse_raw(state)",Load the state of the agent with the given ID.,"def load_agent_state ( self , agent_id : str ) -> AgentSettings : state_file_path = Path ( agent_id ) / ""state.json"" if not self . file_manager . exists ( state_file_path ) : raise FileNotFoundError ( f""Agent with ID '{agent_id}' has no state.json"" ) state = self . file_manager . read_file ( state_file_path ) return AgentSettings . parse_raw ( state )",Load the state of the agent with the given ID.
/yolov5/utils/general.py,check_version,"def check_version(current=""0.0.0"", minimum=""0.0.0"", name=""version "", pinned=False, hard=False, verbose=False):
    """"""Checks if the current version meets the minimum required version, exits or warns based on parameters.""""""
    current, minimum = (pkg.parse_version(x) for x in (current, minimum))
    result = (current == minimum) if pinned else (current >= minimum)  # bool
    s = f""WARNING ⚠️ {name}{minimum} is required by YOLOv5, but {name}{current} is currently installed""  # string
    if hard:
        assert result, emojis(s)  # assert min requirements met
    if verbose and not result:
        LOGGER.warning(s)
    return result","def check_version(current=""0.0.0"", minimum=""0.0.0"", name=""version "", pinned=False, hard=False, verbose=False):
    """"""Checks if the current version meets the minimum required version, exits or warns based on parameters.""""""
    current, minimum = (pkg.parse_version(x) for x in (current, minimum))
    result = (current == minimum) if pinned else (current >= minimum)  # bool
    s = f""WARNING ⚠️ {name}{minimum} is required by YOLOv5, but {name}{current} is currently installed""  # string
    if hard:
        assert result, emojis(s)  # assert min requirements met
    if verbose and not result:
        LOGGER.warning(s)
    return result","Checks if the current version meets the minimum required version, exits or warns based on parameters.","Checks if the current version meets the minimum required version, exits or warns based on parameters.","def check_version(current=""0.0.0"", minimum=""0.0.0"", name=""version "", pinned=False, hard=False, verbose=False):
    
    current, minimum = (pkg.parse_version(x) for x in (current, minimum))
    result = (current == minimum) if pinned else (current >= minimum)  # bool
    s = f""WARNING ⚠️ {name}{minimum} is required by YOLOv5, but {name}{current} is currently installed""  # string
    if hard:
        assert result, emojis(s)  # assert min requirements met
    if verbose and not result:
        LOGGER.warning(s)
    return result","Checks if the current version meets the minimum required version, exits or warns based on parameters.","def check_version ( current = ""0.0.0"" , minimum = ""0.0.0"" , name = ""version "" , pinned = False , hard = False , verbose = False ) : current , minimum = ( pkg . parse_version ( x ) for x in ( current , minimum ) ) result = ( current == minimum ) if pinned else ( current >= minimum ) # bool s = f""WARNING ⚠️ {name}{minimum} is required by YOLOv5, but {name}{current} is currently installed"" # string if hard : assert result , emojis ( s ) # assert min requirements met if verbose and not result : LOGGER . warning ( s ) return result","Checks if the current version meets the minimum required version, exits or warns based on parameters."
/ansible/packaging/release.py,set_setuptools_upper_bound,"def set_setuptools_upper_bound(requested_version: Version) -> None:
    """"""Set the upper bound on setuptools in pyproject.toml.""""""
    current = ANSIBLE_PYPROJECT_TOML_FILE.read_text()
    pattern = re.compile(r'^(?P<begin>requires = \[""setuptools >= )(?P<lower>[^,]+)(?P<middle>, <= )(?P<upper>[^""]+)(?P<end>"".*)$', re.MULTILINE)
    match = pattern.search(current)

    if not match:
        raise ApplicationError(f""Unable to find the 'requires' entry in: {ANSIBLE_PYPROJECT_TOML_FILE.relative_to(CHECKOUT_DIR)}"")

    current_version = Version(match.group('upper'))

    if requested_version == current_version:
        return

    display.show(f""Updating setuptools upper bound from {current_version} to {requested_version} ..."")

    updated = pattern.sub(fr'\g<begin>\g<lower>\g<middle>{requested_version}\g<end>', current)

    if current == updated:
        raise RuntimeError(""Failed to set the setuptools upper bound."")

    ANSIBLE_PYPROJECT_TOML_FILE.write_text(updated)","def set_setuptools_upper_bound(requested_version: Version) -> None:
    """"""Set the upper bound on setuptools in pyproject.toml.""""""
    current = ANSIBLE_PYPROJECT_TOML_FILE.read_text()
    pattern = re.compile(r'^(?P<begin>requires = \[""setuptools >= )(?P<lower>[^,]+)(?P<middle>, <= )(?P<upper>[^""]+)(?P<end>"".*)$', re.MULTILINE)
    match = pattern.search(current)

    if not match:
        raise ApplicationError(f""Unable to find the 'requires' entry in: {ANSIBLE_PYPROJECT_TOML_FILE.relative_to(CHECKOUT_DIR)}"")

    current_version = Version(match.group('upper'))

    if requested_version == current_version:
        return

    display.show(f""Updating setuptools upper bound from {current_version} to {requested_version} ..."")

    updated = pattern.sub(fr'\g<begin>\g<lower>\g<middle>{requested_version}\g<end>', current)

    if current == updated:
        raise RuntimeError(""Failed to set the setuptools upper bound."")

    ANSIBLE_PYPROJECT_TOML_FILE.write_text(updated)",Set the upper bound on setuptools in pyproject.toml.,Set the upper bound on setuptools in pyproject.toml.,"def set_setuptools_upper_bound(requested_version: Version) -> None:
    
    current = ANSIBLE_PYPROJECT_TOML_FILE.read_text()
    pattern = re.compile(r'^(?P<begin>requires = \[""setuptools >= )(?P<lower>[^,]+)(?P<middle>, <= )(?P<upper>[^""]+)(?P<end>"".*)$', re.MULTILINE)
    match = pattern.search(current)

    if not match:
        raise ApplicationError(f""Unable to find the 'requires' entry in: {ANSIBLE_PYPROJECT_TOML_FILE.relative_to(CHECKOUT_DIR)}"")

    current_version = Version(match.group('upper'))

    if requested_version == current_version:
        return

    display.show(f""Updating setuptools upper bound from {current_version} to {requested_version} ..."")

    updated = pattern.sub(fr'\g<begin>\g<lower>\g<middle>{requested_version}\g<end>', current)

    if current == updated:
        raise RuntimeError(""Failed to set the setuptools upper bound."")

    ANSIBLE_PYPROJECT_TOML_FILE.write_text(updated)",Set the upper bound on setuptools in pyproject.toml.,"def set_setuptools_upper_bound ( requested_version : Version ) -> None : current = ANSIBLE_PYPROJECT_TOML_FILE . read_text ( ) pattern = re . compile ( r'^(?P<begin>requires = \[""setuptools >= )(?P<lower>[^,]+)(?P<middle>, <= )(?P<upper>[^""]+)(?P<end>"".*)$' , re . MULTILINE ) match = pattern . search ( current ) if not match : raise ApplicationError ( f""Unable to find the 'requires' entry in: {ANSIBLE_PYPROJECT_TOML_FILE.relative_to(CHECKOUT_DIR)}"" ) current_version = Version ( match . group ( 'upper' ) ) if requested_version == current_version : return display . show ( f""Updating setuptools upper bound from {current_version} to {requested_version} ..."" ) updated = pattern . sub ( fr'\g<begin>\g<lower>\g<middle>{requested_version}\g<end>' , current ) if current == updated : raise RuntimeError ( ""Failed to set the setuptools upper bound."" ) ANSIBLE_PYPROJECT_TOML_FILE . write_text ( updated )",Set the upper bound on setuptools in pyproject.toml.
/odoo/odoo/models.py,toggle_active,"def toggle_active(self):
        ""Inverses the value of :attr:`active` on the records in ``self``.""
        assert self._active_name, f""No 'active' field on model {self._name}""
        active_recs = self.filtered(self._active_name)
        active_recs[self._active_name] = False
        (self - active_recs)[self._active_name] = True","def toggle_active(self):
        ""Inverses the value of :attr:`active` on the records in ``self``.""
        assert self._active_name, f""No 'active' field on model {self._name}""
        active_recs = self.filtered(self._active_name)
        active_recs[self._active_name] = False
        (self - active_recs)[self._active_name] = True",Inverses the value of :attr:`active` on the records in ``self``.,Inverses the value of :attr:`active` on the records in ``self``.,"def toggle_active(self):
        ""Inverses the value of :attr:`active` on the records in ``self``.""
        assert self._active_name, f""No 'active' field on model {self._name}""
        active_recs = self.filtered(self._active_name)
        active_recs[self._active_name] = False
        (self - active_recs)[self._active_name] = True",Inverses the value of :attr:`active` on the records in ``self``.,"def toggle_active ( self ) : ""Inverses the value of :attr:`active` on the records in ``self``."" assert self . _active_name , f""No 'active' field on model {self._name}"" active_recs = self . filtered ( self . _active_name ) active_recs [ self . _active_name ] = False ( self - active_recs ) [ self . _active_name ] = True",Inverses the value of :attr:`active` on the records in ``self``.
/crawl4ai/docs/examples/docker_example.py,crawl_direct,"def crawl_direct(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """"""Directly crawl without using task queue""""""
        response = requests.post(
            f""{self.base_url}/crawl_direct"", json=request_data, headers=self.headers
        )
        response.raise_for_status()
        return response.json()","def crawl_direct(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """"""Directly crawl without using task queue""""""
        response = requests.post(
            f""{self.base_url}/crawl_direct"", json=request_data, headers=self.headers
        )
        response.raise_for_status()
        return response.json()",Directly crawl without using task queue,Directly crawl without using task queue,"def crawl_direct(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        
        response = requests.post(
            f""{self.base_url}/crawl_direct"", json=request_data, headers=self.headers
        )
        response.raise_for_status()
        return response.json()",Directly crawl without using task queue,"def crawl_direct ( self , request_data : Dict [ str , Any ] ) -> Dict [ str , Any ] : response = requests . post ( f""{self.base_url}/crawl_direct"" , json = request_data , headers = self . headers ) response . raise_for_status ( ) return response . json ( )",Directly crawl without using task queue
/ultralytics/ultralytics/data/loaders.py,_single_check,"def _single_check(im: torch.Tensor, stride: int = 32) -> torch.Tensor:
        """"""Validate and format a single image tensor, ensuring correct shape and normalization.""""""
        s = (
            f""torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) ""
            f""divisible by stride {stride}. Input shape{tuple(im.shape)} is incompatible.""
        )
        if len(im.shape) != 4:
            if len(im.shape) != 3:
                raise ValueError(s)
            LOGGER.warning(s)
            im = im.unsqueeze(0)
        if im.shape[2] % stride or im.shape[3] % stride:
            raise ValueError(s)
        if im.max() > 1.0 + torch.finfo(im.dtype).eps:  # torch.float32 eps is 1.2e-07
            LOGGER.warning(
                f""torch.Tensor inputs should be normalized 0.0-1.0 but max value is {im.max()}. Dividing input by 255.""
            )
            im = im.float() / 255.0

        return im","def _single_check(im: torch.Tensor, stride: int = 32) -> torch.Tensor:
        """"""Validate and format a single image tensor, ensuring correct shape and normalization.""""""
        s = (
            f""torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) ""
            f""divisible by stride {stride}. Input shape{tuple(im.shape)} is incompatible.""
        )
        if len(im.shape) != 4:
            if len(im.shape) != 3:
                raise ValueError(s)
            LOGGER.warning(s)
            im = im.unsqueeze(0)
        if im.shape[2] % stride or im.shape[3] % stride:
            raise ValueError(s)
        if im.max() > 1.0 + torch.finfo(im.dtype).eps:  # torch.float32 eps is 1.2e-07
            LOGGER.warning(
                f""torch.Tensor inputs should be normalized 0.0-1.0 but max value is {im.max()}. Dividing input by 255.""
            )
            im = im.float() / 255.0

        return im","Validate and format a single image tensor, ensuring correct shape and normalization.","Validate and format a single image tensor, ensuring correct shape and normalization.","def _single_check(im: torch.Tensor, stride: int = 32) -> torch.Tensor:
        
        s = (
            f""torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) ""
            f""divisible by stride {stride}. Input shape{tuple(im.shape)} is incompatible.""
        )
        if len(im.shape) != 4:
            if len(im.shape) != 3:
                raise ValueError(s)
            LOGGER.warning(s)
            im = im.unsqueeze(0)
        if im.shape[2] % stride or im.shape[3] % stride:
            raise ValueError(s)
        if im.max() > 1.0 + torch.finfo(im.dtype).eps:  # torch.float32 eps is 1.2e-07
            LOGGER.warning(
                f""torch.Tensor inputs should be normalized 0.0-1.0 but max value is {im.max()}. Dividing input by 255.""
            )
            im = im.float() / 255.0

        return im","Validate and format a single image tensor, ensuring correct shape and normalization.","def _single_check ( im : torch . Tensor , stride : int = 32 ) -> torch . Tensor : s = ( f""torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) "" f""divisible by stride {stride}. Input shape{tuple(im.shape)} is incompatible."" ) if len ( im . shape ) != 4 : if len ( im . shape ) != 3 : raise ValueError ( s ) LOGGER . warning ( s ) im = im . unsqueeze ( 0 ) if im . shape [ 2 ] % stride or im . shape [ 3 ] % stride : raise ValueError ( s ) if im . max ( ) > 1.0 + torch . finfo ( im . dtype ) . eps : # torch.float32 eps is 1.2e-07 LOGGER . warning ( f""torch.Tensor inputs should be normalized 0.0-1.0 but max value is {im.max()}. Dividing input by 255."" ) im = im . float ( ) / 255.0 return im","Validate and format a single image tensor, ensuring correct shape and normalization."
/LLaMA-Factory/src/llamafactory/data/mm_plugin.py,_regularize_images,"def _regularize_images(self, images: list[""ImageInput""], **kwargs) -> dict[str, list[""ImageObject""]]:
        r""""""Regularize images to avoid error. Including reading and pre-processing.""""""
        results = []
        for image in images:
            if isinstance(image, (str, BinaryIO)):
                image = Image.open(image)
            elif isinstance(image, bytes):
                image = Image.open(BytesIO(image))
            elif isinstance(image, dict):
                if image[""bytes""] is not None:
                    image = Image.open(BytesIO(image[""bytes""]))
                else:
                    image = Image.open(image[""path""])

            if not isinstance(image, ImageObject):
                raise ValueError(f""Expect input is a list of images, but got {type(image)}."")

            results.append(self._preprocess_image(image, **kwargs))

        return {""images"": results}","def _regularize_images(self, images: list[""ImageInput""], **kwargs) -> dict[str, list[""ImageObject""]]:
        r""""""Regularize images to avoid error. Including reading and pre-processing.""""""
        results = []
        for image in images:
            if isinstance(image, (str, BinaryIO)):
                image = Image.open(image)
            elif isinstance(image, bytes):
                image = Image.open(BytesIO(image))
            elif isinstance(image, dict):
                if image[""bytes""] is not None:
                    image = Image.open(BytesIO(image[""bytes""]))
                else:
                    image = Image.open(image[""path""])

            if not isinstance(image, ImageObject):
                raise ValueError(f""Expect input is a list of images, but got {type(image)}."")

            results.append(self._preprocess_image(image, **kwargs))

        return {""images"": results}",Regularize images to avoid error. Including reading and pre-processing.,Regularize images to avoid error.,"def _regularize_images(self, images: list[""ImageInput""], **kwargs) -> dict[str, list[""ImageObject""]]:
        
        results = []
        for image in images:
            if isinstance(image, (str, BinaryIO)):
                image = Image.open(image)
            elif isinstance(image, bytes):
                image = Image.open(BytesIO(image))
            elif isinstance(image, dict):
                if image[""bytes""] is not None:
                    image = Image.open(BytesIO(image[""bytes""]))
                else:
                    image = Image.open(image[""path""])

            if not isinstance(image, ImageObject):
                raise ValueError(f""Expect input is a list of images, but got {type(image)}."")

            results.append(self._preprocess_image(image, **kwargs))

        return {""images"": results}",Regularize images to avoid error.,"def _regularize_images ( self , images : list [ ""ImageInput"" ] , ** kwargs ) -> dict [ str , list [ ""ImageObject"" ] ] : results = [ ] for image in images : if isinstance ( image , ( str , BinaryIO ) ) : image = Image . open ( image ) elif isinstance ( image , bytes ) : image = Image . open ( BytesIO ( image ) ) elif isinstance ( image , dict ) : if image [ ""bytes"" ] is not None : image = Image . open ( BytesIO ( image [ ""bytes"" ] ) ) else : image = Image . open ( image [ ""path"" ] ) if not isinstance ( image , ImageObject ) : raise ValueError ( f""Expect input is a list of images, but got {type(image)}."" ) results . append ( self . _preprocess_image ( image , ** kwargs ) ) return { ""images"" : results }",Regularize images to avoid error.
/annotated_deep_learning_paper_implementations/labml_nn/transformers/retro/model.py,__init__,"def __init__(self, d: int, base: int = 10_000):
        """"""
        * `d` is the number of features $d$
        * `base` is the constant used for calculating $\Theta$
        """"""
        super().__init__()
        # $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$
        self.theta = nn.Parameter(1. / (base ** (torch.arange(0, d, 2).float() / d)), requires_grad=False)","def __init__(self, d: int, base: int = 10_000):
        """"""
        * `d` is the number of features $d$
        * `base` is the constant used for calculating $\Theta$
        """"""
        super().__init__()
        # $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$
        self.theta = nn.Parameter(1. / (base ** (torch.arange(0, d, 2).float() / d)), requires_grad=False)","* `d` is the number of features $d$
* `base` is the constant used for calculating $\Theta$",`d` is the number of features $d$ `base` is the constant used for calculating $\Theta$,"def __init__(self, d: int, base: int = 10_000):
        
        super().__init__()
        # $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$
        self.theta = nn.Parameter(1. / (base ** (torch.arange(0, d, 2).float() / d)), requires_grad=False)",`d` is the number of features $d$ `base` is the constant used for calculating $\Theta$,"def __init__ ( self , d : int , base : int = 10_000 ) : super ( ) . __init__ ( ) # $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$ self . theta = nn . Parameter ( 1. / ( base ** ( torch . arange ( 0 , d , 2 ) . float ( ) / d ) ) , requires_grad = False )",`d` is the number of features $d$ `base` is the constant used for calculating $\Theta$
/cheat.sh/lib/search.py,_parse_options,"def _parse_options(options):
    """"""Parse search options string into optiond_dict
    """"""

    if options is None:
        return {}

    search_options = {
        'insensitive': 'i' in options,
        'word_boundaries': 'b' in options,
        'recursive': 'r' in options,
    }
    return search_options","def _parse_options(options):
    """"""Parse search options string into optiond_dict
    """"""

    if options is None:
        return {}

    search_options = {
        'insensitive': 'i' in options,
        'word_boundaries': 'b' in options,
        'recursive': 'r' in options,
    }
    return search_options",Parse search options string into optiond_dict,Parse search options string into optiond_dict,"def _parse_options(options):
    

    if options is None:
        return {}

    search_options = {
        'insensitive': 'i' in options,
        'word_boundaries': 'b' in options,
        'recursive': 'r' in options,
    }
    return search_options",Parse search options string into optiond_dict,"def _parse_options ( options ) : if options is None : return { } search_options = { 'insensitive' : 'i' in options , 'word_boundaries' : 'b' in options , 'recursive' : 'r' in options , } return search_options",Parse search options string into optiond_dict
/sentry/src/sentry/runner/commands/permissions.py,list,"def list(user: str) -> None:
    ""List permissions for a user.""
    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)
    up_list = UserPermission.objects.filter(user=user_inst).order_by(""permission"")
    click.echo(f""Permissions for `{user_inst.username}`:"")
    for permission in up_list:
        click.echo(f""- {permission.permission}"")","def list(user: str) -> None:
    ""List permissions for a user.""
    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)
    up_list = UserPermission.objects.filter(user=user_inst).order_by(""permission"")
    click.echo(f""Permissions for `{user_inst.username}`:"")
    for permission in up_list:
        click.echo(f""- {permission.permission}"")",List permissions for a user.,List permissions for a user.,"def list(user: str) -> None:
    ""List permissions for a user.""
    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)
    up_list = UserPermission.objects.filter(user=user_inst).order_by(""permission"")
    click.echo(f""Permissions for `{user_inst.username}`:"")
    for permission in up_list:
        click.echo(f""- {permission.permission}"")",List permissions for a user.,"def list ( user : str ) -> None : ""List permissions for a user."" from sentry . users . models . userpermission import UserPermission user_inst = user_param_to_user ( user ) up_list = UserPermission . objects . filter ( user = user_inst ) . order_by ( ""permission"" ) click . echo ( f""Permissions for `{user_inst.username}`:"" ) for permission in up_list : click . echo ( f""- {permission.permission}"" )",List permissions for a user.
/sentry/src/sentry/runner/commands/permissions.py,remove,"def remove(user: str, permission: str) -> None:
    ""Remove a permission from a user.""
    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)

    try:
        up = UserPermission.objects.get(user=user_inst, permission=permission)
    except UserPermission.DoesNotExist:
        click.echo(f""Permission does not exist for `{user_inst.username}`"")
    else:
        up.delete()
        click.echo(f""Removed permission `{permission}` from `{user_inst.username}`"")","def remove(user: str, permission: str) -> None:
    ""Remove a permission from a user.""
    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)

    try:
        up = UserPermission.objects.get(user=user_inst, permission=permission)
    except UserPermission.DoesNotExist:
        click.echo(f""Permission does not exist for `{user_inst.username}`"")
    else:
        up.delete()
        click.echo(f""Removed permission `{permission}` from `{user_inst.username}`"")",Remove a permission from a user.,Remove a permission from a user.,"def remove(user: str, permission: str) -> None:
    ""Remove a permission from a user.""
    from sentry.users.models.userpermission import UserPermission

    user_inst = user_param_to_user(user)

    try:
        up = UserPermission.objects.get(user=user_inst, permission=permission)
    except UserPermission.DoesNotExist:
        click.echo(f""Permission does not exist for `{user_inst.username}`"")
    else:
        up.delete()
        click.echo(f""Removed permission `{permission}` from `{user_inst.username}`"")",Remove a permission from a user.,"def remove ( user : str , permission : str ) -> None : ""Remove a permission from a user."" from sentry . users . models . userpermission import UserPermission user_inst = user_param_to_user ( user ) try : up = UserPermission . objects . get ( user = user_inst , permission = permission ) except UserPermission . DoesNotExist : click . echo ( f""Permission does not exist for `{user_inst.username}`"" ) else : up . delete ( ) click . echo ( f""Removed permission `{permission}` from `{user_inst.username}`"" )",Remove a permission from a user.
/yolov5/utils/general.py,git_describe,"def git_describe(path=ROOT):
    """"""
    Returns a human-readable git description of the repository at `path`, or an empty string on failure.

    Example output is 'fv5.0-5-g3e25f1e'. See https://git-scm.com/docs/git-describe.
    """"""
    try:
        assert (Path(path) / "".git"").is_dir()
        return check_output(f""git -C {path} describe --tags --long --always"", shell=True).decode()[:-1]
    except Exception:
        return """"","def git_describe(path=ROOT):
    """"""
    Returns a human-readable git description of the repository at `path`, or an empty string on failure.

    """"""
    try:
        assert (Path(path) / "".git"").is_dir()
        return check_output(f""git -C {path} describe --tags --long --always"", shell=True).decode()[:-1]
    except Exception:
        return """"","Returns a human-readable git description of the repository at `path`, or an empty string on failure.

Example output is 'fv5.0-5-g3e25f1e'. See https://git-scm.com/docs/git-describe.","Returns a human-readable git description of the repository at `path`, or an empty string on failure.","def git_describe(path=ROOT):
    
    try:
        assert (Path(path) / "".git"").is_dir()
        return check_output(f""git -C {path} describe --tags --long --always"", shell=True).decode()[:-1]
    except Exception:
        return """"","Returns a human-readable git description of the repository at `path`, or an empty string on failure.","def git_describe ( path = ROOT ) : try : assert ( Path ( path ) / "".git"" ) . is_dir ( ) return check_output ( f""git -C {path} describe --tags --long --always"" , shell = True ) . decode ( ) [ : - 1 ] except Exception : return """"","Returns a human-readable git description of the repository at `path`, or an empty string on failure."
/browser-use/tests/extraction_test.py,count_string_tokens,"def count_string_tokens(string: str, model: str) -> tuple[int, float]:
	""""""Count the number of tokens in a string using a specified model.""""""

	def get_price_per_token(model: str) -> float:
		""""""Get the price per token for a specified model.

		@todo: move to utils, use a package or sth
		""""""
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]

	llm = ChatOpenAI(model=model)
	token_count = llm.get_num_tokens(string)
	price = token_count * get_price_per_token(model)
	return token_count, price","def count_string_tokens(string: str, model: str) -> tuple[int, float]:
	""""""Count the number of tokens in a string using a specified model.""""""

	def get_price_per_token(model: str) -> float:
		""""""Get the price per token for a specified model.

		@todo: move to utils, use a package or sth
		""""""
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]

	llm = ChatOpenAI(model=model)
	token_count = llm.get_num_tokens(string)
	price = token_count * get_price_per_token(model)
	return token_count, price",Count the number of tokens in a string using a specified model.,Count the number of tokens in a string using a specified model.,"def count_string_tokens(string: str, model: str) -> tuple[int, float]:
	

	def get_price_per_token(model: str) -> float:
		
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]

	llm = ChatOpenAI(model=model)
	token_count = llm.get_num_tokens(string)
	price = token_count * get_price_per_token(model)
	return token_count, price",Count the number of tokens in a string using a specified model.,"def count_string_tokens ( string : str , model : str ) -> tuple [ int , float ] : def get_price_per_token ( model : str ) -> float : prices = { 'gpt-4o' : 2.5 / 1e6 , 'gpt-4o-mini' : 0.15 / 1e6 , } return prices [ model ] llm = ChatOpenAI ( model = model ) token_count = llm . get_num_tokens ( string ) price = token_count * get_price_per_token ( model ) return token_count , price",Count the number of tokens in a string using a specified model.
/browser-use/tests/extraction_test.py,get_price_per_token,"def get_price_per_token(model: str) -> float:
		""""""Get the price per token for a specified model.

		@todo: move to utils, use a package or sth
		""""""
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]","def get_price_per_token(model: str) -> float:
		""""""Get the price per token for a specified model.

		@todo: move to utils, use a package or sth
		""""""
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]","Get the price per token for a specified model.

@todo: move to utils, use a package or sth",Get the price per token for a specified model.,"def get_price_per_token(model: str) -> float:
		
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]",Get the price per token for a specified model.,"def get_price_per_token ( model : str ) -> float : prices = { 'gpt-4o' : 2.5 / 1e6 , 'gpt-4o-mini' : 0.15 / 1e6 , } return prices [ model ]",Get the price per token for a specified model.
/sentry/src/sentry/runner/commands/devserver.py,add_daemon,"def add_daemon(name: str, command: list[str]) -> None:
    """"""
    Used by getsentry to add additional workers to the devserver setup.
    """"""
    if name in _DEFAULT_DAEMONS:
        raise KeyError(f""The {name} worker has already been defined"")
    _DEFAULT_DAEMONS[name] = command","def add_daemon(name: str, command: list[str]) -> None:
    """"""
    Used by getsentry to add additional workers to the devserver setup.
    """"""
    if name in _DEFAULT_DAEMONS:
        raise KeyError(f""The {name} worker has already been defined"")
    _DEFAULT_DAEMONS[name] = command",Used by getsentry to add additional workers to the devserver setup.,Used by getsentry to add additional workers to the devserver setup.,"def add_daemon(name: str, command: list[str]) -> None:
    
    if name in _DEFAULT_DAEMONS:
        raise KeyError(f""The {name} worker has already been defined"")
    _DEFAULT_DAEMONS[name] = command",Used by getsentry to add additional workers to the devserver setup.,"def add_daemon ( name : str , command : list [ str ] ) -> None : if name in _DEFAULT_DAEMONS : raise KeyError ( f""The {name} worker has already been defined"" ) _DEFAULT_DAEMONS [ name ] = command",Used by getsentry to add additional workers to the devserver setup.
/LLaMA-Factory/src/llamafactory/data/mm_plugin.py,_regularize_audios,"def _regularize_audios(
        self, audios: list[""AudioInput""], sampling_rate: float, **kwargs
    ) -> dict[str, Union[list[""NDArray""], list[float]]]:
        r""""""Regularizes audios to avoid error. Including reading and resampling.""""""
        results, sampling_rates = [], []
        for audio in audios:
            if isinstance(audio, (str, BinaryIO)):
                audio, sampling_rate = librosa.load(audio, sr=sampling_rate)

            if not isinstance(audio, np.ndarray):
                raise ValueError(f""Expect input is a list of audios, but got {type(audio)}."")

            results.append(audio)
            sampling_rates.append(sampling_rate)

        return {""audios"": results, ""sampling_rates"": sampling_rates}","def _regularize_audios(
        self, audios: list[""AudioInput""], sampling_rate: float, **kwargs
    ) -> dict[str, Union[list[""NDArray""], list[float]]]:
        r""""""Regularizes audios to avoid error. Including reading and resampling.""""""
        results, sampling_rates = [], []
        for audio in audios:
            if isinstance(audio, (str, BinaryIO)):
                audio, sampling_rate = librosa.load(audio, sr=sampling_rate)

            if not isinstance(audio, np.ndarray):
                raise ValueError(f""Expect input is a list of audios, but got {type(audio)}."")

            results.append(audio)
            sampling_rates.append(sampling_rate)

        return {""audios"": results, ""sampling_rates"": sampling_rates}",Regularizes audios to avoid error. Including reading and resampling.,Regularizes audios to avoid error.,"def _regularize_audios(
        self, audios: list[""AudioInput""], sampling_rate: float, **kwargs
    ) -> dict[str, Union[list[""NDArray""], list[float]]]:
        
        results, sampling_rates = [], []
        for audio in audios:
            if isinstance(audio, (str, BinaryIO)):
                audio, sampling_rate = librosa.load(audio, sr=sampling_rate)

            if not isinstance(audio, np.ndarray):
                raise ValueError(f""Expect input is a list of audios, but got {type(audio)}."")

            results.append(audio)
            sampling_rates.append(sampling_rate)

        return {""audios"": results, ""sampling_rates"": sampling_rates}",Regularizes audios to avoid error.,"def _regularize_audios ( self , audios : list [ ""AudioInput"" ] , sampling_rate : float , ** kwargs ) -> dict [ str , Union [ list [ ""NDArray"" ] , list [ float ] ] ] : results , sampling_rates = [ ] , [ ] for audio in audios : if isinstance ( audio , ( str , BinaryIO ) ) : audio , sampling_rate = librosa . load ( audio , sr = sampling_rate ) if not isinstance ( audio , np . ndarray ) : raise ValueError ( f""Expect input is a list of audios, but got {type(audio)}."" ) results . append ( audio ) sampling_rates . append ( sampling_rate ) return { ""audios"" : results , ""sampling_rates"" : sampling_rates }",Regularizes audios to avoid error.
/yolov5/utils/loggers/__init__.py,log_images,"def log_images(self, files, name=""Images"", epoch=0):
        """"""Logs images to all loggers with optional naming and epoch specification.""""""
        files = [Path(f) for f in (files if isinstance(files, (tuple, list)) else [files])]  # to Path
        files = [f for f in files if f.exists()]  # filter by exists

        if self.tb:
            for f in files:
                self.tb.add_image(f.stem, cv2.imread(str(f))[..., ::-1], epoch, dataformats=""HWC"")

        if self.wandb:
            self.wandb.log({name: [wandb.Image(str(f), caption=f.name) for f in files]}, step=epoch)

        if self.clearml:
            if name == ""Results"":
                [self.clearml.log_plot(f.stem, f) for f in files]
            else:
                self.clearml.log_debug_samples(files, title=name)","def log_images(self, files, name=""Images"", epoch=0):
        """"""Logs images to all loggers with optional naming and epoch specification.""""""
        files = [Path(f) for f in (files if isinstance(files, (tuple, list)) else [files])]  # to Path
        files = [f for f in files if f.exists()]  # filter by exists

        if self.tb:
            for f in files:
                self.tb.add_image(f.stem, cv2.imread(str(f))[..., ::-1], epoch, dataformats=""HWC"")

        if self.wandb:
            self.wandb.log({name: [wandb.Image(str(f), caption=f.name) for f in files]}, step=epoch)

        if self.clearml:
            if name == ""Results"":
                [self.clearml.log_plot(f.stem, f) for f in files]
            else:
                self.clearml.log_debug_samples(files, title=name)",Logs images to all loggers with optional naming and epoch specification.,Logs images to all loggers with optional naming and epoch specification.,"def log_images(self, files, name=""Images"", epoch=0):
        
        files = [Path(f) for f in (files if isinstance(files, (tuple, list)) else [files])]  # to Path
        files = [f for f in files if f.exists()]  # filter by exists

        if self.tb:
            for f in files:
                self.tb.add_image(f.stem, cv2.imread(str(f))[..., ::-1], epoch, dataformats=""HWC"")

        if self.wandb:
            self.wandb.log({name: [wandb.Image(str(f), caption=f.name) for f in files]}, step=epoch)

        if self.clearml:
            if name == ""Results"":
                [self.clearml.log_plot(f.stem, f) for f in files]
            else:
                self.clearml.log_debug_samples(files, title=name)",Logs images to all loggers with optional naming and epoch specification.,"def log_images ( self , files , name = ""Images"" , epoch = 0 ) : files = [ Path ( f ) for f in ( files if isinstance ( files , ( tuple , list ) ) else [ files ] ) ] # to Path files = [ f for f in files if f . exists ( ) ] # filter by exists if self . tb : for f in files : self . tb . add_image ( f . stem , cv2 . imread ( str ( f ) ) [ ... , : : - 1 ] , epoch , dataformats = ""HWC"" ) if self . wandb : self . wandb . log ( { name : [ wandb . Image ( str ( f ) , caption = f . name ) for f in files ] } , step = epoch ) if self . clearml : if name == ""Results"" : [ self . clearml . log_plot ( f . stem , f ) for f in files ] else : self . clearml . log_debug_samples ( files , title = name )",Logs images to all loggers with optional naming and epoch specification.
/colossalai/colossalai/checkpoint_io/utils.py,cast,"def cast(param, value, key=None):
        r""""""Make a deep copy of value, casting all tensors to device of param.""""""
        if isinstance(value, torch.Tensor):
            # Floating-point types are a bit special here. They are the only ones
            # that are assumed to always match the type of params.
            # Make sure state['step'] is not casted https://github.com/pytorch/pytorch/issues/74424
            if key != ""step"":
                if param.is_floating_point():
                    value = value.to(param.dtype)
                value = value.to(param.device, non_blocking=True)
            return value
        elif isinstance(value, dict):
            return {k: cast(param, v, key=k) for k, v in value.items()}
        elif isinstance(value, container_abcs.Iterable):
            return type(value)(cast(param, v) for v in value)
        else:
            return value","def cast(param, value, key=None):
        r""""""Make a deep copy of value, casting all tensors to device of param.""""""
        if isinstance(value, torch.Tensor):
            # Floating-point types are a bit special here. They are the only ones
            # that are assumed to always match the type of params.
            if key != ""step"":
                if param.is_floating_point():
                    value = value.to(param.dtype)
                value = value.to(param.device, non_blocking=True)
            return value
        elif isinstance(value, dict):
            return {k: cast(param, v, key=k) for k, v in value.items()}
        elif isinstance(value, container_abcs.Iterable):
            return type(value)(cast(param, v) for v in value)
        else:
            return value","Make a deep copy of value, casting all tensors to device of param.","Make a deep copy of value, casting all tensors to device of param.","def cast(param, value, key=None):
        
        if isinstance(value, torch.Tensor):
            # Floating-point types are a bit special here. They are the only ones
            # that are assumed to always match the type of params.
            if key != ""step"":
                if param.is_floating_point():
                    value = value.to(param.dtype)
                value = value.to(param.device, non_blocking=True)
            return value
        elif isinstance(value, dict):
            return {k: cast(param, v, key=k) for k, v in value.items()}
        elif isinstance(value, container_abcs.Iterable):
            return type(value)(cast(param, v) for v in value)
        else:
            return value","Make a deep copy of value, casting all tensors to device of param.","def cast ( param , value , key = None ) : if isinstance ( value , torch . Tensor ) : # Floating-point types are a bit special here. They are the only ones # that are assumed to always match the type of params. if key != ""step"" : if param . is_floating_point ( ) : value = value . to ( param . dtype ) value = value . to ( param . device , non_blocking = True ) return value elif isinstance ( value , dict ) : return { k : cast ( param , v , key = k ) for k , v in value . items ( ) } elif isinstance ( value , container_abcs . Iterable ) : return type ( value ) ( cast ( param , v ) for v in value ) else : return value","Make a deep copy of value, casting all tensors to device of param."
/black/src/blib2to3/pytree.py,__repr__,"def __repr__(self) -> str:
        """"""Return a canonical string representation.""""""
        from .pgen2.token import tok_name

        assert self.type is not None
        return ""{}({}, {!r})"".format(
            self.__class__.__name__,
            tok_name.get(self.type, self.type),
            self.value,
        )","def __repr__(self) -> str:
        """"""Return a canonical string representation.""""""
        from .pgen2.token import tok_name

        assert self.type is not None
        return ""{}({}, {!r})"".format(
            self.__class__.__name__,
            tok_name.get(self.type, self.type),
            self.value,
        )",Return a canonical string representation.,Return a canonical string representation.,"def __repr__(self) -> str:
        
        from .pgen2.token import tok_name

        assert self.type is not None
        return ""{}({}, {!r})"".format(
            self.__class__.__name__,
            tok_name.get(self.type, self.type),
            self.value,
        )",Return a canonical string representation.,"def __repr__ ( self ) -> str : from . pgen2 . token import tok_name assert self . type is not None return ""{}({}, {!r})"" . format ( self . __class__ . __name__ , tok_name . get ( self . type , self . type ) , self . value , )",Return a canonical string representation.
/cpython/Tools/build/check_warnings.py,get_warnings_by_file,"def get_warnings_by_file(warnings: list[dict]) -> dict[str, list[dict]]:
    """"""
    Returns a dictionary where the key is the file and the data is the
    warnings in that file. Does not include duplicate warnings for a
    file from list of provided warnings.
    """"""
    warnings_by_file = defaultdict(list)
    warnings_added = set()
    for warning in warnings:
        warning_key = (
            f""{warning['file']}-{warning['line']}-""
            f""{warning['column']}-{warning['option']}""
        )
        if warning_key not in warnings_added:
            warnings_added.add(warning_key)
            warnings_by_file[warning[""file""]].append(warning)

    return warnings_by_file","def get_warnings_by_file(warnings: list[dict]) -> dict[str, list[dict]]:
    """"""
    Returns a dictionary where the key is the file and the data is the
    warnings in that file. Does not include duplicate warnings for a
    file from list of provided warnings.
    """"""
    warnings_by_file = defaultdict(list)
    warnings_added = set()
    for warning in warnings:
        warning_key = (
            f""{warning['file']}-{warning['line']}-""
            f""{warning['column']}-{warning['option']}""
        )
        if warning_key not in warnings_added:
            warnings_added.add(warning_key)
            warnings_by_file[warning[""file""]].append(warning)

    return warnings_by_file","Returns a dictionary where the key is the file and the data is the
warnings in that file. Does not include duplicate warnings for a
file from list of provided warnings.",Returns a dictionary where the key is the file and the data is the warnings in that file.,"def get_warnings_by_file(warnings: list[dict]) -> dict[str, list[dict]]:
    
    warnings_by_file = defaultdict(list)
    warnings_added = set()
    for warning in warnings:
        warning_key = (
            f""{warning['file']}-{warning['line']}-""
            f""{warning['column']}-{warning['option']}""
        )
        if warning_key not in warnings_added:
            warnings_added.add(warning_key)
            warnings_by_file[warning[""file""]].append(warning)

    return warnings_by_file",Returns a dictionary where the key is the file and the data is the warnings in that file.,"def get_warnings_by_file ( warnings : list [ dict ] ) -> dict [ str , list [ dict ] ] : warnings_by_file = defaultdict ( list ) warnings_added = set ( ) for warning in warnings : warning_key = ( f""{warning['file']}-{warning['line']}-"" f""{warning['column']}-{warning['option']}"" ) if warning_key not in warnings_added : warnings_added . add ( warning_key ) warnings_by_file [ warning [ ""file"" ] ] . append ( warning ) return warnings_by_file",Returns a dictionary where the key is the file and the data is the warnings in that file.
/gpt4free/g4f/Provider/LambdaChat.py,get_model,"def get_model(cls, model: str) -> str:
        """"""Get the internal model name from the user-provided model name.""""""
        
        if not model:
            return cls.default_model
        
        # Check if the model exists directly in our models list
        if model in cls.models:
            return model
        
        # Check if there's an alias for this model
        if model in cls.model_aliases:
            alias = cls.model_aliases[model]
            # If the alias is a list, randomly select one of the options
            if isinstance(alias, list):
                selected_model = random.choice(alias)
                debug.log(f""{cls.__name__}: Selected model '{selected_model}' from alias '{model}'"")
                return selected_model
            debug.log(f""{cls.__name__}: Using model '{alias}' for alias '{model}'"")
            return alias
        
        raise ModelNotFoundError(f""Model {model} not found"")","def get_model(cls, model: str) -> str:
        """"""Get the internal model name from the user-provided model name.""""""
        
        if not model:
            return cls.default_model
        
        # Check if the model exists directly in our models list
        if model in cls.models:
            return model
        
        # Check if there's an alias for this model
        if model in cls.model_aliases:
            alias = cls.model_aliases[model]
            # If the alias is a list, randomly select one of the options
            if isinstance(alias, list):
                selected_model = random.choice(alias)
                debug.log(f""{cls.__name__}: Selected model '{selected_model}' from alias '{model}'"")
                return selected_model
            debug.log(f""{cls.__name__}: Using model '{alias}' for alias '{model}'"")
            return alias
        
        raise ModelNotFoundError(f""Model {model} not found"")",Get the internal model name from the user-provided model name.,Get the internal model name from the user-provided model name.,"def get_model(cls, model: str) -> str:
        
        
        if not model:
            return cls.default_model
        
        # Check if the model exists directly in our models list
        if model in cls.models:
            return model
        
        # Check if there's an alias for this model
        if model in cls.model_aliases:
            alias = cls.model_aliases[model]
            # If the alias is a list, randomly select one of the options
            if isinstance(alias, list):
                selected_model = random.choice(alias)
                debug.log(f""{cls.__name__}: Selected model '{selected_model}' from alias '{model}'"")
                return selected_model
            debug.log(f""{cls.__name__}: Using model '{alias}' for alias '{model}'"")
            return alias
        
        raise ModelNotFoundError(f""Model {model} not found"")",Get the internal model name from the user-provided model name.,"def get_model ( cls , model : str ) -> str : if not model : return cls . default_model # Check if the model exists directly in our models list if model in cls . models : return model # Check if there's an alias for this model if model in cls . model_aliases : alias = cls . model_aliases [ model ] # If the alias is a list, randomly select one of the options if isinstance ( alias , list ) : selected_model = random . choice ( alias ) debug . log ( f""{cls.__name__}: Selected model '{selected_model}' from alias '{model}'"" ) return selected_model debug . log ( f""{cls.__name__}: Using model '{alias}' for alias '{model}'"" ) return alias raise ModelNotFoundError ( f""Model {model} not found"" )",Get the internal model name from the user-provided model name.
/FastChat/fastchat/train/train_lora_t5.py,safe_save_model_for_hf_trainer,"def safe_save_model_for_hf_trainer(
    trainer: transformers.Trainer, output_dir: str, state_dict: dict
):
    """"""Collects the state dict and dump to disk.""""""

    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)","def safe_save_model_for_hf_trainer(
    trainer: transformers.Trainer, output_dir: str, state_dict: dict
):
    """"""Collects the state dict and dump to disk.""""""

    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,Collects the state dict and dump to disk.,"def safe_save_model_for_hf_trainer(
    trainer: transformers.Trainer, output_dir: str, state_dict: dict
):
    

    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,"def safe_save_model_for_hf_trainer ( trainer : transformers . Trainer , output_dir : str , state_dict : dict ) : if trainer . args . should_save : cpu_state_dict = { key : value . cpu ( ) for key , value in state_dict . items ( ) } del state_dict trainer . _save ( output_dir , state_dict = cpu_state_dict )",Collects the state dict and dump to disk.
/odoo/odoo/models.py,__and__,"def __and__(self, other) -> Self:
        """""" Return the intersection of two recordsets.
            Note that first occurrence order is preserved.
        """"""
        try:
            if self._name != other._name:
                raise TypeError(f""inconsistent models in: {self} & {other}"")
            other_ids = set(other._ids)
            return self.browse(OrderedSet(id for id in self._ids if id in other_ids))
        except AttributeError:
            raise TypeError(f""unsupported operand types in: {self} & {other!r}"")","def __and__(self, other) -> Self:
        """""" Return the intersection of two recordsets.
            Note that first occurrence order is preserved.
        """"""
        try:
            if self._name != other._name:
                raise TypeError(f""inconsistent models in: {self} & {other}"")
            other_ids = set(other._ids)
            return self.browse(OrderedSet(id for id in self._ids if id in other_ids))
        except AttributeError:
            raise TypeError(f""unsupported operand types in: {self} & {other!r}"")","Return the intersection of two recordsets.
Note that first occurrence order is preserved.",Return the intersection of two recordsets.,"def __and__(self, other) -> Self:
        
        try:
            if self._name != other._name:
                raise TypeError(f""inconsistent models in: {self} & {other}"")
            other_ids = set(other._ids)
            return self.browse(OrderedSet(id for id in self._ids if id in other_ids))
        except AttributeError:
            raise TypeError(f""unsupported operand types in: {self} & {other!r}"")",Return the intersection of two recordsets.,"def __and__ ( self , other ) -> Self : try : if self . _name != other . _name : raise TypeError ( f""inconsistent models in: {self} & {other}"" ) other_ids = set ( other . _ids ) return self . browse ( OrderedSet ( id for id in self . _ids if id in other_ids ) ) except AttributeError : raise TypeError ( f""unsupported operand types in: {self} & {other!r}"" )",Return the intersection of two recordsets.
/sentry/src/sentry/runner/commands/killswitches.py,_list,"def _list() -> None:
    """"""
    List all killswitches and whether they are enabled (and how).
    """"""

    from sentry import killswitches, options

    for name, info in killswitches.ALL_KILLSWITCH_OPTIONS.items():
        click.echo()
        click.echo(f""{name}"")
        click.echo(f""  # {info.description}"")
        conditions = killswitches.print_conditions(name, options.get(name))
        click.echo(f""{conditions}"")","def _list() -> None:
    """"""
    List all killswitches and whether they are enabled (and how).
    """"""

    from sentry import killswitches, options

    for name, info in killswitches.ALL_KILLSWITCH_OPTIONS.items():
        click.echo()
        click.echo(f""{name}"")
        click.echo(f""  # {info.description}"")
        conditions = killswitches.print_conditions(name, options.get(name))
        click.echo(f""{conditions}"")",List all killswitches and whether they are enabled (and how).,List all killswitches and whether they are enabled (and how).,"def _list() -> None:
    

    from sentry import killswitches, options

    for name, info in killswitches.ALL_KILLSWITCH_OPTIONS.items():
        click.echo()
        click.echo(f""{name}"")
        click.echo(f""  # {info.description}"")
        conditions = killswitches.print_conditions(name, options.get(name))
        click.echo(f""{conditions}"")",List all killswitches and whether they are enabled (and how).,"def _list ( ) -> None : from sentry import killswitches , options for name , info in killswitches . ALL_KILLSWITCH_OPTIONS . items ( ) : click . echo ( ) click . echo ( f""{name}"" ) click . echo ( f""  # {info.description}"" ) conditions = killswitches . print_conditions ( name , options . get ( name ) ) click . echo ( f""{conditions}"" )",List all killswitches and whether they are enabled (and how).
/gradio/gradio/blocks.py,clear,"def clear(self):
        """"""Resets the layout of the Blocks object.""""""
        self.default_config.blocks = {}
        self.default_config.fns = {}
        self.children = []
        return self","def clear(self):
        """"""Resets the layout of the Blocks object.""""""
        self.default_config.blocks = {}
        self.default_config.fns = {}
        self.children = []
        return self",Resets the layout of the Blocks object.,Resets the layout of the Blocks object.,"def clear(self):
        
        self.default_config.blocks = {}
        self.default_config.fns = {}
        self.children = []
        return self",Resets the layout of the Blocks object.,def clear ( self ) : self . default_config . blocks = { } self . default_config . fns = { } self . children = [ ] return self,Resets the layout of the Blocks object.
/yolov5/utils/loggers/__init__.py,on_train_batch_end,"def on_train_batch_end(self, model, ni, imgs, targets, paths, vals):
        """"""Logs training batch end events, plots images, and updates external loggers with batch-end data.""""""
        log_dict = dict(zip(self.keys[:3], vals))
        # Callback runs on train batch end
        # ni: number integrated batches (since train start)
        if self.plots:
            if ni < 3:
                f = self.save_dir / f""train_batch{ni}.jpg""  # filename
                plot_images(imgs, targets, paths, f)
                if ni == 0 and self.tb and not self.opt.sync_bn:
                    log_tensorboard_graph(self.tb, model, imgsz=(self.opt.imgsz, self.opt.imgsz))
            if ni == 10 and (self.wandb or self.clearml):
                files = sorted(self.save_dir.glob(""train*.jpg""))
                if self.wandb:
                    self.wandb.log({""Mosaics"": [wandb.Image(str(f), caption=f.name) for f in files if f.exists()]})
                if self.clearml:
                    self.clearml.log_debug_samples(files, title=""Mosaics"")

        if self.comet_logger:
            self.comet_logger.on_train_batch_end(log_dict, step=ni)","def on_train_batch_end(self, model, ni, imgs, targets, paths, vals):
        """"""Logs training batch end events, plots images, and updates external loggers with batch-end data.""""""
        log_dict = dict(zip(self.keys[:3], vals))
        # Callback runs on train batch end
        # ni: number integrated batches (since train start)
        if self.plots:
            if ni < 3:
                f = self.save_dir / f""train_batch{ni}.jpg""  # filename
                plot_images(imgs, targets, paths, f)
                if ni == 0 and self.tb and not self.opt.sync_bn:
                    log_tensorboard_graph(self.tb, model, imgsz=(self.opt.imgsz, self.opt.imgsz))
            if ni == 10 and (self.wandb or self.clearml):
                files = sorted(self.save_dir.glob(""train*.jpg""))
                if self.wandb:
                    self.wandb.log({""Mosaics"": [wandb.Image(str(f), caption=f.name) for f in files if f.exists()]})
                if self.clearml:
                    self.clearml.log_debug_samples(files, title=""Mosaics"")

        if self.comet_logger:
            self.comet_logger.on_train_batch_end(log_dict, step=ni)","Logs training batch end events, plots images, and updates external loggers with batch-end data.","Logs training batch end events, plots images, and updates external loggers with batch-end data.","def on_train_batch_end(self, model, ni, imgs, targets, paths, vals):
        
        log_dict = dict(zip(self.keys[:3], vals))
        # Callback runs on train batch end
        # ni: number integrated batches (since train start)
        if self.plots:
            if ni < 3:
                f = self.save_dir / f""train_batch{ni}.jpg""  # filename
                plot_images(imgs, targets, paths, f)
                if ni == 0 and self.tb and not self.opt.sync_bn:
                    log_tensorboard_graph(self.tb, model, imgsz=(self.opt.imgsz, self.opt.imgsz))
            if ni == 10 and (self.wandb or self.clearml):
                files = sorted(self.save_dir.glob(""train*.jpg""))
                if self.wandb:
                    self.wandb.log({""Mosaics"": [wandb.Image(str(f), caption=f.name) for f in files if f.exists()]})
                if self.clearml:
                    self.clearml.log_debug_samples(files, title=""Mosaics"")

        if self.comet_logger:
            self.comet_logger.on_train_batch_end(log_dict, step=ni)","Logs training batch end events, plots images, and updates external loggers with batch-end data.","def on_train_batch_end ( self , model , ni , imgs , targets , paths , vals ) : log_dict = dict ( zip ( self . keys [ : 3 ] , vals ) ) # Callback runs on train batch end # ni: number integrated batches (since train start) if self . plots : if ni < 3 : f = self . save_dir / f""train_batch{ni}.jpg"" # filename plot_images ( imgs , targets , paths , f ) if ni == 0 and self . tb and not self . opt . sync_bn : log_tensorboard_graph ( self . tb , model , imgsz = ( self . opt . imgsz , self . opt . imgsz ) ) if ni == 10 and ( self . wandb or self . clearml ) : files = sorted ( self . save_dir . glob ( ""train*.jpg"" ) ) if self . wandb : self . wandb . log ( { ""Mosaics"" : [ wandb . Image ( str ( f ) , caption = f . name ) for f in files if f . exists ( ) ] } ) if self . clearml : self . clearml . log_debug_samples ( files , title = ""Mosaics"" ) if self . comet_logger : self . comet_logger . on_train_batch_end ( log_dict , step = ni )","Logs training batch end events, plots images, and updates external loggers with batch-end data."
/MinerU/tests/test_cli/lib/common.py,check_latex_table_exists,"def check_latex_table_exists(file_path):
    """"""check latex table exists.""""""
    pattern = r'\\begin\{tabular\}.*?\\end\{tabular\}'
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    matches = re.findall(pattern, content, re.DOTALL)
    return len(matches) > 0","def check_latex_table_exists(file_path):
    """"""check latex table exists.""""""
    pattern = r'\\begin\{tabular\}.*?\\end\{tabular\}'
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    matches = re.findall(pattern, content, re.DOTALL)
    return len(matches) > 0",check latex table exists.,check latex table exists.,"def check_latex_table_exists(file_path):
    
    pattern = r'\\begin\{tabular\}.*?\\end\{tabular\}'
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    matches = re.findall(pattern, content, re.DOTALL)
    return len(matches) > 0",check latex table exists.,"def check_latex_table_exists ( file_path ) : pattern = r'\\begin\{tabular\}.*?\\end\{tabular\}' with open ( file_path , 'r' , encoding = 'utf-8' ) as file : content = file . read ( ) matches = re . findall ( pattern , content , re . DOTALL ) return len ( matches ) > 0",check latex table exists.
/MetaGPT/metagpt/roles/role.py,_watch,"def _watch(self, actions: Iterable[Type[Action]] | Iterable[Action]):
        """"""Watch Actions of interest. Role will select Messages caused by these Actions from its personal message
        buffer during _observe.
        """"""
        self.rc.watch = {any_to_str(t) for t in actions}","def _watch(self, actions: Iterable[Type[Action]] | Iterable[Action]):
        """"""Watch Actions of interest. Role will select Messages caused by these Actions from its personal message
        buffer during _observe.
        """"""
        self.rc.watch = {any_to_str(t) for t in actions}","Watch Actions of interest. Role will select Messages caused by these Actions from its personal message
buffer during _observe.",Watch Actions of interest.,"def _watch(self, actions: Iterable[Type[Action]] | Iterable[Action]):
        
        self.rc.watch = {any_to_str(t) for t in actions}",Watch Actions of interest.,"def _watch ( self , actions : Iterable [ Type [ Action ] ] | Iterable [ Action ] ) : self . rc . watch = { any_to_str ( t ) for t in actions }",Watch Actions of interest.
/yolov5/models/yolo.py,_forward_augment,"def _forward_augment(self, x):
        """"""Performs augmented inference across different scales and flips, returning combined detections.""""""
        img_size = x.shape[-2:]  # height, width
        s = [1, 0.83, 0.67]  # scales
        f = [None, 3, None]  # flips (2-ud, 3-lr)
        y = []  # outputs
        for si, fi in zip(s, f):
            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))
            yi = self._forward_once(xi)[0]  # forward
            # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save
            yi = self._descale_pred(yi, fi, si, img_size)
            y.append(yi)
        y = self._clip_augmented(y)  # clip augmented tails
        return torch.cat(y, 1), None","def _forward_augment(self, x):
        """"""Performs augmented inference across different scales and flips, returning combined detections.""""""
        img_size = x.shape[-2:]  # height, width
        s = [1, 0.83, 0.67]  # scales
        f = [None, 3, None]  # flips (2-ud, 3-lr)
        y = []  # outputs
        for si, fi in zip(s, f):
            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))
            yi = self._forward_once(xi)[0]  # forward
            # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save
            yi = self._descale_pred(yi, fi, si, img_size)
            y.append(yi)
        y = self._clip_augmented(y)  # clip augmented tails
        return torch.cat(y, 1), None","Performs augmented inference across different scales and flips, returning combined detections.","Performs augmented inference across different scales and flips, returning combined detections.","def _forward_augment(self, x):
        
        img_size = x.shape[-2:]  # height, width
        s = [1, 0.83, 0.67]  # scales
        f = [None, 3, None]  # flips (2-ud, 3-lr)
        y = []  # outputs
        for si, fi in zip(s, f):
            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))
            yi = self._forward_once(xi)[0]  # forward
            # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save
            yi = self._descale_pred(yi, fi, si, img_size)
            y.append(yi)
        y = self._clip_augmented(y)  # clip augmented tails
        return torch.cat(y, 1), None","Performs augmented inference across different scales and flips, returning combined detections.","def _forward_augment ( self , x ) : img_size = x . shape [ - 2 : ] # height, width s = [ 1 , 0.83 , 0.67 ] # scales f = [ None , 3 , None ] # flips (2-ud, 3-lr) y = [ ] # outputs for si , fi in zip ( s , f ) : xi = scale_img ( x . flip ( fi ) if fi else x , si , gs = int ( self . stride . max ( ) ) ) yi = self . _forward_once ( xi ) [ 0 ] # forward # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save yi = self . _descale_pred ( yi , fi , si , img_size ) y . append ( yi ) y = self . _clip_augmented ( y ) # clip augmented tails return torch . cat ( y , 1 ) , None","Performs augmented inference across different scales and flips, returning combined detections."
/colossalai/colossalai/checkpoint_io/utils.py,get_shard_filename,"def get_shard_filename(weights_name: str, idx: int):
    """"""
    get shard file name
    """"""
    shard_file = weights_name.replace("".bin"", f""-{idx+1:05d}.bin"")
    shard_file = shard_file.replace("".safetensors"", f""-{idx+1:05d}.safetensors"")
    return shard_file","def get_shard_filename(weights_name: str, idx: int):
    """"""
    get shard file name
    """"""
    shard_file = weights_name.replace("".bin"", f""-{idx+1:05d}.bin"")
    shard_file = shard_file.replace("".safetensors"", f""-{idx+1:05d}.safetensors"")
    return shard_file",get shard file name,get shard file name,"def get_shard_filename(weights_name: str, idx: int):
    
    shard_file = weights_name.replace("".bin"", f""-{idx+1:05d}.bin"")
    shard_file = shard_file.replace("".safetensors"", f""-{idx+1:05d}.safetensors"")
    return shard_file",get shard file name,"def get_shard_filename ( weights_name : str , idx : int ) : shard_file = weights_name . replace ( "".bin"" , f""-{idx+1:05d}.bin"" ) shard_file = shard_file . replace ( "".safetensors"" , f""-{idx+1:05d}.safetensors"" ) return shard_file",get shard file name
/python-patterns/patterns/structural/proxy.py,do_the_job,"def do_the_job(self, user: str) -> None:
        """"""
        logging and controlling access are some examples of proxy usages.
        """"""

        print(f""[log] Doing the job for {user} is requested."")

        if user == ""admin"":
            self._real_subject.do_the_job(user)
        else:
            print(""[log] I can do the job just for `admins`."")","def do_the_job(self, user: str) -> None:
        """"""
        logging and controlling access are some examples of proxy usages.
        """"""

        print(f""[log] Doing the job for {user} is requested."")

        if user == ""admin"":
            self._real_subject.do_the_job(user)
        else:
            print(""[log] I can do the job just for `admins`."")",logging and controlling access are some examples of proxy usages.,logging and controlling access are some examples of proxy usages.,"def do_the_job(self, user: str) -> None:
        

        print(f""[log] Doing the job for {user} is requested."")

        if user == ""admin"":
            self._real_subject.do_the_job(user)
        else:
            print(""[log] I can do the job just for `admins`."")",logging and controlling access are some examples of proxy usages.,"def do_the_job ( self , user : str ) -> None : print ( f""[log] Doing the job for {user} is requested."" ) if user == ""admin"" : self . _real_subject . do_the_job ( user ) else : print ( ""[log] I can do the job just for `admins`."" )",logging and controlling access are some examples of proxy usages.
/cpython/Tools/build/compute-changes.py,get_changed_files,"def get_changed_files(
    ref_a: str = GITHUB_DEFAULT_BRANCH, ref_b: str = ""HEAD""
) -> Set[Path]:
    """"""List the files changed between two Git refs, filtered by change type.""""""
    args = (""git"", ""diff"", ""--name-only"", f""{ref_a}...{ref_b}"", ""--"")
    print(*args)
    changed_files_result = subprocess.run(
        args, stdout=subprocess.PIPE, check=True, encoding=""utf-8""
    )
    changed_files = changed_files_result.stdout.strip().splitlines()
    return frozenset(map(Path, filter(None, map(str.strip, changed_files))))","def get_changed_files(
    ref_a: str = GITHUB_DEFAULT_BRANCH, ref_b: str = ""HEAD""
) -> Set[Path]:
    """"""List the files changed between two Git refs, filtered by change type.""""""
    args = (""git"", ""diff"", ""--name-only"", f""{ref_a}...{ref_b}"", ""--"")
    print(*args)
    changed_files_result = subprocess.run(
        args, stdout=subprocess.PIPE, check=True, encoding=""utf-8""
    )
    changed_files = changed_files_result.stdout.strip().splitlines()
    return frozenset(map(Path, filter(None, map(str.strip, changed_files))))","List the files changed between two Git refs, filtered by change type.","List the files changed between two Git refs, filtered by change type.","def get_changed_files(
    ref_a: str = GITHUB_DEFAULT_BRANCH, ref_b: str = ""HEAD""
) -> Set[Path]:
    
    args = (""git"", ""diff"", ""--name-only"", f""{ref_a}...{ref_b}"", ""--"")
    print(*args)
    changed_files_result = subprocess.run(
        args, stdout=subprocess.PIPE, check=True, encoding=""utf-8""
    )
    changed_files = changed_files_result.stdout.strip().splitlines()
    return frozenset(map(Path, filter(None, map(str.strip, changed_files))))","List the files changed between two Git refs, filtered by change type.","def get_changed_files ( ref_a : str = GITHUB_DEFAULT_BRANCH , ref_b : str = ""HEAD"" ) -> Set [ Path ] : args = ( ""git"" , ""diff"" , ""--name-only"" , f""{ref_a}...{ref_b}"" , ""--"" ) print ( * args ) changed_files_result = subprocess . run ( args , stdout = subprocess . PIPE , check = True , encoding = ""utf-8"" ) changed_files = changed_files_result . stdout . strip ( ) . splitlines ( ) return frozenset ( map ( Path , filter ( None , map ( str . strip , changed_files ) ) ) )","List the files changed between two Git refs, filtered by change type."
/sentry/src/sentry/runner/commands/plugins.py,list,"def list() -> None:
    ""List all installed plugins""
    plugins = [
        (ep.name, dist.metadata[""name""], dist.version, str(dist.locate_file(""."")))
        for dist in importlib.metadata.distributions()
        for ep in dist.entry_points
        if ep.group == ""sentry.plugins""
    ]
    plugins.sort()
    for name, project_name, version, location in plugins:
        click.echo(f""{name}: {project_name} {version} ({location})"")","def list() -> None:
    ""List all installed plugins""
    plugins = [
        (ep.name, dist.metadata[""name""], dist.version, str(dist.locate_file(""."")))
        for dist in importlib.metadata.distributions()
        for ep in dist.entry_points
        if ep.group == ""sentry.plugins""
    ]
    plugins.sort()
    for name, project_name, version, location in plugins:
        click.echo(f""{name}: {project_name} {version} ({location})"")",List all installed plugins,List all installed plugins,"def list() -> None:
    ""List all installed plugins""
    plugins = [
        (ep.name, dist.metadata[""name""], dist.version, str(dist.locate_file(""."")))
        for dist in importlib.metadata.distributions()
        for ep in dist.entry_points
        if ep.group == ""sentry.plugins""
    ]
    plugins.sort()
    for name, project_name, version, location in plugins:
        click.echo(f""{name}: {project_name} {version} ({location})"")",List all installed plugins,"def list ( ) -> None : ""List all installed plugins"" plugins = [ ( ep . name , dist . metadata [ ""name"" ] , dist . version , str ( dist . locate_file ( ""."" ) ) ) for dist in importlib . metadata . distributions ( ) for ep in dist . entry_points if ep . group == ""sentry.plugins"" ] plugins . sort ( ) for name , project_name , version , location in plugins : click . echo ( f""{name}: {project_name} {version} ({location})"" )",List all installed plugins
/localstack/localstack-core/localstack/testing/pytest/fixtures.py,echo_http_server,"def echo_http_server(httpserver: HTTPServer):
    """"""Spins up a local HTTP echo server and returns the endpoint URL""""""

    def _echo(request: Request) -> Response:
        request_json = None
        if request.is_json:
            with contextlib.suppress(ValueError):
                request_json = json.loads(request.data)
        result = {
            ""data"": request.data or ""{}"",
            ""headers"": dict(request.headers),
            ""url"": request.url,
            ""method"": request.method,
            ""json"": request_json,
        }
        response_body = json.dumps(json_safe(result))
        return Response(response_body, status=200)

    httpserver.expect_request("""").respond_with_handler(_echo)
    http_endpoint = httpserver.url_for(""/"")

    return http_endpoint","def echo_http_server(httpserver: HTTPServer):
    """"""Spins up a local HTTP echo server and returns the endpoint URL""""""

    def _echo(request: Request) -> Response:
        request_json = None
        if request.is_json:
            with contextlib.suppress(ValueError):
                request_json = json.loads(request.data)
        result = {
            ""data"": request.data or ""{}"",
            ""headers"": dict(request.headers),
            ""url"": request.url,
            ""method"": request.method,
            ""json"": request_json,
        }
        response_body = json.dumps(json_safe(result))
        return Response(response_body, status=200)

    httpserver.expect_request("""").respond_with_handler(_echo)
    http_endpoint = httpserver.url_for(""/"")

    return http_endpoint",Spins up a local HTTP echo server and returns the endpoint URL,Spins up a local HTTP echo server and returns the endpoint URL,"def echo_http_server(httpserver: HTTPServer):
    

    def _echo(request: Request) -> Response:
        request_json = None
        if request.is_json:
            with contextlib.suppress(ValueError):
                request_json = json.loads(request.data)
        result = {
            ""data"": request.data or ""{}"",
            ""headers"": dict(request.headers),
            ""url"": request.url,
            ""method"": request.method,
            ""json"": request_json,
        }
        response_body = json.dumps(json_safe(result))
        return Response(response_body, status=200)

    httpserver.expect_request("""").respond_with_handler(_echo)
    http_endpoint = httpserver.url_for(""/"")

    return http_endpoint",Spins up a local HTTP echo server and returns the endpoint URL,"def echo_http_server ( httpserver : HTTPServer ) : def _echo ( request : Request ) -> Response : request_json = None if request . is_json : with contextlib . suppress ( ValueError ) : request_json = json . loads ( request . data ) result = { ""data"" : request . data or ""{}"" , ""headers"" : dict ( request . headers ) , ""url"" : request . url , ""method"" : request . method , ""json"" : request_json , } response_body = json . dumps ( json_safe ( result ) ) return Response ( response_body , status = 200 ) httpserver . expect_request ( """" ) . respond_with_handler ( _echo ) http_endpoint = httpserver . url_for ( ""/"" ) return http_endpoint",Spins up a local HTTP echo server and returns the endpoint URL
/gpt4free/g4f/Provider/LegacyLMArena.py,get_model,"def get_model(cls, model: str) -> str:
        """"""Get the internal model name from the user-provided model name.""""""
        if not model:
            return cls.default_model
        
        # Check if the model exists directly in our models list
        if model in cls.models:
            return model
        
        # Check if there's an alias for this model
        if model in cls.model_aliases:
            alias = cls.model_aliases[model]
            # If the alias is a list, randomly select one of the options
            if isinstance(alias, list):
                selected_model = random.choice(alias)
                debug.log(f""LMArena: Selected model '{selected_model}' from alias '{model}'"")
                return selected_model
            debug.log(f""LMArena: Using model '{alias}' for alias '{model}'"")
            return alias
        
        raise ModelNotFoundError(f""Model {model} not found"")","def get_model(cls, model: str) -> str:
        """"""Get the internal model name from the user-provided model name.""""""
        if not model:
            return cls.default_model
        
        # Check if the model exists directly in our models list
        if model in cls.models:
            return model
        
        # Check if there's an alias for this model
        if model in cls.model_aliases:
            alias = cls.model_aliases[model]
            # If the alias is a list, randomly select one of the options
            if isinstance(alias, list):
                selected_model = random.choice(alias)
                debug.log(f""LMArena: Selected model '{selected_model}' from alias '{model}'"")
                return selected_model
            debug.log(f""LMArena: Using model '{alias}' for alias '{model}'"")
            return alias
        
        raise ModelNotFoundError(f""Model {model} not found"")",Get the internal model name from the user-provided model name.,Get the internal model name from the user-provided model name.,"def get_model(cls, model: str) -> str:
        
        if not model:
            return cls.default_model
        
        # Check if the model exists directly in our models list
        if model in cls.models:
            return model
        
        # Check if there's an alias for this model
        if model in cls.model_aliases:
            alias = cls.model_aliases[model]
            # If the alias is a list, randomly select one of the options
            if isinstance(alias, list):
                selected_model = random.choice(alias)
                debug.log(f""LMArena: Selected model '{selected_model}' from alias '{model}'"")
                return selected_model
            debug.log(f""LMArena: Using model '{alias}' for alias '{model}'"")
            return alias
        
        raise ModelNotFoundError(f""Model {model} not found"")",Get the internal model name from the user-provided model name.,"def get_model ( cls , model : str ) -> str : if not model : return cls . default_model # Check if the model exists directly in our models list if model in cls . models : return model # Check if there's an alias for this model if model in cls . model_aliases : alias = cls . model_aliases [ model ] # If the alias is a list, randomly select one of the options if isinstance ( alias , list ) : selected_model = random . choice ( alias ) debug . log ( f""LMArena: Selected model '{selected_model}' from alias '{model}'"" ) return selected_model debug . log ( f""LMArena: Using model '{alias}' for alias '{model}'"" ) return alias raise ModelNotFoundError ( f""Model {model} not found"" )",Get the internal model name from the user-provided model name.
/llama_index/llama-dev/llama_dev/pkg/bump.py,bump_version,"def bump_version(current_version: str, bump_type: BumpType) -> str:
    """"""Bump a version string according to semver rules.""""""
    v = Version(current_version)

    # Parse the version components
    release = v.release
    major = release[0] if len(release) > 0 else 0
    minor = release[1] if len(release) > 1 else 0
    micro = release[2] if len(release) > 2 else 0

    version_str = """"
    if bump_type == BumpType.MAJOR:
        version_str = f""{major + 1}.0.0""
    elif bump_type == BumpType.MINOR:
        version_str = f""{major}.{minor + 1}.0""
    elif bump_type == BumpType.PATCH:
        version_str = f""{major}.{minor}.{micro + 1}""

    return version_str","def bump_version(current_version: str, bump_type: BumpType) -> str:
    """"""Bump a version string according to semver rules.""""""
    v = Version(current_version)

    # Parse the version components
    release = v.release
    major = release[0] if len(release) > 0 else 0
    minor = release[1] if len(release) > 1 else 0
    micro = release[2] if len(release) > 2 else 0

    version_str = """"
    if bump_type == BumpType.MAJOR:
        version_str = f""{major + 1}.0.0""
    elif bump_type == BumpType.MINOR:
        version_str = f""{major}.{minor + 1}.0""
    elif bump_type == BumpType.PATCH:
        version_str = f""{major}.{minor}.{micro + 1}""

    return version_str",Bump a version string according to semver rules.,Bump a version string according to semver rules.,"def bump_version(current_version: str, bump_type: BumpType) -> str:
    
    v = Version(current_version)

    # Parse the version components
    release = v.release
    major = release[0] if len(release) > 0 else 0
    minor = release[1] if len(release) > 1 else 0
    micro = release[2] if len(release) > 2 else 0

    version_str = """"
    if bump_type == BumpType.MAJOR:
        version_str = f""{major + 1}.0.0""
    elif bump_type == BumpType.MINOR:
        version_str = f""{major}.{minor + 1}.0""
    elif bump_type == BumpType.PATCH:
        version_str = f""{major}.{minor}.{micro + 1}""

    return version_str",Bump a version string according to semver rules.,"def bump_version ( current_version : str , bump_type : BumpType ) -> str : v = Version ( current_version ) # Parse the version components release = v . release major = release [ 0 ] if len ( release ) > 0 else 0 minor = release [ 1 ] if len ( release ) > 1 else 0 micro = release [ 2 ] if len ( release ) > 2 else 0 version_str = """" if bump_type == BumpType . MAJOR : version_str = f""{major + 1}.0.0"" elif bump_type == BumpType . MINOR : version_str = f""{major}.{minor + 1}.0"" elif bump_type == BumpType . PATCH : version_str = f""{major}.{minor}.{micro + 1}"" return version_str",Bump a version string according to semver rules.
/crawl4ai/docs/examples/proxy_rotation_demo.py,load_proxies_from_env,"def load_proxies_from_env() -> List[Dict]:
    """"""Load proxies from PROXIES environment variable""""""
    proxies = []
    try:
        proxy_list = os.getenv(""PROXIES"", """").split("","")
        for proxy in proxy_list:
            if not proxy:
                continue
            ip, port, username, password = proxy.split("":"")
            proxies.append({
                ""server"": f""http://{ip}:{port}"",
                ""username"": username,
                ""password"": password,
                ""ip"": ip  # Store original IP for verification
            })
    except Exception as e:
        print(f""Error loading proxies from environment: {e}"")
    return proxies","def load_proxies_from_env() -> List[Dict]:
    """"""Load proxies from PROXIES environment variable""""""
    proxies = []
    try:
        proxy_list = os.getenv(""PROXIES"", """").split("","")
        for proxy in proxy_list:
            if not proxy:
                continue
            ip, port, username, password = proxy.split("":"")
            proxies.append({
                ""username"": username,
                ""password"": password,
                ""ip"": ip  # Store original IP for verification
            })
    except Exception as e:
        print(f""Error loading proxies from environment: {e}"")
    return proxies",Load proxies from PROXIES environment variable,Load proxies from PROXIES environment variable,"def load_proxies_from_env() -> List[Dict]:
    
    proxies = []
    try:
        proxy_list = os.getenv(""PROXIES"", """").split("","")
        for proxy in proxy_list:
            if not proxy:
                continue
            ip, port, username, password = proxy.split("":"")
            proxies.append({
                ""username"": username,
                ""password"": password,
                ""ip"": ip  # Store original IP for verification
            })
    except Exception as e:
        print(f""Error loading proxies from environment: {e}"")
    return proxies",Load proxies from PROXIES environment variable,"def load_proxies_from_env ( ) -> List [ Dict ] : proxies = [ ] try : proxy_list = os . getenv ( ""PROXIES"" , """" ) . split ( "","" ) for proxy in proxy_list : if not proxy : continue ip , port , username , password = proxy . split ( "":"" ) proxies . append ( { ""username"" : username , ""password"" : password , ""ip"" : ip # Store original IP for verification } ) except Exception as e : print ( f""Error loading proxies from environment: {e}"" ) return proxies",Load proxies from PROXIES environment variable
/chatglm-6b/ptuning/trainer.py,_prepare_input,"def _prepare_input(self, data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:
        """"""
        Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.
        """"""
        if isinstance(data, Mapping):
            return type(data)({k: self._prepare_input(v) for k, v in data.items()})
        elif isinstance(data, (tuple, list)):
            return type(data)(self._prepare_input(v) for v in data)
        elif isinstance(data, torch.Tensor):
            kwargs = {""device"": self.args.device}
            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):
                # NLP models inputs are int/uint and those get adjusted to the right dtype of the
                # embedding. Other models such as wav2vec2's inputs are already float and thus
                # may need special handling to match the dtypes of the model
                kwargs.update({""dtype"": self.args.hf_deepspeed_config.dtype()})
            return data.to(**kwargs)
        return data","def _prepare_input(self, data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:
        """"""
        Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.
        """"""
        if isinstance(data, Mapping):
            return type(data)({k: self._prepare_input(v) for k, v in data.items()})
        elif isinstance(data, (tuple, list)):
            return type(data)(self._prepare_input(v) for v in data)
        elif isinstance(data, torch.Tensor):
            kwargs = {""device"": self.args.device}
            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):
                # NLP models inputs are int/uint and those get adjusted to the right dtype of the
                # embedding. Other models such as wav2vec2's inputs are already float and thus
                # may need special handling to match the dtypes of the model
                kwargs.update({""dtype"": self.args.hf_deepspeed_config.dtype()})
            return data.to(**kwargs)
        return data","Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.","Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.","def _prepare_input(self, data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:
        
        if isinstance(data, Mapping):
            return type(data)({k: self._prepare_input(v) for k, v in data.items()})
        elif isinstance(data, (tuple, list)):
            return type(data)(self._prepare_input(v) for v in data)
        elif isinstance(data, torch.Tensor):
            kwargs = {""device"": self.args.device}
            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):
                # NLP models inputs are int/uint and those get adjusted to the right dtype of the
                # embedding. Other models such as wav2vec2's inputs are already float and thus
                # may need special handling to match the dtypes of the model
                kwargs.update({""dtype"": self.args.hf_deepspeed_config.dtype()})
            return data.to(**kwargs)
        return data","Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.","def _prepare_input ( self , data : Union [ torch . Tensor , Any ] ) -> Union [ torch . Tensor , Any ] : if isinstance ( data , Mapping ) : return type ( data ) ( { k : self . _prepare_input ( v ) for k , v in data . items ( ) } ) elif isinstance ( data , ( tuple , list ) ) : return type ( data ) ( self . _prepare_input ( v ) for v in data ) elif isinstance ( data , torch . Tensor ) : kwargs = { ""device"" : self . args . device } if self . deepspeed and ( torch . is_floating_point ( data ) or torch . is_complex ( data ) ) : # NLP models inputs are int/uint and those get adjusted to the right dtype of the # embedding. Other models such as wav2vec2's inputs are already float and thus # may need special handling to match the dtypes of the model kwargs . update ( { ""dtype"" : self . args . hf_deepspeed_config . dtype ( ) } ) return data . to ( ** kwargs ) return data","Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors."
/yolov5/utils/torch_utils.py,smartCrossEntropyLoss,"def smartCrossEntropyLoss(label_smoothing=0.0):
    """"""Returns a CrossEntropyLoss with optional label smoothing for torch>=1.10.0; warns if smoothing on lower
    versions.
    """"""
    if check_version(torch.__version__, ""1.10.0""):
        return nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    if label_smoothing > 0:
        LOGGER.warning(f""WARNING ⚠️ label smoothing {label_smoothing} requires torch>=1.10.0"")
    return nn.CrossEntropyLoss()","def smartCrossEntropyLoss(label_smoothing=0.0):
    """"""Returns a CrossEntropyLoss with optional label smoothing for torch>=1.10.0; warns if smoothing on lower
    versions.
    """"""
    if check_version(torch.__version__, ""1.10.0""):
        return nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    if label_smoothing > 0:
        LOGGER.warning(f""WARNING ⚠️ label smoothing {label_smoothing} requires torch>=1.10.0"")
    return nn.CrossEntropyLoss()","Returns a CrossEntropyLoss with optional label smoothing for torch>=1.10.0; warns if smoothing on lower
versions.",Returns a CrossEntropyLoss with optional label smoothing for torch>=1.10.0; warns if smoothing on lower versions.,"def smartCrossEntropyLoss(label_smoothing=0.0):
    
    if check_version(torch.__version__, ""1.10.0""):
        return nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    if label_smoothing > 0:
        LOGGER.warning(f""WARNING ⚠️ label smoothing {label_smoothing} requires torch>=1.10.0"")
    return nn.CrossEntropyLoss()",Returns a CrossEntropyLoss with optional label smoothing for torch>=1.10.0; warns if smoothing on lower versions.,"def smartCrossEntropyLoss ( label_smoothing = 0.0 ) : if check_version ( torch . __version__ , ""1.10.0"" ) : return nn . CrossEntropyLoss ( label_smoothing = label_smoothing ) if label_smoothing > 0 : LOGGER . warning ( f""WARNING ⚠️ label smoothing {label_smoothing} requires torch>=1.10.0"" ) return nn . CrossEntropyLoss ( )",Returns a CrossEntropyLoss with optional label smoothing for torch>=1.10.0; warns if smoothing on lower versions.
/yolov5/utils/torch_utils.py,prune,"def prune(model, amount=0.3):
    """"""Prunes Conv2d layers in a model to a specified sparsity using L1 unstructured pruning.""""""
    import torch.nn.utils.prune as prune

    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):
            prune.l1_unstructured(m, name=""weight"", amount=amount)  # prune
            prune.remove(m, ""weight"")  # make permanent
    LOGGER.info(f""Model pruned to {sparsity(model):.3g} global sparsity"")","def prune(model, amount=0.3):
    """"""Prunes Conv2d layers in a model to a specified sparsity using L1 unstructured pruning.""""""
    import torch.nn.utils.prune as prune

    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):
            prune.l1_unstructured(m, name=""weight"", amount=amount)  # prune
            prune.remove(m, ""weight"")  # make permanent
    LOGGER.info(f""Model pruned to {sparsity(model):.3g} global sparsity"")",Prunes Conv2d layers in a model to a specified sparsity using L1 unstructured pruning.,Prunes Conv2d layers in a model to a specified sparsity using L1 unstructured pruning.,"def prune(model, amount=0.3):
    
    import torch.nn.utils.prune as prune

    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):
            prune.l1_unstructured(m, name=""weight"", amount=amount)  # prune
            prune.remove(m, ""weight"")  # make permanent
    LOGGER.info(f""Model pruned to {sparsity(model):.3g} global sparsity"")",Prunes Conv2d layers in a model to a specified sparsity using L1 unstructured pruning.,"def prune ( model , amount = 0.3 ) : import torch . nn . utils . prune as prune for name , m in model . named_modules ( ) : if isinstance ( m , nn . Conv2d ) : prune . l1_unstructured ( m , name = ""weight"" , amount = amount ) # prune prune . remove ( m , ""weight"" ) # make permanent LOGGER . info ( f""Model pruned to {sparsity(model):.3g} global sparsity"" )",Prunes Conv2d layers in a model to a specified sparsity using L1 unstructured pruning.
/LLaMA-Factory/src/llamafactory/data/mm_plugin.py,get_mm_plugin,"def get_mm_plugin(
    name: str,
    image_token: Optional[str] = None,
    video_token: Optional[str] = None,
    audio_token: Optional[str] = None,
) -> ""BasePlugin"":
    r""""""Get plugin for multimodal inputs.""""""
    if name not in PLUGINS:
        raise ValueError(f""Multimodal plugin `{name}` not found."")

    return PLUGINS[name](image_token, video_token, audio_token)","def get_mm_plugin(
    name: str,
    image_token: Optional[str] = None,
    video_token: Optional[str] = None,
    audio_token: Optional[str] = None,
) -> ""BasePlugin"":
    r""""""Get plugin for multimodal inputs.""""""
    if name not in PLUGINS:
        raise ValueError(f""Multimodal plugin `{name}` not found."")

    return PLUGINS[name](image_token, video_token, audio_token)",Get plugin for multimodal inputs.,Get plugin for multimodal inputs.,"def get_mm_plugin(
    name: str,
    image_token: Optional[str] = None,
    video_token: Optional[str] = None,
    audio_token: Optional[str] = None,
) -> ""BasePlugin"":
    
    if name not in PLUGINS:
        raise ValueError(f""Multimodal plugin `{name}` not found."")

    return PLUGINS[name](image_token, video_token, audio_token)",Get plugin for multimodal inputs.,"def get_mm_plugin ( name : str , image_token : Optional [ str ] = None , video_token : Optional [ str ] = None , audio_token : Optional [ str ] = None , ) -> ""BasePlugin"" : if name not in PLUGINS : raise ValueError ( f""Multimodal plugin `{name}` not found."" ) return PLUGINS [ name ] ( image_token , video_token , audio_token )",Get plugin for multimodal inputs.
/black/src/black/nodes.py,is_arith_like,"def is_arith_like(node: LN) -> bool:
    """"""Whether node is an arithmetic or a binary arithmetic expression""""""
    return node.type in {
        syms.arith_expr,
        syms.shift_expr,
        syms.xor_expr,
        syms.and_expr,
    }","def is_arith_like(node: LN) -> bool:
    """"""Whether node is an arithmetic or a binary arithmetic expression""""""
    return node.type in {
        syms.arith_expr,
        syms.shift_expr,
        syms.xor_expr,
        syms.and_expr,
    }",Whether node is an arithmetic or a binary arithmetic expression,Whether node is an arithmetic or a binary arithmetic expression,"def is_arith_like(node: LN) -> bool:
    
    return node.type in {
        syms.arith_expr,
        syms.shift_expr,
        syms.xor_expr,
        syms.and_expr,
    }",Whether node is an arithmetic or a binary arithmetic expression,"def is_arith_like ( node : LN ) -> bool : return node . type in { syms . arith_expr , syms . shift_expr , syms . xor_expr , syms . and_expr , }",Whether node is an arithmetic or a binary arithmetic expression
/cpython/Tools/wasm/wasi/__main__.py,build_python_path,"def build_python_path():
    """"""The path to the build Python binary.""""""
    binary = BUILD_DIR / ""python""
    if not binary.is_file():
        binary = binary.with_suffix("".exe"")
        if not binary.is_file():
            raise FileNotFoundError(""Unable to find `python(.exe)` in ""
                                    f""{BUILD_DIR}"")

    return binary","def build_python_path():
    """"""The path to the build Python binary.""""""
    binary = BUILD_DIR / ""python""
    if not binary.is_file():
        binary = binary.with_suffix("".exe"")
        if not binary.is_file():
            raise FileNotFoundError(""Unable to find `python(.exe)` in ""
                                    f""{BUILD_DIR}"")

    return binary",The path to the build Python binary.,The path to the build Python binary.,"def build_python_path():
    
    binary = BUILD_DIR / ""python""
    if not binary.is_file():
        binary = binary.with_suffix("".exe"")
        if not binary.is_file():
            raise FileNotFoundError(""Unable to find `python(.exe)` in ""
                                    f""{BUILD_DIR}"")

    return binary",The path to the build Python binary.,"def build_python_path ( ) : binary = BUILD_DIR / ""python"" if not binary . is_file ( ) : binary = binary . with_suffix ( "".exe"" ) if not binary . is_file ( ) : raise FileNotFoundError ( ""Unable to find `python(.exe)` in "" f""{BUILD_DIR}"" ) return binary",The path to the build Python binary.
/ultralytics/ultralytics/data/base.py,cache_images,"def cache_images(self) -> None:
        """"""Cache images to memory or disk for faster training.""""""
        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        fcn, storage = (self.cache_images_to_disk, ""Disk"") if self.cache == ""disk"" else (self.load_image, ""RAM"")
        with ThreadPool(NUM_THREADS) as pool:
            results = pool.imap(fcn, range(self.ni))
            pbar = TQDM(enumerate(results), total=self.ni, disable=LOCAL_RANK > 0)
            for i, x in pbar:
                if self.cache == ""disk"":
                    b += self.npy_files[i].stat().st_size
                else:  # 'ram'
                    self.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)
                    b += self.ims[i].nbytes
                pbar.desc = f""{self.prefix}Caching images ({b / gb:.1f}GB {storage})""
            pbar.close()","def cache_images(self) -> None:
        """"""Cache images to memory or disk for faster training.""""""
        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        fcn, storage = (self.cache_images_to_disk, ""Disk"") if self.cache == ""disk"" else (self.load_image, ""RAM"")
        with ThreadPool(NUM_THREADS) as pool:
            results = pool.imap(fcn, range(self.ni))
            pbar = TQDM(enumerate(results), total=self.ni, disable=LOCAL_RANK > 0)
            for i, x in pbar:
                if self.cache == ""disk"":
                    b += self.npy_files[i].stat().st_size
                else:  # 'ram'
                    self.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)
                    b += self.ims[i].nbytes
                pbar.desc = f""{self.prefix}Caching images ({b / gb:.1f}GB {storage})""
            pbar.close()",Cache images to memory or disk for faster training.,Cache images to memory or disk for faster training.,"def cache_images(self) -> None:
        
        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        fcn, storage = (self.cache_images_to_disk, ""Disk"") if self.cache == ""disk"" else (self.load_image, ""RAM"")
        with ThreadPool(NUM_THREADS) as pool:
            results = pool.imap(fcn, range(self.ni))
            pbar = TQDM(enumerate(results), total=self.ni, disable=LOCAL_RANK > 0)
            for i, x in pbar:
                if self.cache == ""disk"":
                    b += self.npy_files[i].stat().st_size
                else:  # 'ram'
                    self.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)
                    b += self.ims[i].nbytes
                pbar.desc = f""{self.prefix}Caching images ({b / gb:.1f}GB {storage})""
            pbar.close()",Cache images to memory or disk for faster training.,"def cache_images ( self ) -> None : b , gb = 0 , 1 << 30 # bytes of cached images, bytes per gigabytes fcn , storage = ( self . cache_images_to_disk , ""Disk"" ) if self . cache == ""disk"" else ( self . load_image , ""RAM"" ) with ThreadPool ( NUM_THREADS ) as pool : results = pool . imap ( fcn , range ( self . ni ) ) pbar = TQDM ( enumerate ( results ) , total = self . ni , disable = LOCAL_RANK > 0 ) for i , x in pbar : if self . cache == ""disk"" : b += self . npy_files [ i ] . stat ( ) . st_size else : # 'ram' self . ims [ i ] , self . im_hw0 [ i ] , self . im_hw [ i ] = x # im, hw_orig, hw_resized = load_image(self, i) b += self . ims [ i ] . nbytes pbar . desc = f""{self.prefix}Caching images ({b / gb:.1f}GB {storage})"" pbar . close ( )",Cache images to memory or disk for faster training.
/ansible/packaging/release.py,get_release_artifact_details,"def get_release_artifact_details(repository: str, version: Version, validate: bool) -> list[ReleaseArtifact]:
    """"""Return information about the release artifacts hosted on PyPI.""""""
    data = get_pypi_project(repository, 'ansible-core', version)
    artifacts = [describe_release_artifact(version, item, validate) for item in data[""urls""]]

    expected_artifact_types = {""bdist_wheel"", ""sdist""}
    found_artifact_types = set(artifact.package_type for artifact in artifacts)

    if found_artifact_types != expected_artifact_types:
        raise RuntimeError(f""Expected {expected_artifact_types} artifact types, but found {found_artifact_types} instead."")

    return artifacts","def get_release_artifact_details(repository: str, version: Version, validate: bool) -> list[ReleaseArtifact]:
    """"""Return information about the release artifacts hosted on PyPI.""""""
    data = get_pypi_project(repository, 'ansible-core', version)
    artifacts = [describe_release_artifact(version, item, validate) for item in data[""urls""]]

    expected_artifact_types = {""bdist_wheel"", ""sdist""}
    found_artifact_types = set(artifact.package_type for artifact in artifacts)

    if found_artifact_types != expected_artifact_types:
        raise RuntimeError(f""Expected {expected_artifact_types} artifact types, but found {found_artifact_types} instead."")

    return artifacts",Return information about the release artifacts hosted on PyPI.,Return information about the release artifacts hosted on PyPI.,"def get_release_artifact_details(repository: str, version: Version, validate: bool) -> list[ReleaseArtifact]:
    
    data = get_pypi_project(repository, 'ansible-core', version)
    artifacts = [describe_release_artifact(version, item, validate) for item in data[""urls""]]

    expected_artifact_types = {""bdist_wheel"", ""sdist""}
    found_artifact_types = set(artifact.package_type for artifact in artifacts)

    if found_artifact_types != expected_artifact_types:
        raise RuntimeError(f""Expected {expected_artifact_types} artifact types, but found {found_artifact_types} instead."")

    return artifacts",Return information about the release artifacts hosted on PyPI.,"def get_release_artifact_details ( repository : str , version : Version , validate : bool ) -> list [ ReleaseArtifact ] : data = get_pypi_project ( repository , 'ansible-core' , version ) artifacts = [ describe_release_artifact ( version , item , validate ) for item in data [ ""urls"" ] ] expected_artifact_types = { ""bdist_wheel"" , ""sdist"" } found_artifact_types = set ( artifact . package_type for artifact in artifacts ) if found_artifact_types != expected_artifact_types : raise RuntimeError ( f""Expected {expected_artifact_types} artifact types, but found {found_artifact_types} instead."" ) return artifacts",Return information about the release artifacts hosted on PyPI.
/AutoGPT/.github/workflows/scripts/check_actions_status.py,make_api_request,"def make_api_request(url: str, headers: Dict[str, str]) -> Dict:
    """"""Make an API request and return the JSON response.""""""
    try:
        print(""Making API request to:"", url)
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f""Error: API request failed. {e}"")
        sys.exit(1)","def make_api_request(url: str, headers: Dict[str, str]) -> Dict:
    """"""Make an API request and return the JSON response.""""""
    try:
        print(""Making API request to:"", url)
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f""Error: API request failed. {e}"")
        sys.exit(1)",Make an API request and return the JSON response.,Make an API request and return the JSON response.,"def make_api_request(url: str, headers: Dict[str, str]) -> Dict:
    
    try:
        print(""Making API request to:"", url)
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f""Error: API request failed. {e}"")
        sys.exit(1)",Make an API request and return the JSON response.,"def make_api_request ( url : str , headers : Dict [ str , str ] ) -> Dict : try : print ( ""Making API request to:"" , url ) response = requests . get ( url , headers = headers , timeout = 10 ) response . raise_for_status ( ) return response . json ( ) except requests . RequestException as e : print ( f""Error: API request failed. {e}"" ) sys . exit ( 1 )",Make an API request and return the JSON response.
/ultralytics/ultralytics/solutions/config.py,update,"def update(self, **kwargs):
        """"""Update configuration parameters with new values provided as keyword arguments.""""""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                url = ""https://docs.ultralytics.com/solutions/#solutions-arguments""
                raise ValueError(f""{key} is not a valid solution argument, see {url}"")

        return self","def update(self, **kwargs):
        """"""Update configuration parameters with new values provided as keyword arguments.""""""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                raise ValueError(f""{key} is not a valid solution argument, see {url}"")

        return self",Update configuration parameters with new values provided as keyword arguments.,Update configuration parameters with new values provided as keyword arguments.,"def update(self, **kwargs):
        
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                raise ValueError(f""{key} is not a valid solution argument, see {url}"")

        return self",Update configuration parameters with new values provided as keyword arguments.,"def update ( self , ** kwargs ) : for key , value in kwargs . items ( ) : if hasattr ( self , key ) : setattr ( self , key , value ) else : raise ValueError ( f""{key} is not a valid solution argument, see {url}"" ) return self",Update configuration parameters with new values provided as keyword arguments.
/AutoGPT/classic/original_autogpt/autogpt/app/config.py,build_config_from_env,"def build_config_from_env(cls, project_root: Path = PROJECT_ROOT) -> AppConfig:
        """"""Initialize the Config class""""""

        config = cls.build_agent_configuration()
        config.project_root = project_root

        # Make relative paths absolute
        for k in {
            ""azure_config_file"",  # TODO: move from project root
        }:
            setattr(config, k, project_root / getattr(config, k))

        if (
            config.openai_credentials
            and config.openai_credentials.api_type == SecretStr(""azure"")
            and (config_file := config.azure_config_file)
        ):
            config.openai_credentials.load_azure_config(config_file)

        return config","def build_config_from_env(cls, project_root: Path = PROJECT_ROOT) -> AppConfig:
        """"""Initialize the Config class""""""

        config = cls.build_agent_configuration()
        config.project_root = project_root

        # Make relative paths absolute
        for k in {
            ""azure_config_file"",  # TODO: move from project root
        }:
            setattr(config, k, project_root / getattr(config, k))

        if (
            config.openai_credentials
            and config.openai_credentials.api_type == SecretStr(""azure"")
            and (config_file := config.azure_config_file)
        ):
            config.openai_credentials.load_azure_config(config_file)

        return config",Initialize the Config class,Initialize the Config class,"def build_config_from_env(cls, project_root: Path = PROJECT_ROOT) -> AppConfig:
        

        config = cls.build_agent_configuration()
        config.project_root = project_root

        # Make relative paths absolute
        for k in {
            ""azure_config_file"",  # TODO: move from project root
        }:
            setattr(config, k, project_root / getattr(config, k))

        if (
            config.openai_credentials
            and config.openai_credentials.api_type == SecretStr(""azure"")
            and (config_file := config.azure_config_file)
        ):
            config.openai_credentials.load_azure_config(config_file)

        return config",Initialize the Config class,"def build_config_from_env ( cls , project_root : Path = PROJECT_ROOT ) -> AppConfig : config = cls . build_agent_configuration ( ) config . project_root = project_root # Make relative paths absolute for k in { ""azure_config_file"" , # TODO: move from project root } : setattr ( config , k , project_root / getattr ( config , k ) ) if ( config . openai_credentials and config . openai_credentials . api_type == SecretStr ( ""azure"" ) and ( config_file := config . azure_config_file ) ) : config . openai_credentials . load_azure_config ( config_file ) return config",Initialize the Config class
/pytorch/functorch/einops/rearrange.py,composition_to_dims,"def composition_to_dims(
        composition: Sequence[Union[list[Union[str, AnonymousAxis]], str]],
    ) -> list[Union[str, tuple[str, ...]]]:
        """"""Convert a `ParsedExpression.composition` into a `Tensor.__getitem__` index of strings representing first
        class dims.""""""
        dim_composition: list[Union[str, tuple[str, ...]]] = []
        for dimension in composition:
            if isinstance(dimension, list):
                dim_composition.append(
                    tuple(
                        dim
                        for identifier in dimension
                        for dim in identifier_dim_map[identifier]
                    )
                )
            elif dimension == _ellipsis:
                dim_composition.extend(identifier_dim_map[_ellipsis])
            else:
                raise ValueError(f""Unexpected dimension: {dimension}"")
        return dim_composition","def composition_to_dims(
        composition: Sequence[Union[list[Union[str, AnonymousAxis]], str]],
    ) -> list[Union[str, tuple[str, ...]]]:
        """"""Convert a `ParsedExpression.composition` into a `Tensor.__getitem__` index of strings representing first
        class dims.""""""
        dim_composition: list[Union[str, tuple[str, ...]]] = []
        for dimension in composition:
            if isinstance(dimension, list):
                dim_composition.append(
                    tuple(
                        dim
                        for identifier in dimension
                        for dim in identifier_dim_map[identifier]
                    )
                )
            elif dimension == _ellipsis:
                dim_composition.extend(identifier_dim_map[_ellipsis])
            else:
                raise ValueError(f""Unexpected dimension: {dimension}"")
        return dim_composition","Convert a `ParsedExpression.composition` into a `Tensor.__getitem__` index of strings representing first
class dims.",Convert a `ParsedExpression.composition` into a `Tensor.__getitem__` index of strings representing first class dims.,"def composition_to_dims(
        composition: Sequence[Union[list[Union[str, AnonymousAxis]], str]],
    ) -> list[Union[str, tuple[str, ...]]]:
        
        dim_composition: list[Union[str, tuple[str, ...]]] = []
        for dimension in composition:
            if isinstance(dimension, list):
                dim_composition.append(
                    tuple(
                        dim
                        for identifier in dimension
                        for dim in identifier_dim_map[identifier]
                    )
                )
            elif dimension == _ellipsis:
                dim_composition.extend(identifier_dim_map[_ellipsis])
            else:
                raise ValueError(f""Unexpected dimension: {dimension}"")
        return dim_composition",Convert a  into a `Tensor.__getitem__` index of strings representing first class dims.,"def composition_to_dims ( composition : Sequence [ Union [ list [ Union [ str , AnonymousAxis ] ] , str ] ] , ) -> list [ Union [ str , tuple [ str , ... ] ] ] : dim_composition : list [ Union [ str , tuple [ str , ... ] ] ] = [ ] for dimension in composition : if isinstance ( dimension , list ) : dim_composition . append ( tuple ( dim for identifier in dimension for dim in identifier_dim_map [ identifier ] ) ) elif dimension == _ellipsis : dim_composition . extend ( identifier_dim_map [ _ellipsis ] ) else : raise ValueError ( f""Unexpected dimension: {dimension}"" ) return dim_composition",Convert a into a `Tensor.__getitem__` index of strings representing first class dims.
/yolov5/utils/dataloaders.py,_unzip,"def _unzip(self, path):
        """"""Unzips a .zip file at 'path', returning success status, unzipped directory, and path to YAML file within.""""""
        if not str(path).endswith("".zip""):  # path is data.yaml
            return False, None, path
        assert Path(path).is_file(), f""Error unzipping {path}, file not found""
        unzip_file(path, path=path.parent)
        dir = path.with_suffix("""")  # dataset directory == zip name
        assert dir.is_dir(), f""Error unzipping {path}, {dir} not found. path/to/abc.zip MUST unzip to path/to/abc/""
        return True, str(dir), self._find_yaml(dir)","def _unzip(self, path):
        """"""Unzips a .zip file at 'path', returning success status, unzipped directory, and path to YAML file within.""""""
        if not str(path).endswith("".zip""):  # path is data.yaml
            return False, None, path
        assert Path(path).is_file(), f""Error unzipping {path}, file not found""
        unzip_file(path, path=path.parent)
        dir = path.with_suffix("""")  # dataset directory == zip name
        assert dir.is_dir(), f""Error unzipping {path}, {dir} not found. path/to/abc.zip MUST unzip to path/to/abc/""
        return True, str(dir), self._find_yaml(dir)","Unzips a .zip file at 'path', returning success status, unzipped directory, and path to YAML file within.","Unzips a .zip file at 'path', returning success status, unzipped directory, and path to YAML file within.","def _unzip(self, path):
        
        if not str(path).endswith("".zip""):  # path is data.yaml
            return False, None, path
        assert Path(path).is_file(), f""Error unzipping {path}, file not found""
        unzip_file(path, path=path.parent)
        dir = path.with_suffix("""")  # dataset directory == zip name
        assert dir.is_dir(), f""Error unzipping {path}, {dir} not found. path/to/abc.zip MUST unzip to path/to/abc/""
        return True, str(dir), self._find_yaml(dir)","Unzips a .zip file at 'path', returning success status, unzipped directory, and path to YAML file within.","def _unzip ( self , path ) : if not str ( path ) . endswith ( "".zip"" ) : # path is data.yaml return False , None , path assert Path ( path ) . is_file ( ) , f""Error unzipping {path}, file not found"" unzip_file ( path , path = path . parent ) dir = path . with_suffix ( """" ) # dataset directory == zip name assert dir . is_dir ( ) , f""Error unzipping {path}, {dir} not found. path/to/abc.zip MUST unzip to path/to/abc/"" return True , str ( dir ) , self . _find_yaml ( dir )","Unzips a .zip file at 'path', returning success status, unzipped directory, and path to YAML file within."
/llama_index/llama-dev/llama_dev/pkg/bump.py,update_pyproject_version,"def update_pyproject_version(package_path: Path, new_version: str) -> None:
    """"""Update the version in a pyproject.toml file.""""""
    pyproject_path = package_path / ""pyproject.toml""

    # Read the file content
    with open(pyproject_path, ""r"") as f:
        content = f.read()

    pattern = r'(\[project\][^\[]*?version\s*=\s*[""\'])([^""\']+)([""\'])'
    new_content = re.sub(pattern, rf""\g<1>{new_version}\g<3>"", content, flags=re.DOTALL)

    # Write the updated content back
    with open(pyproject_path, ""w"") as f:
        f.write(new_content)","def update_pyproject_version(package_path: Path, new_version: str) -> None:
    """"""Update the version in a pyproject.toml file.""""""
    pyproject_path = package_path / ""pyproject.toml""

    # Read the file content
    with open(pyproject_path, ""r"") as f:
        content = f.read()

    pattern = r'(\[project\][^\[]*?version\s*=\s*[""\'])([^""\']+)([""\'])'
    new_content = re.sub(pattern, rf""\g<1>{new_version}\g<3>"", content, flags=re.DOTALL)

    # Write the updated content back
    with open(pyproject_path, ""w"") as f:
        f.write(new_content)",Update the version in a pyproject.toml file.,Update the version in a pyproject.toml file.,"def update_pyproject_version(package_path: Path, new_version: str) -> None:
    
    pyproject_path = package_path / ""pyproject.toml""

    # Read the file content
    with open(pyproject_path, ""r"") as f:
        content = f.read()

    pattern = r'(\[project\][^\[]*?version\s*=\s*[""\'])([^""\']+)([""\'])'
    new_content = re.sub(pattern, rf""\g<1>{new_version}\g<3>"", content, flags=re.DOTALL)

    # Write the updated content back
    with open(pyproject_path, ""w"") as f:
        f.write(new_content)",Update the version in a pyproject.toml file.,"def update_pyproject_version ( package_path : Path , new_version : str ) -> None : pyproject_path = package_path / ""pyproject.toml"" # Read the file content with open ( pyproject_path , ""r"" ) as f : content = f . read ( ) pattern = r'(\[project\][^\[]*?version\s*=\s*[""\'])([^""\']+)([""\'])' new_content = re . sub ( pattern , rf""\g<1>{new_version}\g<3>"" , content , flags = re . DOTALL ) # Write the updated content back with open ( pyproject_path , ""w"" ) as f : f . write ( new_content )",Update the version in a pyproject.toml file.
/ansible/packaging/release.py,check_ansible_version,"def check_ansible_version(current_version: Version, requested_version: Version) -> None:
    """"""Verify the requested version is valid for the current version.""""""
    if requested_version.release[:2] != current_version.release[:2]:
        raise ApplicationError(f""Version {requested_version} does not match the major and minor portion of the current version: {current_version}"")

    if requested_version < current_version:
        raise ApplicationError(f""Version {requested_version} is older than the current version: {current_version}"")","def check_ansible_version(current_version: Version, requested_version: Version) -> None:
    """"""Verify the requested version is valid for the current version.""""""
    if requested_version.release[:2] != current_version.release[:2]:
        raise ApplicationError(f""Version {requested_version} does not match the major and minor portion of the current version: {current_version}"")

    if requested_version < current_version:
        raise ApplicationError(f""Version {requested_version} is older than the current version: {current_version}"")",Verify the requested version is valid for the current version.,Verify the requested version is valid for the current version.,"def check_ansible_version(current_version: Version, requested_version: Version) -> None:
    
    if requested_version.release[:2] != current_version.release[:2]:
        raise ApplicationError(f""Version {requested_version} does not match the major and minor portion of the current version: {current_version}"")

    if requested_version < current_version:
        raise ApplicationError(f""Version {requested_version} is older than the current version: {current_version}"")",Verify the requested version is valid for the current version.,"def check_ansible_version ( current_version : Version , requested_version : Version ) -> None : if requested_version . release [ : 2 ] != current_version . release [ : 2 ] : raise ApplicationError ( f""Version {requested_version} does not match the major and minor portion of the current version: {current_version}"" ) if requested_version < current_version : raise ApplicationError ( f""Version {requested_version} is older than the current version: {current_version}"" )",Verify the requested version is valid for the current version.
/yolov5/utils/dataloaders.py,__init__,"def __init__(self, path=""coco128.yaml"", autodownload=False):
        """"""Initializes HUBDatasetStats with optional auto-download for datasets, given a path to dataset YAML or ZIP
        file.
        """"""
        zipped, data_dir, yaml_path = self._unzip(Path(path))
        try:
            with open(check_yaml(yaml_path), errors=""ignore"") as f:
                data = yaml.safe_load(f)  # data dict
                if zipped:
                    data[""path""] = data_dir
        except Exception as e:
            raise Exception(""error/HUB/dataset_stats/yaml_load"") from e

        check_dataset(data, autodownload)  # download dataset if missing
        self.hub_dir = Path(data[""path""] + ""-hub"")
        self.im_dir = self.hub_dir / ""images""
        self.im_dir.mkdir(parents=True, exist_ok=True)  # makes /images
        self.stats = {""nc"": data[""nc""], ""names"": list(data[""names""].values())}  # statistics dictionary
        self.data = data","def __init__(self, path=""coco128.yaml"", autodownload=False):
        """"""Initializes HUBDatasetStats with optional auto-download for datasets, given a path to dataset YAML or ZIP
        file.
        """"""
        zipped, data_dir, yaml_path = self._unzip(Path(path))
        try:
            with open(check_yaml(yaml_path), errors=""ignore"") as f:
                data = yaml.safe_load(f)  # data dict
                if zipped:
                    data[""path""] = data_dir
        except Exception as e:
            raise Exception(""error/HUB/dataset_stats/yaml_load"") from e

        check_dataset(data, autodownload)  # download dataset if missing
        self.hub_dir = Path(data[""path""] + ""-hub"")
        self.im_dir = self.hub_dir / ""images""
        self.im_dir.mkdir(parents=True, exist_ok=True)  # makes /images
        self.stats = {""nc"": data[""nc""], ""names"": list(data[""names""].values())}  # statistics dictionary
        self.data = data","Initializes HUBDatasetStats with optional auto-download for datasets, given a path to dataset YAML or ZIP
file.","Initializes HUBDatasetStats with optional auto-download for datasets, given a path to dataset YAML or ZIP file.","def __init__(self, path=""coco128.yaml"", autodownload=False):
        
        zipped, data_dir, yaml_path = self._unzip(Path(path))
        try:
            with open(check_yaml(yaml_path), errors=""ignore"") as f:
                data = yaml.safe_load(f)  # data dict
                if zipped:
                    data[""path""] = data_dir
        except Exception as e:
            raise Exception(""error/HUB/dataset_stats/yaml_load"") from e

        check_dataset(data, autodownload)  # download dataset if missing
        self.hub_dir = Path(data[""path""] + ""-hub"")
        self.im_dir = self.hub_dir / ""images""
        self.im_dir.mkdir(parents=True, exist_ok=True)  # makes /images
        self.stats = {""nc"": data[""nc""], ""names"": list(data[""names""].values())}  # statistics dictionary
        self.data = data","Initializes HUBDatasetStats with optional auto-download for datasets, given a path to dataset YAML or ZIP file.","def __init__ ( self , path = ""coco128.yaml"" , autodownload = False ) : zipped , data_dir , yaml_path = self . _unzip ( Path ( path ) ) try : with open ( check_yaml ( yaml_path ) , errors = ""ignore"" ) as f : data = yaml . safe_load ( f ) # data dict if zipped : data [ ""path"" ] = data_dir except Exception as e : raise Exception ( ""error/HUB/dataset_stats/yaml_load"" ) from e check_dataset ( data , autodownload ) # download dataset if missing self . hub_dir = Path ( data [ ""path"" ] + ""-hub"" ) self . im_dir = self . hub_dir / ""images"" self . im_dir . mkdir ( parents = True , exist_ok = True ) # makes /images self . stats = { ""nc"" : data [ ""nc"" ] , ""names"" : list ( data [ ""names"" ] . values ( ) ) } # statistics dictionary self . data = data","Initializes HUBDatasetStats with optional auto-download for datasets, given a path to dataset YAML or ZIP file."
/transformers/benchmark/benchmarks_entrypoint.py,initialise_benchmark,"def initialise_benchmark(self, metadata: Dict[str, str]) -> int:
        """"""
        Creates a new benchmark, returns the benchmark id
        """"""
        # gpu_name: str, model_id: str
        with self.conn.cursor() as cur:
            cur.execute(
                ""INSERT INTO benchmarks (branch, commit_id, commit_message, metadata) VALUES (%s, %s, %s, %s) RETURNING benchmark_id"",
                (self.branch, self.commit_id, self.commit_msg, metadata),
            )
            benchmark_id = cur.fetchone()[0]
            logger.debug(f""initialised benchmark #{benchmark_id}"")
            return benchmark_id","def initialise_benchmark(self, metadata: Dict[str, str]) -> int:
        """"""
        Creates a new benchmark, returns the benchmark id
        """"""
        # gpu_name: str, model_id: str
        with self.conn.cursor() as cur:
            cur.execute(
                ""INSERT INTO benchmarks (branch, commit_id, commit_message, metadata) VALUES (%s, %s, %s, %s) RETURNING benchmark_id"",
                (self.branch, self.commit_id, self.commit_msg, metadata),
            )
            benchmark_id = cur.fetchone()[0]
            logger.debug(f""initialised benchmark #{benchmark_id}"")
            return benchmark_id","Creates a new benchmark, returns the benchmark id","Creates a new benchmark, returns the benchmark id","def initialise_benchmark(self, metadata: Dict[str, str]) -> int:
        
        # gpu_name: str, model_id: str
        with self.conn.cursor() as cur:
            cur.execute(
                ""INSERT INTO benchmarks (branch, commit_id, commit_message, metadata) VALUES (%s, %s, %s, %s) RETURNING benchmark_id"",
                (self.branch, self.commit_id, self.commit_msg, metadata),
            )
            benchmark_id = cur.fetchone()[0]
            logger.debug(f""initialised benchmark #{benchmark_id}"")
            return benchmark_id","Creates a new benchmark, returns the benchmark id","def initialise_benchmark ( self , metadata : Dict [ str , str ] ) -> int : # gpu_name: str, model_id: str with self . conn . cursor ( ) as cur : cur . execute ( ""INSERT INTO benchmarks (branch, commit_id, commit_message, metadata) VALUES (%s, %s, %s, %s) RETURNING benchmark_id"" , ( self . branch , self . commit_id , self . commit_msg , metadata ) , ) benchmark_id = cur . fetchone ( ) [ 0 ] logger . debug ( f""initialised benchmark #{benchmark_id}"" ) return benchmark_id","Creates a new benchmark, returns the benchmark id"
/manim/manimlib/mobject/svg/old_tex_mobject.py,balance_braces,"def balance_braces(self, tex: str) -> str:
        """"""
        Makes Tex resiliant to unmatched braces
        """"""
        num_unclosed_brackets = 0
        for i in range(len(tex)):
            if i > 0 and tex[i - 1] == ""\\"":
                # So as to not count '\{' type expressions
                continue
            char = tex[i]
            if char == ""{"":
                num_unclosed_brackets += 1
            elif char == ""}"":
                if num_unclosed_brackets == 0:
                    tex = ""{"" + tex
                else:
                    num_unclosed_brackets -= 1
        tex += num_unclosed_brackets * ""}""
        return tex","def balance_braces(self, tex: str) -> str:
        """"""
        Makes Tex resiliant to unmatched braces
        """"""
        num_unclosed_brackets = 0
        for i in range(len(tex)):
            if i > 0 and tex[i - 1] == ""\\"":
                # So as to not count '\{' type expressions
                continue
            char = tex[i]
            if char == ""{"":
                num_unclosed_brackets += 1
            elif char == ""}"":
                if num_unclosed_brackets == 0:
                    tex = ""{"" + tex
                else:
                    num_unclosed_brackets -= 1
        tex += num_unclosed_brackets * ""}""
        return tex",Makes Tex resiliant to unmatched braces,Makes Tex resiliant to unmatched braces,"def balance_braces(self, tex: str) -> str:
        
        num_unclosed_brackets = 0
        for i in range(len(tex)):
            if i > 0 and tex[i - 1] == ""\\"":
                # So as to not count '\{' type expressions
                continue
            char = tex[i]
            if char == ""{"":
                num_unclosed_brackets += 1
            elif char == ""}"":
                if num_unclosed_brackets == 0:
                    tex = ""{"" + tex
                else:
                    num_unclosed_brackets -= 1
        tex += num_unclosed_brackets * ""}""
        return tex",Makes Tex resiliant to unmatched braces,"def balance_braces ( self , tex : str ) -> str : num_unclosed_brackets = 0 for i in range ( len ( tex ) ) : if i > 0 and tex [ i - 1 ] == ""\\"" : # So as to not count '\{' type expressions continue char = tex [ i ] if char == ""{"" : num_unclosed_brackets += 1 elif char == ""}"" : if num_unclosed_brackets == 0 : tex = ""{"" + tex else : num_unclosed_brackets -= 1 tex += num_unclosed_brackets * ""}"" return tex",Makes Tex resiliant to unmatched braces
/yolov5/utils/loggers/comet/__init__.py,check_dataset,"def check_dataset(self, data_file):
        """"""Validates the dataset configuration by loading the YAML file specified in `data_file`.""""""
        with open(data_file) as f:
            data_config = yaml.safe_load(f)

        path = data_config.get(""path"")
        if path and path.startswith(COMET_PREFIX):
            path = data_config[""path""].replace(COMET_PREFIX, """")
            return self.download_dataset_artifact(path)
        self.log_asset(self.opt.data, metadata={""type"": ""data-config-file""})

        return check_dataset(data_file)","def check_dataset(self, data_file):
        """"""Validates the dataset configuration by loading the YAML file specified in `data_file`.""""""
        with open(data_file) as f:
            data_config = yaml.safe_load(f)

        path = data_config.get(""path"")
        if path and path.startswith(COMET_PREFIX):
            path = data_config[""path""].replace(COMET_PREFIX, """")
            return self.download_dataset_artifact(path)
        self.log_asset(self.opt.data, metadata={""type"": ""data-config-file""})

        return check_dataset(data_file)",Validates the dataset configuration by loading the YAML file specified in `data_file`.,Validates the dataset configuration by loading the YAML file specified in `data_file`.,"def check_dataset(self, data_file):
        
        with open(data_file) as f:
            data_config = yaml.safe_load(f)

        path = data_config.get(""path"")
        if path and path.startswith(COMET_PREFIX):
            path = data_config[""path""].replace(COMET_PREFIX, """")
            return self.download_dataset_artifact(path)
        self.log_asset(self.opt.data, metadata={""type"": ""data-config-file""})

        return check_dataset(data_file)",Validates the dataset configuration by loading the YAML file specified in `data_file`.,"def check_dataset ( self , data_file ) : with open ( data_file ) as f : data_config = yaml . safe_load ( f ) path = data_config . get ( ""path"" ) if path and path . startswith ( COMET_PREFIX ) : path = data_config [ ""path"" ] . replace ( COMET_PREFIX , """" ) return self . download_dataset_artifact ( path ) self . log_asset ( self . opt . data , metadata = { ""type"" : ""data-config-file"" } ) return check_dataset ( data_file )",Validates the dataset configuration by loading the YAML file specified in `data_file`.
/ansible/packaging/release.py,show_version,"def show_version(final: bool = False, pre: str | None = None) -> None:
    """"""Show the current and next ansible-core version.""""""
    current_version = get_ansible_version(mode=VersionMode.ALLOW_DEV_POST)
    display.show(f""Current version: {current_version}"")

    try:
        next_version = get_next_version(current_version, final=final, pre=pre)
    except ApplicationError as ex:
        display.show(f""   Next version: Unknown - {ex}"")
    else:
        display.show(f""   Next version: {next_version}"")

        check_ansible_version(current_version, next_version)","def show_version(final: bool = False, pre: str | None = None) -> None:
    """"""Show the current and next ansible-core version.""""""
    current_version = get_ansible_version(mode=VersionMode.ALLOW_DEV_POST)
    display.show(f""Current version: {current_version}"")

    try:
        next_version = get_next_version(current_version, final=final, pre=pre)
    except ApplicationError as ex:
        display.show(f""   Next version: Unknown - {ex}"")
    else:
        display.show(f""   Next version: {next_version}"")

        check_ansible_version(current_version, next_version)",Show the current and next ansible-core version.,Show the current and next ansible-core version.,"def show_version(final: bool = False, pre: str | None = None) -> None:
    
    current_version = get_ansible_version(mode=VersionMode.ALLOW_DEV_POST)
    display.show(f""Current version: {current_version}"")

    try:
        next_version = get_next_version(current_version, final=final, pre=pre)
    except ApplicationError as ex:
        display.show(f""   Next version: Unknown - {ex}"")
    else:
        display.show(f""   Next version: {next_version}"")

        check_ansible_version(current_version, next_version)",Show the current and next ansible-core version.,"def show_version ( final : bool = False , pre : str | None = None ) -> None : current_version = get_ansible_version ( mode = VersionMode . ALLOW_DEV_POST ) display . show ( f""Current version: {current_version}"" ) try : next_version = get_next_version ( current_version , final = final , pre = pre ) except ApplicationError as ex : display . show ( f""   Next version: Unknown - {ex}"" ) else : display . show ( f""   Next version: {next_version}"" ) check_ansible_version ( current_version , next_version )",Show the current and next ansible-core version.
/odoo/odoo/fields.py,_compute,"def _compute(self, records):
        """"""Add the default properties value when the container is changed.""""""
        for record in records:
            record[self.name] = self._add_default_values(
                record.env,
                {self.name: record[self.name], self.definition_record: record[self.definition_record]},
            )","def _compute(self, records):
        """"""Add the default properties value when the container is changed.""""""
        for record in records:
            record[self.name] = self._add_default_values(
                record.env,
                {self.name: record[self.name], self.definition_record: record[self.definition_record]},
            )",Add the default properties value when the container is changed.,Add the default properties value when the container is changed.,"def _compute(self, records):
        
        for record in records:
            record[self.name] = self._add_default_values(
                record.env,
                {self.name: record[self.name], self.definition_record: record[self.definition_record]},
            )",Add the default properties value when the container is changed.,"def _compute ( self , records ) : for record in records : record [ self . name ] = self . _add_default_values ( record . env , { self . name : record [ self . name ] , self . definition_record : record [ self . definition_record ] } , )",Add the default properties value when the container is changed.
/Fooocus/ldm_patched/t2ia/adapter.py,conv_nd,"def conv_nd(dims, *args, **kwargs):
    """"""
    Create a 1D, 2D, or 3D convolution module.
    """"""
    if dims == 1:
        return nn.Conv1d(*args, **kwargs)
    elif dims == 2:
        return nn.Conv2d(*args, **kwargs)
    elif dims == 3:
        return nn.Conv3d(*args, **kwargs)
    raise ValueError(f""unsupported dimensions: {dims}"")","def conv_nd(dims, *args, **kwargs):
    """"""
    Create a 1D, 2D, or 3D convolution module.
    """"""
    if dims == 1:
        return nn.Conv1d(*args, **kwargs)
    elif dims == 2:
        return nn.Conv2d(*args, **kwargs)
    elif dims == 3:
        return nn.Conv3d(*args, **kwargs)
    raise ValueError(f""unsupported dimensions: {dims}"")","Create a 1D, 2D, or 3D convolution module.","Create a 1D, 2D, or 3D convolution module.","def conv_nd(dims, *args, **kwargs):
    
    if dims == 1:
        return nn.Conv1d(*args, **kwargs)
    elif dims == 2:
        return nn.Conv2d(*args, **kwargs)
    elif dims == 3:
        return nn.Conv3d(*args, **kwargs)
    raise ValueError(f""unsupported dimensions: {dims}"")","Create a 1D, 2D, or 3D convolution module.","def conv_nd ( dims , * args , ** kwargs ) : if dims == 1 : return nn . Conv1d ( * args , ** kwargs ) elif dims == 2 : return nn . Conv2d ( * args , ** kwargs ) elif dims == 3 : return nn . Conv3d ( * args , ** kwargs ) raise ValueError ( f""unsupported dimensions: {dims}"" )","Create a 1D, 2D, or 3D convolution module."
/core/script/scaffold/model.py,update_manifest,"def update_manifest(self, **kwargs) -> None:
        """"""Update the integration manifest.""""""
        print(f""Updating {self.domain} manifest: {kwargs}"")

        # Sort keys in manifest so we don't trigger hassfest errors.
        manifest: dict[str, Any] = {**self.manifest(), **kwargs}
        sort_manifest(manifest)

        self.manifest_path.write_text(json.dumps(manifest, indent=2) + ""\n"")","def update_manifest(self, **kwargs) -> None:
        """"""Update the integration manifest.""""""
        print(f""Updating {self.domain} manifest: {kwargs}"")

        # Sort keys in manifest so we don't trigger hassfest errors.
        manifest: dict[str, Any] = {**self.manifest(), **kwargs}
        sort_manifest(manifest)

        self.manifest_path.write_text(json.dumps(manifest, indent=2) + ""\n"")",Update the integration manifest.,Update the integration manifest.,"def update_manifest(self, **kwargs) -> None:
        
        print(f""Updating {self.domain} manifest: {kwargs}"")

        # Sort keys in manifest so we don't trigger hassfest errors.
        manifest: dict[str, Any] = {**self.manifest(), **kwargs}
        sort_manifest(manifest)

        self.manifest_path.write_text(json.dumps(manifest, indent=2) + ""\n"")",Update the integration manifest.,"def update_manifest ( self , ** kwargs ) -> None : print ( f""Updating {self.domain} manifest: {kwargs}"" ) # Sort keys in manifest so we don't trigger hassfest errors. manifest : dict [ str , Any ] = { ** self . manifest ( ) , ** kwargs } sort_manifest ( manifest ) self . manifest_path . write_text ( json . dumps ( manifest , indent = 2 ) + ""\n"" )",Update the integration manifest.
/GPT-SoVITS/GPT_SoVITS/BigVGAN/bigvgan.py,_save_pretrained,"def _save_pretrained(self, save_directory: Path) -> None:
        """"""Save weights and config.json from a Pytorch model to a local directory.""""""

        model_path = save_directory / ""bigvgan_generator.pt""
        torch.save({""generator"": self.state_dict()}, model_path)

        config_path = save_directory / ""config.json""
        with open(config_path, ""w"") as config_file:
            json.dump(self.h, config_file, indent=4)","def _save_pretrained(self, save_directory: Path) -> None:
        """"""Save weights and config.json from a Pytorch model to a local directory.""""""

        model_path = save_directory / ""bigvgan_generator.pt""
        torch.save({""generator"": self.state_dict()}, model_path)

        config_path = save_directory / ""config.json""
        with open(config_path, ""w"") as config_file:
            json.dump(self.h, config_file, indent=4)",Save weights and config.json from a Pytorch model to a local directory.,Save weights and config.json from a Pytorch model to a local directory.,"def _save_pretrained(self, save_directory: Path) -> None:
        

        model_path = save_directory / ""bigvgan_generator.pt""
        torch.save({""generator"": self.state_dict()}, model_path)

        config_path = save_directory / ""config.json""
        with open(config_path, ""w"") as config_file:
            json.dump(self.h, config_file, indent=4)",Save weights and config.json from a Pytorch model to a local directory.,"def _save_pretrained ( self , save_directory : Path ) -> None : model_path = save_directory / ""bigvgan_generator.pt"" torch . save ( { ""generator"" : self . state_dict ( ) } , model_path ) config_path = save_directory / ""config.json"" with open ( config_path , ""w"" ) as config_file : json . dump ( self . h , config_file , indent = 4 )",Save weights and config.json from a Pytorch model to a local directory.
/faceswap/tests/simple_tests.py,download_file,"def download_file(url, filename):  # TODO: retry
    """""" Download a file from given url """"""
    if os.path.isfile(filename):
        print_status(f""[?] '{url}' already cached as '{filename}'"")
        return filename
    try:
        print_status(f""[?] Downloading '{url}' to '{filename}'"")
        video, _ = urlretrieve(url, filename)
        return video
    except urllib.error.URLError as err:
        print_fail(f""[-] Failed downloading: {err}"")
        return None","def download_file(url, filename):  # TODO: retry
    """""" Download a file from given url """"""
    if os.path.isfile(filename):
        print_status(f""[?] '{url}' already cached as '{filename}'"")
        return filename
    try:
        print_status(f""[?] Downloading '{url}' to '{filename}'"")
        video, _ = urlretrieve(url, filename)
        return video
    except urllib.error.URLError as err:
        print_fail(f""[-] Failed downloading: {err}"")
        return None",Download a file from given url,Download a file from given url,"def download_file(url, filename):  # TODO: retry
    
    if os.path.isfile(filename):
        print_status(f""[?] '{url}' already cached as '{filename}'"")
        return filename
    try:
        print_status(f""[?] Downloading '{url}' to '{filename}'"")
        video, _ = urlretrieve(url, filename)
        return video
    except urllib.error.URLError as err:
        print_fail(f""[-] Failed downloading: {err}"")
        return None",Download a file from given url,"def download_file ( url , filename ) : # TODO: retry if os . path . isfile ( filename ) : print_status ( f""[?] '{url}' already cached as '{filename}'"" ) return filename try : print_status ( f""[?] Downloading '{url}' to '{filename}'"" ) video , _ = urlretrieve ( url , filename ) return video except urllib . error . URLError as err : print_fail ( f""[-] Failed downloading: {err}"" ) return None",Download a file from given url
/core/script/scaffold/templates/config_flow_helper/tests/test_config_flow.py,get_suggested,"def get_suggested(schema, key):
    """"""Get suggested value for key in voluptuous schema.""""""
    for k in schema:
        if k == key:
            if k.description is None or ""suggested_value"" not in k.description:
                return None
            return k.description[""suggested_value""]
    # Wanted key absent from schema
    raise KeyError(f""Key `{key}` is missing from schema"")","def get_suggested(schema, key):
    """"""Get suggested value for key in voluptuous schema.""""""
    for k in schema:
        if k == key:
            if k.description is None or ""suggested_value"" not in k.description:
                return None
            return k.description[""suggested_value""]
    # Wanted key absent from schema
    raise KeyError(f""Key `{key}` is missing from schema"")",Get suggested value for key in voluptuous schema.,Get suggested value for key in voluptuous schema.,"def get_suggested(schema, key):
    
    for k in schema:
        if k == key:
            if k.description is None or ""suggested_value"" not in k.description:
                return None
            return k.description[""suggested_value""]
    # Wanted key absent from schema
    raise KeyError(f""Key `{key}` is missing from schema"")",Get suggested value for key in voluptuous schema.,"def get_suggested ( schema , key ) : for k in schema : if k == key : if k . description is None or ""suggested_value"" not in k . description : return None return k . description [ ""suggested_value"" ] # Wanted key absent from schema raise KeyError ( f""Key `{key}` is missing from schema"" )",Get suggested value for key in voluptuous schema.
/faceswap/plugins/train/model/_base/model.py,save,"def save(self) -> None:
        """""" Save the state values to the serialized state file. """"""
        logger.debug(""Saving State"")
        state = {""name"": self._name,
                 ""sessions"": self._sessions,
                 ""lowest_avg_loss"": self._lowest_avg_loss,
                 ""iterations"": self._iterations,
                 ""mixed_precision_layers"": self._mixed_precision_layers,
                 ""config"": _CONFIG}
        self._serializer.save(self._filename, state)
        logger.debug(""Saved State"")","def save(self) -> None:
        """""" Save the state values to the serialized state file. """"""
        logger.debug(""Saving State"")
        state = {""name"": self._name,
                 ""sessions"": self._sessions,
                 ""lowest_avg_loss"": self._lowest_avg_loss,
                 ""iterations"": self._iterations,
                 ""mixed_precision_layers"": self._mixed_precision_layers,
                 ""config"": _CONFIG}
        self._serializer.save(self._filename, state)
        logger.debug(""Saved State"")",Save the state values to the serialized state file.,Save the state values to the serialized state file.,"def save(self) -> None:
        
        logger.debug(""Saving State"")
        state = {""name"": self._name,
                 ""sessions"": self._sessions,
                 ""lowest_avg_loss"": self._lowest_avg_loss,
                 ""iterations"": self._iterations,
                 ""mixed_precision_layers"": self._mixed_precision_layers,
                 ""config"": _CONFIG}
        self._serializer.save(self._filename, state)
        logger.debug(""Saved State"")",Save the state values to the serialized state file.,"def save ( self ) -> None : logger . debug ( ""Saving State"" ) state = { ""name"" : self . _name , ""sessions"" : self . _sessions , ""lowest_avg_loss"" : self . _lowest_avg_loss , ""iterations"" : self . _iterations , ""mixed_precision_layers"" : self . _mixed_precision_layers , ""config"" : _CONFIG } self . _serializer . save ( self . _filename , state ) logger . debug ( ""Saved State"" )",Save the state values to the serialized state file.
/pandas/pandas/tests/reshape/merge/test_multi.py,left,"def left():
    """"""left dataframe (not multi-indexed) for multi-index join tests""""""
    # a little relevant example with NAs
    key1 = [""bar"", ""bar"", ""bar"", ""foo"", ""foo"", ""baz"", ""baz"", ""qux"", ""qux"", ""snap""]
    key2 = [""two"", ""one"", ""three"", ""one"", ""two"", ""one"", ""two"", ""two"", ""three"", ""one""]

    data = np.random.default_rng(2).standard_normal(len(key1))
    return DataFrame({""key1"": key1, ""key2"": key2, ""data"": data})","def left():
    """"""left dataframe (not multi-indexed) for multi-index join tests""""""
    # a little relevant example with NAs
    key1 = [""bar"", ""bar"", ""bar"", ""foo"", ""foo"", ""baz"", ""baz"", ""qux"", ""qux"", ""snap""]
    key2 = [""two"", ""one"", ""three"", ""one"", ""two"", ""one"", ""two"", ""two"", ""three"", ""one""]

    data = np.random.default_rng(2).standard_normal(len(key1))
    return DataFrame({""key1"": key1, ""key2"": key2, ""data"": data})",left dataframe (not multi-indexed) for multi-index join tests,left dataframe (not multi-indexed) for multi-index join tests,"def left():
    
    # a little relevant example with NAs
    key1 = [""bar"", ""bar"", ""bar"", ""foo"", ""foo"", ""baz"", ""baz"", ""qux"", ""qux"", ""snap""]
    key2 = [""two"", ""one"", ""three"", ""one"", ""two"", ""one"", ""two"", ""two"", ""three"", ""one""]

    data = np.random.default_rng(2).standard_normal(len(key1))
    return DataFrame({""key1"": key1, ""key2"": key2, ""data"": data})",left dataframe (not multi-indexed) for multi-index join tests,"def left ( ) : # a little relevant example with NAs key1 = [ ""bar"" , ""bar"" , ""bar"" , ""foo"" , ""foo"" , ""baz"" , ""baz"" , ""qux"" , ""qux"" , ""snap"" ] key2 = [ ""two"" , ""one"" , ""three"" , ""one"" , ""two"" , ""one"" , ""two"" , ""two"" , ""three"" , ""one"" ] data = np . random . default_rng ( 2 ) . standard_normal ( len ( key1 ) ) return DataFrame ( { ""key1"" : key1 , ""key2"" : key2 , ""data"" : data } )",left dataframe (not multi-indexed) for multi-index join tests
/ansible/packaging/release.py,set_ansible_version,"def set_ansible_version(current_version: Version, requested_version: Version) -> None:
    """"""Set the current ansible-core version.""""""
    check_ansible_version(current_version, requested_version)

    if requested_version == current_version:
        return

    display.show(f""Updating version {current_version} to {requested_version} ..."")

    current = ANSIBLE_RELEASE_FILE.read_text()
    updated = ANSIBLE_VERSION_PATTERN.sub(ANSIBLE_VERSION_FORMAT.format(version=requested_version), current)

    if current == updated:
        raise RuntimeError(""Failed to set the ansible-core version."")

    ANSIBLE_RELEASE_FILE.write_text(updated)","def set_ansible_version(current_version: Version, requested_version: Version) -> None:
    """"""Set the current ansible-core version.""""""
    check_ansible_version(current_version, requested_version)

    if requested_version == current_version:
        return

    display.show(f""Updating version {current_version} to {requested_version} ..."")

    current = ANSIBLE_RELEASE_FILE.read_text()
    updated = ANSIBLE_VERSION_PATTERN.sub(ANSIBLE_VERSION_FORMAT.format(version=requested_version), current)

    if current == updated:
        raise RuntimeError(""Failed to set the ansible-core version."")

    ANSIBLE_RELEASE_FILE.write_text(updated)",Set the current ansible-core version.,Set the current ansible-core version.,"def set_ansible_version(current_version: Version, requested_version: Version) -> None:
    
    check_ansible_version(current_version, requested_version)

    if requested_version == current_version:
        return

    display.show(f""Updating version {current_version} to {requested_version} ..."")

    current = ANSIBLE_RELEASE_FILE.read_text()
    updated = ANSIBLE_VERSION_PATTERN.sub(ANSIBLE_VERSION_FORMAT.format(version=requested_version), current)

    if current == updated:
        raise RuntimeError(""Failed to set the ansible-core version."")

    ANSIBLE_RELEASE_FILE.write_text(updated)",Set the current ansible-core version.,"def set_ansible_version ( current_version : Version , requested_version : Version ) -> None : check_ansible_version ( current_version , requested_version ) if requested_version == current_version : return display . show ( f""Updating version {current_version} to {requested_version} ..."" ) current = ANSIBLE_RELEASE_FILE . read_text ( ) updated = ANSIBLE_VERSION_PATTERN . sub ( ANSIBLE_VERSION_FORMAT . format ( version = requested_version ) , current ) if current == updated : raise RuntimeError ( ""Failed to set the ansible-core version."" ) ANSIBLE_RELEASE_FILE . write_text ( updated )",Set the current ansible-core version.
/cpython/Tools/ssl/make_ssl_data.py,gen_library_codes,"def gen_library_codes(args):
    """"""Generate table short libname to numeric code.""""""
    yield ""/* generated from args.lib2errnum */""
    yield ""static struct py_ssl_library_code library_codes[] = {""
    for libname in sorted(args.lib2errnum):
        yield f""#ifdef ERR_LIB_{libname}""
        yield f'    {{""{libname}"", ERR_LIB_{libname}}},'
        yield ""#endif""
    yield ""    {NULL, 0}  /* sentinel */""
    yield ""};""","def gen_library_codes(args):
    """"""Generate table short libname to numeric code.""""""
    yield ""/* generated from args.lib2errnum */""
    yield ""static struct py_ssl_library_code library_codes[] = {""
    for libname in sorted(args.lib2errnum):
        yield f""#ifdef ERR_LIB_{libname}""
        yield f'    {{""{libname}"", ERR_LIB_{libname}}},'
        yield ""#endif""
    yield ""    {NULL, 0}  /* sentinel */""
    yield ""};""",Generate table short libname to numeric code.,Generate table short libname to numeric code.,"def gen_library_codes(args):
    
    yield ""/* generated from args.lib2errnum */""
    yield ""static struct py_ssl_library_code library_codes[] = {""
    for libname in sorted(args.lib2errnum):
        yield f""#ifdef ERR_LIB_{libname}""
        yield f'    {{""{libname}"", ERR_LIB_{libname}}},'
        yield ""#endif""
    yield ""    {NULL, 0}  /* sentinel */""
    yield ""};""",Generate table short libname to numeric code.,"def gen_library_codes ( args ) : yield ""/* generated from args.lib2errnum */"" yield ""static struct py_ssl_library_code library_codes[] = {"" for libname in sorted ( args . lib2errnum ) : yield f""#ifdef ERR_LIB_{libname}"" yield f'    {{""{libname}"", ERR_LIB_{libname}}},' yield ""#endif"" yield ""    {NULL, 0}  /* sentinel */"" yield ""};""",Generate table short libname to numeric code.
/private-gpt/private_gpt/settings/yaml.py,load_env_var,"def load_env_var(_, node) -> str:
        """"""Extract the matched value, expand env variable, and replace the match.""""""
        value = str(node.value).removeprefix(""${"").removesuffix(""}"")
        split = value.split("":"", 1)
        env_var = split[0]
        value = environ.get(env_var)
        default = None if len(split) == 1 else split[1]
        if value is None and default is None:
            raise ValueError(
                f""Environment variable {env_var} is not set and not default was provided""
            )
        return value or default","def load_env_var(_, node) -> str:
        """"""Extract the matched value, expand env variable, and replace the match.""""""
        value = str(node.value).removeprefix(""${"").removesuffix(""}"")
        split = value.split("":"", 1)
        env_var = split[0]
        value = environ.get(env_var)
        default = None if len(split) == 1 else split[1]
        if value is None and default is None:
            raise ValueError(
                f""Environment variable {env_var} is not set and not default was provided""
            )
        return value or default","Extract the matched value, expand env variable, and replace the match.","Extract the matched value, expand env variable, and replace the match.","def load_env_var(_, node) -> str:
        
        value = str(node.value).removeprefix(""${"").removesuffix(""}"")
        split = value.split("":"", 1)
        env_var = split[0]
        value = environ.get(env_var)
        default = None if len(split) == 1 else split[1]
        if value is None and default is None:
            raise ValueError(
                f""Environment variable {env_var} is not set and not default was provided""
            )
        return value or default","Extract the matched value, expand env variable, and replace the match.","def load_env_var ( _ , node ) -> str : value = str ( node . value ) . removeprefix ( ""${"" ) . removesuffix ( ""}"" ) split = value . split ( "":"" , 1 ) env_var = split [ 0 ] value = environ . get ( env_var ) default = None if len ( split ) == 1 else split [ 1 ] if value is None and default is None : raise ValueError ( f""Environment variable {env_var} is not set and not default was provided"" ) return value or default","Extract the matched value, expand env variable, and replace the match."
/yt-dlp/yt_dlp/networking/common.py,register_rh,"def register_rh(handler):
    """"""Register a RequestHandler class""""""
    assert issubclass(handler, RequestHandler), f'{handler} must be a subclass of RequestHandler'
    assert handler.RH_KEY not in _REQUEST_HANDLERS, f'RequestHandler {handler.RH_KEY} already registered'
    _REQUEST_HANDLERS[handler.RH_KEY] = handler
    return handler","def register_rh(handler):
    """"""Register a RequestHandler class""""""
    assert issubclass(handler, RequestHandler), f'{handler} must be a subclass of RequestHandler'
    assert handler.RH_KEY not in _REQUEST_HANDLERS, f'RequestHandler {handler.RH_KEY} already registered'
    _REQUEST_HANDLERS[handler.RH_KEY] = handler
    return handler",Register a RequestHandler class,Register a RequestHandler class,"def register_rh(handler):
    
    assert issubclass(handler, RequestHandler), f'{handler} must be a subclass of RequestHandler'
    assert handler.RH_KEY not in _REQUEST_HANDLERS, f'RequestHandler {handler.RH_KEY} already registered'
    _REQUEST_HANDLERS[handler.RH_KEY] = handler
    return handler",Register a RequestHandler class,"def register_rh ( handler ) : assert issubclass ( handler , RequestHandler ) , f'{handler} must be a subclass of RequestHandler' assert handler . RH_KEY not in _REQUEST_HANDLERS , f'RequestHandler {handler.RH_KEY} already registered' _REQUEST_HANDLERS [ handler . RH_KEY ] = handler return handler",Register a RequestHandler class
/OpenBB/openbb_platform/extensions/technical/tests/test_technical_helpers.py,mock_data,"def mock_data(top_range: int = 100):
    """"""Mock data for testing.""""""
    _open = pd.Series(np.arange(1, top_range))
    _high = pd.Series(np.arange(1, top_range))
    _low = pd.Series(np.arange(1, top_range))
    _close = pd.Series(np.arange(1, top_range))
    _volume = pd.Series(np.arange(1, top_range))
    _date = pd.Series(pd.date_range(""2021-01-01"", periods=top_range - 1, freq=""D""))
    return pd.DataFrame(
        {
            ""open"": _open,
            ""high"": _high,
            ""low"": _low,
            ""close"": _close,
            ""volume"": _volume,
            ""date"": _date,
        }
    ).set_index(""date"")","def mock_data(top_range: int = 100):
    """"""Mock data for testing.""""""
    _open = pd.Series(np.arange(1, top_range))
    _high = pd.Series(np.arange(1, top_range))
    _low = pd.Series(np.arange(1, top_range))
    _close = pd.Series(np.arange(1, top_range))
    _volume = pd.Series(np.arange(1, top_range))
    _date = pd.Series(pd.date_range(""2021-01-01"", periods=top_range - 1, freq=""D""))
    return pd.DataFrame(
        {
            ""open"": _open,
            ""high"": _high,
            ""low"": _low,
            ""close"": _close,
            ""volume"": _volume,
            ""date"": _date,
        }
    ).set_index(""date"")",Mock data for testing.,Mock data for testing.,"def mock_data(top_range: int = 100):
    
    _open = pd.Series(np.arange(1, top_range))
    _high = pd.Series(np.arange(1, top_range))
    _low = pd.Series(np.arange(1, top_range))
    _close = pd.Series(np.arange(1, top_range))
    _volume = pd.Series(np.arange(1, top_range))
    _date = pd.Series(pd.date_range(""2021-01-01"", periods=top_range - 1, freq=""D""))
    return pd.DataFrame(
        {
            ""open"": _open,
            ""high"": _high,
            ""low"": _low,
            ""close"": _close,
            ""volume"": _volume,
            ""date"": _date,
        }
    ).set_index(""date"")",Mock data for testing.,"def mock_data ( top_range : int = 100 ) : _open = pd . Series ( np . arange ( 1 , top_range ) ) _high = pd . Series ( np . arange ( 1 , top_range ) ) _low = pd . Series ( np . arange ( 1 , top_range ) ) _close = pd . Series ( np . arange ( 1 , top_range ) ) _volume = pd . Series ( np . arange ( 1 , top_range ) ) _date = pd . Series ( pd . date_range ( ""2021-01-01"" , periods = top_range - 1 , freq = ""D"" ) ) return pd . DataFrame ( { ""open"" : _open , ""high"" : _high , ""low"" : _low , ""close"" : _close , ""volume"" : _volume , ""date"" : _date , } ) . set_index ( ""date"" )",Mock data for testing.
/faceswap/setup.py,_process_arguments,"def _process_arguments(self) -> None:
        """""" Process any cli arguments and dummy in cli arguments if calling from updater. """"""
        args = [arg for arg in sys.argv]  # pylint:disable=unnecessary-comprehension
        if self.updater:
            from lib.utils import get_backend  # pylint:disable=import-outside-toplevel
            args.append(f""--{get_backend()}"")

        logger.debug(args)
        for arg in args:
            if arg == ""--installer"":
                self.is_installer = True
            if not self.backend and (arg.startswith(""--"") and
                                     arg.replace(""--"", """") in self._backends):
                self.backend = arg.replace(""--"", """").lower()","def _process_arguments(self) -> None:
        """""" Process any cli arguments and dummy in cli arguments if calling from updater. """"""
        args = [arg for arg in sys.argv]  # pylint:disable=unnecessary-comprehension
        if self.updater:
            from lib.utils import get_backend  # pylint:disable=import-outside-toplevel
            args.append(f""--{get_backend()}"")

        logger.debug(args)
        for arg in args:
            if arg == ""--installer"":
                self.is_installer = True
            if not self.backend and (arg.startswith(""--"") and
                                     arg.replace(""--"", """") in self._backends):
                self.backend = arg.replace(""--"", """").lower()",Process any cli arguments and dummy in cli arguments if calling from updater.,Process any cli arguments and dummy in cli arguments if calling from updater.,"def _process_arguments(self) -> None:
        
        args = [arg for arg in sys.argv]  # pylint:disable=unnecessary-comprehension
        if self.updater:
            from lib.utils import get_backend  # pylint:disable=import-outside-toplevel
            args.append(f""--{get_backend()}"")

        logger.debug(args)
        for arg in args:
            if arg == ""--installer"":
                self.is_installer = True
            if not self.backend and (arg.startswith(""--"") and
                                     arg.replace(""--"", """") in self._backends):
                self.backend = arg.replace(""--"", """").lower()",Process any cli arguments and dummy in cli arguments if calling from updater.,"def _process_arguments ( self ) -> None : args = [ arg for arg in sys . argv ] # pylint:disable=unnecessary-comprehension if self . updater : from lib . utils import get_backend # pylint:disable=import-outside-toplevel args . append ( f""--{get_backend()}"" ) logger . debug ( args ) for arg in args : if arg == ""--installer"" : self . is_installer = True if not self . backend and ( arg . startswith ( ""--"" ) and arg . replace ( ""--"" , """" ) in self . _backends ) : self . backend = arg . replace ( ""--"" , """" ) . lower ( )",Process any cli arguments and dummy in cli arguments if calling from updater.
/faceswap/plugins/train/model/dfl_sae.py,_legacy_mapping,"def _legacy_mapping(self):
        """""" The mapping of legacy separate model names to single model names """"""
        mappings = {""df"": {f""{self.name}_encoder.h5"": ""encoder_df"",
                           f""{self.name}_decoder_A.h5"": ""decoder_a"",
                           f""{self.name}_decoder_B.h5"": ""decoder_b""},
                    ""liae"": {f""{self.name}_encoder.h5"": ""encoder_liae"",
                             f""{self.name}_intermediate_B.h5"": ""intermediate_both"",
                             f""{self.name}_intermediate.h5"": ""intermediate_b"",
                             f""{self.name}_decoder.h5"": ""decoder_both""}}
        return mappings[self.config[""architecture""]]","def _legacy_mapping(self):
        """""" The mapping of legacy separate model names to single model names """"""
        mappings = {""df"": {f""{self.name}_encoder.h5"": ""encoder_df"",
                           f""{self.name}_decoder_A.h5"": ""decoder_a"",
                           f""{self.name}_decoder_B.h5"": ""decoder_b""},
                    ""liae"": {f""{self.name}_encoder.h5"": ""encoder_liae"",
                             f""{self.name}_intermediate_B.h5"": ""intermediate_both"",
                             f""{self.name}_intermediate.h5"": ""intermediate_b"",
                             f""{self.name}_decoder.h5"": ""decoder_both""}}
        return mappings[self.config[""architecture""]]",The mapping of legacy separate model names to single model names,The mapping of legacy separate model names to single model names,"def _legacy_mapping(self):
        
        mappings = {""df"": {f""{self.name}_encoder.h5"": ""encoder_df"",
                           f""{self.name}_decoder_A.h5"": ""decoder_a"",
                           f""{self.name}_decoder_B.h5"": ""decoder_b""},
                    ""liae"": {f""{self.name}_encoder.h5"": ""encoder_liae"",
                             f""{self.name}_intermediate_B.h5"": ""intermediate_both"",
                             f""{self.name}_intermediate.h5"": ""intermediate_b"",
                             f""{self.name}_decoder.h5"": ""decoder_both""}}
        return mappings[self.config[""architecture""]]",The mapping of legacy separate model names to single model names,"def _legacy_mapping ( self ) : mappings = { ""df"" : { f""{self.name}_encoder.h5"" : ""encoder_df"" , f""{self.name}_decoder_A.h5"" : ""decoder_a"" , f""{self.name}_decoder_B.h5"" : ""decoder_b"" } , ""liae"" : { f""{self.name}_encoder.h5"" : ""encoder_liae"" , f""{self.name}_intermediate_B.h5"" : ""intermediate_both"" , f""{self.name}_intermediate.h5"" : ""intermediate_b"" , f""{self.name}_decoder.h5"" : ""decoder_both"" } } return mappings [ self . config [ ""architecture"" ] ]",The mapping of legacy separate model names to single model names
/yolov5/utils/dataloaders.py,process_images,"def process_images(self):
        """"""Compresses images for Ultralytics HUB across 'train', 'val', 'test' splits and saves to specified
        directory.
        """"""
        for split in ""train"", ""val"", ""test"":
            if self.data.get(split) is None:
                continue
            dataset = LoadImagesAndLabels(self.data[split])  # load dataset
            desc = f""{split} images""
            for _ in tqdm(ThreadPool(NUM_THREADS).imap(self._hub_ops, dataset.im_files), total=dataset.n, desc=desc):
                pass
        print(f""Done. All images saved to {self.im_dir}"")
        return self.im_dir","def process_images(self):
        """"""Compresses images for Ultralytics HUB across 'train', 'val', 'test' splits and saves to specified
        directory.
        """"""
        for split in ""train"", ""val"", ""test"":
            if self.data.get(split) is None:
                continue
            dataset = LoadImagesAndLabels(self.data[split])  # load dataset
            desc = f""{split} images""
            for _ in tqdm(ThreadPool(NUM_THREADS).imap(self._hub_ops, dataset.im_files), total=dataset.n, desc=desc):
                pass
        print(f""Done. All images saved to {self.im_dir}"")
        return self.im_dir","Compresses images for Ultralytics HUB across 'train', 'val', 'test' splits and saves to specified
directory.","Compresses images for Ultralytics HUB across 'train', 'val', 'test' splits and saves to specified directory.","def process_images(self):
        
        for split in ""train"", ""val"", ""test"":
            if self.data.get(split) is None:
                continue
            dataset = LoadImagesAndLabels(self.data[split])  # load dataset
            desc = f""{split} images""
            for _ in tqdm(ThreadPool(NUM_THREADS).imap(self._hub_ops, dataset.im_files), total=dataset.n, desc=desc):
                pass
        print(f""Done. All images saved to {self.im_dir}"")
        return self.im_dir","Compresses images for Ultralytics HUB across 'train', 'val', 'test' splits and saves to specified directory.","def process_images ( self ) : for split in ""train"" , ""val"" , ""test"" : if self . data . get ( split ) is None : continue dataset = LoadImagesAndLabels ( self . data [ split ] ) # load dataset desc = f""{split} images"" for _ in tqdm ( ThreadPool ( NUM_THREADS ) . imap ( self . _hub_ops , dataset . im_files ) , total = dataset . n , desc = desc ) : pass print ( f""Done. All images saved to {self.im_dir}"" ) return self . im_dir","Compresses images for Ultralytics HUB across 'train', 'val', 'test' splits and saves to specified directory."
/core/script/gen_requirements_all.py,requirements_pre_commit_output,"def requirements_pre_commit_output() -> str:
    """"""Generate output for pre-commit dependencies.""""""
    source = "".pre-commit-config.yaml""
    pre_commit_conf: dict[str, list[dict[str, Any]]]
    pre_commit_conf = load_yaml(source)  # type: ignore[assignment]
    reqs: list[str] = []
    hook: dict[str, Any]
    for repo in (x for x in pre_commit_conf[""repos""] if x.get(""rev"")):
        rev: str = repo[""rev""]
        for hook in repo[""hooks""]:
            if hook[""id""] not in IGNORE_PRE_COMMIT_HOOK_ID:
                reqs.append(f""{hook['id']}=={rev.lstrip('v')}"")
                reqs.extend(x for x in hook.get(""additional_dependencies"", ()))
    output = [
        f""# Automatically generated ""
        f""from {source} by {Path(__file__).name}, do not edit"",
        """",
    ]
    output.extend(sorted(reqs))
    return ""\n"".join(output) + ""\n""","def requirements_pre_commit_output() -> str:
    """"""Generate output for pre-commit dependencies.""""""
    source = "".pre-commit-config.yaml""
    pre_commit_conf: dict[str, list[dict[str, Any]]]
    pre_commit_conf = load_yaml(source)  # type: ignore[assignment]
    reqs: list[str] = []
    hook: dict[str, Any]
    for repo in (x for x in pre_commit_conf[""repos""] if x.get(""rev"")):
        rev: str = repo[""rev""]
        for hook in repo[""hooks""]:
            if hook[""id""] not in IGNORE_PRE_COMMIT_HOOK_ID:
                reqs.append(f""{hook['id']}=={rev.lstrip('v')}"")
                reqs.extend(x for x in hook.get(""additional_dependencies"", ()))
    output = [
        f""# Automatically generated ""
        f""from {source} by {Path(__file__).name}, do not edit"",
        """",
    ]
    output.extend(sorted(reqs))
    return ""\n"".join(output) + ""\n""",Generate output for pre-commit dependencies.,Generate output for pre-commit dependencies.,"def requirements_pre_commit_output() -> str:
    
    source = "".pre-commit-config.yaml""
    pre_commit_conf: dict[str, list[dict[str, Any]]]
    pre_commit_conf = load_yaml(source)  # type: ignore[assignment]
    reqs: list[str] = []
    hook: dict[str, Any]
    for repo in (x for x in pre_commit_conf[""repos""] if x.get(""rev"")):
        rev: str = repo[""rev""]
        for hook in repo[""hooks""]:
            if hook[""id""] not in IGNORE_PRE_COMMIT_HOOK_ID:
                reqs.append(f""{hook['id']}=={rev.lstrip('v')}"")
                reqs.extend(x for x in hook.get(""additional_dependencies"", ()))
    output = [
        f""# Automatically generated ""
        f""from {source} by {Path(__file__).name}, do not edit"",
        """",
    ]
    output.extend(sorted(reqs))
    return ""\n"".join(output) + ""\n""",Generate output for pre-commit dependencies.,"def requirements_pre_commit_output ( ) -> str : source = "".pre-commit-config.yaml"" pre_commit_conf : dict [ str , list [ dict [ str , Any ] ] ] pre_commit_conf = load_yaml ( source ) # type: ignore[assignment] reqs : list [ str ] = [ ] hook : dict [ str , Any ] for repo in ( x for x in pre_commit_conf [ ""repos"" ] if x . get ( ""rev"" ) ) : rev : str = repo [ ""rev"" ] for hook in repo [ ""hooks"" ] : if hook [ ""id"" ] not in IGNORE_PRE_COMMIT_HOOK_ID : reqs . append ( f""{hook['id']}=={rev.lstrip('v')}"" ) reqs . extend ( x for x in hook . get ( ""additional_dependencies"" , ( ) ) ) output = [ f""# Automatically generated "" f""from {source} by {Path(__file__).name}, do not edit"" , """" , ] output . extend ( sorted ( reqs ) ) return ""\n"" . join ( output ) + ""\n""",Generate output for pre-commit dependencies.
/requests/src/requests/models.py,register_hook,"def register_hook(self, event, hook):
        """"""Properly register a hook.""""""

        if event not in self.hooks:
            raise ValueError(f'Unsupported event specified, with event name ""{event}""')

        if isinstance(hook, Callable):
            self.hooks[event].append(hook)
        elif hasattr(hook, ""__iter__""):
            self.hooks[event].extend(h for h in hook if isinstance(h, Callable))","def register_hook(self, event, hook):
        """"""Properly register a hook.""""""

        if event not in self.hooks:
            raise ValueError(f'Unsupported event specified, with event name ""{event}""')

        if isinstance(hook, Callable):
            self.hooks[event].append(hook)
        elif hasattr(hook, ""__iter__""):
            self.hooks[event].extend(h for h in hook if isinstance(h, Callable))",Properly register a hook.,Properly register a hook.,"def register_hook(self, event, hook):
        

        if event not in self.hooks:
            raise ValueError(f'Unsupported event specified, with event name ""{event}""')

        if isinstance(hook, Callable):
            self.hooks[event].append(hook)
        elif hasattr(hook, ""__iter__""):
            self.hooks[event].extend(h for h in hook if isinstance(h, Callable))",Properly register a hook.,"def register_hook ( self , event , hook ) : if event not in self . hooks : raise ValueError ( f'Unsupported event specified, with event name ""{event}""' ) if isinstance ( hook , Callable ) : self . hooks [ event ] . append ( hook ) elif hasattr ( hook , ""__iter__"" ) : self . hooks [ event ] . extend ( h for h in hook if isinstance ( h , Callable ) )",Properly register a hook.
/faceswap/plugins/train/model/dfl_sae.py,_patch_weights_management,"def _patch_weights_management(self):
        """""" Patch in the correct encoder name into the config dictionary for freezing and loading
        weights based on architecture.
        """"""
        self.config[""freeze_layers""] = [f""encoder_{self.architecture}""]
        self.config[""load_layers""] = [f""encoder_{self.architecture}""]
        logger.debug(""Patched encoder layers to config: %s"",
                     {k: v for k, v in self.config.items()
                      if k in (""freeze_layers"", ""load_layers"")})","def _patch_weights_management(self):
        """""" Patch in the correct encoder name into the config dictionary for freezing and loading
        weights based on architecture.
        """"""
        self.config[""freeze_layers""] = [f""encoder_{self.architecture}""]
        self.config[""load_layers""] = [f""encoder_{self.architecture}""]
        logger.debug(""Patched encoder layers to config: %s"",
                     {k: v for k, v in self.config.items()
                      if k in (""freeze_layers"", ""load_layers"")})","Patch in the correct encoder name into the config dictionary for freezing and loading
weights based on architecture.",Patch in the correct encoder name into the config dictionary for freezing and loading weights based on architecture.,"def _patch_weights_management(self):
        
        self.config[""freeze_layers""] = [f""encoder_{self.architecture}""]
        self.config[""load_layers""] = [f""encoder_{self.architecture}""]
        logger.debug(""Patched encoder layers to config: %s"",
                     {k: v for k, v in self.config.items()
                      if k in (""freeze_layers"", ""load_layers"")})",Patch in the correct encoder name into the config dictionary for freezing and loading weights based on architecture.,"def _patch_weights_management ( self ) : self . config [ ""freeze_layers"" ] = [ f""encoder_{self.architecture}"" ] self . config [ ""load_layers"" ] = [ f""encoder_{self.architecture}"" ] logger . debug ( ""Patched encoder layers to config: %s"" , { k : v for k , v in self . config . items ( ) if k in ( ""freeze_layers"" , ""load_layers"" ) } )",Patch in the correct encoder name into the config dictionary for freezing and loading weights based on architecture.
/requests/src/requests/sessions.py,get_adapter,"def get_adapter(self, url):
        """"""
        Returns the appropriate connection adapter for the given URL.

        :rtype: requests.adapters.BaseAdapter
        """"""
        for prefix, adapter in self.adapters.items():
            if url.lower().startswith(prefix.lower()):
                return adapter

        # Nothing matches :-/
        raise InvalidSchema(f""No connection adapters were found for {url!r}"")","def get_adapter(self, url):
        """"""
        Returns the appropriate connection adapter for the given URL.

        :rtype: requests.adapters.BaseAdapter
        """"""
        for prefix, adapter in self.adapters.items():
            if url.lower().startswith(prefix.lower()):
                return adapter

        # Nothing matches :-/
        raise InvalidSchema(f""No connection adapters were found for {url!r}"")","Returns the appropriate connection adapter for the given URL.

:rtype: requests.adapters.BaseAdapter",Returns the appropriate connection adapter for the given URL.,"def get_adapter(self, url):
        
        for prefix, adapter in self.adapters.items():
            if url.lower().startswith(prefix.lower()):
                return adapter

        # Nothing matches :-/
        raise InvalidSchema(f""No connection adapters were found for {url!r}"")",Returns the appropriate connection adapter for the given URL.,"def get_adapter ( self , url ) : for prefix , adapter in self . adapters . items ( ) : if url . lower ( ) . startswith ( prefix . lower ( ) ) : return adapter # Nothing matches :-/ raise InvalidSchema ( f""No connection adapters were found for {url!r}"" )",Returns the appropriate connection adapter for the given URL.
/ansible/test/sanity/code-smell/release-names.py,main,"def main():
    """"""Entrypoint to the script""""""

    releases = pathlib.Path('.github/RELEASE_NAMES.txt').read_text().splitlines()

    # Why this format?  The file's sole purpose is to be read by a human when they need to know
    # which release names have already been used.  So:
    # 1) It's easier for a human to find the release names when there's one on each line
    # 2) It helps keep other people from using the file and then asking for new features in it
    for name in (r.split(maxsplit=1)[1] for r in releases):
        if __codename__ == name:
            break
    else:
        print(f'.github/RELEASE_NAMES.txt: Current codename {__codename__!r} not present in the file')","def main():
    """"""Entrypoint to the script""""""

    releases = pathlib.Path('.github/RELEASE_NAMES.txt').read_text().splitlines()

    # Why this format?  The file's sole purpose is to be read by a human when they need to know
    # which release names have already been used.  So:
    # 1) It's easier for a human to find the release names when there's one on each line
    # 2) It helps keep other people from using the file and then asking for new features in it
    for name in (r.split(maxsplit=1)[1] for r in releases):
        if __codename__ == name:
            break
    else:
        print(f'.github/RELEASE_NAMES.txt: Current codename {__codename__!r} not present in the file')",Entrypoint to the script,Entrypoint to the script,"def main():
    

    releases = pathlib.Path('.github/RELEASE_NAMES.txt').read_text().splitlines()

    # Why this format?  The file's sole purpose is to be read by a human when they need to know
    # which release names have already been used.  So:
    # 1) It's easier for a human to find the release names when there's one on each line
    # 2) It helps keep other people from using the file and then asking for new features in it
    for name in (r.split(maxsplit=1)[1] for r in releases):
        if __codename__ == name:
            break
    else:
        print(f'.github/RELEASE_NAMES.txt: Current codename {__codename__!r} not present in the file')",Entrypoint to the script,def main ( ) : releases = pathlib . Path ( '.github/RELEASE_NAMES.txt' ) . read_text ( ) . splitlines ( ) # Why this format?  The file's sole purpose is to be read by a human when they need to know # which release names have already been used.  So: # 1) It's easier for a human to find the release names when there's one on each line # 2) It helps keep other people from using the file and then asking for new features in it for name in ( r . split ( maxsplit = 1 ) [ 1 ] for r in releases ) : if __codename__ == name : break else : print ( f'.github/RELEASE_NAMES.txt: Current codename {__codename__!r} not present in the file' ),Entrypoint to the script
/yolov5/utils/loggers/__init__.py,log_tensorboard_graph,"def log_tensorboard_graph(tb, model, imgsz=(640, 640)):
    """"""Logs the model graph to TensorBoard with specified image size and model.""""""
    try:
        p = next(model.parameters())  # for device, type
        imgsz = (imgsz, imgsz) if isinstance(imgsz, int) else imgsz  # expand
        im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)  # input image (WARNING: must be zeros, not empty)
        with warnings.catch_warnings():
            warnings.simplefilter(""ignore"")  # suppress jit trace warning
            tb.add_graph(torch.jit.trace(de_parallel(model), im, strict=False), [])
    except Exception as e:
        LOGGER.warning(f""WARNING ⚠️ TensorBoard graph visualization failure {e}"")","def log_tensorboard_graph(tb, model, imgsz=(640, 640)):
    """"""Logs the model graph to TensorBoard with specified image size and model.""""""
    try:
        p = next(model.parameters())  # for device, type
        imgsz = (imgsz, imgsz) if isinstance(imgsz, int) else imgsz  # expand
        im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)  # input image (WARNING: must be zeros, not empty)
        with warnings.catch_warnings():
            warnings.simplefilter(""ignore"")  # suppress jit trace warning
            tb.add_graph(torch.jit.trace(de_parallel(model), im, strict=False), [])
    except Exception as e:
        LOGGER.warning(f""WARNING ⚠️ TensorBoard graph visualization failure {e}"")",Logs the model graph to TensorBoard with specified image size and model.,Logs the model graph to TensorBoard with specified image size and model.,"def log_tensorboard_graph(tb, model, imgsz=(640, 640)):
    
    try:
        p = next(model.parameters())  # for device, type
        imgsz = (imgsz, imgsz) if isinstance(imgsz, int) else imgsz  # expand
        im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)  # input image (WARNING: must be zeros, not empty)
        with warnings.catch_warnings():
            warnings.simplefilter(""ignore"")  # suppress jit trace warning
            tb.add_graph(torch.jit.trace(de_parallel(model), im, strict=False), [])
    except Exception as e:
        LOGGER.warning(f""WARNING ⚠️ TensorBoard graph visualization failure {e}"")",Logs the model graph to TensorBoard with specified image size and model.,"def log_tensorboard_graph ( tb , model , imgsz = ( 640 , 640 ) ) : try : p = next ( model . parameters ( ) ) # for device, type imgsz = ( imgsz , imgsz ) if isinstance ( imgsz , int ) else imgsz # expand im = torch . zeros ( ( 1 , 3 , * imgsz ) ) . to ( p . device ) . type_as ( p ) # input image (WARNING: must be zeros, not empty) with warnings . catch_warnings ( ) : warnings . simplefilter ( ""ignore"" ) # suppress jit trace warning tb . add_graph ( torch . jit . trace ( de_parallel ( model ) , im , strict = False ) , [ ] ) except Exception as e : LOGGER . warning ( f""WARNING ⚠️ TensorBoard graph visualization failure {e}"" )",Logs the model graph to TensorBoard with specified image size and model.
/black/src/black/files.py,get_gitignore,"def get_gitignore(root: Path) -> PathSpec:
    """"""Return a PathSpec matching gitignore content if present.""""""
    gitignore = root / "".gitignore""
    lines: list[str] = []
    if gitignore.is_file():
        with gitignore.open(encoding=""utf-8"") as gf:
            lines = gf.readlines()
    try:
        return PathSpec.from_lines(""gitwildmatch"", lines)
    except GitWildMatchPatternError as e:
        err(f""Could not parse {gitignore}: {e}"")
        raise","def get_gitignore(root: Path) -> PathSpec:
    """"""Return a PathSpec matching gitignore content if present.""""""
    gitignore = root / "".gitignore""
    lines: list[str] = []
    if gitignore.is_file():
        with gitignore.open(encoding=""utf-8"") as gf:
            lines = gf.readlines()
    try:
        return PathSpec.from_lines(""gitwildmatch"", lines)
    except GitWildMatchPatternError as e:
        err(f""Could not parse {gitignore}: {e}"")
        raise",Return a PathSpec matching gitignore content if present.,Return a PathSpec matching gitignore content if present.,"def get_gitignore(root: Path) -> PathSpec:
    
    gitignore = root / "".gitignore""
    lines: list[str] = []
    if gitignore.is_file():
        with gitignore.open(encoding=""utf-8"") as gf:
            lines = gf.readlines()
    try:
        return PathSpec.from_lines(""gitwildmatch"", lines)
    except GitWildMatchPatternError as e:
        err(f""Could not parse {gitignore}: {e}"")
        raise",Return a PathSpec matching gitignore content if present.,"def get_gitignore ( root : Path ) -> PathSpec : gitignore = root / "".gitignore"" lines : list [ str ] = [ ] if gitignore . is_file ( ) : with gitignore . open ( encoding = ""utf-8"" ) as gf : lines = gf . readlines ( ) try : return PathSpec . from_lines ( ""gitwildmatch"" , lines ) except GitWildMatchPatternError as e : err ( f""Could not parse {gitignore}: {e}"" ) raise",Return a PathSpec matching gitignore content if present.
/odoo/odoo/http.py,_serve_static,"def _serve_static(self):
        """""" Serve a static file from the file system. """"""
        module, _, path = self.httprequest.path[1:].partition('/static/')
        try:
            directory = root.statics[module]
            filepath = werkzeug.security.safe_join(directory, path)
            debug = (
                'assets' in self.session.debug and
                ' wkhtmltopdf ' not in self.httprequest.user_agent.string
            )
            res = Stream.from_path(filepath, public=True).get_response(
                max_age=0 if debug else STATIC_CACHE,
                content_security_policy=None,
            )
            root.set_csp(res)
            return res
        except KeyError:
            raise NotFound(f'Module ""{module}"" not found.\n')
        except OSError:  # cover both missing file and invalid permissions
            raise NotFound(f'File ""{path}"" not found in module {module}.\n')","def _serve_static(self):
        """""" Serve a static file from the file system. """"""
        module, _, path = self.httprequest.path[1:].partition('/static/')
        try:
            directory = root.statics[module]
            filepath = werkzeug.security.safe_join(directory, path)
            debug = (
                'assets' in self.session.debug and
                ' wkhtmltopdf ' not in self.httprequest.user_agent.string
            )
            res = Stream.from_path(filepath, public=True).get_response(
                max_age=0 if debug else STATIC_CACHE,
                content_security_policy=None,
            )
            root.set_csp(res)
            return res
        except KeyError:
            raise NotFound(f'Module ""{module}"" not found.\n')
        except OSError:  # cover both missing file and invalid permissions
            raise NotFound(f'File ""{path}"" not found in module {module}.\n')",Serve a static file from the file system.,Serve a static file from the file system.,"def _serve_static(self):
        
        module, _, path = self.httprequest.path[1:].partition('/static/')
        try:
            directory = root.statics[module]
            filepath = werkzeug.security.safe_join(directory, path)
            debug = (
                'assets' in self.session.debug and
                ' wkhtmltopdf ' not in self.httprequest.user_agent.string
            )
            res = Stream.from_path(filepath, public=True).get_response(
                max_age=0 if debug else STATIC_CACHE,
                content_security_policy=None,
            )
            root.set_csp(res)
            return res
        except KeyError:
            raise NotFound(f'Module ""{module}"" not found.\n')
        except OSError:  # cover both missing file and invalid permissions
            raise NotFound(f'File ""{path}"" not found in module {module}.\n')",Serve a static file from the file system.,"def _serve_static ( self ) : module , _ , path = self . httprequest . path [ 1 : ] . partition ( '/static/' ) try : directory = root . statics [ module ] filepath = werkzeug . security . safe_join ( directory , path ) debug = ( 'assets' in self . session . debug and ' wkhtmltopdf ' not in self . httprequest . user_agent . string ) res = Stream . from_path ( filepath , public = True ) . get_response ( max_age = 0 if debug else STATIC_CACHE , content_security_policy = None , ) root . set_csp ( res ) return res except KeyError : raise NotFound ( f'Module ""{module}"" not found.\n' ) except OSError : # cover both missing file and invalid permissions raise NotFound ( f'File ""{path}"" not found in module {module}.\n' )",Serve a static file from the file system.
/odoo/odoo/fields.py,update,"def update(cls, id: int, values: dict):
        """"""
        Write ``values`` on the related record.

        Return the command triple :samp:`(UPDATE, {id}, {values})`
        """"""
        return (cls.UPDATE, id, values)","def update(cls, id: int, values: dict):
        """"""
        Write ``values`` on the related record.

        Return the command triple :samp:`(UPDATE, {id}, {values})`
        """"""
        return (cls.UPDATE, id, values)","Write ``values`` on the related record.

Return the command triple :samp:`(UPDATE, {id}, {values})`",Write ``values`` on the related record.,"def update(cls, id: int, values: dict):
        
        return (cls.UPDATE, id, values)",Write ``values`` on the related record.,"def update ( cls , id : int , values : dict ) : return ( cls . UPDATE , id , values )",Write ``values`` on the related record.
/odoo/odoo/tests/form.py,__getitem__,"def __getitem__(self, field_name):
        """""" Return the current value of the given field. """"""
        field_info = self._view['fields'].get(field_name)
        assert field_info is not None, f""{field_name!r} was not found in the view""

        value = self._values[field_name]
        if field_info['type'] == 'many2one':
            Model = self._env[field_info['relation']]
            return Model.browse(value)
        elif field_info['type'] == 'one2many':
            return O2MProxy(self, field_name)
        elif field_info['type'] == 'many2many':
            return M2MProxy(self, field_name)
        return value","def __getitem__(self, field_name):
        """""" Return the current value of the given field. """"""
        field_info = self._view['fields'].get(field_name)
        assert field_info is not None, f""{field_name!r} was not found in the view""

        value = self._values[field_name]
        if field_info['type'] == 'many2one':
            Model = self._env[field_info['relation']]
            return Model.browse(value)
        elif field_info['type'] == 'one2many':
            return O2MProxy(self, field_name)
        elif field_info['type'] == 'many2many':
            return M2MProxy(self, field_name)
        return value",Return the current value of the given field.,Return the current value of the given field.,"def __getitem__(self, field_name):
        
        field_info = self._view['fields'].get(field_name)
        assert field_info is not None, f""{field_name!r} was not found in the view""

        value = self._values[field_name]
        if field_info['type'] == 'many2one':
            Model = self._env[field_info['relation']]
            return Model.browse(value)
        elif field_info['type'] == 'one2many':
            return O2MProxy(self, field_name)
        elif field_info['type'] == 'many2many':
            return M2MProxy(self, field_name)
        return value",Return the current value of the given field.,"def __getitem__ ( self , field_name ) : field_info = self . _view [ 'fields' ] . get ( field_name ) assert field_info is not None , f""{field_name!r} was not found in the view"" value = self . _values [ field_name ] if field_info [ 'type' ] == 'many2one' : Model = self . _env [ field_info [ 'relation' ] ] return Model . browse ( value ) elif field_info [ 'type' ] == 'one2many' : return O2MProxy ( self , field_name ) elif field_info [ 'type' ] == 'many2many' : return M2MProxy ( self , field_name ) return value",Return the current value of the given field.
/ultralytics/ultralytics/data/loaders.py,close,"def close(self):
        """"""Terminate stream loader, stop threads, and release video capture resources.""""""
        self.running = False  # stop flag for Thread
        for thread in self.threads:
            if thread.is_alive():
                thread.join(timeout=5)  # Add timeout
        for cap in self.caps:  # Iterate through the stored VideoCapture objects
            try:
                cap.release()  # release video capture
            except Exception as e:
                LOGGER.warning(f""Could not release VideoCapture object: {e}"")
        cv2.destroyAllWindows()","def close(self):
        """"""Terminate stream loader, stop threads, and release video capture resources.""""""
        self.running = False  # stop flag for Thread
        for thread in self.threads:
            if thread.is_alive():
                thread.join(timeout=5)  # Add timeout
        for cap in self.caps:  # Iterate through the stored VideoCapture objects
            try:
                cap.release()  # release video capture
            except Exception as e:
                LOGGER.warning(f""Could not release VideoCapture object: {e}"")
        cv2.destroyAllWindows()","Terminate stream loader, stop threads, and release video capture resources.","Terminate stream loader, stop threads, and release video capture resources.","def close(self):
        
        self.running = False  # stop flag for Thread
        for thread in self.threads:
            if thread.is_alive():
                thread.join(timeout=5)  # Add timeout
        for cap in self.caps:  # Iterate through the stored VideoCapture objects
            try:
                cap.release()  # release video capture
            except Exception as e:
                LOGGER.warning(f""Could not release VideoCapture object: {e}"")
        cv2.destroyAllWindows()","Terminate stream loader, stop threads, and release video capture resources.","def close ( self ) : self . running = False # stop flag for Thread for thread in self . threads : if thread . is_alive ( ) : thread . join ( timeout = 5 ) # Add timeout for cap in self . caps : # Iterate through the stored VideoCapture objects try : cap . release ( ) # release video capture except Exception as e : LOGGER . warning ( f""Could not release VideoCapture object: {e}"" ) cv2 . destroyAllWindows ( )","Terminate stream loader, stop threads, and release video capture resources."
/cpython/Tools/build/stable_abi.py,binutils_check_library,"def binutils_check_library(manifest, library, expected_symbols, dynamic):
    """"""Check that library exports all expected_symbols""""""
    available_symbols = set(binutils_get_exported_symbols(library, dynamic))
    missing_symbols = expected_symbols - available_symbols
    if missing_symbols:
        print(textwrap.dedent(f""""""\
            Some symbols from the limited API are missing from {library}:
                {', '.join(missing_symbols)}

            This error means that there are some missing symbols among the
            ones exported in the library.
            This normally means that some symbol, function implementation or
            a prototype belonging to a symbol in the limited API has been
            deleted or is missing.
        """"""), file=sys.stderr)
        return False
    return True","def binutils_check_library(manifest, library, expected_symbols, dynamic):
    """"""Check that library exports all expected_symbols""""""
    available_symbols = set(binutils_get_exported_symbols(library, dynamic))
    missing_symbols = expected_symbols - available_symbols
    if missing_symbols:
        print(textwrap.dedent(f""""""\
            Some symbols from the limited API are missing from {library}:
                {', '.join(missing_symbols)}

            This error means that there are some missing symbols among the
            ones exported in the library.
            This normally means that some symbol, function implementation or
            a prototype belonging to a symbol in the limited API has been
            deleted or is missing.
        """"""), file=sys.stderr)
        return False
    return True",Check that library exports all expected_symbols,Check that library exports all expected_symbols,"def binutils_check_library(manifest, library, expected_symbols, dynamic):
    
    available_symbols = set(binutils_get_exported_symbols(library, dynamic))
    missing_symbols = expected_symbols - available_symbols
    if missing_symbols:
        print(textwrap.dedent(f), file=sys.stderr)
        return False
    return True",Check that library exports all expected_symbols,"def binutils_check_library ( manifest , library , expected_symbols , dynamic ) : available_symbols = set ( binutils_get_exported_symbols ( library , dynamic ) ) missing_symbols = expected_symbols - available_symbols if missing_symbols : print ( textwrap . dedent ( f ) , file = sys . stderr ) return False return True",Check that library exports all expected_symbols
/cpython/Tools/cases_generator/target_generator.py,write_opcode_targets,"def write_opcode_targets(analysis: Analysis, out: CWriter) -> None:
    """"""Write header file that defines the jump target table""""""
    targets = [""&&_unknown_opcode,\n""] * 256
    for name, op in analysis.opmap.items():
        if op < 256:
            targets[op] = f""&&TARGET_{name},\n""
    out.emit(""#if !Py_TAIL_CALL_INTERP\n"")
    out.emit(""static void *opcode_targets[256] = {\n"")
    for target in targets:
        out.emit(target)
    out.emit(""};\n"")
    out.emit(""#else /* Py_TAIL_CALL_INTERP */\n"")","def write_opcode_targets(analysis: Analysis, out: CWriter) -> None:
    """"""Write header file that defines the jump target table""""""
    targets = [""&&_unknown_opcode,\n""] * 256
    for name, op in analysis.opmap.items():
        if op < 256:
            targets[op] = f""&&TARGET_{name},\n""
    out.emit(""#if !Py_TAIL_CALL_INTERP\n"")
    out.emit(""static void *opcode_targets[256] = {\n"")
    for target in targets:
        out.emit(target)
    out.emit(""};\n"")
    out.emit(""#else /* Py_TAIL_CALL_INTERP */\n"")",Write header file that defines the jump target table,Write header file that defines the jump target table,"def write_opcode_targets(analysis: Analysis, out: CWriter) -> None:
    
    targets = [""&&_unknown_opcode,\n""] * 256
    for name, op in analysis.opmap.items():
        if op < 256:
            targets[op] = f""&&TARGET_{name},\n""
    out.emit(""#if !Py_TAIL_CALL_INTERP\n"")
    out.emit(""static void *opcode_targets[256] = {\n"")
    for target in targets:
        out.emit(target)
    out.emit(""};\n"")
    out.emit(""#else /* Py_TAIL_CALL_INTERP */\n"")",Write header file that defines the jump target table,"def write_opcode_targets ( analysis : Analysis , out : CWriter ) -> None : targets = [ ""&&_unknown_opcode,\n"" ] * 256 for name , op in analysis . opmap . items ( ) : if op < 256 : targets [ op ] = f""&&TARGET_{name},\n"" out . emit ( ""#if !Py_TAIL_CALL_INTERP\n"" ) out . emit ( ""static void *opcode_targets[256] = {\n"" ) for target in targets : out . emit ( target ) out . emit ( ""};\n"" ) out . emit ( ""#else /* Py_TAIL_CALL_INTERP */\n"" )",Write header file that defines the jump target table
/OpenManus/app/mcp/server.py,run,"def run(self, transport: str = ""stdio"") -> None:
        """"""Run the MCP server.""""""
        # Register all tools
        self.register_all_tools()

        # Register cleanup function (match original behavior)
        atexit.register(lambda: asyncio.run(self.cleanup()))

        # Start server (with same logging as original)
        logger.info(f""Starting OpenManus server ({transport} mode)"")
        self.server.run(transport=transport)","def run(self, transport: str = ""stdio"") -> None:
        """"""Run the MCP server.""""""
        # Register all tools
        self.register_all_tools()

        # Register cleanup function (match original behavior)
        atexit.register(lambda: asyncio.run(self.cleanup()))

        # Start server (with same logging as original)
        logger.info(f""Starting OpenManus server ({transport} mode)"")
        self.server.run(transport=transport)",Run the MCP server.,Run the MCP server.,"def run(self, transport: str = ""stdio"") -> None:
        
        # Register all tools
        self.register_all_tools()

        # Register cleanup function (match original behavior)
        atexit.register(lambda: asyncio.run(self.cleanup()))

        # Start server (with same logging as original)
        logger.info(f""Starting OpenManus server ({transport} mode)"")
        self.server.run(transport=transport)",Run the MCP server.,"def run ( self , transport : str = ""stdio"" ) -> None : # Register all tools self . register_all_tools ( ) # Register cleanup function (match original behavior) atexit . register ( lambda : asyncio . run ( self . cleanup ( ) ) ) # Start server (with same logging as original) logger . info ( f""Starting OpenManus server ({transport} mode)"" ) self . server . run ( transport = transport )",Run the MCP server.
/ultralytics/ultralytics/data/build.py,build_grounding,"def build_grounding(cfg, img_path, json_file, batch, mode=""train"", rect=False, stride=32):
    """"""Build and return a GroundingDataset based on configuration parameters.""""""
    return GroundingDataset(
        img_path=img_path,
        json_file=json_file,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == ""train"",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == ""train"" else 0.5,
        prefix=colorstr(f""{mode}: ""),
        task=cfg.task,
        classes=cfg.classes,
        fraction=cfg.fraction if mode == ""train"" else 1.0,
    )","def build_grounding(cfg, img_path, json_file, batch, mode=""train"", rect=False, stride=32):
    """"""Build and return a GroundingDataset based on configuration parameters.""""""
    return GroundingDataset(
        img_path=img_path,
        json_file=json_file,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == ""train"",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == ""train"" else 0.5,
        prefix=colorstr(f""{mode}: ""),
        task=cfg.task,
        classes=cfg.classes,
        fraction=cfg.fraction if mode == ""train"" else 1.0,
    )",Build and return a GroundingDataset based on configuration parameters.,Build and return a GroundingDataset based on configuration parameters.,"def build_grounding(cfg, img_path, json_file, batch, mode=""train"", rect=False, stride=32):
    
    return GroundingDataset(
        img_path=img_path,
        json_file=json_file,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == ""train"",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == ""train"" else 0.5,
        prefix=colorstr(f""{mode}: ""),
        task=cfg.task,
        classes=cfg.classes,
        fraction=cfg.fraction if mode == ""train"" else 1.0,
    )",Build and return a GroundingDataset based on configuration parameters.,"def build_grounding ( cfg , img_path , json_file , batch , mode = ""train"" , rect = False , stride = 32 ) : return GroundingDataset ( img_path = img_path , json_file = json_file , imgsz = cfg . imgsz , batch_size = batch , augment = mode == ""train"" , # augmentation hyp = cfg , # TODO: probably add a get_hyps_from_cfg function rect = cfg . rect or rect , # rectangular batches cache = cfg . cache or None , single_cls = cfg . single_cls or False , stride = int ( stride ) , pad = 0.0 if mode == ""train"" else 0.5 , prefix = colorstr ( f""{mode}: "" ) , task = cfg . task , classes = cfg . classes , fraction = cfg . fraction if mode == ""train"" else 1.0 , )",Build and return a GroundingDataset based on configuration parameters.
/odoo/odoo/models.py,parse_read_group_spec,"def parse_read_group_spec(spec: str) -> tuple:
    """""" Return a triplet corresponding to the given groupby/path/aggregate specification. """"""
    res_match = regex_read_group_spec.match(spec)
    if not res_match:
        raise ValueError(
            f'Invalid aggregate/groupby specification {spec!r}.\n'
            '- Valid aggregate specification looks like ""<field_name>:<agg>"" example: ""quantity:sum"".\n'
            '- Valid groupby specification looks like ""<no_datish_field_name>"" or ""<datish_field_name>:<granularity>"" example: ""date:month"" or ""<properties_field_name>.<property>:<granularity>"".'
        )

    groups = res_match.groups()
    return groups[0], groups[2], groups[3]","def parse_read_group_spec(spec: str) -> tuple:
    """""" Return a triplet corresponding to the given groupby/path/aggregate specification. """"""
    res_match = regex_read_group_spec.match(spec)
    if not res_match:
        raise ValueError(
            f'Invalid aggregate/groupby specification {spec!r}.\n'
            '- Valid aggregate specification looks like ""<field_name>:<agg>"" example: ""quantity:sum"".\n'
            '- Valid groupby specification looks like ""<no_datish_field_name>"" or ""<datish_field_name>:<granularity>"" example: ""date:month"" or ""<properties_field_name>.<property>:<granularity>"".'
        )

    groups = res_match.groups()
    return groups[0], groups[2], groups[3]",Return a triplet corresponding to the given groupby/path/aggregate specification.,Return a triplet corresponding to the given groupby/path/aggregate specification.,"def parse_read_group_spec(spec: str) -> tuple:
    
    res_match = regex_read_group_spec.match(spec)
    if not res_match:
        raise ValueError(
            f'Invalid aggregate/groupby specification {spec!r}.\n'
            '- Valid aggregate specification looks like ""<field_name>:<agg>"" example: ""quantity:sum"".\n'
            '- Valid groupby specification looks like ""<no_datish_field_name>"" or ""<datish_field_name>:<granularity>"" example: ""date:month"" or ""<properties_field_name>.<property>:<granularity>"".'
        )

    groups = res_match.groups()
    return groups[0], groups[2], groups[3]",Return a triplet corresponding to the given groupby/path/aggregate specification.,"def parse_read_group_spec ( spec : str ) -> tuple : res_match = regex_read_group_spec . match ( spec ) if not res_match : raise ValueError ( f'Invalid aggregate/groupby specification {spec!r}.\n' '- Valid aggregate specification looks like ""<field_name>:<agg>"" example: ""quantity:sum"".\n' '- Valid groupby specification looks like ""<no_datish_field_name>"" or ""<datish_field_name>:<granularity>"" example: ""date:month"" or ""<properties_field_name>.<property>:<granularity>"".' ) groups = res_match . groups ( ) return groups [ 0 ] , groups [ 2 ] , groups [ 3 ]",Return a triplet corresponding to the given groupby/path/aggregate specification.
/odoo/odoo/_monkeypatches/werkzeug_urls.py,_check_str_tuple,"def _check_str_tuple(value: t.Tuple[t.AnyStr, ...]) -> None:
    """"""Ensure tuple items are all strings or all bytes.""""""
    if not value:
        return

    item_type = str if isinstance(value[0], str) else bytes

    if any(not isinstance(item, item_type) for item in value):
        raise TypeError(f""Cannot mix str and bytes arguments (got {value!r})"")","def _check_str_tuple(value: t.Tuple[t.AnyStr, ...]) -> None:
    """"""Ensure tuple items are all strings or all bytes.""""""
    if not value:
        return

    item_type = str if isinstance(value[0], str) else bytes

    if any(not isinstance(item, item_type) for item in value):
        raise TypeError(f""Cannot mix str and bytes arguments (got {value!r})"")",Ensure tuple items are all strings or all bytes.,Ensure tuple items are all strings or all bytes.,"def _check_str_tuple(value: t.Tuple[t.AnyStr, ...]) -> None:
    
    if not value:
        return

    item_type = str if isinstance(value[0], str) else bytes

    if any(not isinstance(item, item_type) for item in value):
        raise TypeError(f""Cannot mix str and bytes arguments (got {value!r})"")",Ensure tuple items are all strings or all bytes.,"def _check_str_tuple ( value : t . Tuple [ t . AnyStr , ... ] ) -> None : if not value : return item_type = str if isinstance ( value [ 0 ] , str ) else bytes if any ( not isinstance ( item , item_type ) for item in value ) : raise TypeError ( f""Cannot mix str and bytes arguments (got {value!r})"" )",Ensure tuple items are all strings or all bytes.
/streamlit/scripts/update_version.py,update_files,"def update_files(data: dict[str, str], version: str) -> None:
    """"""Update files with new version number.""""""

    for filename, regex in data.items():
        file_path = os.path.join(BASE_DIR, filename)
        matched = False
        pattern = re.compile(regex)
        for line in fileinput.input(file_path, inplace=True):
            if pattern.match(line.rstrip()):
                matched = True
            updated_line = re.sub(regex, rf""\g<pre>{version}\g<post>"", line.rstrip())
            print(updated_line)
        if not matched:
            msg = f'In file ""{file_path}"", did not find regex ""{regex}""'
            raise Exception(msg)","def update_files(data: dict[str, str], version: str) -> None:
    """"""Update files with new version number.""""""

    for filename, regex in data.items():
        file_path = os.path.join(BASE_DIR, filename)
        matched = False
        pattern = re.compile(regex)
        for line in fileinput.input(file_path, inplace=True):
            if pattern.match(line.rstrip()):
                matched = True
            updated_line = re.sub(regex, rf""\g<pre>{version}\g<post>"", line.rstrip())
            print(updated_line)
        if not matched:
            msg = f'In file ""{file_path}"", did not find regex ""{regex}""'
            raise Exception(msg)",Update files with new version number.,Update files with new version number.,"def update_files(data: dict[str, str], version: str) -> None:
    

    for filename, regex in data.items():
        file_path = os.path.join(BASE_DIR, filename)
        matched = False
        pattern = re.compile(regex)
        for line in fileinput.input(file_path, inplace=True):
            if pattern.match(line.rstrip()):
                matched = True
            updated_line = re.sub(regex, rf""\g<pre>{version}\g<post>"", line.rstrip())
            print(updated_line)
        if not matched:
            msg = f'In file ""{file_path}"", did not find regex ""{regex}""'
            raise Exception(msg)",Update files with new version number.,"def update_files ( data : dict [ str , str ] , version : str ) -> None : for filename , regex in data . items ( ) : file_path = os . path . join ( BASE_DIR , filename ) matched = False pattern = re . compile ( regex ) for line in fileinput . input ( file_path , inplace = True ) : if pattern . match ( line . rstrip ( ) ) : matched = True updated_line = re . sub ( regex , rf""\g<pre>{version}\g<post>"" , line . rstrip ( ) ) print ( updated_line ) if not matched : msg = f'In file ""{file_path}"", did not find regex ""{regex}""' raise Exception ( msg )",Update files with new version number.
/quivr/core/quivr_core/llm/llm_endpoint.py,_calculate_tokenizer_size,"def _calculate_tokenizer_size(self) -> int:
        """"""Calculate size of tokenizer by summing the sizes of its vocabulary and model files""""""
        # By default, return a size of 5 MB
        if not hasattr(self.tokenizer, ""vocab_files_names"") or not hasattr(
            self.tokenizer, ""init_kwargs""
        ):
            return self._default_size

        total_size = 0

        # Get the file keys from vocab_files_names
        file_keys = self.tokenizer.vocab_files_names.keys()
        # Look up these files in init_kwargs
        for key in file_keys:
            if file_path := self.tokenizer.init_kwargs.get(key):
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, FileNotFoundError):
                    logger.debug(f""Could not access tokenizer file: {file_path}"")

        return total_size if total_size > 0 else self._default_size","def _calculate_tokenizer_size(self) -> int:
        """"""Calculate size of tokenizer by summing the sizes of its vocabulary and model files""""""
        # By default, return a size of 5 MB
        if not hasattr(self.tokenizer, ""vocab_files_names"") or not hasattr(
            self.tokenizer, ""init_kwargs""
        ):
            return self._default_size

        total_size = 0

        # Get the file keys from vocab_files_names
        file_keys = self.tokenizer.vocab_files_names.keys()
        # Look up these files in init_kwargs
        for key in file_keys:
            if file_path := self.tokenizer.init_kwargs.get(key):
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, FileNotFoundError):
                    logger.debug(f""Could not access tokenizer file: {file_path}"")

        return total_size if total_size > 0 else self._default_size",Calculate size of tokenizer by summing the sizes of its vocabulary and model files,Calculate size of tokenizer by summing the sizes of its vocabulary and model files,"def _calculate_tokenizer_size(self) -> int:
        
        # By default, return a size of 5 MB
        if not hasattr(self.tokenizer, ""vocab_files_names"") or not hasattr(
            self.tokenizer, ""init_kwargs""
        ):
            return self._default_size

        total_size = 0

        # Get the file keys from vocab_files_names
        file_keys = self.tokenizer.vocab_files_names.keys()
        # Look up these files in init_kwargs
        for key in file_keys:
            if file_path := self.tokenizer.init_kwargs.get(key):
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, FileNotFoundError):
                    logger.debug(f""Could not access tokenizer file: {file_path}"")

        return total_size if total_size > 0 else self._default_size",Calculate size of tokenizer by summing the sizes of its vocabulary and model files,"def _calculate_tokenizer_size ( self ) -> int : # By default, return a size of 5 MB if not hasattr ( self . tokenizer , ""vocab_files_names"" ) or not hasattr ( self . tokenizer , ""init_kwargs"" ) : return self . _default_size total_size = 0 # Get the file keys from vocab_files_names file_keys = self . tokenizer . vocab_files_names . keys ( ) # Look up these files in init_kwargs for key in file_keys : if file_path := self . tokenizer . init_kwargs . get ( key ) : try : total_size += os . path . getsize ( file_path ) except ( OSError , FileNotFoundError ) : logger . debug ( f""Could not access tokenizer file: {file_path}"" ) return total_size if total_size > 0 else self . _default_size",Calculate size of tokenizer by summing the sizes of its vocabulary and model files
/core/script/scaffold/model.py,update_strings,"def update_strings(self, **kwargs) -> None:
        """"""Update the integration strings.""""""
        print(f""Updating {self.domain} strings: {list(kwargs)}"")
        self.strings_path.write_text(
            json.dumps({**self.strings(), **kwargs}, indent=2) + ""\n""
        )","def update_strings(self, **kwargs) -> None:
        """"""Update the integration strings.""""""
        print(f""Updating {self.domain} strings: {list(kwargs)}"")
        self.strings_path.write_text(
            json.dumps({**self.strings(), **kwargs}, indent=2) + ""\n""
        )",Update the integration strings.,Update the integration strings.,"def update_strings(self, **kwargs) -> None:
        
        print(f""Updating {self.domain} strings: {list(kwargs)}"")
        self.strings_path.write_text(
            json.dumps({**self.strings(), **kwargs}, indent=2) + ""\n""
        )",Update the integration strings.,"def update_strings ( self , ** kwargs ) -> None : print ( f""Updating {self.domain} strings: {list(kwargs)}"" ) self . strings_path . write_text ( json . dumps ( { ** self . strings ( ) , ** kwargs } , indent = 2 ) + ""\n"" )",Update the integration strings.
/open-interpreter/interpreter/core/computer/terminal/languages/java.py,preprocess_java,"def preprocess_java(code):
    """"""
    Add active line markers
    Add end of execution marker
    """"""
    lines = code.split(""\n"")
    processed_lines = []

    for i, line in enumerate(lines, 1):
        # Add active line print
        processed_lines.append(f'System.out.println(""##active_line{i}##"");')
        processed_lines.append(line)

    # Join lines to form the processed code
    code = ""\n"".join(processed_lines)

    # Add end of execution marker
    code += '\nSystem.out.println(""##end_of_execution##"");'
    return code","def preprocess_java(code):
    """"""
    Add active line markers
    Add end of execution marker
    """"""
    lines = code.split(""\n"")
    processed_lines = []

    for i, line in enumerate(lines, 1):
        # Add active line print
        processed_lines.append(f'System.out.println(""##active_line{i}##"");')
        processed_lines.append(line)

    # Join lines to form the processed code
    code = ""\n"".join(processed_lines)

    # Add end of execution marker
    code += '\nSystem.out.println(""##end_of_execution##"");'
    return code","Add active line markers
Add end of execution marker",Add active line markers,"def preprocess_java(code):
    
    lines = code.split(""\n"")
    processed_lines = []

    for i, line in enumerate(lines, 1):
        # Add active line print
        processed_lines.append(f'System.out.println(""##active_line{i}##"");')
        processed_lines.append(line)

    # Join lines to form the processed code
    code = ""\n"".join(processed_lines)

    # Add end of execution marker
    code += '\nSystem.out.println(""##end_of_execution##"");'
    return code",Add active line markers,"def preprocess_java ( code ) : lines = code . split ( ""\n"" ) processed_lines = [ ] for i , line in enumerate ( lines , 1 ) : # Add active line print processed_lines . append ( f'System.out.println(""##active_line{i}##"");' ) processed_lines . append ( line ) # Join lines to form the processed code code = ""\n"" . join ( processed_lines ) # Add end of execution marker code += '\nSystem.out.println(""##end_of_execution##"");' return code",Add active line markers
/streamlit/scripts/update_e2e_snapshots.py,get_token_from_credential_manager,"def get_token_from_credential_manager() -> str:
    """"""Get the GitHub token from the git credential manager.
    The token can also be provided via the --token argument.
    """"""
    cmd = [""git"", ""credential"", ""fill""]
    input_data = ""protocol=https\nhost=github.com\n\n""
    result = subprocess.run(
        cmd, input=input_data, capture_output=True, text=True, check=False
    )
    if result.returncode != 0:
        print(
            f""Error getting credentials from git credential manager: {result.stderr.strip()}""
        )
        return """"
    output = result.stdout
    # Parse the output to get the token
    for line in output.splitlines():
        if line.startswith(""password=""):
            return line[len(""password="") :]
    return """"","def get_token_from_credential_manager() -> str:
    """"""Get the GitHub token from the git credential manager.
    The token can also be provided via the --token argument.
    """"""
    cmd = [""git"", ""credential"", ""fill""]
    input_data = ""protocol=https\nhost=github.com\n\n""
    result = subprocess.run(
        cmd, input=input_data, capture_output=True, text=True, check=False
    )
    if result.returncode != 0:
        print(
            f""Error getting credentials from git credential manager: {result.stderr.strip()}""
        )
        return """"
    output = result.stdout
    # Parse the output to get the token
    for line in output.splitlines():
        if line.startswith(""password=""):
            return line[len(""password="") :]
    return """"","Get the GitHub token from the git credential manager.
The token can also be provided via the --token argument.",Get the GitHub token from the git credential manager.,"def get_token_from_credential_manager() -> str:
    
    cmd = [""git"", ""credential"", ""fill""]
    input_data = ""protocol=https\nhost=github.com\n\n""
    result = subprocess.run(
        cmd, input=input_data, capture_output=True, text=True, check=False
    )
    if result.returncode != 0:
        print(
            f""Error getting credentials from git credential manager: {result.stderr.strip()}""
        )
        return """"
    output = result.stdout
    # Parse the output to get the token
    for line in output.splitlines():
        if line.startswith(""password=""):
            return line[len(""password="") :]
    return """"",Get the GitHub token from the git credential manager.,"def get_token_from_credential_manager ( ) -> str : cmd = [ ""git"" , ""credential"" , ""fill"" ] input_data = ""protocol=https\nhost=github.com\n\n"" result = subprocess . run ( cmd , input = input_data , capture_output = True , text = True , check = False ) if result . returncode != 0 : print ( f""Error getting credentials from git credential manager: {result.stderr.strip()}"" ) return """" output = result . stdout # Parse the output to get the token for line in output . splitlines ( ) : if line . startswith ( ""password="" ) : return line [ len ( ""password="" ) : ] return """"",Get the GitHub token from the git credential manager.
/ansible/packaging/release.py,get_next_release_date,"def get_next_release_date(start: datetime.date, step: int, after: datetime.date) -> datetime.date:
    """"""Return the next release date.""""""
    if start > after:
        raise ValueError(f""{start=} is greater than {after=}"")

    current_delta = after - start
    release_delta = datetime.timedelta(days=(math.floor(current_delta.days / step) + 1) * step)

    release = start + release_delta

    return release","def get_next_release_date(start: datetime.date, step: int, after: datetime.date) -> datetime.date:
    """"""Return the next release date.""""""
    if start > after:
        raise ValueError(f""{start=} is greater than {after=}"")

    current_delta = after - start
    release_delta = datetime.timedelta(days=(math.floor(current_delta.days / step) + 1) * step)

    release = start + release_delta

    return release",Return the next release date.,Return the next release date.,"def get_next_release_date(start: datetime.date, step: int, after: datetime.date) -> datetime.date:
    
    if start > after:
        raise ValueError(f""{start=} is greater than {after=}"")

    current_delta = after - start
    release_delta = datetime.timedelta(days=(math.floor(current_delta.days / step) + 1) * step)

    release = start + release_delta

    return release",Return the next release date.,"def get_next_release_date ( start : datetime . date , step : int , after : datetime . date ) -> datetime . date : if start > after : raise ValueError ( f""{start=} is greater than {after=}"" ) current_delta = after - start release_delta = datetime . timedelta ( days = ( math . floor ( current_delta . days / step ) + 1 ) * step ) release = start + release_delta return release",Return the next release date.
/faceswap/setup.py,docker_no_cuda,"def docker_no_cuda(cls) -> None:
        """""" Output Tips for Docker without Cuda """"""
        logger.info(
            ""1. Install Docker from: https://www.docker.com/get-started\n\n""
            ""2. Enter the Faceswap folder and build the Docker Image For Faceswap:\n""
            ""   docker build -t faceswap-cpu -f Dockerfile.cpu .\n\n""
            ""3. Launch and enter the Faceswap container:\n""
            ""  a. Headless:\n""
            ""     docker run --rm -it -v ./:/srv faceswap-cpu\n\n""
            ""  b. GUI:\n""
            ""     xhost +local: && \\ \n""
            ""     docker run --rm -it \\ \n""
            ""     -v ./:/srv \\ \n""
            ""     -v /tmp/.X11-unix:/tmp/.X11-unix \\ \n""
            ""     -e DISPLAY=${DISPLAY} \\ \n""
            ""     faceswap-cpu \n"")
        logger.info(""That's all you need to do with docker. Have fun."")","def docker_no_cuda(cls) -> None:
        """""" Output Tips for Docker without Cuda """"""
        logger.info(
            ""2. Enter the Faceswap folder and build the Docker Image For Faceswap:\n""
            ""   docker build -t faceswap-cpu -f Dockerfile.cpu .\n\n""
            ""3. Launch and enter the Faceswap container:\n""
            ""  a. Headless:\n""
            ""     docker run --rm -it -v ./:/srv faceswap-cpu\n\n""
            ""  b. GUI:\n""
            ""     xhost +local: && \\ \n""
            ""     docker run --rm -it \\ \n""
            ""     -v ./:/srv \\ \n""
            ""     -v /tmp/.X11-unix:/tmp/.X11-unix \\ \n""
            ""     -e DISPLAY=${DISPLAY} \\ \n""
            ""     faceswap-cpu \n"")
        logger.info(""That's all you need to do with docker. Have fun."")",Output Tips for Docker without Cuda,Output Tips for Docker without Cuda,"def docker_no_cuda(cls) -> None:
        
        logger.info(
            ""2. Enter the Faceswap folder and build the Docker Image For Faceswap:\n""
            ""   docker build -t faceswap-cpu -f Dockerfile.cpu .\n\n""
            ""3. Launch and enter the Faceswap container:\n""
            ""  a. Headless:\n""
            ""     docker run --rm -it -v ./:/srv faceswap-cpu\n\n""
            ""  b. GUI:\n""
            ""     xhost +local: && \\ \n""
            ""     docker run --rm -it \\ \n""
            ""     -v ./:/srv \\ \n""
            ""     -v /tmp/.X11-unix:/tmp/.X11-unix \\ \n""
            ""     -e DISPLAY=${DISPLAY} \\ \n""
            ""     faceswap-cpu \n"")
        logger.info(""That's all you need to do with docker. Have fun."")",Output Tips for Docker without Cuda,"def docker_no_cuda ( cls ) -> None : logger . info ( ""2. Enter the Faceswap folder and build the Docker Image For Faceswap:\n"" ""   docker build -t faceswap-cpu -f Dockerfile.cpu .\n\n"" ""3. Launch and enter the Faceswap container:\n"" ""  a. Headless:\n"" ""     docker run --rm -it -v ./:/srv faceswap-cpu\n\n"" ""  b. GUI:\n"" ""     xhost +local: && \\ \n"" ""     docker run --rm -it \\ \n"" ""     -v ./:/srv \\ \n"" ""     -v /tmp/.X11-unix:/tmp/.X11-unix \\ \n"" ""     -e DISPLAY=${DISPLAY} \\ \n"" ""     faceswap-cpu \n"" ) logger . info ( ""That's all you need to do with docker. Have fun."" )",Output Tips for Docker without Cuda
/OpenHands/openhands/llm/metrics.py,get,"def get(self) -> dict:
        """"""Return the metrics in a dictionary.""""""
        return {
            'accumulated_cost': self._accumulated_cost,
            'accumulated_token_usage': self.accumulated_token_usage.model_dump(),
            'costs': [cost.model_dump() for cost in self._costs],
            'response_latencies': [
                latency.model_dump() for latency in self._response_latencies
            ],
            'token_usages': [usage.model_dump() for usage in self._token_usages],
        }","def get(self) -> dict:
        """"""Return the metrics in a dictionary.""""""
        return {
            'accumulated_cost': self._accumulated_cost,
            'accumulated_token_usage': self.accumulated_token_usage.model_dump(),
            'costs': [cost.model_dump() for cost in self._costs],
            'response_latencies': [
                latency.model_dump() for latency in self._response_latencies
            ],
            'token_usages': [usage.model_dump() for usage in self._token_usages],
        }",Return the metrics in a dictionary.,Return the metrics in a dictionary.,"def get(self) -> dict:
        
        return {
            'accumulated_cost': self._accumulated_cost,
            'accumulated_token_usage': self.accumulated_token_usage.model_dump(),
            'costs': [cost.model_dump() for cost in self._costs],
            'response_latencies': [
                latency.model_dump() for latency in self._response_latencies
            ],
            'token_usages': [usage.model_dump() for usage in self._token_usages],
        }",Return the metrics in a dictionary.,"def get ( self ) -> dict : return { 'accumulated_cost' : self . _accumulated_cost , 'accumulated_token_usage' : self . accumulated_token_usage . model_dump ( ) , 'costs' : [ cost . model_dump ( ) for cost in self . _costs ] , 'response_latencies' : [ latency . model_dump ( ) for latency in self . _response_latencies ] , 'token_usages' : [ usage . model_dump ( ) for usage in self . _token_usages ] , }",Return the metrics in a dictionary.
/streamlit/scripts/update_e2e_snapshots.py,download_artifact,"def download_artifact(artifact_url: str, token: str, download_path: str) -> None:
    """"""Download an artifact from a given URL.""""""
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""token {token}"",
    }
    response = requests.get(artifact_url, headers=headers, stream=True)
    if response.status_code != 200:
        raise Exception(
            f""Error downloading artifact: {response.status_code} {response.text}""
        )

    with open(download_path, ""wb"") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)","def download_artifact(artifact_url: str, token: str, download_path: str) -> None:
    """"""Download an artifact from a given URL.""""""
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""token {token}"",
    }
    response = requests.get(artifact_url, headers=headers, stream=True)
    if response.status_code != 200:
        raise Exception(
            f""Error downloading artifact: {response.status_code} {response.text}""
        )

    with open(download_path, ""wb"") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)",Download an artifact from a given URL.,Download an artifact from a given URL.,"def download_artifact(artifact_url: str, token: str, download_path: str) -> None:
    
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""token {token}"",
    }
    response = requests.get(artifact_url, headers=headers, stream=True)
    if response.status_code != 200:
        raise Exception(
            f""Error downloading artifact: {response.status_code} {response.text}""
        )

    with open(download_path, ""wb"") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)",Download an artifact from a given URL.,"def download_artifact ( artifact_url : str , token : str , download_path : str ) -> None : headers = { ""Accept"" : ""application/vnd.github.v3+json"" , ""Authorization"" : f""token {token}"" , } response = requests . get ( artifact_url , headers = headers , stream = True ) if response . status_code != 200 : raise Exception ( f""Error downloading artifact: {response.status_code} {response.text}"" ) with open ( download_path , ""wb"" ) as f : for chunk in response . iter_content ( chunk_size = 8192 ) : f . write ( chunk )",Download an artifact from a given URL.
/core/script/scaffold/generate.py,_ensure_tests_dir_exists,"def _ensure_tests_dir_exists(info: Info) -> None:
    """"""Ensure a test dir exists.""""""
    if info.tests_dir.exists():
        return

    info.tests_dir.mkdir()
    print(f""Writing {info.tests_dir / '__init__.py'}"")
    (info.tests_dir / ""__init__.py"").write_text(
        f'""""""Tests for the {info.name} integration.""""""\n'
    )","def _ensure_tests_dir_exists(info: Info) -> None:
    """"""Ensure a test dir exists.""""""
    if info.tests_dir.exists():
        return

    info.tests_dir.mkdir()
    print(f""Writing {info.tests_dir / '__init__.py'}"")
    (info.tests_dir / ""__init__.py"").write_text(
        f'""""""Tests for the {info.name} integration.""""""\n'
    )",Ensure a test dir exists.,Ensure a test dir exists.,"def _ensure_tests_dir_exists(info: Info) -> None:
    
    if info.tests_dir.exists():
        return

    info.tests_dir.mkdir()
    print(f""Writing {info.tests_dir / '__init__.py'}"")
    (info.tests_dir / ""__init__.py"").write_text(
        f'\n'
    )",Ensure a test dir exists.,"def _ensure_tests_dir_exists ( info : Info ) -> None : if info . tests_dir . exists ( ) : return info . tests_dir . mkdir ( ) print ( f""Writing {info.tests_dir / '__init__.py'}"" ) ( info . tests_dir / ""__init__.py"" ) . write_text ( f'\n' )",Ensure a test dir exists.
/annotated_deep_learning_paper_implementations/labml_nn/transformers/mha.py,get_scores,"def get_scores(self, query: torch.Tensor, key: torch.Tensor):
        """"""
        ### Calculate scores between queries and keys

        This method can be overridden for other variations like relative attention.
        """"""

        # Calculate $Q K^\top$ or $S_{ijbh} = \sum_d Q_{ibhd} K_{jbhd}$
        return torch.einsum('ibhd,jbhd->ijbh', query, key)","def get_scores(self, query: torch.Tensor, key: torch.Tensor):
        """"""
        ### Calculate scores between queries and keys

        This method can be overridden for other variations like relative attention.
        """"""

        # Calculate $Q K^\top$ or $S_{ijbh} = \sum_d Q_{ibhd} K_{jbhd}$
        return torch.einsum('ibhd,jbhd->ijbh', query, key)","### Calculate scores between queries and keys

This method can be overridden for other variations like relative attention.",### Calculate scores between queries and keys,"def get_scores(self, query: torch.Tensor, key: torch.Tensor):
        

        # Calculate $Q K^\top$ or $S_{ijbh} = \sum_d Q_{ibhd} K_{jbhd}$
        return torch.einsum('ibhd,jbhd->ijbh', query, key)",### Calculate scores between queries and keys,"def get_scores ( self , query : torch . Tensor , key : torch . Tensor ) : # Calculate $Q K^\top$ or $S_{ijbh} = \sum_d Q_{ibhd} K_{jbhd}$ return torch . einsum ( 'ibhd,jbhd->ijbh' , query , key )",### Calculate scores between queries and keys
/OpenManus/app/tool/create_chat_completion.py,_get_type_info,"def _get_type_info(self, type_hint: Type) -> dict:
        """"""Get type information for a single type.""""""
        if isinstance(type_hint, type) and issubclass(type_hint, BaseModel):
            return type_hint.model_json_schema()

        return {
            ""type"": self.type_mapping.get(type_hint, ""string""),
            ""description"": f""Value of type {getattr(type_hint, '__name__', 'any')}"",
        }","def _get_type_info(self, type_hint: Type) -> dict:
        """"""Get type information for a single type.""""""
        if isinstance(type_hint, type) and issubclass(type_hint, BaseModel):
            return type_hint.model_json_schema()

        return {
            ""type"": self.type_mapping.get(type_hint, ""string""),
            ""description"": f""Value of type {getattr(type_hint, '__name__', 'any')}"",
        }",Get type information for a single type.,Get type information for a single type.,"def _get_type_info(self, type_hint: Type) -> dict:
        
        if isinstance(type_hint, type) and issubclass(type_hint, BaseModel):
            return type_hint.model_json_schema()

        return {
            ""type"": self.type_mapping.get(type_hint, ""string""),
            ""description"": f""Value of type {getattr(type_hint, '__name__', 'any')}"",
        }",Get type information for a single type.,"def _get_type_info ( self , type_hint : Type ) -> dict : if isinstance ( type_hint , type ) and issubclass ( type_hint , BaseModel ) : return type_hint . model_json_schema ( ) return { ""type"" : self . type_mapping . get ( type_hint , ""string"" ) , ""description"" : f""Value of type {getattr(type_hint, '__name__', 'any')}"" , }",Get type information for a single type.
/faceswap/plugins/train/model/_base/model.py,_load_config,"def _load_config(self) -> None:
        """""" Load the global config for reference in :attr:`config` and set the faceswap blocks
        configuration options in `lib.model.nn_blocks` """"""
        global _CONFIG  # pylint:disable=global-statement
        if not _CONFIG:
            model_name = self._config_section
            logger.debug(""Loading config for: %s"", model_name)
            _CONFIG = Config(model_name, configfile=self._configfile).config_dict

        nn_block_keys = ['icnr_init', 'conv_aware_init', 'reflect_padding']
        set_nnblock_config({key: _CONFIG.pop(key)
                            for key in nn_block_keys})","def _load_config(self) -> None:
        """""" Load the global config for reference in :attr:`config` and set the faceswap blocks
        configuration options in `lib.model.nn_blocks` """"""
        global _CONFIG  # pylint:disable=global-statement
        if not _CONFIG:
            model_name = self._config_section
            logger.debug(""Loading config for: %s"", model_name)
            _CONFIG = Config(model_name, configfile=self._configfile).config_dict

        nn_block_keys = ['icnr_init', 'conv_aware_init', 'reflect_padding']
        set_nnblock_config({key: _CONFIG.pop(key)
                            for key in nn_block_keys})","Load the global config for reference in :attr:`config` and set the faceswap blocks
configuration options in `lib.model.nn_blocks`",Load the global config for reference in :attr:`config` and set the faceswap blocks configuration options in `lib.model.nn_blocks`,"def _load_config(self) -> None:
        
        global _CONFIG  # pylint:disable=global-statement
        if not _CONFIG:
            model_name = self._config_section
            logger.debug(""Loading config for: %s"", model_name)
            _CONFIG = Config(model_name, configfile=self._configfile).config_dict

        nn_block_keys = ['icnr_init', 'conv_aware_init', 'reflect_padding']
        set_nnblock_config({key: _CONFIG.pop(key)
                            for key in nn_block_keys})",Load the global config for reference in :attr:`config` and set the faceswap blocks configuration options in `lib.model.nn_blocks`,"def _load_config ( self ) -> None : global _CONFIG # pylint:disable=global-statement if not _CONFIG : model_name = self . _config_section logger . debug ( ""Loading config for: %s"" , model_name ) _CONFIG = Config ( model_name , configfile = self . _configfile ) . config_dict nn_block_keys = [ 'icnr_init' , 'conv_aware_init' , 'reflect_padding' ] set_nnblock_config ( { key : _CONFIG . pop ( key ) for key in nn_block_keys } )",Load the global config for reference in :attr:`config` and set the faceswap blocks configuration options in `lib.model.nn_blocks`
/yolov5/utils/torch_utils.py,__call__,"def __call__(self, epoch, fitness):
        """"""Evaluates if training should stop based on fitness improvement and patience, returning a boolean.""""""
        if fitness >= self.best_fitness:  # >= 0 to allow for early zero-fitness stage of training
            self.best_epoch = epoch
            self.best_fitness = fitness
        delta = epoch - self.best_epoch  # epochs without improvement
        self.possible_stop = delta >= (self.patience - 1)  # possible stop may occur next epoch
        stop = delta >= self.patience  # stop training if patience exceeded
        if stop:
            LOGGER.info(
                f""Stopping training early as no improvement observed in last {self.patience} epochs. ""
                f""Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n""
                f""To update EarlyStopping(patience={self.patience}) pass a new patience value, ""
                f""i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.""
            )
        return stop","def __call__(self, epoch, fitness):
        """"""Evaluates if training should stop based on fitness improvement and patience, returning a boolean.""""""
        if fitness >= self.best_fitness:  # >= 0 to allow for early zero-fitness stage of training
            self.best_epoch = epoch
            self.best_fitness = fitness
        delta = epoch - self.best_epoch  # epochs without improvement
        self.possible_stop = delta >= (self.patience - 1)  # possible stop may occur next epoch
        stop = delta >= self.patience  # stop training if patience exceeded
        if stop:
            LOGGER.info(
                f""Stopping training early as no improvement observed in last {self.patience} epochs. ""
                f""Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n""
                f""To update EarlyStopping(patience={self.patience}) pass a new patience value, ""
                f""i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.""
            )
        return stop","Evaluates if training should stop based on fitness improvement and patience, returning a boolean.","Evaluates if training should stop based on fitness improvement and patience, returning a boolean.","def __call__(self, epoch, fitness):
        
        if fitness >= self.best_fitness:  # >= 0 to allow for early zero-fitness stage of training
            self.best_epoch = epoch
            self.best_fitness = fitness
        delta = epoch - self.best_epoch  # epochs without improvement
        self.possible_stop = delta >= (self.patience - 1)  # possible stop may occur next epoch
        stop = delta >= self.patience  # stop training if patience exceeded
        if stop:
            LOGGER.info(
                f""Stopping training early as no improvement observed in last {self.patience} epochs. ""
                f""Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n""
                f""To update EarlyStopping(patience={self.patience}) pass a new patience value, ""
                f""i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.""
            )
        return stop","Evaluates if training should stop based on fitness improvement and patience, returning a boolean.","def __call__ ( self , epoch , fitness ) : if fitness >= self . best_fitness : # >= 0 to allow for early zero-fitness stage of training self . best_epoch = epoch self . best_fitness = fitness delta = epoch - self . best_epoch # epochs without improvement self . possible_stop = delta >= ( self . patience - 1 ) # possible stop may occur next epoch stop = delta >= self . patience # stop training if patience exceeded if stop : LOGGER . info ( f""Stopping training early as no improvement observed in last {self.patience} epochs. "" f""Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n"" f""To update EarlyStopping(patience={self.patience}) pass a new patience value, "" f""i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping."" ) return stop","Evaluates if training should stop based on fitness improvement and patience, returning a boolean."
/cpython/Tools/build/generate_global_objects.py,open_for_changes,"def open_for_changes(filename, orig):
    """"""Like open() but only write to the file if it changed.""""""
    outfile = io.StringIO()
    yield outfile
    text = outfile.getvalue()
    if text != orig:
        with open(filename, 'w', encoding='utf-8') as outfile:
            outfile.write(text)
    else:
        print(f'# not changed: {filename}')","def open_for_changes(filename, orig):
    """"""Like open() but only write to the file if it changed.""""""
    outfile = io.StringIO()
    yield outfile
    text = outfile.getvalue()
    if text != orig:
        with open(filename, 'w', encoding='utf-8') as outfile:
            outfile.write(text)
    else:
        print(f'# not changed: {filename}')",Like open() but only write to the file if it changed.,Like open() but only write to the file if it changed.,"def open_for_changes(filename, orig):
    
    outfile = io.StringIO()
    yield outfile
    text = outfile.getvalue()
    if text != orig:
        with open(filename, 'w', encoding='utf-8') as outfile:
            outfile.write(text)
    else:
        print(f'# not changed: {filename}')",Like open() but only write to the file if it changed.,"def open_for_changes ( filename , orig ) : outfile = io . StringIO ( ) yield outfile text = outfile . getvalue ( ) if text != orig : with open ( filename , 'w' , encoding = 'utf-8' ) as outfile : outfile . write ( text ) else : print ( f'# not changed: {filename}' )",Like open() but only write to the file if it changed.
/annotated_deep_learning_paper_implementations/labml_nn/transformers/gmlp/__init__.py,__init__,"def __init__(self, d_z: int, seq_len: int):
        """"""
        * `d_z` is the dimensionality of $Z$
        * `seq_len` is the sequence length
        """"""
        super().__init__()
        # Normalization layer before applying $f_{W,b}(\cdot)$
        self.norm = nn.LayerNorm([d_z // 2])
        # Weight $W$ in $f_{W,b}(\cdot)$.
        #
        # The paper notes that it's important to initialize weights to small values and the bias to $1$,
        # so that during the initial training $s(\cdot)$ is close to identity (apart from the split).
        self.weight = nn.Parameter(torch.zeros(seq_len, seq_len).uniform_(-0.01, 0.01), requires_grad=True)
        # Weight $b$ in $f_{W,b}(\cdot)$
        #
        # The paper notes that it's important to initialize bias to $1$.
        self.bias = nn.Parameter(torch.ones(seq_len), requires_grad=True)","def __init__(self, d_z: int, seq_len: int):
        """"""
        * `d_z` is the dimensionality of $Z$
        * `seq_len` is the sequence length
        """"""
        super().__init__()
        # Normalization layer before applying $f_{W,b}(\cdot)$
        # Weight $W$ in $f_{W,b}(\cdot)$.
        #
        # The paper notes that it's important to initialize weights to small values and the bias to $1$,
        # so that during the initial training $s(\cdot)$ is close to identity (apart from the split).
        self.weight = nn.Parameter(torch.zeros(seq_len, seq_len).uniform_(-0.01, 0.01), requires_grad=True)
        # Weight $b$ in $f_{W,b}(\cdot)$
        #
        # The paper notes that it's important to initialize bias to $1$.
        self.bias = nn.Parameter(torch.ones(seq_len), requires_grad=True)","* `d_z` is the dimensionality of $Z$
* `seq_len` is the sequence length",`d_z` is the dimensionality of $Z$ `seq_len` is the sequence length,"def __init__(self, d_z: int, seq_len: int):
        
        super().__init__()
        # Normalization layer before applying $f_{W,b}(\cdot)$
        # Weight $W$ in $f_{W,b}(\cdot)$.
        #
        # The paper notes that it's important to initialize weights to small values and the bias to $1$,
        # so that during the initial training $s(\cdot)$ is close to identity (apart from the split).
        self.weight = nn.Parameter(torch.zeros(seq_len, seq_len).uniform_(-0.01, 0.01), requires_grad=True)
        # Weight $b$ in $f_{W,b}(\cdot)$
        #
        # The paper notes that it's important to initialize bias to $1$.
        self.bias = nn.Parameter(torch.ones(seq_len), requires_grad=True)",`d_z` is the dimensionality of $Z$ `seq_len` is the sequence length,"def __init__ ( self , d_z : int , seq_len : int ) : super ( ) . __init__ ( ) # Normalization layer before applying $f_{W,b}(\cdot)$ # Weight $W$ in $f_{W,b}(\cdot)$. # # The paper notes that it's important to initialize weights to small values and the bias to $1$, # so that during the initial training $s(\cdot)$ is close to identity (apart from the split). self . weight = nn . Parameter ( torch . zeros ( seq_len , seq_len ) . uniform_ ( - 0.01 , 0.01 ) , requires_grad = True ) # Weight $b$ in $f_{W,b}(\cdot)$ # # The paper notes that it's important to initialize bias to $1$. self . bias = nn . Parameter ( torch . ones ( seq_len ) , requires_grad = True )",`d_z` is the dimensionality of $Z$ `seq_len` is the sequence length
/Fooocus/ldm_patched/t2ia/adapter.py,avg_pool_nd,"def avg_pool_nd(dims, *args, **kwargs):
    """"""
    Create a 1D, 2D, or 3D average pooling module.
    """"""
    if dims == 1:
        return nn.AvgPool1d(*args, **kwargs)
    elif dims == 2:
        return nn.AvgPool2d(*args, **kwargs)
    elif dims == 3:
        return nn.AvgPool3d(*args, **kwargs)
    raise ValueError(f""unsupported dimensions: {dims}"")","def avg_pool_nd(dims, *args, **kwargs):
    """"""
    Create a 1D, 2D, or 3D average pooling module.
    """"""
    if dims == 1:
        return nn.AvgPool1d(*args, **kwargs)
    elif dims == 2:
        return nn.AvgPool2d(*args, **kwargs)
    elif dims == 3:
        return nn.AvgPool3d(*args, **kwargs)
    raise ValueError(f""unsupported dimensions: {dims}"")","Create a 1D, 2D, or 3D average pooling module.","Create a 1D, 2D, or 3D average pooling module.","def avg_pool_nd(dims, *args, **kwargs):
    
    if dims == 1:
        return nn.AvgPool1d(*args, **kwargs)
    elif dims == 2:
        return nn.AvgPool2d(*args, **kwargs)
    elif dims == 3:
        return nn.AvgPool3d(*args, **kwargs)
    raise ValueError(f""unsupported dimensions: {dims}"")","Create a 1D, 2D, or 3D average pooling module.","def avg_pool_nd ( dims , * args , ** kwargs ) : if dims == 1 : return nn . AvgPool1d ( * args , ** kwargs ) elif dims == 2 : return nn . AvgPool2d ( * args , ** kwargs ) elif dims == 3 : return nn . AvgPool3d ( * args , ** kwargs ) raise ValueError ( f""unsupported dimensions: {dims}"" )","Create a 1D, 2D, or 3D average pooling module."
/black/src/black/numerics.py,format_hex,"def format_hex(text: str) -> str:
    """"""
    Formats a hexadecimal string like ""0x12B3""
    """"""
    before, after = text[:2], text[2:]
    return f""{before}{after.upper()}""","def format_hex(text: str) -> str:
    """"""
    Formats a hexadecimal string like ""0x12B3""
    """"""
    before, after = text[:2], text[2:]
    return f""{before}{after.upper()}""","Formats a hexadecimal string like ""0x12B3""","Formats a hexadecimal string like ""0x12B3""","def format_hex(text: str) -> str:
    
    before, after = text[:2], text[2:]
    return f""{before}{after.upper()}""","Formats a hexadecimal string like ""0x12B3""","def format_hex ( text : str ) -> str : before , after = text [ : 2 ] , text [ 2 : ] return f""{before}{after.upper()}""","Formats a hexadecimal string like ""0x12B3"""
/devops-exercises/coding/python/merge_sort.py,main,"def main():
    """"""
    Executes the merge sort algorithm with a randomly generated list.
    Time Complexity: O(n log n)
    """"""
    rand_num_li = generate_random_list()
    print(f""Unsorted List: {rand_num_li}"")
    sorted_list = merge_sort(rand_num_li)
    print(f""Sorted List: {sorted_list}"")","def main():
    """"""
    Executes the merge sort algorithm with a randomly generated list.
    Time Complexity: O(n log n)
    """"""
    rand_num_li = generate_random_list()
    print(f""Unsorted List: {rand_num_li}"")
    sorted_list = merge_sort(rand_num_li)
    print(f""Sorted List: {sorted_list}"")","Executes the merge sort algorithm with a randomly generated list.
Time Complexity: O(n log n)",Executes the merge sort algorithm with a randomly generated list.,"def main():
    
    rand_num_li = generate_random_list()
    print(f""Unsorted List: {rand_num_li}"")
    sorted_list = merge_sort(rand_num_li)
    print(f""Sorted List: {sorted_list}"")",Executes the merge sort algorithm with a randomly generated list.,"def main ( ) : rand_num_li = generate_random_list ( ) print ( f""Unsorted List: {rand_num_li}"" ) sorted_list = merge_sort ( rand_num_li ) print ( f""Sorted List: {sorted_list}"" )",Executes the merge sort algorithm with a randomly generated list.
/mitmproxy/examples/contrib/http_manipulate_cookies.py,stringify_cookies,"def stringify_cookies(cookies: list[dict[str, str | None]]) -> str:
    """"""
    Creates a cookie string from a list of cookie dicts.
    """"""
    return ""; "".join(
        [
            f""{c['name']}={c['value']}""
            if c.get(""value"", None) is not None
            else f""{c['name']}""
            for c in cookies
        ]
    )","def stringify_cookies(cookies: list[dict[str, str | None]]) -> str:
    """"""
    Creates a cookie string from a list of cookie dicts.
    """"""
    return ""; "".join(
        [
            f""{c['name']}={c['value']}""
            if c.get(""value"", None) is not None
            else f""{c['name']}""
            for c in cookies
        ]
    )",Creates a cookie string from a list of cookie dicts.,Creates a cookie string from a list of cookie dicts.,"def stringify_cookies(cookies: list[dict[str, str | None]]) -> str:
    
    return ""; "".join(
        [
            f""{c['name']}={c['value']}""
            if c.get(""value"", None) is not None
            else f""{c['name']}""
            for c in cookies
        ]
    )",Creates a cookie string from a list of cookie dicts.,"def stringify_cookies ( cookies : list [ dict [ str , str | None ] ] ) -> str : return ""; "" . join ( [ f""{c['name']}={c['value']}"" if c . get ( ""value"" , None ) is not None else f""{c['name']}"" for c in cookies ] )",Creates a cookie string from a list of cookie dicts.
/markitdown/packages/markitdown/src/markitdown/_stream_info.py,copy_and_update,"def copy_and_update(self, *args, **kwargs):
        """"""Copy the StreamInfo object and update it with the given StreamInfo
        instance and/or other keyword arguments.""""""
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if len(kwargs) > 0:
            new_info.update(kwargs)

        return StreamInfo(**new_info)","def copy_and_update(self, *args, **kwargs):
        """"""Copy the StreamInfo object and update it with the given StreamInfo
        instance and/or other keyword arguments.""""""
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if len(kwargs) > 0:
            new_info.update(kwargs)

        return StreamInfo(**new_info)","Copy the StreamInfo object and update it with the given StreamInfo
instance and/or other keyword arguments.",Copy the StreamInfo object and update it with the given StreamInfo instance and/or other keyword arguments.,"def copy_and_update(self, *args, **kwargs):
        
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if len(kwargs) > 0:
            new_info.update(kwargs)

        return StreamInfo(**new_info)",Copy the StreamInfo object and update it with the given StreamInfo instance and/or other keyword arguments.,"def copy_and_update ( self , * args , ** kwargs ) : new_info = asdict ( self ) for si in args : assert isinstance ( si , StreamInfo ) new_info . update ( { k : v for k , v in asdict ( si ) . items ( ) if v is not None } ) if len ( kwargs ) > 0 : new_info . update ( kwargs ) return StreamInfo ( ** new_info )",Copy the StreamInfo object and update it with the given StreamInfo instance and/or other keyword arguments.
/MetaGPT/metagpt/roles/role.py,_set_state,"def _set_state(self, state: int):
        """"""Update the current state.""""""
        self.rc.state = state
        logger.debug(f""actions={self.actions}, state={state}"")
        self.set_todo(self.actions[self.rc.state] if state >= 0 else None)","def _set_state(self, state: int):
        """"""Update the current state.""""""
        self.rc.state = state
        logger.debug(f""actions={self.actions}, state={state}"")
        self.set_todo(self.actions[self.rc.state] if state >= 0 else None)",Update the current state.,Update the current state.,"def _set_state(self, state: int):
        
        self.rc.state = state
        logger.debug(f""actions={self.actions}, state={state}"")
        self.set_todo(self.actions[self.rc.state] if state >= 0 else None)",Update the current state.,"def _set_state ( self , state : int ) : self . rc . state = state logger . debug ( f""actions={self.actions}, state={state}"" ) self . set_todo ( self . actions [ self . rc . state ] if state >= 0 else None )",Update the current state.
/yolov5/utils/dataloaders.py,img2label_paths,"def img2label_paths(img_paths):
    """"""Generates label file paths from corresponding image file paths by replacing `/images/` with `/labels/` and
    extension with `.txt`.
    """"""
    sa, sb = f""{os.sep}images{os.sep}"", f""{os.sep}labels{os.sep}""  # /images/, /labels/ substrings
    return [sb.join(x.rsplit(sa, 1)).rsplit(""."", 1)[0] + "".txt"" for x in img_paths]","def img2label_paths(img_paths):
    """"""Generates label file paths from corresponding image file paths by replacing `/images/` with `/labels/` and
    extension with `.txt`.
    """"""
    sa, sb = f""{os.sep}images{os.sep}"", f""{os.sep}labels{os.sep}""  # /images/, /labels/ substrings
    return [sb.join(x.rsplit(sa, 1)).rsplit(""."", 1)[0] + "".txt"" for x in img_paths]","Generates label file paths from corresponding image file paths by replacing `/images/` with `/labels/` and
extension with `.txt`.",Generates label file paths from corresponding image file paths by replacing `/images/` with `/labels/` and extension with `.txt`.,"def img2label_paths(img_paths):
    
    sa, sb = f""{os.sep}images{os.sep}"", f""{os.sep}labels{os.sep}""  # /images/, /labels/ substrings
    return [sb.join(x.rsplit(sa, 1)).rsplit(""."", 1)[0] + "".txt"" for x in img_paths]",Generates label file paths from corresponding image file paths by replacing `/images/` with `/labels/` and extension with `.txt`.,"def img2label_paths ( img_paths ) : sa , sb = f""{os.sep}images{os.sep}"" , f""{os.sep}labels{os.sep}"" # /images/, /labels/ substrings return [ sb . join ( x . rsplit ( sa , 1 ) ) . rsplit ( ""."" , 1 ) [ 0 ] + "".txt"" for x in img_paths ]",Generates label file paths from corresponding image file paths by replacing `/images/` with `/labels/` and extension with `.txt`.
/ultralytics/ultralytics/data/utils.py,exif_size,"def exif_size(img: Image.Image) -> Tuple[int, int]:
    """"""Return exif-corrected PIL size.""""""
    s = img.size  # (width, height)
    if img.format == ""JPEG"":  # only support JPEG images
        try:
            if exif := img.getexif():
                rotation = exif.get(274, None)  # the EXIF key for the orientation tag is 274
                if rotation in {6, 8}:  # rotation 270 or 90
                    s = s[1], s[0]
        except Exception:
            pass
    return s","def exif_size(img: Image.Image) -> Tuple[int, int]:
    """"""Return exif-corrected PIL size.""""""
    s = img.size  # (width, height)
    if img.format == ""JPEG"":  # only support JPEG images
        try:
            if exif := img.getexif():
                rotation = exif.get(274, None)  # the EXIF key for the orientation tag is 274
                if rotation in {6, 8}:  # rotation 270 or 90
                    s = s[1], s[0]
        except Exception:
            pass
    return s",Return exif-corrected PIL size.,Return exif-corrected PIL size.,"def exif_size(img: Image.Image) -> Tuple[int, int]:
    
    s = img.size  # (width, height)
    if img.format == ""JPEG"":  # only support JPEG images
        try:
            if exif := img.getexif():
                rotation = exif.get(274, None)  # the EXIF key for the orientation tag is 274
                if rotation in {6, 8}:  # rotation 270 or 90
                    s = s[1], s[0]
        except Exception:
            pass
    return s",Return exif-corrected PIL size.,"def exif_size ( img : Image . Image ) -> Tuple [ int , int ] : s = img . size # (width, height) if img . format == ""JPEG"" : # only support JPEG images try : if exif := img . getexif ( ) : rotation = exif . get ( 274 , None ) # the EXIF key for the orientation tag is 274 if rotation in { 6 , 8 } : # rotation 270 or 90 s = s [ 1 ] , s [ 0 ] except Exception : pass return s",Return exif-corrected PIL size.
/odoo/odoo/models.py,union,"def union(self, *args) -> Self:
        """""" Return the union of ``self`` with all the arguments (in linear time
            complexity, with first occurrence order preserved).
        """"""
        ids = list(self._ids)
        for arg in args:
            try:
                if arg._name != self._name:
                    raise TypeError(f""inconsistent models in: {self} | {arg}"")
                ids.extend(arg._ids)
            except AttributeError:
                raise TypeError(f""unsupported operand types in: {self} | {arg!r}"")
        return self.browse(OrderedSet(ids))","def union(self, *args) -> Self:
        """""" Return the union of ``self`` with all the arguments (in linear time
            complexity, with first occurrence order preserved).
        """"""
        ids = list(self._ids)
        for arg in args:
            try:
                if arg._name != self._name:
                    raise TypeError(f""inconsistent models in: {self} | {arg}"")
                ids.extend(arg._ids)
            except AttributeError:
                raise TypeError(f""unsupported operand types in: {self} | {arg!r}"")
        return self.browse(OrderedSet(ids))","Return the union of ``self`` with all the arguments (in linear time
complexity, with first occurrence order preserved).","Return the union of ``self`` with all the arguments (in linear time complexity, with first occurrence order preserved).","def union(self, *args) -> Self:
        
        ids = list(self._ids)
        for arg in args:
            try:
                if arg._name != self._name:
                    raise TypeError(f""inconsistent models in: {self} | {arg}"")
                ids.extend(arg._ids)
            except AttributeError:
                raise TypeError(f""unsupported operand types in: {self} | {arg!r}"")
        return self.browse(OrderedSet(ids))","Return the union of ``self`` with all the arguments (in linear time complexity, with first occurrence order preserved).","def union ( self , * args ) -> Self : ids = list ( self . _ids ) for arg in args : try : if arg . _name != self . _name : raise TypeError ( f""inconsistent models in: {self} | {arg}"" ) ids . extend ( arg . _ids ) except AttributeError : raise TypeError ( f""unsupported operand types in: {self} | {arg!r}"" ) return self . browse ( OrderedSet ( ids ) )","Return the union of ``self`` with all the arguments (in linear time complexity, with first occurrence order preserved)."
/core/script/gen_requirements_all.py,generate_requirements_list,"def generate_requirements_list(reqs: dict[str, list[str]]) -> str:
    """"""Generate a pip file based on requirements.""""""
    output = []
    for pkg, requirements in sorted(reqs.items(), key=itemgetter(0)):
        output.extend(f""\n# {req}"" for req in sorted(requirements))

        if comment_requirement(pkg):
            output.append(f""\n# {pkg}\n"")
        else:
            output.append(f""\n{pkg}\n"")
    return """".join(output)","def generate_requirements_list(reqs: dict[str, list[str]]) -> str:
    """"""Generate a pip file based on requirements.""""""
    output = []
    for pkg, requirements in sorted(reqs.items(), key=itemgetter(0)):
        output.extend(f""\n# {req}"" for req in sorted(requirements))

        if comment_requirement(pkg):
            output.append(f""\n# {pkg}\n"")
        else:
            output.append(f""\n{pkg}\n"")
    return """".join(output)",Generate a pip file based on requirements.,Generate a pip file based on requirements.,"def generate_requirements_list(reqs: dict[str, list[str]]) -> str:
    
    output = []
    for pkg, requirements in sorted(reqs.items(), key=itemgetter(0)):
        output.extend(f""\n# {req}"" for req in sorted(requirements))

        if comment_requirement(pkg):
            output.append(f""\n# {pkg}\n"")
        else:
            output.append(f""\n{pkg}\n"")
    return """".join(output)",Generate a pip file based on requirements.,"def generate_requirements_list ( reqs : dict [ str , list [ str ] ] ) -> str : output = [ ] for pkg , requirements in sorted ( reqs . items ( ) , key = itemgetter ( 0 ) ) : output . extend ( f""\n# {req}"" for req in sorted ( requirements ) ) if comment_requirement ( pkg ) : output . append ( f""\n# {pkg}\n"" ) else : output . append ( f""\n{pkg}\n"" ) return """" . join ( output )",Generate a pip file based on requirements.
/ultralytics/docs/build_docs.py,remove_macros,"def remove_macros():
    """"""Remove the /macros directory and related entries in sitemap.xml from the built site.""""""
    shutil.rmtree(SITE / ""macros"", ignore_errors=True)
    (SITE / ""sitemap.xml.gz"").unlink(missing_ok=True)

    # Process sitemap.xml
    sitemap = SITE / ""sitemap.xml""
    lines = sitemap.read_text(encoding=""utf-8"").splitlines(keepends=True)

    # Find indices of '/macros/' lines
    macros_indices = [i for i, line in enumerate(lines) if ""/macros/"" in line]

    # Create a set of indices to remove (including lines before and after)
    indices_to_remove = set()
    for i in macros_indices:
        indices_to_remove.update(range(i - 1, i + 3))  # i-1, i, i+1, i+2, i+3

    # Create new list of lines, excluding the ones to remove
    new_lines = [line for i, line in enumerate(lines) if i not in indices_to_remove]

    # Write the cleaned content back to the file
    sitemap.write_text("""".join(new_lines), encoding=""utf-8"")

    print(f""Removed {len(macros_indices)} URLs containing '/macros/' from {sitemap}"")","def remove_macros():
    """"""Remove the /macros directory and related entries in sitemap.xml from the built site.""""""
    shutil.rmtree(SITE / ""macros"", ignore_errors=True)
    (SITE / ""sitemap.xml.gz"").unlink(missing_ok=True)

    # Process sitemap.xml
    sitemap = SITE / ""sitemap.xml""
    lines = sitemap.read_text(encoding=""utf-8"").splitlines(keepends=True)

    # Find indices of '/macros/' lines
    macros_indices = [i for i, line in enumerate(lines) if ""/macros/"" in line]

    # Create a set of indices to remove (including lines before and after)
    indices_to_remove = set()
    for i in macros_indices:
        indices_to_remove.update(range(i - 1, i + 3))  # i-1, i, i+1, i+2, i+3

    # Create new list of lines, excluding the ones to remove
    new_lines = [line for i, line in enumerate(lines) if i not in indices_to_remove]

    # Write the cleaned content back to the file
    sitemap.write_text("""".join(new_lines), encoding=""utf-8"")

    print(f""Removed {len(macros_indices)} URLs containing '/macros/' from {sitemap}"")",Remove the /macros directory and related entries in sitemap.xml from the built site.,Remove the /macros directory and related entries in sitemap.xml from the built site.,"def remove_macros():
    
    shutil.rmtree(SITE / ""macros"", ignore_errors=True)
    (SITE / ""sitemap.xml.gz"").unlink(missing_ok=True)

    # Process sitemap.xml
    sitemap = SITE / ""sitemap.xml""
    lines = sitemap.read_text(encoding=""utf-8"").splitlines(keepends=True)

    # Find indices of '/macros/' lines
    macros_indices = [i for i, line in enumerate(lines) if ""/macros/"" in line]

    # Create a set of indices to remove (including lines before and after)
    indices_to_remove = set()
    for i in macros_indices:
        indices_to_remove.update(range(i - 1, i + 3))  # i-1, i, i+1, i+2, i+3

    # Create new list of lines, excluding the ones to remove
    new_lines = [line for i, line in enumerate(lines) if i not in indices_to_remove]

    # Write the cleaned content back to the file
    sitemap.write_text("""".join(new_lines), encoding=""utf-8"")

    print(f""Removed {len(macros_indices)} URLs containing '/macros/' from {sitemap}"")",Remove the /macros directory and related entries in sitemap.xml from the built site.,"def remove_macros ( ) : shutil . rmtree ( SITE / ""macros"" , ignore_errors = True ) ( SITE / ""sitemap.xml.gz"" ) . unlink ( missing_ok = True ) # Process sitemap.xml sitemap = SITE / ""sitemap.xml"" lines = sitemap . read_text ( encoding = ""utf-8"" ) . splitlines ( keepends = True ) # Find indices of '/macros/' lines macros_indices = [ i for i , line in enumerate ( lines ) if ""/macros/"" in line ] # Create a set of indices to remove (including lines before and after) indices_to_remove = set ( ) for i in macros_indices : indices_to_remove . update ( range ( i - 1 , i + 3 ) ) # i-1, i, i+1, i+2, i+3 # Create new list of lines, excluding the ones to remove new_lines = [ line for i , line in enumerate ( lines ) if i not in indices_to_remove ] # Write the cleaned content back to the file sitemap . write_text ( """" . join ( new_lines ) , encoding = ""utf-8"" ) print ( f""Removed {len(macros_indices)} URLs containing '/macros/' from {sitemap}"" )",Remove the /macros directory and related entries in sitemap.xml from the built site.
/cpython/Tools/wasm/wasi/__main__.py,make_wasi_python,"def make_wasi_python(context, working_dir):
    """"""Run `make` for the WASI/host build.""""""
    call([""make"", ""--jobs"", str(cpu_count()), ""all""],
             env=updated_env(),
             quiet=context.quiet)

    exec_script = working_dir / ""python.sh""
    call([exec_script, ""--version""], quiet=False)
    print(
        f""🎉 Use `{exec_script.relative_to(context.init_dir)}` ""
        ""to run CPython w/ the WASI host specified by --host-runner""
    )","def make_wasi_python(context, working_dir):
    """"""Run `make` for the WASI/host build.""""""
    call([""make"", ""--jobs"", str(cpu_count()), ""all""],
             env=updated_env(),
             quiet=context.quiet)

    exec_script = working_dir / ""python.sh""
    call([exec_script, ""--version""], quiet=False)
    print(
        f""🎉 Use `{exec_script.relative_to(context.init_dir)}` ""
        ""to run CPython w/ the WASI host specified by --host-runner""
    )",Run `make` for the WASI/host build.,Run `make` for the WASI/host build.,"def make_wasi_python(context, working_dir):
    
    call([""make"", ""--jobs"", str(cpu_count()), ""all""],
             env=updated_env(),
             quiet=context.quiet)

    exec_script = working_dir / ""python.sh""
    call([exec_script, ""--version""], quiet=False)
    print(
        f""🎉 Use `{exec_script.relative_to(context.init_dir)}` ""
        ""to run CPython w/ the WASI host specified by --host-runner""
    )",Run `make` for the WASI/host build.,"def make_wasi_python ( context , working_dir ) : call ( [ ""make"" , ""--jobs"" , str ( cpu_count ( ) ) , ""all"" ] , env = updated_env ( ) , quiet = context . quiet ) exec_script = working_dir / ""python.sh"" call ( [ exec_script , ""--version"" ] , quiet = False ) print ( f""🎉 Use `{exec_script.relative_to(context.init_dir)}` "" ""to run CPython w/ the WASI host specified by --host-runner"" )",Run `make` for the WASI/host build.
/pandas/pandas/util/_decorators.py,future_version_msg,"def future_version_msg(version: str | None) -> str:
    """"""Specify which version of pandas the deprecation will take place in.""""""
    if version is None:
        return ""In a future version of pandas""
    else:
        return f""Starting with pandas version {version}""","def future_version_msg(version: str | None) -> str:
    """"""Specify which version of pandas the deprecation will take place in.""""""
    if version is None:
        return ""In a future version of pandas""
    else:
        return f""Starting with pandas version {version}""",Specify which version of pandas the deprecation will take place in.,Specify which version of pandas the deprecation will take place in.,"def future_version_msg(version: str | None) -> str:
    
    if version is None:
        return ""In a future version of pandas""
    else:
        return f""Starting with pandas version {version}""",Specify which version of pandas the deprecation will take place in.,"def future_version_msg ( version : str | None ) -> str : if version is None : return ""In a future version of pandas"" else : return f""Starting with pandas version {version}""",Specify which version of pandas the deprecation will take place in.
/mitmproxy/examples/contrib/http_manipulate_cookies.py,parse_cookies,"def parse_cookies(cookie_string: str) -> list[dict[str, str | None]]:
    """"""
    Parses a cookie string into a list of cookie dicts.
    """"""
    return [
        {""name"": g[0], ""value"": g[1]} if len(g) == 2 else {""name"": g[0], ""value"": None}
        for g in [
            k.split(""="", 1) for k in [c.strip() for c in cookie_string.split("";"")] if k
        ]
    ]","def parse_cookies(cookie_string: str) -> list[dict[str, str | None]]:
    """"""
    Parses a cookie string into a list of cookie dicts.
    """"""
    return [
        {""name"": g[0], ""value"": g[1]} if len(g) == 2 else {""name"": g[0], ""value"": None}
        for g in [
            k.split(""="", 1) for k in [c.strip() for c in cookie_string.split("";"")] if k
        ]
    ]",Parses a cookie string into a list of cookie dicts.,Parses a cookie string into a list of cookie dicts.,"def parse_cookies(cookie_string: str) -> list[dict[str, str | None]]:
    
    return [
        {""name"": g[0], ""value"": g[1]} if len(g) == 2 else {""name"": g[0], ""value"": None}
        for g in [
            k.split(""="", 1) for k in [c.strip() for c in cookie_string.split("";"")] if k
        ]
    ]",Parses a cookie string into a list of cookie dicts.,"def parse_cookies ( cookie_string : str ) -> list [ dict [ str , str | None ] ] : return [ { ""name"" : g [ 0 ] , ""value"" : g [ 1 ] } if len ( g ) == 2 else { ""name"" : g [ 0 ] , ""value"" : None } for g in [ k . split ( ""="" , 1 ) for k in [ c . strip ( ) for c in cookie_string . split ( "";"" ) ] if k ] ]",Parses a cookie string into a list of cookie dicts.
/cpython/Tools/build/parse_html5_entities.py,compare_dicts,"def compare_dicts(old, new):
    """"""Compare the old and new dicts and print the differences.""""""
    added = new.keys() - old.keys()
    if added:
        print(f'{len(added)} entitie(s) have been added:')
        for name in sorted(added):
            print(f'  {name!r}: {new[name]!r}')
    removed = old.keys() - new.keys()
    if removed:
        print(f'{len(removed)} entitie(s) have been removed:')
        for name in sorted(removed):
            print(f'  {name!r}: {old[name]!r}')
    changed = set()
    for name in (old.keys() & new.keys()):
        if old[name] != new[name]:
            changed.add((name, old[name], new[name]))
    if changed:
        print(f'{len(changed)} entitie(s) have been modified:')
        for item in sorted(changed):
            print('  {!r}: {!r} -> {!r}'.format(*item))","def compare_dicts(old, new):
    """"""Compare the old and new dicts and print the differences.""""""
    added = new.keys() - old.keys()
    if added:
        print(f'{len(added)} entitie(s) have been added:')
        for name in sorted(added):
            print(f'  {name!r}: {new[name]!r}')
    removed = old.keys() - new.keys()
    if removed:
        print(f'{len(removed)} entitie(s) have been removed:')
        for name in sorted(removed):
            print(f'  {name!r}: {old[name]!r}')
    changed = set()
    for name in (old.keys() & new.keys()):
        if old[name] != new[name]:
            changed.add((name, old[name], new[name]))
    if changed:
        print(f'{len(changed)} entitie(s) have been modified:')
        for item in sorted(changed):
            print('  {!r}: {!r} -> {!r}'.format(*item))",Compare the old and new dicts and print the differences.,Compare the old and new dicts and print the differences.,"def compare_dicts(old, new):
    
    added = new.keys() - old.keys()
    if added:
        print(f'{len(added)} entitie(s) have been added:')
        for name in sorted(added):
            print(f'  {name!r}: {new[name]!r}')
    removed = old.keys() - new.keys()
    if removed:
        print(f'{len(removed)} entitie(s) have been removed:')
        for name in sorted(removed):
            print(f'  {name!r}: {old[name]!r}')
    changed = set()
    for name in (old.keys() & new.keys()):
        if old[name] != new[name]:
            changed.add((name, old[name], new[name]))
    if changed:
        print(f'{len(changed)} entitie(s) have been modified:')
        for item in sorted(changed):
            print('  {!r}: {!r} -> {!r}'.format(*item))",Compare the old and new dicts and print the differences.,"def compare_dicts ( old , new ) : added = new . keys ( ) - old . keys ( ) if added : print ( f'{len(added)} entitie(s) have been added:' ) for name in sorted ( added ) : print ( f'  {name!r}: {new[name]!r}' ) removed = old . keys ( ) - new . keys ( ) if removed : print ( f'{len(removed)} entitie(s) have been removed:' ) for name in sorted ( removed ) : print ( f'  {name!r}: {old[name]!r}' ) changed = set ( ) for name in ( old . keys ( ) & new . keys ( ) ) : if old [ name ] != new [ name ] : changed . add ( ( name , old [ name ] , new [ name ] ) ) if changed : print ( f'{len(changed)} entitie(s) have been modified:' ) for item in sorted ( changed ) : print ( '  {!r}: {!r} -> {!r}' . format ( * item ) )",Compare the old and new dicts and print the differences.
/python-patterns/patterns/behavioral/state.py,scan,"def scan(self) -> None:
        """"""Scan the dial to the next station""""""
        self.pos += 1
        if self.pos == len(self.stations):
            self.pos = 0
        print(f""Scanning... Station is {self.stations[self.pos]} {self.name}"")","def scan(self) -> None:
        """"""Scan the dial to the next station""""""
        self.pos += 1
        if self.pos == len(self.stations):
            self.pos = 0
        print(f""Scanning... Station is {self.stations[self.pos]} {self.name}"")",Scan the dial to the next station,Scan the dial to the next station,"def scan(self) -> None:
        
        self.pos += 1
        if self.pos == len(self.stations):
            self.pos = 0
        print(f""Scanning... Station is {self.stations[self.pos]} {self.name}"")",Scan the dial to the next station,"def scan ( self ) -> None : self . pos += 1 if self . pos == len ( self . stations ) : self . pos = 0 print ( f""Scanning... Station is {self.stations[self.pos]} {self.name}"" )",Scan the dial to the next station
/OpenBB/openbb_platform/extensions/economy/integration/test_economy_api.py,headers,"def headers():
    """"""Get the headers for the API request.""""""
    userpass = f""{Env().API_USERNAME}:{Env().API_PASSWORD}""
    userpass_bytes = userpass.encode(""ascii"")
    base64_bytes = base64.b64encode(userpass_bytes)

    return {""Authorization"": f""Basic {base64_bytes.decode('ascii')}""}","def headers():
    """"""Get the headers for the API request.""""""
    userpass = f""{Env().API_USERNAME}:{Env().API_PASSWORD}""
    userpass_bytes = userpass.encode(""ascii"")
    base64_bytes = base64.b64encode(userpass_bytes)

    return {""Authorization"": f""Basic {base64_bytes.decode('ascii')}""}",Get the headers for the API request.,Get the headers for the API request.,"def headers():
    
    userpass = f""{Env().API_USERNAME}:{Env().API_PASSWORD}""
    userpass_bytes = userpass.encode(""ascii"")
    base64_bytes = base64.b64encode(userpass_bytes)

    return {""Authorization"": f""Basic {base64_bytes.decode('ascii')}""}",Get the headers for the API request.,"def headers ( ) : userpass = f""{Env().API_USERNAME}:{Env().API_PASSWORD}"" userpass_bytes = userpass . encode ( ""ascii"" ) base64_bytes = base64 . b64encode ( userpass_bytes ) return { ""Authorization"" : f""Basic {base64_bytes.decode('ascii')}"" }",Get the headers for the API request.
/Fooocus/ldm_patched/k_diffusion/sampling.py,sample_euler,"def sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):
    """"""Implements Algorithm 2 (Euler steps) from Karras et al. (2022).""""""
    extra_args = {} if extra_args is None else extra_args
    s_in = x.new_ones([x.shape[0]])
    for i in trange(len(sigmas) - 1, disable=disable):
        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.
        sigma_hat = sigmas[i] * (gamma + 1)
        if gamma > 0:
            eps = torch.randn_like(x) * s_noise
            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5
        denoised = model(x, sigma_hat * s_in, **extra_args)
        d = to_d(x, sigma_hat, denoised)
        if callback is not None:
            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})
        dt = sigmas[i + 1] - sigma_hat
        # Euler method
        x = x + d * dt
    return x","def sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):
    """"""Implements Algorithm 2 (Euler steps) from Karras et al. (2022).""""""
    extra_args = {} if extra_args is None else extra_args
    s_in = x.new_ones([x.shape[0]])
    for i in trange(len(sigmas) - 1, disable=disable):
        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.
        sigma_hat = sigmas[i] * (gamma + 1)
        if gamma > 0:
            eps = torch.randn_like(x) * s_noise
            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5
        denoised = model(x, sigma_hat * s_in, **extra_args)
        d = to_d(x, sigma_hat, denoised)
        if callback is not None:
            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})
        dt = sigmas[i + 1] - sigma_hat
        # Euler method
        x = x + d * dt
    return x",Implements Algorithm 2 (Euler steps) from Karras et al. (2022).,Implements Algorithm 2 (Euler steps) from Karras et al.,"def sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):
    
    extra_args = {} if extra_args is None else extra_args
    s_in = x.new_ones([x.shape[0]])
    for i in trange(len(sigmas) - 1, disable=disable):
        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.
        sigma_hat = sigmas[i] * (gamma + 1)
        if gamma > 0:
            eps = torch.randn_like(x) * s_noise
            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5
        denoised = model(x, sigma_hat * s_in, **extra_args)
        d = to_d(x, sigma_hat, denoised)
        if callback is not None:
            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})
        dt = sigmas[i + 1] - sigma_hat
        # Euler method
        x = x + d * dt
    return x",Implements Algorithm 2 (Euler steps) from Karras et al.,"def sample_euler ( model , x , sigmas , extra_args = None , callback = None , disable = None , s_churn = 0. , s_tmin = 0. , s_tmax = float ( 'inf' ) , s_noise = 1. ) : extra_args = { } if extra_args is None else extra_args s_in = x . new_ones ( [ x . shape [ 0 ] ] ) for i in trange ( len ( sigmas ) - 1 , disable = disable ) : gamma = min ( s_churn / ( len ( sigmas ) - 1 ) , 2 ** 0.5 - 1 ) if s_tmin <= sigmas [ i ] <= s_tmax else 0. sigma_hat = sigmas [ i ] * ( gamma + 1 ) if gamma > 0 : eps = torch . randn_like ( x ) * s_noise x = x + eps * ( sigma_hat ** 2 - sigmas [ i ] ** 2 ) ** 0.5 denoised = model ( x , sigma_hat * s_in , ** extra_args ) d = to_d ( x , sigma_hat , denoised ) if callback is not None : callback ( { 'x' : x , 'i' : i , 'sigma' : sigmas [ i ] , 'sigma_hat' : sigma_hat , 'denoised' : denoised } ) dt = sigmas [ i + 1 ] - sigma_hat # Euler method x = x + d * dt return x",Implements Algorithm 2 (Euler steps) from Karras et al.
/odoo/odoo/api.py,lang,"def lang(self):
        """"""Return the current language code.

        :rtype: str
        """"""
        lang = self.context.get('lang')
        if lang and lang != 'en_US' and not self['res.lang']._get_data(code=lang):
            # cannot translate here because we do not have a valid language
            raise UserError(f'Invalid language code: {lang}')  # pylint: disable
        return lang or None","def lang(self):
        """"""Return the current language code.

        :rtype: str
        """"""
        lang = self.context.get('lang')
        if lang and lang != 'en_US' and not self['res.lang']._get_data(code=lang):
            # cannot translate here because we do not have a valid language
            raise UserError(f'Invalid language code: {lang}')  # pylint: disable
        return lang or None","Return the current language code.

:rtype: str",Return the current language code.,"def lang(self):
        
        lang = self.context.get('lang')
        if lang and lang != 'en_US' and not self['res.lang']._get_data(code=lang):
            # cannot translate here because we do not have a valid language
            raise UserError(f'Invalid language code: {lang}')  # pylint: disable
        return lang or None",Return the current language code.,def lang ( self ) : lang = self . context . get ( 'lang' ) if lang and lang != 'en_US' and not self [ 'res.lang' ] . _get_data ( code = lang ) : # cannot translate here because we do not have a valid language raise UserError ( f'Invalid language code: {lang}' ) # pylint: disable return lang or None,Return the current language code.
/odoo/odoo/models.py,action_archive,"def action_archive(self):
        """"""Sets :attr:`active` to ``False`` on a recordset, by calling
         :meth:`toggle_active` on its currently active records.
        """"""
        assert self._active_name, f""No 'active' field on model {self._name}""
        return self.filtered(lambda record: record[self._active_name]).toggle_active()","def action_archive(self):
        """"""Sets :attr:`active` to ``False`` on a recordset, by calling
         :meth:`toggle_active` on its currently active records.
        """"""
        assert self._active_name, f""No 'active' field on model {self._name}""
        return self.filtered(lambda record: record[self._active_name]).toggle_active()","Sets :attr:`active` to ``False`` on a recordset, by calling
:meth:`toggle_active` on its currently active records.","Sets :attr:`active` to ``False`` on a recordset, by calling :meth:`toggle_active` on its currently active records.","def action_archive(self):
        
        assert self._active_name, f""No 'active' field on model {self._name}""
        return self.filtered(lambda record: record[self._active_name]).toggle_active()","Sets :attr:`active` to ``False`` on a recordset, by calling :meth:`toggle_active` on its currently active records.","def action_archive ( self ) : assert self . _active_name , f""No 'active' field on model {self._name}"" return self . filtered ( lambda record : record [ self . _active_name ] ) . toggle_active ( )","Sets :attr:`active` to ``False`` on a recordset, by calling :meth:`toggle_active` on its currently active records."
/MetaGPT/metagpt/schema.py,load,"def load(val):
        """"""Convert the json string to object.""""""

        try:
            m = json.loads(val)
            id = m.get(""id"")
            if ""id"" in m:
                del m[""id""]
            msg = Message(**m)
            if id:
                msg.id = id
            return msg
        except JSONDecodeError as err:
            logger.error(f""parse json failed: {val}, error:{err}"")
        return None","def load(val):
        """"""Convert the json string to object.""""""

        try:
            m = json.loads(val)
            id = m.get(""id"")
            if ""id"" in m:
                del m[""id""]
            msg = Message(**m)
            if id:
                msg.id = id
            return msg
        except JSONDecodeError as err:
            logger.error(f""parse json failed: {val}, error:{err}"")
        return None",Convert the json string to object.,Convert the json string to object.,"def load(val):
        

        try:
            m = json.loads(val)
            id = m.get(""id"")
            if ""id"" in m:
                del m[""id""]
            msg = Message(**m)
            if id:
                msg.id = id
            return msg
        except JSONDecodeError as err:
            logger.error(f""parse json failed: {val}, error:{err}"")
        return None",Convert the json string to object.,"def load ( val ) : try : m = json . loads ( val ) id = m . get ( ""id"" ) if ""id"" in m : del m [ ""id"" ] msg = Message ( ** m ) if id : msg . id = id return msg except JSONDecodeError as err : logger . error ( f""parse json failed: {val}, error:{err}"" ) return None",Convert the json string to object.
/black/src/black/strings.py,normalize_string_prefix,"def normalize_string_prefix(s: str) -> str:
    """"""Make all string prefixes lowercase.""""""
    match = STRING_PREFIX_RE.match(s)
    assert match is not None, f""failed to match string {s!r}""
    orig_prefix = match.group(1)
    new_prefix = (
        orig_prefix.replace(""F"", ""f"")
        .replace(""B"", ""b"")
        .replace(""U"", """")
        .replace(""u"", """")
    )

    # Python syntax guarantees max 2 prefixes and that one of them is ""r""
    if len(new_prefix) == 2 and ""r"" != new_prefix[0].lower():
        new_prefix = new_prefix[::-1]
    return f""{new_prefix}{match.group(2)}""","def normalize_string_prefix(s: str) -> str:
    """"""Make all string prefixes lowercase.""""""
    match = STRING_PREFIX_RE.match(s)
    assert match is not None, f""failed to match string {s!r}""
    orig_prefix = match.group(1)
    new_prefix = (
        orig_prefix.replace(""F"", ""f"")
        .replace(""B"", ""b"")
        .replace(""U"", """")
        .replace(""u"", """")
    )

    # Python syntax guarantees max 2 prefixes and that one of them is ""r""
    if len(new_prefix) == 2 and ""r"" != new_prefix[0].lower():
        new_prefix = new_prefix[::-1]
    return f""{new_prefix}{match.group(2)}""",Make all string prefixes lowercase.,Make all string prefixes lowercase.,"def normalize_string_prefix(s: str) -> str:
    
    match = STRING_PREFIX_RE.match(s)
    assert match is not None, f""failed to match string {s!r}""
    orig_prefix = match.group(1)
    new_prefix = (
        orig_prefix.replace(""F"", ""f"")
        .replace(""B"", ""b"")
        .replace(""U"", """")
        .replace(""u"", """")
    )

    # Python syntax guarantees max 2 prefixes and that one of them is ""r""
    if len(new_prefix) == 2 and ""r"" != new_prefix[0].lower():
        new_prefix = new_prefix[::-1]
    return f""{new_prefix}{match.group(2)}""",Make all string prefixes lowercase.,"def normalize_string_prefix ( s : str ) -> str : match = STRING_PREFIX_RE . match ( s ) assert match is not None , f""failed to match string {s!r}"" orig_prefix = match . group ( 1 ) new_prefix = ( orig_prefix . replace ( ""F"" , ""f"" ) . replace ( ""B"" , ""b"" ) . replace ( ""U"" , """" ) . replace ( ""u"" , """" ) ) # Python syntax guarantees max 2 prefixes and that one of them is ""r"" if len ( new_prefix ) == 2 and ""r"" != new_prefix [ 0 ] . lower ( ) : new_prefix = new_prefix [ : : - 1 ] return f""{new_prefix}{match.group(2)}""",Make all string prefixes lowercase.
/annotated_deep_learning_paper_implementations/labml_nn/transformers/gmlp/__init__.py,forward,"def forward(self, *, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        """"""
        * `x` is the input embedding tensor $X$ of shape `[seq_len, batch_size, d_model]`
        * `mask` is a boolean mask of shape `[seq_len, seq_len, 1]` that controls the visibility of tokens
         among each other.
        """"""
        # Keep a copy for shortcut connection
        shortcut = x
        # Normalize $X$
        x = self.norm(x)
        # Projection and activation $Z = \sigma(XU)$
        z = self.activation(self.proj1(x))
        # Spacial Gating Unit $\tilde{Z} = s(Z)$
        z = self.sgu(z, mask)
        # Final projection $Y = \tilde{Z}V$
        z = self.proj2(z)

        # Add the shortcut connection
        return z + shortcut","def forward(self, *, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        """"""
        * `x` is the input embedding tensor $X$ of shape `[seq_len, batch_size, d_model]`
        * `mask` is a boolean mask of shape `[seq_len, seq_len, 1]` that controls the visibility of tokens
         among each other.
        """"""
        # Keep a copy for shortcut connection
        shortcut = x
        # Normalize $X$
        x = self.norm(x)
        # Projection and activation $Z = \sigma(XU)$
        z = self.activation(self.proj1(x))
        # Spacial Gating Unit $\tilde{Z} = s(Z)$
        z = self.sgu(z, mask)
        # Final projection $Y = \tilde{Z}V$
        z = self.proj2(z)

        # Add the shortcut connection
        return z + shortcut","* `x` is the input embedding tensor $X$ of shape `[seq_len, batch_size, d_model]`
* `mask` is a boolean mask of shape `[seq_len, seq_len, 1]` that controls the visibility of tokens
 among each other.","`x` is the input embedding tensor $X$ of shape `[seq_len, batch_size, d_model]` `mask` is a boolean mask of shape `[seq_len, seq_len, 1]` that controls the visibility of tokens among each other.","def forward(self, *, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        
        # Keep a copy for shortcut connection
        shortcut = x
        # Normalize $X$
        x = self.norm(x)
        # Projection and activation $Z = \sigma(XU)$
        z = self.activation(self.proj1(x))
        # Spacial Gating Unit $\tilde{Z} = s(Z)$
        z = self.sgu(z, mask)
        # Final projection $Y = \tilde{Z}V$
        z = self.proj2(z)

        # Add the shortcut connection
        return z + shortcut","`x` is the input embedding tensor $X$ of shape `[seq_len, batch_size, d_model]` `mask` is a boolean mask of shape `[seq_len, seq_len, 1]` that controls the visibility of tokens among each other.","def forward ( self , * , x : torch . Tensor , mask : Optional [ torch . Tensor ] = None ) : # Keep a copy for shortcut connection shortcut = x # Normalize $X$ x = self . norm ( x ) # Projection and activation $Z = \sigma(XU)$ z = self . activation ( self . proj1 ( x ) ) # Spacial Gating Unit $\tilde{Z} = s(Z)$ z = self . sgu ( z , mask ) # Final projection $Y = \tilde{Z}V$ z = self . proj2 ( z ) # Add the shortcut connection return z + shortcut","`x` is the input embedding tensor $X$ of shape `[seq_len, batch_size, d_model]` `mask` is a boolean mask of shape `[seq_len, seq_len, 1]` that controls the visibility of tokens among each other."
/odoo/odoo/fields.py,convert_to_column_insert,"def convert_to_column_insert(self, value, record, values=None, validate=True):
        """""" Convert ``value`` from the ``write`` format to the SQL parameter
        format for INSERT queries. This method handles the case of fields that
        store multiple values (translated or company-dependent).
        """"""
        value = self.convert_to_column(value, record, values, validate)
        if not self.company_dependent:
            return value
        fallback = record.env['ir.default']._get_model_defaults(record._name).get(self.name)
        if value == self.convert_to_column(fallback, record):
            return None
        return PsycopgJson({record.env.company.id: value})","def convert_to_column_insert(self, value, record, values=None, validate=True):
        """""" Convert ``value`` from the ``write`` format to the SQL parameter
        format for INSERT queries. This method handles the case of fields that
        store multiple values (translated or company-dependent).
        """"""
        value = self.convert_to_column(value, record, values, validate)
        if not self.company_dependent:
            return value
        fallback = record.env['ir.default']._get_model_defaults(record._name).get(self.name)
        if value == self.convert_to_column(fallback, record):
            return None
        return PsycopgJson({record.env.company.id: value})","Convert ``value`` from the ``write`` format to the SQL parameter
format for INSERT queries. This method handles the case of fields that
store multiple values (translated or company-dependent).",Convert ``value`` from the ``write`` format to the SQL parameter format for INSERT queries.,"def convert_to_column_insert(self, value, record, values=None, validate=True):
        
        value = self.convert_to_column(value, record, values, validate)
        if not self.company_dependent:
            return value
        fallback = record.env['ir.default']._get_model_defaults(record._name).get(self.name)
        if value == self.convert_to_column(fallback, record):
            return None
        return PsycopgJson({record.env.company.id: value})",Convert ``value`` from the ``write`` format to the SQL parameter format for INSERT queries.,"def convert_to_column_insert ( self , value , record , values = None , validate = True ) : value = self . convert_to_column ( value , record , values , validate ) if not self . company_dependent : return value fallback = record . env [ 'ir.default' ] . _get_model_defaults ( record . _name ) . get ( self . name ) if value == self . convert_to_column ( fallback , record ) : return None return PsycopgJson ( { record . env . company . id : value } )",Convert ``value`` from the ``write`` format to the SQL parameter format for INSERT queries.
/yolov5/utils/loggers/comet/__init__.py,log_model,"def log_model(self, path, opt, epoch, fitness_score, best_model=False):
        """"""Logs model checkpoint to experiment with path, options, epoch, fitness, and best model flag.""""""
        if not self.save_model:
            return

        model_metadata = {
            ""fitness_score"": fitness_score[-1],
            ""epochs_trained"": epoch + 1,
            ""save_period"": opt.save_period,
            ""total_epochs"": opt.epochs,
        }

        model_files = glob.glob(f""{path}/*.pt"")
        for model_path in model_files:
            name = Path(model_path).name

            self.experiment.log_model(
                self.model_name,
                file_or_folder=model_path,
                file_name=name,
                metadata=model_metadata,
                overwrite=True,
            )","def log_model(self, path, opt, epoch, fitness_score, best_model=False):
        """"""Logs model checkpoint to experiment with path, options, epoch, fitness, and best model flag.""""""
        if not self.save_model:
            return

        model_metadata = {
            ""fitness_score"": fitness_score[-1],
            ""epochs_trained"": epoch + 1,
            ""save_period"": opt.save_period,
            ""total_epochs"": opt.epochs,
        }

        model_files = glob.glob(f""{path}/*.pt"")
        for model_path in model_files:
            name = Path(model_path).name

            self.experiment.log_model(
                self.model_name,
                file_or_folder=model_path,
                file_name=name,
                metadata=model_metadata,
                overwrite=True,
            )","Logs model checkpoint to experiment with path, options, epoch, fitness, and best model flag.","Logs model checkpoint to experiment with path, options, epoch, fitness, and best model flag.","def log_model(self, path, opt, epoch, fitness_score, best_model=False):
        
        if not self.save_model:
            return

        model_metadata = {
            ""fitness_score"": fitness_score[-1],
            ""epochs_trained"": epoch + 1,
            ""save_period"": opt.save_period,
            ""total_epochs"": opt.epochs,
        }

        model_files = glob.glob(f""{path}/*.pt"")
        for model_path in model_files:
            name = Path(model_path).name

            self.experiment.log_model(
                self.model_name,
                file_or_folder=model_path,
                file_name=name,
                metadata=model_metadata,
                overwrite=True,
            )","Logs model checkpoint to experiment with path, options, epoch, fitness, and best model flag.","def log_model ( self , path , opt , epoch , fitness_score , best_model = False ) : if not self . save_model : return model_metadata = { ""fitness_score"" : fitness_score [ - 1 ] , ""epochs_trained"" : epoch + 1 , ""save_period"" : opt . save_period , ""total_epochs"" : opt . epochs , } model_files = glob . glob ( f""{path}/*.pt"" ) for model_path in model_files : name = Path ( model_path ) . name self . experiment . log_model ( self . model_name , file_or_folder = model_path , file_name = name , metadata = model_metadata , overwrite = True , )","Logs model checkpoint to experiment with path, options, epoch, fitness, and best model flag."
/cpython/Tools/wasm/wasi/__main__.py,configure_build_python,"def configure_build_python(context, working_dir):
    """"""Configure the build/host Python.""""""
    if LOCAL_SETUP.exists():
        print(f""👍 {LOCAL_SETUP} exists ..."")
    else:
        print(f""📝 Touching {LOCAL_SETUP} ..."")
        LOCAL_SETUP.write_bytes(LOCAL_SETUP_MARKER)

    configure = [os.path.relpath(CHECKOUT / 'configure', working_dir)]
    if context.args:
        configure.extend(context.args)

    call(configure, quiet=context.quiet)","def configure_build_python(context, working_dir):
    """"""Configure the build/host Python.""""""
    if LOCAL_SETUP.exists():
        print(f""👍 {LOCAL_SETUP} exists ..."")
    else:
        print(f""📝 Touching {LOCAL_SETUP} ..."")
        LOCAL_SETUP.write_bytes(LOCAL_SETUP_MARKER)

    configure = [os.path.relpath(CHECKOUT / 'configure', working_dir)]
    if context.args:
        configure.extend(context.args)

    call(configure, quiet=context.quiet)",Configure the build/host Python.,Configure the build/host Python.,"def configure_build_python(context, working_dir):
    
    if LOCAL_SETUP.exists():
        print(f""👍 {LOCAL_SETUP} exists ..."")
    else:
        print(f""📝 Touching {LOCAL_SETUP} ..."")
        LOCAL_SETUP.write_bytes(LOCAL_SETUP_MARKER)

    configure = [os.path.relpath(CHECKOUT / 'configure', working_dir)]
    if context.args:
        configure.extend(context.args)

    call(configure, quiet=context.quiet)",Configure the build/host Python.,"def configure_build_python ( context , working_dir ) : if LOCAL_SETUP . exists ( ) : print ( f""👍 {LOCAL_SETUP} exists ..."" ) else : print ( f""📝 Touching {LOCAL_SETUP} ..."" ) LOCAL_SETUP . write_bytes ( LOCAL_SETUP_MARKER ) configure = [ os . path . relpath ( CHECKOUT / 'configure' , working_dir ) ] if context . args : configure . extend ( context . args ) call ( configure , quiet = context . quiet )",Configure the build/host Python.
/odoo/odoo/models.py,__sub__,"def __sub__(self, other) -> Self:
        """""" Return the recordset of all the records in ``self`` that are not in
            ``other``. Note that recordset order is preserved.
        """"""
        try:
            if self._name != other._name:
                raise TypeError(f""inconsistent models in: {self} - {other}"")
            other_ids = set(other._ids)
            return self.browse([id for id in self._ids if id not in other_ids])
        except AttributeError:
            raise TypeError(f""unsupported operand types in: {self} - {other!r}"")","def __sub__(self, other) -> Self:
        """""" Return the recordset of all the records in ``self`` that are not in
            ``other``. Note that recordset order is preserved.
        """"""
        try:
            if self._name != other._name:
                raise TypeError(f""inconsistent models in: {self} - {other}"")
            other_ids = set(other._ids)
            return self.browse([id for id in self._ids if id not in other_ids])
        except AttributeError:
            raise TypeError(f""unsupported operand types in: {self} - {other!r}"")","Return the recordset of all the records in ``self`` that are not in
``other``. Note that recordset order is preserved.",Return the recordset of all the records in ``self`` that are not in ``other``.,"def __sub__(self, other) -> Self:
        
        try:
            if self._name != other._name:
                raise TypeError(f""inconsistent models in: {self} - {other}"")
            other_ids = set(other._ids)
            return self.browse([id for id in self._ids if id not in other_ids])
        except AttributeError:
            raise TypeError(f""unsupported operand types in: {self} - {other!r}"")",Return the recordset of all the records in ``self`` that are not in ``other``.,"def __sub__ ( self , other ) -> Self : try : if self . _name != other . _name : raise TypeError ( f""inconsistent models in: {self} - {other}"" ) other_ids = set ( other . _ids ) return self . browse ( [ id for id in self . _ids if id not in other_ids ] ) except AttributeError : raise TypeError ( f""unsupported operand types in: {self} - {other!r}"" )",Return the recordset of all the records in ``self`` that are not in ``other``.
/annotated_deep_learning_paper_implementations/labml_nn/transformers/alibi/experiment.py,other_metrics,"def other_metrics(self, output: torch.Tensor, target: torch.Tensor):
        """"""
        Log losses at the initial and final tokens
        """"""
        # If there are more tokens that the training sequence length (during validation),
        if self.seq_len < output.shape[0]:
            # Log the loss at training sequence length
            tracker.add(f'loss.{self.seq_len - 1}.', self.loss_func(output[self.seq_len - 1], target[self.seq_len - 1]))
            # Log the loss at the first token
            tracker.add(f'loss.0.', self.loss_func(output[0], target[0]))
        # Log the loss at the final token
        tracker.add(f'loss.{int(output.shape[0]) - 1}.', self.loss_func(output[-1], target[-1]))","def other_metrics(self, output: torch.Tensor, target: torch.Tensor):
        """"""
        Log losses at the initial and final tokens
        """"""
        # If there are more tokens that the training sequence length (during validation),
        if self.seq_len < output.shape[0]:
            # Log the loss at training sequence length
            tracker.add(f'loss.{self.seq_len - 1}.', self.loss_func(output[self.seq_len - 1], target[self.seq_len - 1]))
            # Log the loss at the first token
            tracker.add(f'loss.0.', self.loss_func(output[0], target[0]))
        # Log the loss at the final token
        tracker.add(f'loss.{int(output.shape[0]) - 1}.', self.loss_func(output[-1], target[-1]))",Log losses at the initial and final tokens,Log losses at the initial and final tokens,"def other_metrics(self, output: torch.Tensor, target: torch.Tensor):
        
        # If there are more tokens that the training sequence length (during validation),
        if self.seq_len < output.shape[0]:
            # Log the loss at training sequence length
            tracker.add(f'loss.{self.seq_len - 1}.', self.loss_func(output[self.seq_len - 1], target[self.seq_len - 1]))
            # Log the loss at the first token
            tracker.add(f'loss.0.', self.loss_func(output[0], target[0]))
        # Log the loss at the final token
        tracker.add(f'loss.{int(output.shape[0]) - 1}.', self.loss_func(output[-1], target[-1]))",Log losses at the initial and final tokens,"def other_metrics ( self , output : torch . Tensor , target : torch . Tensor ) : # If there are more tokens that the training sequence length (during validation), if self . seq_len < output . shape [ 0 ] : # Log the loss at training sequence length tracker . add ( f'loss.{self.seq_len - 1}.' , self . loss_func ( output [ self . seq_len - 1 ] , target [ self . seq_len - 1 ] ) ) # Log the loss at the first token tracker . add ( f'loss.0.' , self . loss_func ( output [ 0 ] , target [ 0 ] ) ) # Log the loss at the final token tracker . add ( f'loss.{int(output.shape[0]) - 1}.' , self . loss_func ( output [ - 1 ] , target [ - 1 ] ) )",Log losses at the initial and final tokens
/yt-dlp/yt_dlp/options.py,format_option_strings,"def format_option_strings(option):
        """""" ('-o', '--option') -> -o, --format METAVAR """"""
        opts = join_nonempty(
            option._short_opts and option._short_opts[0],
            option._long_opts and option._long_opts[0],
            delim=', ')
        if option.takes_value():
            opts += f' {option.metavar}'
        return opts","def format_option_strings(option):
        """""" ('-o', '--option') -> -o, --format METAVAR """"""
        opts = join_nonempty(
            option._short_opts and option._short_opts[0],
            option._long_opts and option._long_opts[0],
            delim=', ')
        if option.takes_value():
            opts += f' {option.metavar}'
        return opts","('-o', '--option') -> -o, --format METAVAR","('-o', '--option') -> -o, --format METAVAR","def format_option_strings(option):
        
        opts = join_nonempty(
            option._short_opts and option._short_opts[0],
            option._long_opts and option._long_opts[0],
            delim=', ')
        if option.takes_value():
            opts += f' {option.metavar}'
        return opts","('-o', '--option') -> -o, --format METAVAR","def format_option_strings ( option ) : opts = join_nonempty ( option . _short_opts and option . _short_opts [ 0 ] , option . _long_opts and option . _long_opts [ 0 ] , delim = ', ' ) if option . takes_value ( ) : opts += f' {option.metavar}' return opts","('-o', '--option') -> -o, --format METAVAR"
/odoo/odoo/tests/form.py,_get_save_values,"def _get_save_values(self):
        """""" Validate and return field values modified since load/save. """"""
        values = UpdateDict(self._values)

        for field_name in self._view['fields']:
            if self._get_modifier(field_name, 'required') and not (
                self._get_modifier(field_name, 'column_invisible')
                or self._get_modifier(field_name, 'invisible')
            ):
                assert values[field_name] is not False, f""{field_name!r} is a required field""

        return values","def _get_save_values(self):
        """""" Validate and return field values modified since load/save. """"""
        values = UpdateDict(self._values)

        for field_name in self._view['fields']:
            if self._get_modifier(field_name, 'required') and not (
                self._get_modifier(field_name, 'column_invisible')
                or self._get_modifier(field_name, 'invisible')
            ):
                assert values[field_name] is not False, f""{field_name!r} is a required field""

        return values",Validate and return field values modified since load/save.,Validate and return field values modified since load/save.,"def _get_save_values(self):
        
        values = UpdateDict(self._values)

        for field_name in self._view['fields']:
            if self._get_modifier(field_name, 'required') and not (
                self._get_modifier(field_name, 'column_invisible')
                or self._get_modifier(field_name, 'invisible')
            ):
                assert values[field_name] is not False, f""{field_name!r} is a required field""

        return values",Validate and return field values modified since load/save.,"def _get_save_values ( self ) : values = UpdateDict ( self . _values ) for field_name in self . _view [ 'fields' ] : if self . _get_modifier ( field_name , 'required' ) and not ( self . _get_modifier ( field_name , 'column_invisible' ) or self . _get_modifier ( field_name , 'invisible' ) ) : assert values [ field_name ] is not False , f""{field_name!r} is a required field"" return values",Validate and return field values modified since load/save.
/LLaMA-Factory/src/llamafactory/eval/template.py,format_example,"def format_example(
        self, target_data: dict[str, str], support_set: list[dict[str, str]], subject_name: str
    ) -> list[dict[str, str]]:
        r""""""Convert dataset examples to messages.""""""
        messages = []
        for k in range(len(support_set)):
            prompt, response = self._parse_example(support_set[k])
            messages.append({""role"": Role.USER.value, ""content"": prompt})
            messages.append({""role"": Role.ASSISTANT.value, ""content"": response})

        prompt, response = self._parse_example(target_data)
        messages.append({""role"": Role.USER.value, ""content"": prompt})
        messages.append({""role"": Role.ASSISTANT.value, ""content"": response})
        messages[0][""content""] = self.system.format(subject=subject_name) + messages[0][""content""]
        return messages","def format_example(
        self, target_data: dict[str, str], support_set: list[dict[str, str]], subject_name: str
    ) -> list[dict[str, str]]:
        r""""""Convert dataset examples to messages.""""""
        messages = []
        for k in range(len(support_set)):
            prompt, response = self._parse_example(support_set[k])
            messages.append({""role"": Role.USER.value, ""content"": prompt})
            messages.append({""role"": Role.ASSISTANT.value, ""content"": response})

        prompt, response = self._parse_example(target_data)
        messages.append({""role"": Role.USER.value, ""content"": prompt})
        messages.append({""role"": Role.ASSISTANT.value, ""content"": response})
        messages[0][""content""] = self.system.format(subject=subject_name) + messages[0][""content""]
        return messages",Convert dataset examples to messages.,Convert dataset examples to messages.,"def format_example(
        self, target_data: dict[str, str], support_set: list[dict[str, str]], subject_name: str
    ) -> list[dict[str, str]]:
        
        messages = []
        for k in range(len(support_set)):
            prompt, response = self._parse_example(support_set[k])
            messages.append({""role"": Role.USER.value, ""content"": prompt})
            messages.append({""role"": Role.ASSISTANT.value, ""content"": response})

        prompt, response = self._parse_example(target_data)
        messages.append({""role"": Role.USER.value, ""content"": prompt})
        messages.append({""role"": Role.ASSISTANT.value, ""content"": response})
        messages[0][""content""] = self.system.format(subject=subject_name) + messages[0][""content""]
        return messages",Convert dataset examples to messages.,"def format_example ( self , target_data : dict [ str , str ] , support_set : list [ dict [ str , str ] ] , subject_name : str ) -> list [ dict [ str , str ] ] : messages = [ ] for k in range ( len ( support_set ) ) : prompt , response = self . _parse_example ( support_set [ k ] ) messages . append ( { ""role"" : Role . USER . value , ""content"" : prompt } ) messages . append ( { ""role"" : Role . ASSISTANT . value , ""content"" : response } ) prompt , response = self . _parse_example ( target_data ) messages . append ( { ""role"" : Role . USER . value , ""content"" : prompt } ) messages . append ( { ""role"" : Role . ASSISTANT . value , ""content"" : response } ) messages [ 0 ] [ ""content"" ] = self . system . format ( subject = subject_name ) + messages [ 0 ] [ ""content"" ] return messages",Convert dataset examples to messages.
/black/src/black/cache.py,write,"def write(self, sources: Iterable[Path]) -> None:
        """"""Update the cache file data and write a new cache file.""""""
        self.file_data.update(
            **{str(src.resolve()): Cache.get_file_data(src) for src in sources}
        )
        try:
            CACHE_DIR.mkdir(parents=True, exist_ok=True)
            with tempfile.NamedTemporaryFile(
                dir=str(self.cache_file.parent), delete=False
            ) as f:
                # We store raw tuples in the cache because it's faster.
                data: dict[str, tuple[float, int, str]] = {
                    k: (*v,) for k, v in self.file_data.items()
                }
                pickle.dump(data, f, protocol=4)
            os.replace(f.name, self.cache_file)
        except OSError:
            pass","def write(self, sources: Iterable[Path]) -> None:
        """"""Update the cache file data and write a new cache file.""""""
        self.file_data.update(
            **{str(src.resolve()): Cache.get_file_data(src) for src in sources}
        )
        try:
            CACHE_DIR.mkdir(parents=True, exist_ok=True)
            with tempfile.NamedTemporaryFile(
                dir=str(self.cache_file.parent), delete=False
            ) as f:
                # We store raw tuples in the cache because it's faster.
                data: dict[str, tuple[float, int, str]] = {
                    k: (*v,) for k, v in self.file_data.items()
                }
                pickle.dump(data, f, protocol=4)
            os.replace(f.name, self.cache_file)
        except OSError:
            pass",Update the cache file data and write a new cache file.,Update the cache file data and write a new cache file.,"def write(self, sources: Iterable[Path]) -> None:
        
        self.file_data.update(
            **{str(src.resolve()): Cache.get_file_data(src) for src in sources}
        )
        try:
            CACHE_DIR.mkdir(parents=True, exist_ok=True)
            with tempfile.NamedTemporaryFile(
                dir=str(self.cache_file.parent), delete=False
            ) as f:
                # We store raw tuples in the cache because it's faster.
                data: dict[str, tuple[float, int, str]] = {
                    k: (*v,) for k, v in self.file_data.items()
                }
                pickle.dump(data, f, protocol=4)
            os.replace(f.name, self.cache_file)
        except OSError:
            pass",Update the cache file data and write a new cache file.,"def write ( self , sources : Iterable [ Path ] ) -> None : self . file_data . update ( ** { str ( src . resolve ( ) ) : Cache . get_file_data ( src ) for src in sources } ) try : CACHE_DIR . mkdir ( parents = True , exist_ok = True ) with tempfile . NamedTemporaryFile ( dir = str ( self . cache_file . parent ) , delete = False ) as f : # We store raw tuples in the cache because it's faster. data : dict [ str , tuple [ float , int , str ] ] = { k : ( * v , ) for k , v in self . file_data . items ( ) } pickle . dump ( data , f , protocol = 4 ) os . replace ( f . name , self . cache_file ) except OSError : pass",Update the cache file data and write a new cache file.
/gradio/gradio/external_utils.py,streamline_spaces_interface,"def streamline_spaces_interface(config: dict) -> dict:
    """"""Streamlines the interface config dictionary to remove unnecessary keys.""""""
    config[""inputs""] = [
        components.get_component_instance(component)
        for component in config[""input_components""]
    ]
    config[""outputs""] = [
        components.get_component_instance(component)
        for component in config[""output_components""]
    ]
    parameters = {
        ""article"",
        ""description"",
        ""flagging_options"",
        ""inputs"",
        ""outputs"",
        ""title"",
    }
    config = {k: config[k] for k in parameters}
    return config","def streamline_spaces_interface(config: dict) -> dict:
    """"""Streamlines the interface config dictionary to remove unnecessary keys.""""""
    config[""inputs""] = [
        components.get_component_instance(component)
        for component in config[""input_components""]
    ]
    config[""outputs""] = [
        components.get_component_instance(component)
        for component in config[""output_components""]
    ]
    parameters = {
        ""article"",
        ""description"",
        ""flagging_options"",
        ""inputs"",
        ""outputs"",
        ""title"",
    }
    config = {k: config[k] for k in parameters}
    return config",Streamlines the interface config dictionary to remove unnecessary keys.,Streamlines the interface config dictionary to remove unnecessary keys.,"def streamline_spaces_interface(config: dict) -> dict:
    
    config[""inputs""] = [
        components.get_component_instance(component)
        for component in config[""input_components""]
    ]
    config[""outputs""] = [
        components.get_component_instance(component)
        for component in config[""output_components""]
    ]
    parameters = {
        ""article"",
        ""description"",
        ""flagging_options"",
        ""inputs"",
        ""outputs"",
        ""title"",
    }
    config = {k: config[k] for k in parameters}
    return config",Streamlines the interface config dictionary to remove unnecessary keys.,"def streamline_spaces_interface ( config : dict ) -> dict : config [ ""inputs"" ] = [ components . get_component_instance ( component ) for component in config [ ""input_components"" ] ] config [ ""outputs"" ] = [ components . get_component_instance ( component ) for component in config [ ""output_components"" ] ] parameters = { ""article"" , ""description"" , ""flagging_options"" , ""inputs"" , ""outputs"" , ""title"" , } config = { k : config [ k ] for k in parameters } return config",Streamlines the interface config dictionary to remove unnecessary keys.
/ultralytics/ultralytics/data/loaders.py,_single_check,"def _single_check(im: Union[Image.Image, np.ndarray], flag: str = ""RGB"") -> np.ndarray:
        """"""Validate and format an image to numpy array, ensuring RGB order and contiguous memory.""""""
        assert isinstance(im, (Image.Image, np.ndarray)), f""Expected PIL/np.ndarray image type, but got {type(im)}""
        if isinstance(im, Image.Image):
            im = np.asarray(im.convert(flag))
            # adding new axis if it's grayscale, and converting to BGR if it's RGB
            im = im[..., None] if flag == ""L"" else im[..., ::-1]
            im = np.ascontiguousarray(im)  # contiguous
        elif im.ndim == 2:  # grayscale in numpy form
            im = im[..., None]
        return im","def _single_check(im: Union[Image.Image, np.ndarray], flag: str = ""RGB"") -> np.ndarray:
        """"""Validate and format an image to numpy array, ensuring RGB order and contiguous memory.""""""
        assert isinstance(im, (Image.Image, np.ndarray)), f""Expected PIL/np.ndarray image type, but got {type(im)}""
        if isinstance(im, Image.Image):
            im = np.asarray(im.convert(flag))
            # adding new axis if it's grayscale, and converting to BGR if it's RGB
            im = im[..., None] if flag == ""L"" else im[..., ::-1]
            im = np.ascontiguousarray(im)  # contiguous
        elif im.ndim == 2:  # grayscale in numpy form
            im = im[..., None]
        return im","Validate and format an image to numpy array, ensuring RGB order and contiguous memory.","Validate and format an image to numpy array, ensuring RGB order and contiguous memory.","def _single_check(im: Union[Image.Image, np.ndarray], flag: str = ""RGB"") -> np.ndarray:
        
        assert isinstance(im, (Image.Image, np.ndarray)), f""Expected PIL/np.ndarray image type, but got {type(im)}""
        if isinstance(im, Image.Image):
            im = np.asarray(im.convert(flag))
            # adding new axis if it's grayscale, and converting to BGR if it's RGB
            im = im[..., None] if flag == ""L"" else im[..., ::-1]
            im = np.ascontiguousarray(im)  # contiguous
        elif im.ndim == 2:  # grayscale in numpy form
            im = im[..., None]
        return im","Validate and format an image to numpy array, ensuring RGB order and contiguous memory.","def _single_check ( im : Union [ Image . Image , np . ndarray ] , flag : str = ""RGB"" ) -> np . ndarray : assert isinstance ( im , ( Image . Image , np . ndarray ) ) , f""Expected PIL/np.ndarray image type, but got {type(im)}"" if isinstance ( im , Image . Image ) : im = np . asarray ( im . convert ( flag ) ) # adding new axis if it's grayscale, and converting to BGR if it's RGB im = im [ ... , None ] if flag == ""L"" else im [ ... , : : - 1 ] im = np . ascontiguousarray ( im ) # contiguous elif im . ndim == 2 : # grayscale in numpy form im = im [ ... , None ] return im","Validate and format an image to numpy array, ensuring RGB order and contiguous memory."
/yolov5/models/yolo.py,_profile_one_layer,"def _profile_one_layer(self, m, x, dt):
        """"""Profiles a single layer's performance by computing GFLOPs, execution time, and parameters.""""""
        c = m == self.model[-1]  # is final layer, copy input as inplace fix
        o = thop.profile(m, inputs=(x.copy() if c else x,), verbose=False)[0] / 1e9 * 2 if thop else 0  # FLOPs
        t = time_sync()
        for _ in range(10):
            m(x.copy() if c else x)
        dt.append((time_sync() - t) * 100)
        if m == self.model[0]:
            LOGGER.info(f""{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  module"")
        LOGGER.info(f""{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}"")
        if c:
            LOGGER.info(f""{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total"")","def _profile_one_layer(self, m, x, dt):
        """"""Profiles a single layer's performance by computing GFLOPs, execution time, and parameters.""""""
        c = m == self.model[-1]  # is final layer, copy input as inplace fix
        o = thop.profile(m, inputs=(x.copy() if c else x,), verbose=False)[0] / 1e9 * 2 if thop else 0  # FLOPs
        t = time_sync()
        for _ in range(10):
            m(x.copy() if c else x)
        dt.append((time_sync() - t) * 100)
        if m == self.model[0]:
            LOGGER.info(f""{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  module"")
        LOGGER.info(f""{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}"")
        if c:
            LOGGER.info(f""{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total"")","Profiles a single layer's performance by computing GFLOPs, execution time, and parameters.","Profiles a single layer's performance by computing GFLOPs, execution time, and parameters.","def _profile_one_layer(self, m, x, dt):
        
        c = m == self.model[-1]  # is final layer, copy input as inplace fix
        o = thop.profile(m, inputs=(x.copy() if c else x,), verbose=False)[0] / 1e9 * 2 if thop else 0  # FLOPs
        t = time_sync()
        for _ in range(10):
            m(x.copy() if c else x)
        dt.append((time_sync() - t) * 100)
        if m == self.model[0]:
            LOGGER.info(f""{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  module"")
        LOGGER.info(f""{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}"")
        if c:
            LOGGER.info(f""{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total"")","Profiles a single layer's performance by computing GFLOPs, execution time, and parameters.","def _profile_one_layer ( self , m , x , dt ) : c = m == self . model [ - 1 ] # is final layer, copy input as inplace fix o = thop . profile ( m , inputs = ( x . copy ( ) if c else x , ) , verbose = False ) [ 0 ] / 1e9 * 2 if thop else 0 # FLOPs t = time_sync ( ) for _ in range ( 10 ) : m ( x . copy ( ) if c else x ) dt . append ( ( time_sync ( ) - t ) * 100 ) if m == self . model [ 0 ] : LOGGER . info ( f""{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  module"" ) LOGGER . info ( f""{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}"" ) if c : LOGGER . info ( f""{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total"" )","Profiles a single layer's performance by computing GFLOPs, execution time, and parameters."
/yolov5/utils/dataloaders.py,_hub_ops,"def _hub_ops(self, f, max_dim=1920):
        """"""Resizes and saves an image at reduced quality for web/app viewing, supporting both PIL and OpenCV.""""""
        f_new = self.im_dir / Path(f).name  # dataset-hub image filename
        try:  # use PIL
            im = Image.open(f)
            r = max_dim / max(im.height, im.width)  # ratio
            if r < 1.0:  # image too large
                im = im.resize((int(im.width * r), int(im.height * r)))
            im.save(f_new, ""JPEG"", quality=50, optimize=True)  # save
        except Exception as e:  # use OpenCV
            LOGGER.info(f""WARNING ⚠️ HUB ops PIL failure {f}: {e}"")
            im = cv2.imread(f)
            im_height, im_width = im.shape[:2]
            r = max_dim / max(im_height, im_width)  # ratio
            if r < 1.0:  # image too large
                im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_AREA)
            cv2.imwrite(str(f_new), im)","def _hub_ops(self, f, max_dim=1920):
        """"""Resizes and saves an image at reduced quality for web/app viewing, supporting both PIL and OpenCV.""""""
        f_new = self.im_dir / Path(f).name  # dataset-hub image filename
        try:  # use PIL
            im = Image.open(f)
            r = max_dim / max(im.height, im.width)  # ratio
            if r < 1.0:  # image too large
                im = im.resize((int(im.width * r), int(im.height * r)))
            im.save(f_new, ""JPEG"", quality=50, optimize=True)  # save
        except Exception as e:  # use OpenCV
            LOGGER.info(f""WARNING ⚠️ HUB ops PIL failure {f}: {e}"")
            im = cv2.imread(f)
            im_height, im_width = im.shape[:2]
            r = max_dim / max(im_height, im_width)  # ratio
            if r < 1.0:  # image too large
                im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_AREA)
            cv2.imwrite(str(f_new), im)","Resizes and saves an image at reduced quality for web/app viewing, supporting both PIL and OpenCV.","Resizes and saves an image at reduced quality for web/app viewing, supporting both PIL and OpenCV.","def _hub_ops(self, f, max_dim=1920):
        
        f_new = self.im_dir / Path(f).name  # dataset-hub image filename
        try:  # use PIL
            im = Image.open(f)
            r = max_dim / max(im.height, im.width)  # ratio
            if r < 1.0:  # image too large
                im = im.resize((int(im.width * r), int(im.height * r)))
            im.save(f_new, ""JPEG"", quality=50, optimize=True)  # save
        except Exception as e:  # use OpenCV
            LOGGER.info(f""WARNING ⚠️ HUB ops PIL failure {f}: {e}"")
            im = cv2.imread(f)
            im_height, im_width = im.shape[:2]
            r = max_dim / max(im_height, im_width)  # ratio
            if r < 1.0:  # image too large
                im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_AREA)
            cv2.imwrite(str(f_new), im)","Resizes and saves an image at reduced quality for web/app viewing, supporting both PIL and OpenCV.","def _hub_ops ( self , f , max_dim = 1920 ) : f_new = self . im_dir / Path ( f ) . name # dataset-hub image filename try : # use PIL im = Image . open ( f ) r = max_dim / max ( im . height , im . width ) # ratio if r < 1.0 : # image too large im = im . resize ( ( int ( im . width * r ) , int ( im . height * r ) ) ) im . save ( f_new , ""JPEG"" , quality = 50 , optimize = True ) # save except Exception as e : # use OpenCV LOGGER . info ( f""WARNING ⚠️ HUB ops PIL failure {f}: {e}"" ) im = cv2 . imread ( f ) im_height , im_width = im . shape [ : 2 ] r = max_dim / max ( im_height , im_width ) # ratio if r < 1.0 : # image too large im = cv2 . resize ( im , ( int ( im_width * r ) , int ( im_height * r ) ) , interpolation = cv2 . INTER_AREA ) cv2 . imwrite ( str ( f_new ) , im )","Resizes and saves an image at reduced quality for web/app viewing, supporting both PIL and OpenCV."
/yolov5/utils/general.py,check_git_info,"def check_git_info(path="".""):
    """"""Checks YOLOv5 git info, returning a dict with remote URL, branch name, and commit hash.""""""
    check_requirements(""gitpython"")
    import git

    try:
        repo = git.Repo(path)
        remote = repo.remotes.origin.url.replace("".git"", """")  # i.e. 'https://github.com/ultralytics/yolov5'
        commit = repo.head.commit.hexsha  # i.e. '3134699c73af83aac2a481435550b968d5792c0d'
        try:
            branch = repo.active_branch.name  # i.e. 'main'
        except TypeError:  # not on any branch
            branch = None  # i.e. 'detached HEAD' state
        return {""remote"": remote, ""branch"": branch, ""commit"": commit}
    except git.exc.InvalidGitRepositoryError:  # path is not a git dir
        return {""remote"": None, ""branch"": None, ""commit"": None}","def check_git_info(path="".""):
    """"""Checks YOLOv5 git info, returning a dict with remote URL, branch name, and commit hash.""""""
    check_requirements(""gitpython"")
    import git

    try:
        repo = git.Repo(path)
        commit = repo.head.commit.hexsha  # i.e. '3134699c73af83aac2a481435550b968d5792c0d'
        try:
            branch = repo.active_branch.name  # i.e. 'main'
        except TypeError:  # not on any branch
            branch = None  # i.e. 'detached HEAD' state
        return {""remote"": remote, ""branch"": branch, ""commit"": commit}
    except git.exc.InvalidGitRepositoryError:  # path is not a git dir
        return {""remote"": None, ""branch"": None, ""commit"": None}","Checks YOLOv5 git info, returning a dict with remote URL, branch name, and commit hash.","Checks YOLOv5 git info, returning a dict with remote URL, branch name, and commit hash.","def check_git_info(path="".""):
    
    check_requirements(""gitpython"")
    import git

    try:
        repo = git.Repo(path)
        commit = repo.head.commit.hexsha  # i.e. '3134699c73af83aac2a481435550b968d5792c0d'
        try:
            branch = repo.active_branch.name  # i.e. 'main'
        except TypeError:  # not on any branch
            branch = None  # i.e. 'detached HEAD' state
        return {""remote"": remote, ""branch"": branch, ""commit"": commit}
    except git.exc.InvalidGitRepositoryError:  # path is not a git dir
        return {""remote"": None, ""branch"": None, ""commit"": None}","Checks YOLOv5 git info, returning a dict with remote URL, branch name, and commit hash.","def check_git_info ( path = ""."" ) : check_requirements ( ""gitpython"" ) import git try : repo = git . Repo ( path ) commit = repo . head . commit . hexsha # i.e. '3134699c73af83aac2a481435550b968d5792c0d' try : branch = repo . active_branch . name # i.e. 'main' except TypeError : # not on any branch branch = None # i.e. 'detached HEAD' state return { ""remote"" : remote , ""branch"" : branch , ""commit"" : commit } except git . exc . InvalidGitRepositoryError : # path is not a git dir return { ""remote"" : None , ""branch"" : None , ""commit"" : None }","Checks YOLOv5 git info, returning a dict with remote URL, branch name, and commit hash."
/OpenBB/openbb_platform/extensions/tests/utils/router_testers.py,check_general,"def check_general(
    keywords: Dict, examples: List, router_name: str, function: Any
) -> List[str]:
    """"""Check for general violations in the router command examples.""""""
    general_violation: List[str] = []

    # Check if the endpoint has examples
    if ""examples"" not in keywords or not examples:
        general_violation.append(
            f""'{router_name}' > '{function.__name__}': missing examples""
        )
        return general_violation

    return general_violation","def check_general(
    keywords: Dict, examples: List, router_name: str, function: Any
) -> List[str]:
    """"""Check for general violations in the router command examples.""""""
    general_violation: List[str] = []

    # Check if the endpoint has examples
    if ""examples"" not in keywords or not examples:
        general_violation.append(
            f""'{router_name}' > '{function.__name__}': missing examples""
        )
        return general_violation

    return general_violation",Check for general violations in the router command examples.,Check for general violations in the router command examples.,"def check_general(
    keywords: Dict, examples: List, router_name: str, function: Any
) -> List[str]:
    
    general_violation: List[str] = []

    # Check if the endpoint has examples
    if ""examples"" not in keywords or not examples:
        general_violation.append(
            f""'{router_name}' > '{function.__name__}': missing examples""
        )
        return general_violation

    return general_violation",Check for general violations in the router command examples.,"def check_general ( keywords : Dict , examples : List , router_name : str , function : Any ) -> List [ str ] : general_violation : List [ str ] = [ ] # Check if the endpoint has examples if ""examples"" not in keywords or not examples : general_violation . append ( f""'{router_name}' > '{function.__name__}': missing examples"" ) return general_violation return general_violation",Check for general violations in the router command examples.
/cpython/Tools/build/stable_abi.py,gen_testcapi_feature_macros,"def gen_testcapi_feature_macros(manifest, args, outfile):
    """"""Generate/check the stable ABI list for documentation annotations""""""
    write = partial(print, file=outfile)
    write(f'// Generated by {SCRIPT_NAME}')
    write()
    write('// Add an entry in dict `result` for each Stable ABI feature macro.')
    write()
    for macro in manifest.select({'feature_macro'}):
        name = macro.name
        write(f'#ifdef {name}')
        write(f'    res = PyDict_SetItemString(result, ""{name}"", Py_True);')
        write('#else')
        write(f'    res = PyDict_SetItemString(result, ""{name}"", Py_False);')
        write('#endif')
        write('if (res) {')
        write('    Py_DECREF(result); return NULL;')
        write('}')
        write()","def gen_testcapi_feature_macros(manifest, args, outfile):
    """"""Generate/check the stable ABI list for documentation annotations""""""
    write = partial(print, file=outfile)
    write()
    write()
    for macro in manifest.select({'feature_macro'}):
        name = macro.name
        write(f'#ifdef {name}')
        write(f'    res = PyDict_SetItemString(result, ""{name}"", Py_True);')
        write('#else')
        write(f'    res = PyDict_SetItemString(result, ""{name}"", Py_False);')
        write('#endif')
        write('if (res) {')
        write('    Py_DECREF(result); return NULL;')
        write('}')
        write()",Generate/check the stable ABI list for documentation annotations,Generate/check the stable ABI list for documentation annotations,"def gen_testcapi_feature_macros(manifest, args, outfile):
    
    write = partial(print, file=outfile)
    write()
    write()
    for macro in manifest.select({'feature_macro'}):
        name = macro.name
        write(f'#ifdef {name}')
        write(f'    res = PyDict_SetItemString(result, ""{name}"", Py_True);')
        write('#else')
        write(f'    res = PyDict_SetItemString(result, ""{name}"", Py_False);')
        write('#endif')
        write('if (res) {')
        write('    Py_DECREF(result); return NULL;')
        write('}')
        write()",Generate/check the stable ABI list for documentation annotations,"def gen_testcapi_feature_macros ( manifest , args , outfile ) : write = partial ( print , file = outfile ) write ( ) write ( ) for macro in manifest . select ( { 'feature_macro' } ) : name = macro . name write ( f'#ifdef {name}' ) write ( f'    res = PyDict_SetItemString(result, ""{name}"", Py_True);' ) write ( '#else' ) write ( f'    res = PyDict_SetItemString(result, ""{name}"", Py_False);' ) write ( '#endif' ) write ( 'if (res) {' ) write ( '    Py_DECREF(result); return NULL;' ) write ( '}' ) write ( )",Generate/check the stable ABI list for documentation annotations
/sentry/src/sentry/data_secrecy/api/waive_data_secrecy.py,get,"def get(self, request: Request, organization: Organization) -> Response:
        """"""
        Returns the data secrecy waiver for an organization if it exists.
        """"""
        try:
            ds = get_object_or_404(DataSecrecyWaiver, organization=organization)
            return Response(
                serialize(ds, request.user, DataSecrecyWaiverSerializer()),
                status=status.HTTP_200_OK,
            )
        except Http404:
            return Response(
                {""detail"": ""No data secrecy waiver in place.""}, status=status.HTTP_404_NOT_FOUND
            )","def get(self, request: Request, organization: Organization) -> Response:
        """"""
        Returns the data secrecy waiver for an organization if it exists.
        """"""
        try:
            ds = get_object_or_404(DataSecrecyWaiver, organization=organization)
            return Response(
                serialize(ds, request.user, DataSecrecyWaiverSerializer()),
                status=status.HTTP_200_OK,
            )
        except Http404:
            return Response(
                {""detail"": ""No data secrecy waiver in place.""}, status=status.HTTP_404_NOT_FOUND
            )",Returns the data secrecy waiver for an organization if it exists.,Returns the data secrecy waiver for an organization if it exists.,"def get(self, request: Request, organization: Organization) -> Response:
        
        try:
            ds = get_object_or_404(DataSecrecyWaiver, organization=organization)
            return Response(
                serialize(ds, request.user, DataSecrecyWaiverSerializer()),
                status=status.HTTP_200_OK,
            )
        except Http404:
            return Response(
                {""detail"": ""No data secrecy waiver in place.""}, status=status.HTTP_404_NOT_FOUND
            )",Returns the data secrecy waiver for an organization if it exists.,"def get ( self , request : Request , organization : Organization ) -> Response : try : ds = get_object_or_404 ( DataSecrecyWaiver , organization = organization ) return Response ( serialize ( ds , request . user , DataSecrecyWaiverSerializer ( ) ) , status = status . HTTP_200_OK , ) except Http404 : return Response ( { ""detail"" : ""No data secrecy waiver in place."" } , status = status . HTTP_404_NOT_FOUND )",Returns the data secrecy waiver for an organization if it exists.
/ultralytics/ultralytics/data/utils.py,save_dataset_cache_file,"def save_dataset_cache_file(prefix: str, path: Path, x: Dict, version: str):
    """"""Save an Ultralytics dataset *.cache dictionary x to path.""""""
    x[""version""] = version  # add cache version
    if is_dir_writeable(path.parent):
        if path.exists():
            path.unlink()  # remove *.cache file if exists
        with open(str(path), ""wb"") as file:  # context manager here fixes windows async np.save bug
            np.save(file, x)
        LOGGER.info(f""{prefix}New cache created: {path}"")
    else:
        LOGGER.warning(f""{prefix}Cache directory {path.parent} is not writeable, cache not saved."")","def save_dataset_cache_file(prefix: str, path: Path, x: Dict, version: str):
    """"""Save an Ultralytics dataset *.cache dictionary x to path.""""""
    x[""version""] = version  # add cache version
    if is_dir_writeable(path.parent):
        if path.exists():
            path.unlink()  # remove *.cache file if exists
        with open(str(path), ""wb"") as file:  # context manager here fixes windows async np.save bug
            np.save(file, x)
        LOGGER.info(f""{prefix}New cache created: {path}"")
    else:
        LOGGER.warning(f""{prefix}Cache directory {path.parent} is not writeable, cache not saved."")",Save an Ultralytics dataset *.cache dictionary x to path.,Save an Ultralytics dataset *.cache dictionary x to path.,"def save_dataset_cache_file(prefix: str, path: Path, x: Dict, version: str):
    
    x[""version""] = version  # add cache version
    if is_dir_writeable(path.parent):
        if path.exists():
            path.unlink()  # remove *.cache file if exists
        with open(str(path), ""wb"") as file:  # context manager here fixes windows async np.save bug
            np.save(file, x)
        LOGGER.info(f""{prefix}New cache created: {path}"")
    else:
        LOGGER.warning(f""{prefix}Cache directory {path.parent} is not writeable, cache not saved."")",Save an Ultralytics dataset *.cache dictionary x to path.,"def save_dataset_cache_file ( prefix : str , path : Path , x : Dict , version : str ) : x [ ""version"" ] = version # add cache version if is_dir_writeable ( path . parent ) : if path . exists ( ) : path . unlink ( ) # remove *.cache file if exists with open ( str ( path ) , ""wb"" ) as file : # context manager here fixes windows async np.save bug np . save ( file , x ) LOGGER . info ( f""{prefix}New cache created: {path}"" ) else : LOGGER . warning ( f""{prefix}Cache directory {path.parent} is not writeable, cache not saved."" )",Save an Ultralytics dataset *.cache dictionary x to path.
/scrapy/tests/test_feedexport.py,run_and_export,"def run_and_export(self, spider_cls, settings):
        """"""Run spider with specified settings; return exported data.""""""

        FEEDS = settings.get(""FEEDS"") or {}
        settings[""FEEDS""] = {
            build_url(file_path): feed for file_path, feed in FEEDS.items()
        }
        content = defaultdict(list)
        spider_cls.start_urls = [self.mockserver.url(""/"")]
        crawler = get_crawler(spider_cls, settings)
        yield crawler.crawl()

        for path, feed in FEEDS.items():
            dir_name = Path(path).parent
            if not dir_name.exists():
                content[feed[""format""]] = []
                continue
            for file in sorted(dir_name.iterdir()):
                content[feed[""format""]].append(file.read_bytes())
        return content","def run_and_export(self, spider_cls, settings):
        """"""Run spider with specified settings; return exported data.""""""

        FEEDS = settings.get(""FEEDS"") or {}
        settings[""FEEDS""] = {
            build_url(file_path): feed for file_path, feed in FEEDS.items()
        }
        content = defaultdict(list)
        spider_cls.start_urls = [self.mockserver.url(""/"")]
        crawler = get_crawler(spider_cls, settings)
        yield crawler.crawl()

        for path, feed in FEEDS.items():
            dir_name = Path(path).parent
            if not dir_name.exists():
                content[feed[""format""]] = []
                continue
            for file in sorted(dir_name.iterdir()):
                content[feed[""format""]].append(file.read_bytes())
        return content",Run spider with specified settings; return exported data.,Run spider with specified settings; return exported data.,"def run_and_export(self, spider_cls, settings):
        

        FEEDS = settings.get(""FEEDS"") or {}
        settings[""FEEDS""] = {
            build_url(file_path): feed for file_path, feed in FEEDS.items()
        }
        content = defaultdict(list)
        spider_cls.start_urls = [self.mockserver.url(""/"")]
        crawler = get_crawler(spider_cls, settings)
        yield crawler.crawl()

        for path, feed in FEEDS.items():
            dir_name = Path(path).parent
            if not dir_name.exists():
                content[feed[""format""]] = []
                continue
            for file in sorted(dir_name.iterdir()):
                content[feed[""format""]].append(file.read_bytes())
        return content",Run spider with specified settings; return exported data.,"def run_and_export ( self , spider_cls , settings ) : FEEDS = settings . get ( ""FEEDS"" ) or { } settings [ ""FEEDS"" ] = { build_url ( file_path ) : feed for file_path , feed in FEEDS . items ( ) } content = defaultdict ( list ) spider_cls . start_urls = [ self . mockserver . url ( ""/"" ) ] crawler = get_crawler ( spider_cls , settings ) yield crawler . crawl ( ) for path , feed in FEEDS . items ( ) : dir_name = Path ( path ) . parent if not dir_name . exists ( ) : content [ feed [ ""format"" ] ] = [ ] continue for file in sorted ( dir_name . iterdir ( ) ) : content [ feed [ ""format"" ] ] . append ( file . read_bytes ( ) ) return content",Run spider with specified settings; return exported data.
/odoo/odoo/models.py,concat,"def concat(self, *args) -> Self:
        """""" Return the concatenation of ``self`` with all the arguments (in
            linear time complexity).
        """"""
        ids = list(self._ids)
        for arg in args:
            try:
                if arg._name != self._name:
                    raise TypeError(f""inconsistent models in: {self} + {arg}"")
                ids.extend(arg._ids)
            except AttributeError:
                raise TypeError(f""unsupported operand types in: {self} + {arg!r}"")
        return self.browse(ids)","def concat(self, *args) -> Self:
        """""" Return the concatenation of ``self`` with all the arguments (in
            linear time complexity).
        """"""
        ids = list(self._ids)
        for arg in args:
            try:
                if arg._name != self._name:
                    raise TypeError(f""inconsistent models in: {self} + {arg}"")
                ids.extend(arg._ids)
            except AttributeError:
                raise TypeError(f""unsupported operand types in: {self} + {arg!r}"")
        return self.browse(ids)","Return the concatenation of ``self`` with all the arguments (in
linear time complexity).",Return the concatenation of ``self`` with all the arguments (in linear time complexity).,"def concat(self, *args) -> Self:
        
        ids = list(self._ids)
        for arg in args:
            try:
                if arg._name != self._name:
                    raise TypeError(f""inconsistent models in: {self} + {arg}"")
                ids.extend(arg._ids)
            except AttributeError:
                raise TypeError(f""unsupported operand types in: {self} + {arg!r}"")
        return self.browse(ids)",Return the concatenation of ``self`` with all the arguments (in linear time complexity).,"def concat ( self , * args ) -> Self : ids = list ( self . _ids ) for arg in args : try : if arg . _name != self . _name : raise TypeError ( f""inconsistent models in: {self} + {arg}"" ) ids . extend ( arg . _ids ) except AttributeError : raise TypeError ( f""unsupported operand types in: {self} + {arg!r}"" ) return self . browse ( ids )",Return the concatenation of ``self`` with all the arguments (in linear time complexity).
/open-interpreter/interpreter/core/computer/terminal/languages/jupyter_language.py,insert_print_statement,"def insert_print_statement(self, line_number):
        """"""Inserts a print statement for a given line number.""""""
        return ast.Expr(
            value=ast.Call(
                func=ast.Name(id=""print"", ctx=ast.Load()),
                args=[ast.Constant(value=f""##active_line{line_number}##"")],
                keywords=[],
            )
        )","def insert_print_statement(self, line_number):
        """"""Inserts a print statement for a given line number.""""""
        return ast.Expr(
            value=ast.Call(
                func=ast.Name(id=""print"", ctx=ast.Load()),
                args=[ast.Constant(value=f""##active_line{line_number}##"")],
                keywords=[],
            )
        )",Inserts a print statement for a given line number.,Inserts a print statement for a given line number.,"def insert_print_statement(self, line_number):
        
        return ast.Expr(
            value=ast.Call(
                func=ast.Name(id=""print"", ctx=ast.Load()),
                args=[ast.Constant(value=f""##active_line{line_number}##"")],
                keywords=[],
            )
        )",Inserts a print statement for a given line number.,"def insert_print_statement ( self , line_number ) : return ast . Expr ( value = ast . Call ( func = ast . Name ( id = ""print"" , ctx = ast . Load ( ) ) , args = [ ast . Constant ( value = f""##active_line{line_number}##"" ) ] , keywords = [ ] , ) )",Inserts a print statement for a given line number.
/yolov5/utils/torch_utils.py,smart_resume,"def smart_resume(ckpt, optimizer, ema=None, weights=""yolov5s.pt"", epochs=300, resume=True):
    """"""Resumes training from a checkpoint, updating optimizer, ema, and epochs, with optional resume verification.""""""
    best_fitness = 0.0
    start_epoch = ckpt[""epoch""] + 1
    if ckpt[""optimizer""] is not None:
        optimizer.load_state_dict(ckpt[""optimizer""])  # optimizer
        best_fitness = ckpt[""best_fitness""]
    if ema and ckpt.get(""ema""):
        ema.ema.load_state_dict(ckpt[""ema""].float().state_dict())  # EMA
        ema.updates = ckpt[""updates""]
    if resume:
        assert start_epoch > 0, (
            f""{weights} training to {epochs} epochs is finished, nothing to resume.\n""
            f""Start a new training without --resume, i.e. 'python train.py --weights {weights}'""
        )
        LOGGER.info(f""Resuming training from {weights} from epoch {start_epoch} to {epochs} total epochs"")
    if epochs < start_epoch:
        LOGGER.info(f""{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs."")
        epochs += ckpt[""epoch""]  # finetune additional epochs
    return best_fitness, start_epoch, epochs","def smart_resume(ckpt, optimizer, ema=None, weights=""yolov5s.pt"", epochs=300, resume=True):
    """"""Resumes training from a checkpoint, updating optimizer, ema, and epochs, with optional resume verification.""""""
    best_fitness = 0.0
    start_epoch = ckpt[""epoch""] + 1
    if ckpt[""optimizer""] is not None:
        optimizer.load_state_dict(ckpt[""optimizer""])  # optimizer
        best_fitness = ckpt[""best_fitness""]
    if ema and ckpt.get(""ema""):
        ema.ema.load_state_dict(ckpt[""ema""].float().state_dict())  # EMA
        ema.updates = ckpt[""updates""]
    if resume:
        assert start_epoch > 0, (
            f""{weights} training to {epochs} epochs is finished, nothing to resume.\n""
            f""Start a new training without --resume, i.e. 'python train.py --weights {weights}'""
        )
        LOGGER.info(f""Resuming training from {weights} from epoch {start_epoch} to {epochs} total epochs"")
    if epochs < start_epoch:
        LOGGER.info(f""{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs."")
        epochs += ckpt[""epoch""]  # finetune additional epochs
    return best_fitness, start_epoch, epochs","Resumes training from a checkpoint, updating optimizer, ema, and epochs, with optional resume verification.","Resumes training from a checkpoint, updating optimizer, ema, and epochs, with optional resume verification.","def smart_resume(ckpt, optimizer, ema=None, weights=""yolov5s.pt"", epochs=300, resume=True):
    
    best_fitness = 0.0
    start_epoch = ckpt[""epoch""] + 1
    if ckpt[""optimizer""] is not None:
        optimizer.load_state_dict(ckpt[""optimizer""])  # optimizer
        best_fitness = ckpt[""best_fitness""]
    if ema and ckpt.get(""ema""):
        ema.ema.load_state_dict(ckpt[""ema""].float().state_dict())  # EMA
        ema.updates = ckpt[""updates""]
    if resume:
        assert start_epoch > 0, (
            f""{weights} training to {epochs} epochs is finished, nothing to resume.\n""
            f""Start a new training without --resume, i.e. 'python train.py --weights {weights}'""
        )
        LOGGER.info(f""Resuming training from {weights} from epoch {start_epoch} to {epochs} total epochs"")
    if epochs < start_epoch:
        LOGGER.info(f""{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs."")
        epochs += ckpt[""epoch""]  # finetune additional epochs
    return best_fitness, start_epoch, epochs","Resumes training from a checkpoint, updating optimizer, ema, and epochs, with optional resume verification.","def smart_resume ( ckpt , optimizer , ema = None , weights = ""yolov5s.pt"" , epochs = 300 , resume = True ) : best_fitness = 0.0 start_epoch = ckpt [ ""epoch"" ] + 1 if ckpt [ ""optimizer"" ] is not None : optimizer . load_state_dict ( ckpt [ ""optimizer"" ] ) # optimizer best_fitness = ckpt [ ""best_fitness"" ] if ema and ckpt . get ( ""ema"" ) : ema . ema . load_state_dict ( ckpt [ ""ema"" ] . float ( ) . state_dict ( ) ) # EMA ema . updates = ckpt [ ""updates"" ] if resume : assert start_epoch > 0 , ( f""{weights} training to {epochs} epochs is finished, nothing to resume.\n"" f""Start a new training without --resume, i.e. 'python train.py --weights {weights}'"" ) LOGGER . info ( f""Resuming training from {weights} from epoch {start_epoch} to {epochs} total epochs"" ) if epochs < start_epoch : LOGGER . info ( f""{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs."" ) epochs += ckpt [ ""epoch"" ] # finetune additional epochs return best_fitness , start_epoch , epochs","Resumes training from a checkpoint, updating optimizer, ema, and epochs, with optional resume verification."
/LLaMA-Factory/src/llamafactory/train/ppo/ppo_utils.py,get_rewards_from_server,"def get_rewards_from_server(server_url: str, messages: list[str]) -> list[""torch.Tensor""]:
    r""""""Get reward scores from the API server.""""""
    headers = {""Content-Type"": ""application/json""}
    payload = {""model"": ""model"", ""messages"": messages}
    response = requests.post(server_url, json=payload, headers=headers)
    rewards = json.loads(response.text)[""scores""]
    return torch.Tensor(rewards)","def get_rewards_from_server(server_url: str, messages: list[str]) -> list[""torch.Tensor""]:
    r""""""Get reward scores from the API server.""""""
    headers = {""Content-Type"": ""application/json""}
    payload = {""model"": ""model"", ""messages"": messages}
    response = requests.post(server_url, json=payload, headers=headers)
    rewards = json.loads(response.text)[""scores""]
    return torch.Tensor(rewards)",Get reward scores from the API server.,Get reward scores from the API server.,"def get_rewards_from_server(server_url: str, messages: list[str]) -> list[""torch.Tensor""]:
    
    headers = {""Content-Type"": ""application/json""}
    payload = {""model"": ""model"", ""messages"": messages}
    response = requests.post(server_url, json=payload, headers=headers)
    rewards = json.loads(response.text)[""scores""]
    return torch.Tensor(rewards)",Get reward scores from the API server.,"def get_rewards_from_server ( server_url : str , messages : list [ str ] ) -> list [ ""torch.Tensor"" ] : headers = { ""Content-Type"" : ""application/json"" } payload = { ""model"" : ""model"" , ""messages"" : messages } response = requests . post ( server_url , json = payload , headers = headers ) rewards = json . loads ( response . text ) [ ""scores"" ] return torch . Tensor ( rewards )",Get reward scores from the API server.
/faceswap/plugins/train/model/iae.py,_legacy_mapping,"def _legacy_mapping(self):
        """""" The mapping of legacy separate model names to single model names """"""
        return {f""{self.name}_encoder.h5"": ""encoder"",
                f""{self.name}_intermediate_A.h5"": ""inter_a"",
                f""{self.name}_intermediate_B.h5"": ""inter_b"",
                f""{self.name}_inter.h5"": ""inter_both"",
                f""{self.name}_decoder.h5"": ""decoder""}","def _legacy_mapping(self):
        """""" The mapping of legacy separate model names to single model names """"""
        return {f""{self.name}_encoder.h5"": ""encoder"",
                f""{self.name}_intermediate_A.h5"": ""inter_a"",
                f""{self.name}_intermediate_B.h5"": ""inter_b"",
                f""{self.name}_inter.h5"": ""inter_both"",
                f""{self.name}_decoder.h5"": ""decoder""}",The mapping of legacy separate model names to single model names,The mapping of legacy separate model names to single model names,"def _legacy_mapping(self):
        
        return {f""{self.name}_encoder.h5"": ""encoder"",
                f""{self.name}_intermediate_A.h5"": ""inter_a"",
                f""{self.name}_intermediate_B.h5"": ""inter_b"",
                f""{self.name}_inter.h5"": ""inter_both"",
                f""{self.name}_decoder.h5"": ""decoder""}",The mapping of legacy separate model names to single model names,"def _legacy_mapping ( self ) : return { f""{self.name}_encoder.h5"" : ""encoder"" , f""{self.name}_intermediate_A.h5"" : ""inter_a"" , f""{self.name}_intermediate_B.h5"" : ""inter_b"" , f""{self.name}_inter.h5"" : ""inter_both"" , f""{self.name}_decoder.h5"" : ""decoder"" }",The mapping of legacy separate model names to single model names
/flask/src/flask/ctx.py,pop,"def pop(self, exc: BaseException | None = _sentinel) -> None:  # type: ignore
        """"""Pops the app context.""""""
        try:
            if len(self._cv_tokens) == 1:
                if exc is _sentinel:
                    exc = sys.exc_info()[1]
                self.app.do_teardown_appcontext(exc)
        finally:
            ctx = _cv_app.get()
            _cv_app.reset(self._cv_tokens.pop())

        if ctx is not self:
            raise AssertionError(
                f""Popped wrong app context. ({ctx!r} instead of {self!r})""
            )

        appcontext_popped.send(self.app, _async_wrapper=self.app.ensure_sync)","def pop(self, exc: BaseException | None = _sentinel) -> None:  # type: ignore
        """"""Pops the app context.""""""
        try:
            if len(self._cv_tokens) == 1:
                if exc is _sentinel:
                    exc = sys.exc_info()[1]
                self.app.do_teardown_appcontext(exc)
        finally:
            ctx = _cv_app.get()
            _cv_app.reset(self._cv_tokens.pop())

        if ctx is not self:
            raise AssertionError(
                f""Popped wrong app context. ({ctx!r} instead of {self!r})""
            )

        appcontext_popped.send(self.app, _async_wrapper=self.app.ensure_sync)",Pops the app context.,Pops the app context.,"def pop(self, exc: BaseException | None = _sentinel) -> None:  # type: ignore
        
        try:
            if len(self._cv_tokens) == 1:
                if exc is _sentinel:
                    exc = sys.exc_info()[1]
                self.app.do_teardown_appcontext(exc)
        finally:
            ctx = _cv_app.get()
            _cv_app.reset(self._cv_tokens.pop())

        if ctx is not self:
            raise AssertionError(
                f""Popped wrong app context. ({ctx!r} instead of {self!r})""
            )

        appcontext_popped.send(self.app, _async_wrapper=self.app.ensure_sync)",Pops the app context.,"def pop ( self , exc : BaseException | None = _sentinel ) -> None : # type: ignore try : if len ( self . _cv_tokens ) == 1 : if exc is _sentinel : exc = sys . exc_info ( ) [ 1 ] self . app . do_teardown_appcontext ( exc ) finally : ctx = _cv_app . get ( ) _cv_app . reset ( self . _cv_tokens . pop ( ) ) if ctx is not self : raise AssertionError ( f""Popped wrong app context. ({ctx!r} instead of {self!r})"" ) appcontext_popped . send ( self . app , _async_wrapper = self . app . ensure_sync )",Pops the app context.
/streamlit/scripts/update_e2e_snapshots.py,get_last_commit_sha,"def get_last_commit_sha() -> str:
    """"""Get the last commit SHA of the local branch.""""""
    cmd = [""git"", ""rev-parse"", ""HEAD""]
    result = subprocess.run(cmd, capture_output=True, text=True, check=False)
    if result.returncode != 0:
        raise Exception(f""Error getting last commit SHA: {result.stderr.strip()}"")
    return result.stdout.strip()","def get_last_commit_sha() -> str:
    """"""Get the last commit SHA of the local branch.""""""
    cmd = [""git"", ""rev-parse"", ""HEAD""]
    result = subprocess.run(cmd, capture_output=True, text=True, check=False)
    if result.returncode != 0:
        raise Exception(f""Error getting last commit SHA: {result.stderr.strip()}"")
    return result.stdout.strip()",Get the last commit SHA of the local branch.,Get the last commit SHA of the local branch.,"def get_last_commit_sha() -> str:
    
    cmd = [""git"", ""rev-parse"", ""HEAD""]
    result = subprocess.run(cmd, capture_output=True, text=True, check=False)
    if result.returncode != 0:
        raise Exception(f""Error getting last commit SHA: {result.stderr.strip()}"")
    return result.stdout.strip()",Get the last commit SHA of the local branch.,"def get_last_commit_sha ( ) -> str : cmd = [ ""git"" , ""rev-parse"" , ""HEAD"" ] result = subprocess . run ( cmd , capture_output = True , text = True , check = False ) if result . returncode != 0 : raise Exception ( f""Error getting last commit SHA: {result.stderr.strip()}"" ) return result . stdout . strip ( )",Get the last commit SHA of the local branch.
/faceswap/setup.py,set_config,"def set_config(self) -> None:
        """""" Set the backend in the faceswap config file """"""
        config = {""backend"": self.backend}
        pypath = os.path.dirname(os.path.realpath(__file__))
        config_file = os.path.join(pypath, ""config"", "".faceswap"")
        with open(config_file, ""w"", encoding=""utf8"") as cnf:
            json.dump(config, cnf)
        logger.info(""Faceswap config written to: %s"", config_file)","def set_config(self) -> None:
        """""" Set the backend in the faceswap config file """"""
        config = {""backend"": self.backend}
        pypath = os.path.dirname(os.path.realpath(__file__))
        config_file = os.path.join(pypath, ""config"", "".faceswap"")
        with open(config_file, ""w"", encoding=""utf8"") as cnf:
            json.dump(config, cnf)
        logger.info(""Faceswap config written to: %s"", config_file)",Set the backend in the faceswap config file,Set the backend in the faceswap config file,"def set_config(self) -> None:
        
        config = {""backend"": self.backend}
        pypath = os.path.dirname(os.path.realpath(__file__))
        config_file = os.path.join(pypath, ""config"", "".faceswap"")
        with open(config_file, ""w"", encoding=""utf8"") as cnf:
            json.dump(config, cnf)
        logger.info(""Faceswap config written to: %s"", config_file)",Set the backend in the faceswap config file,"def set_config ( self ) -> None : config = { ""backend"" : self . backend } pypath = os . path . dirname ( os . path . realpath ( __file__ ) ) config_file = os . path . join ( pypath , ""config"" , "".faceswap"" ) with open ( config_file , ""w"" , encoding = ""utf8"" ) as cnf : json . dump ( config , cnf ) logger . info ( ""Faceswap config written to: %s"" , config_file )",Set the backend in the faceswap config file
/langflow/src/backend/tests/unit/graph/vertex/test_vertex_base.py,mock_vertex,"def mock_vertex() -> Mock:
    """"""Create a mock vertex for testing.""""""
    vertex = Mock(spec=Vertex)
    # Create a mock graph
    mock_graph = Mock()
    mock_graph.get_vertex = Mock(return_value=""source_vertex"")

    # Set the graph attribute on the vertex
    vertex.graph = mock_graph

    vertex.data = {
        ""node"": {
            ""template"": {
                ""test_field"": {""type"": ""str"", ""value"": ""test_value"", ""show"": True},
                ""file_field"": {""type"": ""file"", ""value"": None, ""file_path"": ""/test/path""},
                ""_type"": {""type"": ""str"", ""value"": ""test_type""},
            }
        }
    }
    vertex.id = ""test-vertex-id""
    vertex.display_name = ""Test Vertex""
    return vertex","def mock_vertex() -> Mock:
    """"""Create a mock vertex for testing.""""""
    vertex = Mock(spec=Vertex)
    # Create a mock graph
    mock_graph = Mock()
    mock_graph.get_vertex = Mock(return_value=""source_vertex"")

    # Set the graph attribute on the vertex
    vertex.graph = mock_graph

    vertex.data = {
        ""node"": {
            ""template"": {
                ""test_field"": {""type"": ""str"", ""value"": ""test_value"", ""show"": True},
                ""file_field"": {""type"": ""file"", ""value"": None, ""file_path"": ""/test/path""},
                ""_type"": {""type"": ""str"", ""value"": ""test_type""},
            }
        }
    }
    vertex.id = ""test-vertex-id""
    vertex.display_name = ""Test Vertex""
    return vertex",Create a mock vertex for testing.,Create a mock vertex for testing.,"def mock_vertex() -> Mock:
    
    vertex = Mock(spec=Vertex)
    # Create a mock graph
    mock_graph = Mock()
    mock_graph.get_vertex = Mock(return_value=""source_vertex"")

    # Set the graph attribute on the vertex
    vertex.graph = mock_graph

    vertex.data = {
        ""node"": {
            ""template"": {
                ""test_field"": {""type"": ""str"", ""value"": ""test_value"", ""show"": True},
                ""file_field"": {""type"": ""file"", ""value"": None, ""file_path"": ""/test/path""},
                ""_type"": {""type"": ""str"", ""value"": ""test_type""},
            }
        }
    }
    vertex.id = ""test-vertex-id""
    vertex.display_name = ""Test Vertex""
    return vertex",Create a mock vertex for testing.,"def mock_vertex ( ) -> Mock : vertex = Mock ( spec = Vertex ) # Create a mock graph mock_graph = Mock ( ) mock_graph . get_vertex = Mock ( return_value = ""source_vertex"" ) # Set the graph attribute on the vertex vertex . graph = mock_graph vertex . data = { ""node"" : { ""template"" : { ""test_field"" : { ""type"" : ""str"" , ""value"" : ""test_value"" , ""show"" : True } , ""file_field"" : { ""type"" : ""file"" , ""value"" : None , ""file_path"" : ""/test/path"" } , ""_type"" : { ""type"" : ""str"" , ""value"" : ""test_type"" } , } } } vertex . id = ""test-vertex-id"" vertex . display_name = ""Test Vertex"" return vertex",Create a mock vertex for testing.
/core/script/gen_requirements_all.py,generate_action_requirements_list,"def generate_action_requirements_list(reqs: dict[str, list[str]], action: str) -> str:
    """"""Generate a pip file based on requirements.""""""
    output = []
    for pkg, requirements in sorted(reqs.items(), key=itemgetter(0)):
        output.extend(f""\n# {req}"" for req in sorted(requirements))
        processed_pkg = process_action_requirement(pkg, action)
        output.append(f""\n{processed_pkg}\n"")
    return """".join(output)","def generate_action_requirements_list(reqs: dict[str, list[str]], action: str) -> str:
    """"""Generate a pip file based on requirements.""""""
    output = []
    for pkg, requirements in sorted(reqs.items(), key=itemgetter(0)):
        output.extend(f""\n# {req}"" for req in sorted(requirements))
        processed_pkg = process_action_requirement(pkg, action)
        output.append(f""\n{processed_pkg}\n"")
    return """".join(output)",Generate a pip file based on requirements.,Generate a pip file based on requirements.,"def generate_action_requirements_list(reqs: dict[str, list[str]], action: str) -> str:
    
    output = []
    for pkg, requirements in sorted(reqs.items(), key=itemgetter(0)):
        output.extend(f""\n# {req}"" for req in sorted(requirements))
        processed_pkg = process_action_requirement(pkg, action)
        output.append(f""\n{processed_pkg}\n"")
    return """".join(output)",Generate a pip file based on requirements.,"def generate_action_requirements_list ( reqs : dict [ str , list [ str ] ] , action : str ) -> str : output = [ ] for pkg , requirements in sorted ( reqs . items ( ) , key = itemgetter ( 0 ) ) : output . extend ( f""\n# {req}"" for req in sorted ( requirements ) ) processed_pkg = process_action_requirement ( pkg , action ) output . append ( f""\n{processed_pkg}\n"" ) return """" . join ( output )",Generate a pip file based on requirements.
/cheat.sh/lib/adapter/adapter.py,all_adapters,"def all_adapters(as_dict=False):
    """"""
    Return list of all known adapters
    If `as_dict` is True, return dict {'name': adapter} instead of a list.
    """"""
    def _all_subclasses(cls):
        return set(cls.__subclasses__()).union(set(
            [s for c in cls.__subclasses__() for s in _all_subclasses(c)]
        ))

    if as_dict:
        return {x.name():x for x in _all_subclasses(Adapter)}
    return list(_all_subclasses(Adapter))","def all_adapters(as_dict=False):
    """"""
    Return list of all known adapters
    If `as_dict` is True, return dict {'name': adapter} instead of a list.
    """"""
    def _all_subclasses(cls):
        return set(cls.__subclasses__()).union(set(
            [s for c in cls.__subclasses__() for s in _all_subclasses(c)]
        ))

    if as_dict:
        return {x.name():x for x in _all_subclasses(Adapter)}
    return list(_all_subclasses(Adapter))","Return list of all known adapters
If `as_dict` is True, return dict {'name': adapter} instead of a list.",Return list of all known adapters,"def all_adapters(as_dict=False):
    
    def _all_subclasses(cls):
        return set(cls.__subclasses__()).union(set(
            [s for c in cls.__subclasses__() for s in _all_subclasses(c)]
        ))

    if as_dict:
        return {x.name():x for x in _all_subclasses(Adapter)}
    return list(_all_subclasses(Adapter))",Return list of all known adapters,def all_adapters ( as_dict = False ) : def _all_subclasses ( cls ) : return set ( cls . __subclasses__ ( ) ) . union ( set ( [ s for c in cls . __subclasses__ ( ) for s in _all_subclasses ( c ) ] ) ) if as_dict : return { x . name ( ) : x for x in _all_subclasses ( Adapter ) } return list ( _all_subclasses ( Adapter ) ),Return list of all known adapters
/core/script/gen_requirements_all.py,gather_constraints,"def gather_constraints() -> str:
    """"""Construct output for constraint file.""""""
    return (
        GENERATED_MESSAGE
        + ""\n"".join(
            [
                *sorted(
                    {
                        *core_requirements(),
                        *gather_recursive_requirements(""default_config""),
                        *gather_recursive_requirements(""mqtt""),
                    },
                    key=str.lower,
                ),
                """",
            ]
        )
        + CONSTRAINT_BASE
    )","def gather_constraints() -> str:
    """"""Construct output for constraint file.""""""
    return (
        GENERATED_MESSAGE
        + ""\n"".join(
            [
                *sorted(
                    {
                        *core_requirements(),
                        *gather_recursive_requirements(""default_config""),
                        *gather_recursive_requirements(""mqtt""),
                    },
                    key=str.lower,
                ),
                """",
            ]
        )
        + CONSTRAINT_BASE
    )",Construct output for constraint file.,Construct output for constraint file.,"def gather_constraints() -> str:
    
    return (
        GENERATED_MESSAGE
        + ""\n"".join(
            [
                *sorted(
                    {
                        *core_requirements(),
                        *gather_recursive_requirements(""default_config""),
                        *gather_recursive_requirements(""mqtt""),
                    },
                    key=str.lower,
                ),
                """",
            ]
        )
        + CONSTRAINT_BASE
    )",Construct output for constraint file.,"def gather_constraints ( ) -> str : return ( GENERATED_MESSAGE + ""\n"" . join ( [ * sorted ( { * core_requirements ( ) , * gather_recursive_requirements ( ""default_config"" ) , * gather_recursive_requirements ( ""mqtt"" ) , } , key = str . lower , ) , """" , ] ) + CONSTRAINT_BASE )",Construct output for constraint file.
/OpenManus/app/flow/planning.py,get_status_marks,"def get_status_marks(cls) -> Dict[str, str]:
        """"""Return a mapping of statuses to their marker symbols""""""
        return {
            cls.COMPLETED.value: ""[✓]"",
            cls.IN_PROGRESS.value: ""[→]"",
            cls.BLOCKED.value: ""[!]"",
            cls.NOT_STARTED.value: ""[ ]"",
        }","def get_status_marks(cls) -> Dict[str, str]:
        """"""Return a mapping of statuses to their marker symbols""""""
        return {
            cls.COMPLETED.value: ""[✓]"",
            cls.IN_PROGRESS.value: ""[→]"",
            cls.BLOCKED.value: ""[!]"",
            cls.NOT_STARTED.value: ""[ ]"",
        }",Return a mapping of statuses to their marker symbols,Return a mapping of statuses to their marker symbols,"def get_status_marks(cls) -> Dict[str, str]:
        
        return {
            cls.COMPLETED.value: ""[✓]"",
            cls.IN_PROGRESS.value: ""[→]"",
            cls.BLOCKED.value: ""[!]"",
            cls.NOT_STARTED.value: ""[ ]"",
        }",Return a mapping of statuses to their marker symbols,"def get_status_marks ( cls ) -> Dict [ str , str ] : return { cls . COMPLETED . value : ""[✓]"" , cls . IN_PROGRESS . value : ""[→]"" , cls . BLOCKED . value : ""[!]"" , cls . NOT_STARTED . value : ""[ ]"" , }",Return a mapping of statuses to their marker symbols
/Fooocus/ldm_patched/k_diffusion/sampling.py,sample_euler_ancestral,"def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):
    """"""Ancestral sampling with Euler method steps.""""""
    extra_args = {} if extra_args is None else extra_args
    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler
    s_in = x.new_ones([x.shape[0]])
    for i in trange(len(sigmas) - 1, disable=disable):
        denoised = model(x, sigmas[i] * s_in, **extra_args)
        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)
        if callback is not None:
            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})
        d = to_d(x, sigmas[i], denoised)
        # Euler method
        dt = sigma_down - sigmas[i]
        x = x + d * dt
        if sigmas[i + 1] > 0:
            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up
    return x","def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):
    """"""Ancestral sampling with Euler method steps.""""""
    extra_args = {} if extra_args is None else extra_args
    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler
    s_in = x.new_ones([x.shape[0]])
    for i in trange(len(sigmas) - 1, disable=disable):
        denoised = model(x, sigmas[i] * s_in, **extra_args)
        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)
        if callback is not None:
            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})
        d = to_d(x, sigmas[i], denoised)
        # Euler method
        dt = sigma_down - sigmas[i]
        x = x + d * dt
        if sigmas[i + 1] > 0:
            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up
    return x",Ancestral sampling with Euler method steps.,Ancestral sampling with Euler method steps.,"def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):
    
    extra_args = {} if extra_args is None else extra_args
    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler
    s_in = x.new_ones([x.shape[0]])
    for i in trange(len(sigmas) - 1, disable=disable):
        denoised = model(x, sigmas[i] * s_in, **extra_args)
        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)
        if callback is not None:
            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})
        d = to_d(x, sigmas[i], denoised)
        # Euler method
        dt = sigma_down - sigmas[i]
        x = x + d * dt
        if sigmas[i + 1] > 0:
            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up
    return x",Ancestral sampling with Euler method steps.,"def sample_euler_ancestral ( model , x , sigmas , extra_args = None , callback = None , disable = None , eta = 1. , s_noise = 1. , noise_sampler = None ) : extra_args = { } if extra_args is None else extra_args noise_sampler = default_noise_sampler ( x ) if noise_sampler is None else noise_sampler s_in = x . new_ones ( [ x . shape [ 0 ] ] ) for i in trange ( len ( sigmas ) - 1 , disable = disable ) : denoised = model ( x , sigmas [ i ] * s_in , ** extra_args ) sigma_down , sigma_up = get_ancestral_step ( sigmas [ i ] , sigmas [ i + 1 ] , eta = eta ) if callback is not None : callback ( { 'x' : x , 'i' : i , 'sigma' : sigmas [ i ] , 'sigma_hat' : sigmas [ i ] , 'denoised' : denoised } ) d = to_d ( x , sigmas [ i ] , denoised ) # Euler method dt = sigma_down - sigmas [ i ] x = x + d * dt if sigmas [ i + 1 ] > 0 : x = x + noise_sampler ( sigmas [ i ] , sigmas [ i + 1 ] ) * s_noise * sigma_up return x",Ancestral sampling with Euler method steps.
/odoo/odoo/fields.py,_inverse_related,"def _inverse_related(self, records):
        """""" Inverse the related field ``self`` on ``records``. """"""
        # store record values, otherwise they may be lost by cache invalidation!
        record_value = {record: record[self.name] for record in records}
        for record in records:
            target, field = self.traverse_related(record)
            # update 'target' only if 'record' and 'target' are both real or
            # both new (see `test_base_objects.py`, `test_basic`)
            if target and bool(target.id) == bool(record.id):
                target[field.name] = record_value[record]","def _inverse_related(self, records):
        """""" Inverse the related field ``self`` on ``records``. """"""
        # store record values, otherwise they may be lost by cache invalidation!
        record_value = {record: record[self.name] for record in records}
        for record in records:
            target, field = self.traverse_related(record)
            # update 'target' only if 'record' and 'target' are both real or
            # both new (see `test_base_objects.py`, `test_basic`)
            if target and bool(target.id) == bool(record.id):
                target[field.name] = record_value[record]",Inverse the related field ``self`` on ``records``.,Inverse the related field ``self`` on ``records``.,"def _inverse_related(self, records):
        
        # store record values, otherwise they may be lost by cache invalidation!
        record_value = {record: record[self.name] for record in records}
        for record in records:
            target, field = self.traverse_related(record)
            # update 'target' only if 'record' and 'target' are both real or
            # both new (see `test_base_objects.py`, `test_basic`)
            if target and bool(target.id) == bool(record.id):
                target[field.name] = record_value[record]",Inverse the related field ``self`` on ``records``.,"def _inverse_related ( self , records ) : # store record values, otherwise they may be lost by cache invalidation! record_value = { record : record [ self . name ] for record in records } for record in records : target , field = self . traverse_related ( record ) # update 'target' only if 'record' and 'target' are both real or # both new (see `test_base_objects.py`, `test_basic`) if target and bool ( target . id ) == bool ( record . id ) : target [ field . name ] = record_value [ record ]",Inverse the related field ``self`` on ``records``.
