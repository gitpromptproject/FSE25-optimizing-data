file,function_name,raw_code,clean_code,raw_docstring,clean_docstring,input_code,summary,code_tokens,docstring_tokens
/preswald/preswald/interfaces/workflow.py,register_component_producer,"def register_component_producer(self, component_id: str, atom_name: str):
        """"""Link a component ID to its producing atom for DAG traceability.""""""
        logger.info(f""[DAG] Registering component producer {component_id=} {atom_name=}"")
        if atom_name in self.atoms:
            self._component_producers[component_id] = atom_name
            if self._current_atom:
                logger.info(f""[DAG] Component registered while atom was active self._current_atom={self._current_atom}"")
        else:
            logger.warning(f""[DAG] Skipping producer registration for unknown atom {atom_name=}"")","def register_component_producer(self, component_id: str, atom_name: str):
        """"""Link a component ID to its producing atom for DAG traceability.""""""
        logger.info(f""[DAG] Registering component producer {component_id=} {atom_name=}"")
        if atom_name in self.atoms:
            self._component_producers[component_id] = atom_name
            if self._current_atom:
                logger.info(f""[DAG] Component registered while atom was active self._current_atom={self._current_atom}"")
        else:
            logger.warning(f""[DAG] Skipping producer registration for unknown atom {atom_name=}"")",Link a component ID to its producing atom for DAG traceability.,Link a component ID to its producing atom for DAG traceability.,"def register_component_producer(self, component_id: str, atom_name: str):
        
        logger.info(f""[DAG] Registering component producer {component_id=} {atom_name=}"")
        if atom_name in self.atoms:
            self._component_producers[component_id] = atom_name
            if self._current_atom:
                logger.info(f""[DAG] Component registered while atom was active self._current_atom={self._current_atom}"")
        else:
            logger.warning(f""[DAG] Skipping producer registration for unknown atom {atom_name=}"")",Link a component ID to its producing atom for DAG traceability.,"def register_component_producer ( self , component_id : str , atom_name : str ) : logger . info ( f""[DAG] Registering component producer {component_id=} {atom_name=}"" ) if atom_name in self . atoms : self . _component_producers [ component_id ] = atom_name if self . _current_atom : logger . info ( f""[DAG] Component registered while atom was active self._current_atom={self._current_atom}"" ) else : logger . warning ( f""[DAG] Skipping producer registration for unknown atom {atom_name=}"" )",Link a component ID to its producing atom for DAG traceability.
/LightRAG/examples/graph_visual_with_neo4j.py,process_in_batches,"def process_in_batches(tx, query, data, batch_size):
    """"""Process data in batches and execute the given query.""""""
    for i in range(0, len(data), batch_size):
        batch = data[i : i + batch_size]
        tx.run(query, {""nodes"": batch} if ""nodes"" in query else {""edges"": batch})","def process_in_batches(tx, query, data, batch_size):
    """"""Process data in batches and execute the given query.""""""
    for i in range(0, len(data), batch_size):
        batch = data[i : i + batch_size]
        tx.run(query, {""nodes"": batch} if ""nodes"" in query else {""edges"": batch})",Process data in batches and execute the given query.,Process data in batches and execute the given query.,"def process_in_batches(tx, query, data, batch_size):
    
    for i in range(0, len(data), batch_size):
        batch = data[i : i + batch_size]
        tx.run(query, {""nodes"": batch} if ""nodes"" in query else {""edges"": batch})",Process data in batches and execute the given query.,"def process_in_batches ( tx , query , data , batch_size ) : for i in range ( 0 , len ( data ) , batch_size ) : batch = data [ i : i + batch_size ] tx . run ( query , { ""nodes"" : batch } if ""nodes"" in query else { ""edges"" : batch } )",Process data in batches and execute the given query.
/ha_xiaomi_home/custom_components/xiaomi_home/miot/miot_spec.py,filter_property,"def filter_property(self, siid: int, piid: int) -> bool:
        """"""Filter property by piid.
        MUST call init_async() and set_spec_spec() first.""""""
        if (self._cache and 'properties' in self._cache and
            (f'{siid}.{piid}' in self._cache['properties'] or
             f'{siid}.*' in self._cache['properties'])):
            return True
        return False","def filter_property(self, siid: int, piid: int) -> bool:
        """"""Filter property by piid.
        MUST call init_async() and set_spec_spec() first.""""""
        if (self._cache and 'properties' in self._cache and
            (f'{siid}.{piid}' in self._cache['properties'] or
             f'{siid}.*' in self._cache['properties'])):
            return True
        return False","Filter property by piid.
MUST call init_async() and set_spec_spec() first.",Filter property by piid.,"def filter_property(self, siid: int, piid: int) -> bool:
        
        if (self._cache and 'properties' in self._cache and
            (f'{siid}.{piid}' in self._cache['properties'] or
             f'{siid}.*' in self._cache['properties'])):
            return True
        return False",Filter property by piid.,"def filter_property ( self , siid : int , piid : int ) -> bool : if ( self . _cache and 'properties' in self . _cache and ( f'{siid}.{piid}' in self . _cache [ 'properties' ] or f'{siid}.*' in self . _cache [ 'properties' ] ) ) : return True return False",Filter property by piid.
/ag2/autogen/agentchat/contrib/graph_rag/falkor_graph_query_engine.py,_save_ontology_to_db,"def _save_ontology_to_db(self, ontology: ""Ontology"") -> None:  # type: ignore[no-any-unimported]
        """"""Save graph ontology to a separate table with {graph_name}_ontology""""""
        if self.ontology_table_name in self.falkordb.list_graphs():
            raise ValueError(f""Knowledge graph {self.name} is already created."")
        graph = self.__get_ontology_storage_graph()
        ontology.save_to_graph(graph)","def _save_ontology_to_db(self, ontology: ""Ontology"") -> None:  # type: ignore[no-any-unimported]
        """"""Save graph ontology to a separate table with {graph_name}_ontology""""""
        if self.ontology_table_name in self.falkordb.list_graphs():
            raise ValueError(f""Knowledge graph {self.name} is already created."")
        graph = self.__get_ontology_storage_graph()
        ontology.save_to_graph(graph)",Save graph ontology to a separate table with {graph_name}_ontology,Save graph ontology to a separate table with {graph_name}_ontology,"def _save_ontology_to_db(self, ontology: ""Ontology"") -> None:  # type: ignore[no-any-unimported]
        
        if self.ontology_table_name in self.falkordb.list_graphs():
            raise ValueError(f""Knowledge graph {self.name} is already created."")
        graph = self.__get_ontology_storage_graph()
        ontology.save_to_graph(graph)",Save graph ontology to a separate table with {graph_name}_ontology,"def _save_ontology_to_db ( self , ontology : ""Ontology"" ) -> None : # type: ignore[no-any-unimported] if self . ontology_table_name in self . falkordb . list_graphs ( ) : raise ValueError ( f""Knowledge graph {self.name} is already created."" ) graph = self . __get_ontology_storage_graph ( ) ontology . save_to_graph ( graph )",Save graph ontology to a separate table with {graph_name}_ontology
/nv-ingest/src/nv_ingest/framework/orchestration/morpheus/stages/meta/multiprocessing_stage.py,_build_merge_function,"def _build_merge_function(self):
        """"""
        Adds tracing metadata to the control message and marks its completion.
        """"""

        def merge_fn(ctrl_msg: IngestControlMessage):
            do_trace_tagging = ctrl_msg.get_metadata(""config::add_trace_tagging"") is True
            if do_trace_tagging:
                ts_exit = datetime.now()
                ctrl_msg.set_timestamp(f""trace::exit::{self._task_desc}"", ts_exit)
                ctrl_msg.set_timestamp(""latency::ts_send"", ts_exit)
            return ctrl_msg

        return merge_fn","def _build_merge_function(self):
        """"""
        Adds tracing metadata to the control message and marks its completion.
        """"""

        def merge_fn(ctrl_msg: IngestControlMessage):
            do_trace_tagging = ctrl_msg.get_metadata(""config::add_trace_tagging"") is True
            if do_trace_tagging:
                ts_exit = datetime.now()
                ctrl_msg.set_timestamp(f""trace::exit::{self._task_desc}"", ts_exit)
                ctrl_msg.set_timestamp(""latency::ts_send"", ts_exit)
            return ctrl_msg

        return merge_fn",Adds tracing metadata to the control message and marks its completion.,Adds tracing metadata to the control message and marks its completion.,"def _build_merge_function(self):
        

        def merge_fn(ctrl_msg: IngestControlMessage):
            do_trace_tagging = ctrl_msg.get_metadata(""config::add_trace_tagging"") is True
            if do_trace_tagging:
                ts_exit = datetime.now()
                ctrl_msg.set_timestamp(f""trace::exit::{self._task_desc}"", ts_exit)
                ctrl_msg.set_timestamp(""latency::ts_send"", ts_exit)
            return ctrl_msg

        return merge_fn",Adds tracing metadata to the control message and marks its completion.,"def _build_merge_function ( self ) : def merge_fn ( ctrl_msg : IngestControlMessage ) : do_trace_tagging = ctrl_msg . get_metadata ( ""config::add_trace_tagging"" ) is True if do_trace_tagging : ts_exit = datetime . now ( ) ctrl_msg . set_timestamp ( f""trace::exit::{self._task_desc}"" , ts_exit ) ctrl_msg . set_timestamp ( ""latency::ts_send"" , ts_exit ) return ctrl_msg return merge_fn",Adds tracing metadata to the control message and marks its completion.
/airweave/fern/scripts/generate_openapi.py,filter_paths,"def filter_paths(openapi_schema: Dict[str, Any]) -> Dict[str, Any]:
    """"""Filter OpenAPI paths to only include allowed endpoints.""""""
    if ""paths"" not in openapi_schema:
        return openapi_schema

    filtered_paths = {}
    for path, path_item in openapi_schema[""paths""].items():
        include_path = False
        filtered_operations = {}

        for method, operation in path_item.items():
            if method.lower() in [""get"", ""post"", ""put"", ""delete"", ""patch""] and is_included_endpoint(
                path, method
            ):
                filtered_operations[method] = operation
                include_path = True

        if include_path:
            filtered_paths[path] = filtered_operations

    openapi_schema[""paths""] = filtered_paths
    return openapi_schema","def filter_paths(openapi_schema: Dict[str, Any]) -> Dict[str, Any]:
    """"""Filter OpenAPI paths to only include allowed endpoints.""""""
    if ""paths"" not in openapi_schema:
        return openapi_schema

    filtered_paths = {}
    for path, path_item in openapi_schema[""paths""].items():
        include_path = False
        filtered_operations = {}

        for method, operation in path_item.items():
            if method.lower() in [""get"", ""post"", ""put"", ""delete"", ""patch""] and is_included_endpoint(
                path, method
            ):
                filtered_operations[method] = operation
                include_path = True

        if include_path:
            filtered_paths[path] = filtered_operations

    openapi_schema[""paths""] = filtered_paths
    return openapi_schema",Filter OpenAPI paths to only include allowed endpoints.,Filter OpenAPI paths to only include allowed endpoints.,"def filter_paths(openapi_schema: Dict[str, Any]) -> Dict[str, Any]:
    
    if ""paths"" not in openapi_schema:
        return openapi_schema

    filtered_paths = {}
    for path, path_item in openapi_schema[""paths""].items():
        include_path = False
        filtered_operations = {}

        for method, operation in path_item.items():
            if method.lower() in [""get"", ""post"", ""put"", ""delete"", ""patch""] and is_included_endpoint(
                path, method
            ):
                filtered_operations[method] = operation
                include_path = True

        if include_path:
            filtered_paths[path] = filtered_operations

    openapi_schema[""paths""] = filtered_paths
    return openapi_schema",Filter OpenAPI paths to only include allowed endpoints.,"def filter_paths ( openapi_schema : Dict [ str , Any ] ) -> Dict [ str , Any ] : if ""paths"" not in openapi_schema : return openapi_schema filtered_paths = { } for path , path_item in openapi_schema [ ""paths"" ] . items ( ) : include_path = False filtered_operations = { } for method , operation in path_item . items ( ) : if method . lower ( ) in [ ""get"" , ""post"" , ""put"" , ""delete"" , ""patch"" ] and is_included_endpoint ( path , method ) : filtered_operations [ method ] = operation include_path = True if include_path : filtered_paths [ path ] = filtered_operations openapi_schema [ ""paths"" ] = filtered_paths return openapi_schema",Filter OpenAPI paths to only include allowed endpoints.
/airweave/backend/airweave/platform/sync/worker_pool.py,_handle_task_completion,"def _handle_task_completion(self, task: asyncio.Task) -> None:
        """"""Handle task completion and clean up.""""""
        task_id = getattr(task, ""task_id"", ""unknown"")
        self.pending_tasks.discard(task)

        if task.cancelled():
            logger.warning(f""🚫 WORKER_CANCELLED [{task_id}] Task was cancelled"")
        elif task.exception() is not None:
            logger.error(
                f""💥 WORKER_EXCEPTION [{task_id}] Task completed with exception: {task.exception()}""
            )
        else:
            logger.info(f""🏁 WORKER_CLEANUP [{task_id}] Task cleaned up successfully"")","def _handle_task_completion(self, task: asyncio.Task) -> None:
        """"""Handle task completion and clean up.""""""
        task_id = getattr(task, ""task_id"", ""unknown"")
        self.pending_tasks.discard(task)

        if task.cancelled():
            logger.warning(f""🚫 WORKER_CANCELLED [{task_id}] Task was cancelled"")
        elif task.exception() is not None:
            logger.error(
                f""💥 WORKER_EXCEPTION [{task_id}] Task completed with exception: {task.exception()}""
            )
        else:
            logger.info(f""🏁 WORKER_CLEANUP [{task_id}] Task cleaned up successfully"")",Handle task completion and clean up.,Handle task completion and clean up.,"def _handle_task_completion(self, task: asyncio.Task) -> None:
        
        task_id = getattr(task, ""task_id"", ""unknown"")
        self.pending_tasks.discard(task)

        if task.cancelled():
            logger.warning(f""🚫 WORKER_CANCELLED [{task_id}] Task was cancelled"")
        elif task.exception() is not None:
            logger.error(
                f""💥 WORKER_EXCEPTION [{task_id}] Task completed with exception: {task.exception()}""
            )
        else:
            logger.info(f""🏁 WORKER_CLEANUP [{task_id}] Task cleaned up successfully"")",Handle task completion and clean up.,"def _handle_task_completion ( self , task : asyncio . Task ) -> None : task_id = getattr ( task , ""task_id"" , ""unknown"" ) self . pending_tasks . discard ( task ) if task . cancelled ( ) : logger . warning ( f""🚫 WORKER_CANCELLED [{task_id}] Task was cancelled"" ) elif task . exception ( ) is not None : logger . error ( f""💥 WORKER_EXCEPTION [{task_id}] Task completed with exception: {task.exception()}"" ) else : logger . info ( f""🏁 WORKER_CLEANUP [{task_id}] Task cleaned up successfully"" )",Handle task completion and clean up.
/python-sdk/src/mcp/server/fastmcp/utilities/types.py,_get_mime_type,"def _get_mime_type(self) -> str:
        """"""Get MIME type from format or guess from file extension.""""""
        if self._format:
            return f""image/{self._format.lower()}""

        if self.path:
            suffix = self.path.suffix.lower()
            return {
                "".png"": ""image/png"",
                "".jpg"": ""image/jpeg"",
                "".jpeg"": ""image/jpeg"",
                "".gif"": ""image/gif"",
                "".webp"": ""image/webp"",
            }.get(suffix, ""application/octet-stream"")
        return ""image/png""","def _get_mime_type(self) -> str:
        """"""Get MIME type from format or guess from file extension.""""""
        if self._format:
            return f""image/{self._format.lower()}""

        if self.path:
            suffix = self.path.suffix.lower()
            return {
                "".png"": ""image/png"",
                "".jpg"": ""image/jpeg"",
                "".jpeg"": ""image/jpeg"",
                "".gif"": ""image/gif"",
                "".webp"": ""image/webp"",
            }.get(suffix, ""application/octet-stream"")
        return ""image/png""",Get MIME type from format or guess from file extension.,Get MIME type from format or guess from file extension.,"def _get_mime_type(self) -> str:
        
        if self._format:
            return f""image/{self._format.lower()}""

        if self.path:
            suffix = self.path.suffix.lower()
            return {
                "".png"": ""image/png"",
                "".jpg"": ""image/jpeg"",
                "".jpeg"": ""image/jpeg"",
                "".gif"": ""image/gif"",
                "".webp"": ""image/webp"",
            }.get(suffix, ""application/octet-stream"")
        return ""image/png""",Get MIME type from format or guess from file extension.,"def _get_mime_type ( self ) -> str : if self . _format : return f""image/{self._format.lower()}"" if self . path : suffix = self . path . suffix . lower ( ) return { "".png"" : ""image/png"" , "".jpg"" : ""image/jpeg"" , "".jpeg"" : ""image/jpeg"" , "".gif"" : ""image/gif"" , "".webp"" : ""image/webp"" , } . get ( suffix , ""application/octet-stream"" ) return ""image/png""",Get MIME type from format or guess from file extension.
/ag2/autogen/mcp/mcp_proxy/mcp_proxy.py,__init__,"def __init__(self, servers: list[dict[str, Any]], title: Optional[str] = None, **kwargs: Any) -> None:
        """"""Proxy class to generate client from OpenAPI schema.""""""
        self._servers = servers
        self._title = title or ""MCP Proxy""
        self._kwargs = kwargs
        self._registered_funcs: list[Callable[..., Any]] = []
        self._globals: dict[str, Any] = {}

        self._security: dict[str, list[BaseSecurity]] = {}
        self._security_params: dict[Optional[str], BaseSecurityParameters] = {}
        self._tags: set[str] = set()

        self._function_group: dict[str, list[str]] = {}","def __init__(self, servers: list[dict[str, Any]], title: Optional[str] = None, **kwargs: Any) -> None:
        """"""Proxy class to generate client from OpenAPI schema.""""""
        self._servers = servers
        self._title = title or ""MCP Proxy""
        self._kwargs = kwargs
        self._registered_funcs: list[Callable[..., Any]] = []
        self._globals: dict[str, Any] = {}

        self._security: dict[str, list[BaseSecurity]] = {}
        self._security_params: dict[Optional[str], BaseSecurityParameters] = {}
        self._tags: set[str] = set()

        self._function_group: dict[str, list[str]] = {}",Proxy class to generate client from OpenAPI schema.,Proxy class to generate client from OpenAPI schema.,"def __init__(self, servers: list[dict[str, Any]], title: Optional[str] = None, **kwargs: Any) -> None:
        
        self._servers = servers
        self._title = title or ""MCP Proxy""
        self._kwargs = kwargs
        self._registered_funcs: list[Callable[..., Any]] = []
        self._globals: dict[str, Any] = {}

        self._security: dict[str, list[BaseSecurity]] = {}
        self._security_params: dict[Optional[str], BaseSecurityParameters] = {}
        self._tags: set[str] = set()

        self._function_group: dict[str, list[str]] = {}",Proxy class to generate client from OpenAPI schema.,"def __init__ ( self , servers : list [ dict [ str , Any ] ] , title : Optional [ str ] = None , ** kwargs : Any ) -> None : self . _servers = servers self . _title = title or ""MCP Proxy"" self . _kwargs = kwargs self . _registered_funcs : list [ Callable [ ... , Any ] ] = [ ] self . _globals : dict [ str , Any ] = { } self . _security : dict [ str , list [ BaseSecurity ] ] = { } self . _security_params : dict [ Optional [ str ] , BaseSecurityParameters ] = { } self . _tags : set [ str ] = set ( ) self . _function_group : dict [ str , list [ str ] ] = { }",Proxy class to generate client from OpenAPI schema.
/LightRAG/lightrag/api/run_with_gunicorn.py,check_and_install_dependencies,"def check_and_install_dependencies():
    """"""Check and install required dependencies""""""
    required_packages = [
        ""gunicorn"",
        ""tiktoken"",
        ""psutil"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")","def check_and_install_dependencies():
    """"""Check and install required dependencies""""""
    required_packages = [
        ""gunicorn"",
        ""tiktoken"",
        ""psutil"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")",Check and install required dependencies,Check and install required dependencies,"def check_and_install_dependencies():
    
    required_packages = [
        ""gunicorn"",
        ""tiktoken"",
        ""psutil"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")",Check and install required dependencies,"def check_and_install_dependencies ( ) : required_packages = [ ""gunicorn"" , ""tiktoken"" , ""psutil"" , # Add other required packages here ] for package in required_packages : if not pm . is_installed ( package ) : print ( f""Installing {package}..."" ) pm . install ( package ) print ( f""{package} installed successfully"" )",Check and install required dependencies
/olmocr/olmocr/eval/dolma_refine/registry.py,get,"def get(cls, name: str, raise_on_missing: bool = True) -> Optional[T]:
        """"""Get a tagger from the registry; raise ValueError if it doesn't exist.""""""

        matches = [registered for registered in cls._get_storage() if re.match(registered, name)]

        if len(matches) > 1:
            raise ValueError(f""Multiple taggers match {name}: {', '.join(matches)}"")

        elif len(matches) == 0:
            if raise_on_missing:
                tagger_names = "", "".join([tn for tn, _ in cls.items()])
                raise ValueError(f""Unknown tagger {name}; available taggers: {tagger_names}"")
            return None

        else:
            name = matches[0]
            t, _ = cls._get_storage()[name]
            return t","def get(cls, name: str, raise_on_missing: bool = True) -> Optional[T]:
        """"""Get a tagger from the registry; raise ValueError if it doesn't exist.""""""

        matches = [registered for registered in cls._get_storage() if re.match(registered, name)]

        if len(matches) > 1:
            raise ValueError(f""Multiple taggers match {name}: {', '.join(matches)}"")

        elif len(matches) == 0:
            if raise_on_missing:
                tagger_names = "", "".join([tn for tn, _ in cls.items()])
                raise ValueError(f""Unknown tagger {name}; available taggers: {tagger_names}"")
            return None

        else:
            name = matches[0]
            t, _ = cls._get_storage()[name]
            return t",Get a tagger from the registry; raise ValueError if it doesn't exist.,Get a tagger from the registry; raise ValueError if it doesn't exist.,"def get(cls, name: str, raise_on_missing: bool = True) -> Optional[T]:
        

        matches = [registered for registered in cls._get_storage() if re.match(registered, name)]

        if len(matches) > 1:
            raise ValueError(f""Multiple taggers match {name}: {', '.join(matches)}"")

        elif len(matches) == 0:
            if raise_on_missing:
                tagger_names = "", "".join([tn for tn, _ in cls.items()])
                raise ValueError(f""Unknown tagger {name}; available taggers: {tagger_names}"")
            return None

        else:
            name = matches[0]
            t, _ = cls._get_storage()[name]
            return t",Get a tagger from the registry; raise ValueError if it doesn't exist.,"def get ( cls , name : str , raise_on_missing : bool = True ) -> Optional [ T ] : matches = [ registered for registered in cls . _get_storage ( ) if re . match ( registered , name ) ] if len ( matches ) > 1 : raise ValueError ( f""Multiple taggers match {name}: {', '.join(matches)}"" ) elif len ( matches ) == 0 : if raise_on_missing : tagger_names = "", "" . join ( [ tn for tn , _ in cls . items ( ) ] ) raise ValueError ( f""Unknown tagger {name}; available taggers: {tagger_names}"" ) return None else : name = matches [ 0 ] t , _ = cls . _get_storage ( ) [ name ] return t",Get a tagger from the registry; raise ValueError if it doesn't exist.
/fastapi_mcp/fastapi_mcp/openapi/utils.py,get_single_param_type_from_schema,"def get_single_param_type_from_schema(param_schema: Dict[str, Any]) -> str:
    """"""
    Get the type of a parameter from the schema.
    If the schema is a union type, return the first type.
    """"""
    if ""anyOf"" in param_schema:
        types = {schema.get(""type"") for schema in param_schema[""anyOf""] if schema.get(""type"")}
        if ""null"" in types:
            types.remove(""null"")
        if types:
            return next(iter(types))
        return ""string""
    return param_schema.get(""type"", ""string"")","def get_single_param_type_from_schema(param_schema: Dict[str, Any]) -> str:
    """"""
    Get the type of a parameter from the schema.
    If the schema is a union type, return the first type.
    """"""
    if ""anyOf"" in param_schema:
        types = {schema.get(""type"") for schema in param_schema[""anyOf""] if schema.get(""type"")}
        if ""null"" in types:
            types.remove(""null"")
        if types:
            return next(iter(types))
        return ""string""
    return param_schema.get(""type"", ""string"")","Get the type of a parameter from the schema.
If the schema is a union type, return the first type.",Get the type of a parameter from the schema.,"def get_single_param_type_from_schema(param_schema: Dict[str, Any]) -> str:
    
    if ""anyOf"" in param_schema:
        types = {schema.get(""type"") for schema in param_schema[""anyOf""] if schema.get(""type"")}
        if ""null"" in types:
            types.remove(""null"")
        if types:
            return next(iter(types))
        return ""string""
    return param_schema.get(""type"", ""string"")",Get the type of a parameter from the schema.,"def get_single_param_type_from_schema ( param_schema : Dict [ str , Any ] ) -> str : if ""anyOf"" in param_schema : types = { schema . get ( ""type"" ) for schema in param_schema [ ""anyOf"" ] if schema . get ( ""type"" ) } if ""null"" in types : types . remove ( ""null"" ) if types : return next ( iter ( types ) ) return ""string"" return param_schema . get ( ""type"" , ""string"" )",Get the type of a parameter from the schema.
/ragaai-catalyst/ragaai_catalyst/tracers/langchain_callback.py,start,"def start(self):
        """"""Start tracing with enhanced error handling and async support""""""
        try:
            self.reset_trace()
            self.current_trace[""start_time""] = datetime.now()
            self._active = True
            self._monkey_patch()

            if self.save_interval:
                loop = asyncio.get_event_loop()
                self._save_task = loop.create_task(self._periodic_save())

            logger.info(""Tracing started"")
        except Exception as e:
            logger.error(f""Error starting tracer: {e}"")
            self.on_error(e, context=""start"")
            raise","def start(self):
        """"""Start tracing with enhanced error handling and async support""""""
        try:
            self.reset_trace()
            self.current_trace[""start_time""] = datetime.now()
            self._active = True
            self._monkey_patch()

            if self.save_interval:
                loop = asyncio.get_event_loop()
                self._save_task = loop.create_task(self._periodic_save())

            logger.info(""Tracing started"")
        except Exception as e:
            logger.error(f""Error starting tracer: {e}"")
            self.on_error(e, context=""start"")
            raise",Start tracing with enhanced error handling and async support,Start tracing with enhanced error handling and async support,"def start(self):
        
        try:
            self.reset_trace()
            self.current_trace[""start_time""] = datetime.now()
            self._active = True
            self._monkey_patch()

            if self.save_interval:
                loop = asyncio.get_event_loop()
                self._save_task = loop.create_task(self._periodic_save())

            logger.info(""Tracing started"")
        except Exception as e:
            logger.error(f""Error starting tracer: {e}"")
            self.on_error(e, context=""start"")
            raise",Start tracing with enhanced error handling and async support,"def start ( self ) : try : self . reset_trace ( ) self . current_trace [ ""start_time"" ] = datetime . now ( ) self . _active = True self . _monkey_patch ( ) if self . save_interval : loop = asyncio . get_event_loop ( ) self . _save_task = loop . create_task ( self . _periodic_save ( ) ) logger . info ( ""Tracing started"" ) except Exception as e : logger . error ( f""Error starting tracer: {e}"" ) self . on_error ( e , context = ""start"" ) raise",Start tracing with enhanced error handling and async support
/Kokoro-FastAPI/dev/Test Phon.py,generate_audio_from_phonemes,"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella""):
    """"""Generate audio from phonemes""""""
    response = requests.post(
        ""http://localhost:8880/dev/generate_from_phonemes"",
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    if response.status_code != 200:
        print(f""Error: {response.text}"")
        return None
    return response.content","def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella""):
    """"""Generate audio from phonemes""""""
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    if response.status_code != 200:
        print(f""Error: {response.text}"")
        return None
    return response.content",Generate audio from phonemes,Generate audio from phonemes,"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella""):
    
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    if response.status_code != 200:
        print(f""Error: {response.text}"")
        return None
    return response.content",Generate audio from phonemes,"def generate_audio_from_phonemes ( phonemes : str , voice : str = ""af_bella"" ) : response = requests . post ( json = { ""phonemes"" : phonemes , ""voice"" : voice } , headers = { ""Accept"" : ""audio/wav"" } ) if response . status_code != 200 : print ( f""Error: {response.text}"" ) return None return response . content",Generate audio from phonemes
/mcp-agent/examples/usecases/reliable_conversation/src/workflows/conversation_workflow.py,_build_conversation_context,"def _build_conversation_context(self) -> str:
        """"""Build context string from conversation history""""""
        context_parts = []

        # Include recent messages (last 5 for now)
        recent_messages = (
            self.state.messages[-5:]
            if len(self.state.messages) > 5
            else self.state.messages
        )

        for msg in recent_messages:
            if msg.role != ""system"":  # Skip system message in context
                role_label = ""User"" if msg.role == ""user"" else ""Assistant""
                context_parts.append(f""{role_label}: {msg.content}"")

        return (
            ""\n"".join(context_parts)
            if context_parts
            else ""This is the start of our conversation.""
        )","def _build_conversation_context(self) -> str:
        """"""Build context string from conversation history""""""
        context_parts = []

        # Include recent messages (last 5 for now)
        recent_messages = (
            self.state.messages[-5:]
            if len(self.state.messages) > 5
            else self.state.messages
        )

        for msg in recent_messages:
            if msg.role != ""system"":  # Skip system message in context
                role_label = ""User"" if msg.role == ""user"" else ""Assistant""
                context_parts.append(f""{role_label}: {msg.content}"")

        return (
            ""\n"".join(context_parts)
            if context_parts
            else ""This is the start of our conversation.""
        )",Build context string from conversation history,Build context string from conversation history,"def _build_conversation_context(self) -> str:
        
        context_parts = []

        # Include recent messages (last 5 for now)
        recent_messages = (
            self.state.messages[-5:]
            if len(self.state.messages) > 5
            else self.state.messages
        )

        for msg in recent_messages:
            if msg.role != ""system"":  # Skip system message in context
                role_label = ""User"" if msg.role == ""user"" else ""Assistant""
                context_parts.append(f""{role_label}: {msg.content}"")

        return (
            ""\n"".join(context_parts)
            if context_parts
            else ""This is the start of our conversation.""
        )",Build context string from conversation history,"def _build_conversation_context ( self ) -> str : context_parts = [ ] # Include recent messages (last 5 for now) recent_messages = ( self . state . messages [ - 5 : ] if len ( self . state . messages ) > 5 else self . state . messages ) for msg in recent_messages : if msg . role != ""system"" : # Skip system message in context role_label = ""User"" if msg . role == ""user"" else ""Assistant"" context_parts . append ( f""{role_label}: {msg.content}"" ) return ( ""\n"" . join ( context_parts ) if context_parts else ""This is the start of our conversation."" )",Build context string from conversation history
/mcp/src/aws-support-mcp-server/awslabs/aws_support_mcp_server/models.py,to_api_params,"def to_api_params(self) -> Dict[str, JsonValue]:
        """"""Convert to AWS API parameters.""""""
        params: Dict[str, JsonValue] = {
            ""includeResolvedCases"": cast(JsonValue, self.include_resolved_cases),
            ""includeCommunications"": cast(JsonValue, self.include_communications),
            ""language"": cast(JsonValue, self.language),
        }

        if self.case_id_list:
            params[""caseIdList""] = cast(JsonValue, self.case_id_list)
        if self.display_id:
            params[""displayId""] = cast(JsonValue, self.display_id)
        if self.after_time:
            params[""afterTime""] = cast(JsonValue, self.after_time)
        if self.before_time:
            params[""beforeTime""] = cast(JsonValue, self.before_time)
        if self.max_results:
            params[""maxResults""] = cast(JsonValue, self.max_results)
        if self.next_token:
            params[""nextToken""] = cast(JsonValue, self.next_token)

        return params","def to_api_params(self) -> Dict[str, JsonValue]:
        """"""Convert to AWS API parameters.""""""
        params: Dict[str, JsonValue] = {
            ""includeResolvedCases"": cast(JsonValue, self.include_resolved_cases),
            ""includeCommunications"": cast(JsonValue, self.include_communications),
            ""language"": cast(JsonValue, self.language),
        }

        if self.case_id_list:
            params[""caseIdList""] = cast(JsonValue, self.case_id_list)
        if self.display_id:
            params[""displayId""] = cast(JsonValue, self.display_id)
        if self.after_time:
            params[""afterTime""] = cast(JsonValue, self.after_time)
        if self.before_time:
            params[""beforeTime""] = cast(JsonValue, self.before_time)
        if self.max_results:
            params[""maxResults""] = cast(JsonValue, self.max_results)
        if self.next_token:
            params[""nextToken""] = cast(JsonValue, self.next_token)

        return params",Convert to AWS API parameters.,Convert to AWS API parameters.,"def to_api_params(self) -> Dict[str, JsonValue]:
        
        params: Dict[str, JsonValue] = {
            ""includeResolvedCases"": cast(JsonValue, self.include_resolved_cases),
            ""includeCommunications"": cast(JsonValue, self.include_communications),
            ""language"": cast(JsonValue, self.language),
        }

        if self.case_id_list:
            params[""caseIdList""] = cast(JsonValue, self.case_id_list)
        if self.display_id:
            params[""displayId""] = cast(JsonValue, self.display_id)
        if self.after_time:
            params[""afterTime""] = cast(JsonValue, self.after_time)
        if self.before_time:
            params[""beforeTime""] = cast(JsonValue, self.before_time)
        if self.max_results:
            params[""maxResults""] = cast(JsonValue, self.max_results)
        if self.next_token:
            params[""nextToken""] = cast(JsonValue, self.next_token)

        return params",Convert to AWS API parameters.,"def to_api_params ( self ) -> Dict [ str , JsonValue ] : params : Dict [ str , JsonValue ] = { ""includeResolvedCases"" : cast ( JsonValue , self . include_resolved_cases ) , ""includeCommunications"" : cast ( JsonValue , self . include_communications ) , ""language"" : cast ( JsonValue , self . language ) , } if self . case_id_list : params [ ""caseIdList"" ] = cast ( JsonValue , self . case_id_list ) if self . display_id : params [ ""displayId"" ] = cast ( JsonValue , self . display_id ) if self . after_time : params [ ""afterTime"" ] = cast ( JsonValue , self . after_time ) if self . before_time : params [ ""beforeTime"" ] = cast ( JsonValue , self . before_time ) if self . max_results : params [ ""maxResults"" ] = cast ( JsonValue , self . max_results ) if self . next_token : params [ ""nextToken"" ] = cast ( JsonValue , self . next_token ) return params",Convert to AWS API parameters.
/agenticSeek/sources/tools/mcpFinder.py,interpreter_feedback,"def interpreter_feedback(self, output: str) -> str:
        """"""
        Not really needed for this tool (use return of execute() directly)
        """"""
        if not output:
            raise ValueError(""No output to interpret."")
        return f""""""
            The following MCPs were found:
            {output}
            """"""","def interpreter_feedback(self, output: str) -> str:
        """"""
        Not really needed for this tool (use return of execute() directly)
        """"""
        if not output:
            raise ValueError(""No output to interpret."")
        return f""""""
            The following MCPs were found:
            {output}
            """"""",Not really needed for this tool (use return of execute() directly),Not really needed for this tool (use return of execute() directly),"def interpreter_feedback(self, output: str) -> str:
        
        if not output:
            raise ValueError(""No output to interpret."")
        return f",Not really needed for this tool (use return of execute() directly),"def interpreter_feedback ( self , output : str ) -> str : if not output : raise ValueError ( ""No output to interpret."" ) return f",Not really needed for this tool (use return of execute() directly)
/optillm/optillm/autothink/classifier.py,_load_model,"def _load_model(self):
        """"""Load the classification model using adaptive-classifier library.""""""
        try:
            # Check if adaptive-classifier is installed
            try:
                import adaptive_classifier
            except ImportError:
                logger.info(""Installing adaptive-classifier library..."")
                os.system(f""{sys.executable} -m pip install adaptive-classifier"")
                import adaptive_classifier
            
            # Import the AdaptiveClassifier class
            from adaptive_classifier import AdaptiveClassifier
            
            logger.info(f""Loading complexity classifier model: {self.model_name}"")
            self.classifier = AdaptiveClassifier.from_pretrained(self.model_name)
            logger.info(""Classifier loaded successfully"")
            
        except Exception as e:
            logger.error(f""Error loading complexity classifier: {e}"")
            # Fallback to basic classification if model fails to load
            self.classifier = None","def _load_model(self):
        """"""Load the classification model using adaptive-classifier library.""""""
        try:
            # Check if adaptive-classifier is installed
            try:
                import adaptive_classifier
            except ImportError:
                logger.info(""Installing adaptive-classifier library..."")
                os.system(f""{sys.executable} -m pip install adaptive-classifier"")
                import adaptive_classifier
            
            # Import the AdaptiveClassifier class
            from adaptive_classifier import AdaptiveClassifier
            
            logger.info(f""Loading complexity classifier model: {self.model_name}"")
            self.classifier = AdaptiveClassifier.from_pretrained(self.model_name)
            logger.info(""Classifier loaded successfully"")
            
        except Exception as e:
            logger.error(f""Error loading complexity classifier: {e}"")
            # Fallback to basic classification if model fails to load
            self.classifier = None",Load the classification model using adaptive-classifier library.,Load the classification model using adaptive-classifier library.,"def _load_model(self):
        
        try:
            # Check if adaptive-classifier is installed
            try:
                import adaptive_classifier
            except ImportError:
                logger.info(""Installing adaptive-classifier library..."")
                os.system(f""{sys.executable} -m pip install adaptive-classifier"")
                import adaptive_classifier
            
            # Import the AdaptiveClassifier class
            from adaptive_classifier import AdaptiveClassifier
            
            logger.info(f""Loading complexity classifier model: {self.model_name}"")
            self.classifier = AdaptiveClassifier.from_pretrained(self.model_name)
            logger.info(""Classifier loaded successfully"")
            
        except Exception as e:
            logger.error(f""Error loading complexity classifier: {e}"")
            # Fallback to basic classification if model fails to load
            self.classifier = None",Load the classification model using adaptive-classifier library.,"def _load_model ( self ) : try : # Check if adaptive-classifier is installed try : import adaptive_classifier except ImportError : logger . info ( ""Installing adaptive-classifier library..."" ) os . system ( f""{sys.executable} -m pip install adaptive-classifier"" ) import adaptive_classifier # Import the AdaptiveClassifier class from adaptive_classifier import AdaptiveClassifier logger . info ( f""Loading complexity classifier model: {self.model_name}"" ) self . classifier = AdaptiveClassifier . from_pretrained ( self . model_name ) logger . info ( ""Classifier loaded successfully"" ) except Exception as e : logger . error ( f""Error loading complexity classifier: {e}"" ) # Fallback to basic classification if model fails to load self . classifier = None",Load the classification model using adaptive-classifier library.
/optillm/scripts/eval_optillmbench.py,load_optillm_bench,"def load_optillm_bench() -> datasets.Dataset:
    """"""Load the OptiLLM Bench dataset.""""""
    try:
        dataset = load_dataset(""codelion/optillmbench"")
        return dataset[""test""]  # We use the test split for evaluation
    except Exception as e:
        logger.error(f""Error loading dataset: {e}"")
        raise","def load_optillm_bench() -> datasets.Dataset:
    """"""Load the OptiLLM Bench dataset.""""""
    try:
        dataset = load_dataset(""codelion/optillmbench"")
        return dataset[""test""]  # We use the test split for evaluation
    except Exception as e:
        logger.error(f""Error loading dataset: {e}"")
        raise",Load the OptiLLM Bench dataset.,Load the OptiLLM Bench dataset.,"def load_optillm_bench() -> datasets.Dataset:
    
    try:
        dataset = load_dataset(""codelion/optillmbench"")
        return dataset[""test""]  # We use the test split for evaluation
    except Exception as e:
        logger.error(f""Error loading dataset: {e}"")
        raise",Load the OptiLLM Bench dataset.,"def load_optillm_bench ( ) -> datasets . Dataset : try : dataset = load_dataset ( ""codelion/optillmbench"" ) return dataset [ ""test"" ] # We use the test split for evaluation except Exception as e : logger . error ( f""Error loading dataset: {e}"" ) raise",Load the OptiLLM Bench dataset.
/mcp-agent/src/mcp_agent/workflows/swarm/swarm.py,__init__,"def __init__(self, agent: SwarmAgent, context_variables: Dict[str, str] = None):
        """"""
        Initialize the LLM planner with an agent, which will be used as the
        starting point for the workflow.
        """"""
        super().__init__(agent=agent)
        self.context_variables = defaultdict(str, context_variables or {})
        self.instruction = (
            agent.instruction(self.context_variables)
            if isinstance(agent.instruction, Callable)
            else agent.instruction
        )
        logger.debug(
            f""Swarm initialized with agent {agent.name}"",
            data={
                ""context_variables"": self.context_variables,
                ""instruction"": self.instruction,
            },
        )","def __init__(self, agent: SwarmAgent, context_variables: Dict[str, str] = None):
        """"""
        Initialize the LLM planner with an agent, which will be used as the
        starting point for the workflow.
        """"""
        super().__init__(agent=agent)
        self.context_variables = defaultdict(str, context_variables or {})
        self.instruction = (
            agent.instruction(self.context_variables)
            if isinstance(agent.instruction, Callable)
            else agent.instruction
        )
        logger.debug(
            f""Swarm initialized with agent {agent.name}"",
            data={
                ""context_variables"": self.context_variables,
                ""instruction"": self.instruction,
            },
        )","Initialize the LLM planner with an agent, which will be used as the
starting point for the workflow.","Initialize the LLM planner with an agent, which will be used as the starting point for the workflow.","def __init__(self, agent: SwarmAgent, context_variables: Dict[str, str] = None):
        
        super().__init__(agent=agent)
        self.context_variables = defaultdict(str, context_variables or {})
        self.instruction = (
            agent.instruction(self.context_variables)
            if isinstance(agent.instruction, Callable)
            else agent.instruction
        )
        logger.debug(
            f""Swarm initialized with agent {agent.name}"",
            data={
                ""context_variables"": self.context_variables,
                ""instruction"": self.instruction,
            },
        )","Initialize the LLM planner with an agent, which will be used as the starting point for the workflow.","def __init__ ( self , agent : SwarmAgent , context_variables : Dict [ str , str ] = None ) : super ( ) . __init__ ( agent = agent ) self . context_variables = defaultdict ( str , context_variables or { } ) self . instruction = ( agent . instruction ( self . context_variables ) if isinstance ( agent . instruction , Callable ) else agent . instruction ) logger . debug ( f""Swarm initialized with agent {agent.name}"" , data = { ""context_variables"" : self . context_variables , ""instruction"" : self . instruction , } , )","Initialize the LLM planner with an agent, which will be used as the starting point for the workflow."
/Second-Me/lpm_kernel/file_data/core/discovery.py,auto_discover_processors,"def auto_discover_processors():
    """"""Automatically discover and import all processors""""""
    # get processors directory path
    processors_path = Path(__file__).parent.parent / ""processors""

    # iterate over all subdirectories in processors directory
    for _, name, _ in pkgutil.iter_modules([str(processors_path)]):
        # if it is a directory and contains processor.py
        processor_file = processors_path / name / ""processor.py""
        if processor_file.exists():
            module_path = f""lpm_kernel.file_data.processors.{name}.processor""
            try:
                importlib.import_module(module_path)
            except ImportError as e:
                print(f""Failed to load processor module {module_path}: {e}"")","def auto_discover_processors():
    """"""Automatically discover and import all processors""""""
    # get processors directory path
    processors_path = Path(__file__).parent.parent / ""processors""

    # iterate over all subdirectories in processors directory
    for _, name, _ in pkgutil.iter_modules([str(processors_path)]):
        # if it is a directory and contains processor.py
        processor_file = processors_path / name / ""processor.py""
        if processor_file.exists():
            module_path = f""lpm_kernel.file_data.processors.{name}.processor""
            try:
                importlib.import_module(module_path)
            except ImportError as e:
                print(f""Failed to load processor module {module_path}: {e}"")",Automatically discover and import all processors,Automatically discover and import all processors,"def auto_discover_processors():
    
    # get processors directory path
    processors_path = Path(__file__).parent.parent / ""processors""

    # iterate over all subdirectories in processors directory
    for _, name, _ in pkgutil.iter_modules([str(processors_path)]):
        # if it is a directory and contains processor.py
        processor_file = processors_path / name / ""processor.py""
        if processor_file.exists():
            module_path = f""lpm_kernel.file_data.processors.{name}.processor""
            try:
                importlib.import_module(module_path)
            except ImportError as e:
                print(f""Failed to load processor module {module_path}: {e}"")",Automatically discover and import all processors,"def auto_discover_processors ( ) : # get processors directory path processors_path = Path ( __file__ ) . parent . parent / ""processors"" # iterate over all subdirectories in processors directory for _ , name , _ in pkgutil . iter_modules ( [ str ( processors_path ) ] ) : # if it is a directory and contains processor.py processor_file = processors_path / name / ""processor.py"" if processor_file . exists ( ) : module_path = f""lpm_kernel.file_data.processors.{name}.processor"" try : importlib . import_module ( module_path ) except ImportError as e : print ( f""Failed to load processor module {module_path}: {e}"" )",Automatically discover and import all processors
/airweave/backend/airweave/platform/sources/linear.py,__init__,"def __init__(self):
        """"""Initialize the LinearSource with rate limiting state.""""""
        super().__init__()
        self._request_times = []
        self._lock = asyncio.Lock()
        self._stats = {
            ""api_calls"": 0,
            ""rate_limit_waits"": 0,
        }","def __init__(self):
        """"""Initialize the LinearSource with rate limiting state.""""""
        super().__init__()
        self._request_times = []
        self._lock = asyncio.Lock()
        self._stats = {
            ""api_calls"": 0,
            ""rate_limit_waits"": 0,
        }",Initialize the LinearSource with rate limiting state.,Initialize the LinearSource with rate limiting state.,"def __init__(self):
        
        super().__init__()
        self._request_times = []
        self._lock = asyncio.Lock()
        self._stats = {
            ""api_calls"": 0,
            ""rate_limit_waits"": 0,
        }",Initialize the LinearSource with rate limiting state.,"def __init__ ( self ) : super ( ) . __init__ ( ) self . _request_times = [ ] self . _lock = asyncio . Lock ( ) self . _stats = { ""api_calls"" : 0 , ""rate_limit_waits"" : 0 , }",Initialize the LinearSource with rate limiting state.
/R1-V/src/eval/test_qwen2vl_geoqa_multigpu.py,infer_on_single_gpu,"def infer_on_single_gpu(model_path, device_id, chunk_of_tested_messages, batch_size, results=None):
    """"""init model on this single gpu and let it answer asign chunk of questions""""""
    model, processor = init_model(model_path, device_id)
    
    ### split batch
    responses = []
    batch_messages_list = [chunk_of_tested_messages[start: start + batch_size] 
               for start in range(0, len(chunk_of_tested_messages), batch_size)]

    for batch_messages in tqdm.auto.tqdm(batch_messages_list, desc=f""GPU {device_id} progress"", position=device_id, leave=False):
        batch_output_text = answer_a_batch_question_qwen(batch_messages, model, processor)
        
        responses.extend(batch_output_text)
    
    results[device_id] = responses
    return","def infer_on_single_gpu(model_path, device_id, chunk_of_tested_messages, batch_size, results=None):
    """"""init model on this single gpu and let it answer asign chunk of questions""""""
    model, processor = init_model(model_path, device_id)
    
    ### split batch
    responses = []
    batch_messages_list = [chunk_of_tested_messages[start: start + batch_size] 
               for start in range(0, len(chunk_of_tested_messages), batch_size)]

    for batch_messages in tqdm.auto.tqdm(batch_messages_list, desc=f""GPU {device_id} progress"", position=device_id, leave=False):
        batch_output_text = answer_a_batch_question_qwen(batch_messages, model, processor)
        
        responses.extend(batch_output_text)
    
    results[device_id] = responses
    return",init model on this single gpu and let it answer asign chunk of questions,init model on this single gpu and let it answer asign chunk of questions,"def infer_on_single_gpu(model_path, device_id, chunk_of_tested_messages, batch_size, results=None):
    
    model, processor = init_model(model_path, device_id)
    
    ### split batch
    responses = []
    batch_messages_list = [chunk_of_tested_messages[start: start + batch_size] 
               for start in range(0, len(chunk_of_tested_messages), batch_size)]

    for batch_messages in tqdm.auto.tqdm(batch_messages_list, desc=f""GPU {device_id} progress"", position=device_id, leave=False):
        batch_output_text = answer_a_batch_question_qwen(batch_messages, model, processor)
        
        responses.extend(batch_output_text)
    
    results[device_id] = responses
    return",init model on this single gpu and let it answer asign chunk of questions,"def infer_on_single_gpu ( model_path , device_id , chunk_of_tested_messages , batch_size , results = None ) : model , processor = init_model ( model_path , device_id ) ### split batch responses = [ ] batch_messages_list = [ chunk_of_tested_messages [ start : start + batch_size ] for start in range ( 0 , len ( chunk_of_tested_messages ) , batch_size ) ] for batch_messages in tqdm . auto . tqdm ( batch_messages_list , desc = f""GPU {device_id} progress"" , position = device_id , leave = False ) : batch_output_text = answer_a_batch_question_qwen ( batch_messages , model , processor ) responses . extend ( batch_output_text ) results [ device_id ] = responses return",init model on this single gpu and let it answer asign chunk of questions
/echomimic_v2/EMTD_dataset/download.py,download_youtube_video,"def download_youtube_video(video_url, output_path):
    """"""
    :param video_url: youtube video url
    :param output_dir: file path to save
    """"""
    # video_url, output_path = info
    try:
        if not os.path.exists(os.path.dirname(output_path)):
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # download command
        command = ['yt-dlp', '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', '--merge-output-format',
               'mp4', '--output', output_path , video_url]
        # subprocess.run
        result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')

        if result.returncode == 0:
            print('Download {:s} successfully!'.format(video_url))
        else:
            print(""Fail to download {:s}, error info:\n{:s}"".format(video_url, result.stderr))
    except Exception as e:
        print(f""error: {e}"")","def download_youtube_video(video_url, output_path):
    """"""
    :param video_url: youtube video url
    :param output_dir: file path to save
    """"""
    # video_url, output_path = info
    try:
        if not os.path.exists(os.path.dirname(output_path)):
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # download command
        command = ['yt-dlp', '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', '--merge-output-format',
               'mp4', '--output', output_path , video_url]
        # subprocess.run
        result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')

        if result.returncode == 0:
            print('Download {:s} successfully!'.format(video_url))
        else:
            print(""Fail to download {:s}, error info:\n{:s}"".format(video_url, result.stderr))
    except Exception as e:
        print(f""error: {e}"")",":param video_url: youtube video url
:param output_dir: file path to save",:param video_url: youtube video url :param output_dir: file path to save,"def download_youtube_video(video_url, output_path):
    
    # video_url, output_path = info
    try:
        if not os.path.exists(os.path.dirname(output_path)):
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # download command
        command = ['yt-dlp', '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', '--merge-output-format',
               'mp4', '--output', output_path , video_url]
        # subprocess.run
        result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')

        if result.returncode == 0:
            print('Download {:s} successfully!'.format(video_url))
        else:
            print(""Fail to download {:s}, error info:\n{:s}"".format(video_url, result.stderr))
    except Exception as e:
        print(f""error: {e}"")",:param video_url: youtube video url :param output_dir: file path to save,"def download_youtube_video ( video_url , output_path ) : # video_url, output_path = info try : if not os . path . exists ( os . path . dirname ( output_path ) ) : os . makedirs ( os . path . dirname ( output_path ) , exist_ok = True ) # download command command = [ 'yt-dlp' , '-f' , 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]' , '--merge-output-format' , 'mp4' , '--output' , output_path , video_url ] # subprocess.run result = subprocess . run ( command , capture_output = True , text = True , encoding = 'utf-8' ) if result . returncode == 0 : print ( 'Download {:s} successfully!' . format ( video_url ) ) else : print ( ""Fail to download {:s}, error info:\n{:s}"" . format ( video_url , result . stderr ) ) except Exception as e : print ( f""error: {e}"" )",:param video_url: youtube video url :param output_dir: file path to save
/ha_xiaomi_home/custom_components/xiaomi_home/miot/miot_client.py,sub_device_state,"def sub_device_state(
        self, did: str, handler: Callable[[str, MIoTDeviceState, Any], None],
        handler_ctx: Any = None
    ) -> bool:
        """"""Call callback handler in main loop""""""
        if did not in self._device_list_cache:
            raise MIoTClientError(f'did not exist, {did}')
        self._sub_device_state[did] = MipsDeviceState(
            did=did, handler=handler, handler_ctx=handler_ctx)
        _LOGGER.debug('client sub device state, %s', did)
        return True","def sub_device_state(
        self, did: str, handler: Callable[[str, MIoTDeviceState, Any], None],
        handler_ctx: Any = None
    ) -> bool:
        """"""Call callback handler in main loop""""""
        if did not in self._device_list_cache:
            raise MIoTClientError(f'did not exist, {did}')
        self._sub_device_state[did] = MipsDeviceState(
            did=did, handler=handler, handler_ctx=handler_ctx)
        _LOGGER.debug('client sub device state, %s', did)
        return True",Call callback handler in main loop,Call callback handler in main loop,"def sub_device_state(
        self, did: str, handler: Callable[[str, MIoTDeviceState, Any], None],
        handler_ctx: Any = None
    ) -> bool:
        
        if did not in self._device_list_cache:
            raise MIoTClientError(f'did not exist, {did}')
        self._sub_device_state[did] = MipsDeviceState(
            did=did, handler=handler, handler_ctx=handler_ctx)
        _LOGGER.debug('client sub device state, %s', did)
        return True",Call callback handler in main loop,"def sub_device_state ( self , did : str , handler : Callable [ [ str , MIoTDeviceState , Any ] , None ] , handler_ctx : Any = None ) -> bool : if did not in self . _device_list_cache : raise MIoTClientError ( f'did not exist, {did}' ) self . _sub_device_state [ did ] = MipsDeviceState ( did = did , handler = handler , handler_ctx = handler_ctx ) _LOGGER . debug ( 'client sub device state, %s' , did ) return True",Call callback handler in main loop
/Kokoro-FastAPI/examples/phoneme_examples/generate_phonemes.py,generate_audio_from_phonemes,"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella"") -> Optional[bytes]:
    """"""Generate audio from phonemes.""""""
    response = requests.post(
        ""http://localhost:8880/dev/generate_from_phonemes"",
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    
    print(f""Response status: {response.status_code}"")
    print(f""Response headers: {dict(response.headers)}"")
    print(f""Response content type: {response.headers.get('Content-Type')}"")
    print(f""Response length: {len(response.content)} bytes"")
    
    if response.status_code != 200:
        print(f""Error response: {response.text}"")
        return None
        
    if not response.content:
        print(""Error: Empty response content"")
        return None
        
    return response.content","def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella"") -> Optional[bytes]:
    """"""Generate audio from phonemes.""""""
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    
    print(f""Response status: {response.status_code}"")
    print(f""Response headers: {dict(response.headers)}"")
    print(f""Response content type: {response.headers.get('Content-Type')}"")
    print(f""Response length: {len(response.content)} bytes"")
    
    if response.status_code != 200:
        print(f""Error response: {response.text}"")
        return None
        
    if not response.content:
        print(""Error: Empty response content"")
        return None
        
    return response.content",Generate audio from phonemes.,Generate audio from phonemes.,"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella"") -> Optional[bytes]:
    
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    
    print(f""Response status: {response.status_code}"")
    print(f""Response headers: {dict(response.headers)}"")
    print(f""Response content type: {response.headers.get('Content-Type')}"")
    print(f""Response length: {len(response.content)} bytes"")
    
    if response.status_code != 200:
        print(f""Error response: {response.text}"")
        return None
        
    if not response.content:
        print(""Error: Empty response content"")
        return None
        
    return response.content",Generate audio from phonemes.,"def generate_audio_from_phonemes ( phonemes : str , voice : str = ""af_bella"" ) -> Optional [ bytes ] : response = requests . post ( json = { ""phonemes"" : phonemes , ""voice"" : voice } , headers = { ""Accept"" : ""audio/wav"" } ) print ( f""Response status: {response.status_code}"" ) print ( f""Response headers: {dict(response.headers)}"" ) print ( f""Response content type: {response.headers.get('Content-Type')}"" ) print ( f""Response length: {len(response.content)} bytes"" ) if response . status_code != 200 : print ( f""Error response: {response.text}"" ) return None if not response . content : print ( ""Error: Empty response content"" ) return None return response . content",Generate audio from phonemes.
/pydoll/pydoll/browser/base.py,_setup_user_dir,"def _setup_user_dir(self):
        """"""Prepares the user data directory if necessary.""""""
        if '--user-data-dir' not in [
            arg.split('=')[0] for arg in self.options.arguments
        ]:
            # For all browsers, use a temporary directory
            temp_dir = self._temp_directory_manager.create_temp_dir()
            self.options.arguments.append(f'--user-data-dir={temp_dir.name}')","def _setup_user_dir(self):
        """"""Prepares the user data directory if necessary.""""""
        if '--user-data-dir' not in [
            arg.split('=')[0] for arg in self.options.arguments
        ]:
            # For all browsers, use a temporary directory
            temp_dir = self._temp_directory_manager.create_temp_dir()
            self.options.arguments.append(f'--user-data-dir={temp_dir.name}')",Prepares the user data directory if necessary.,Prepares the user data directory if necessary.,"def _setup_user_dir(self):
        
        if '--user-data-dir' not in [
            arg.split('=')[0] for arg in self.options.arguments
        ]:
            # For all browsers, use a temporary directory
            temp_dir = self._temp_directory_manager.create_temp_dir()
            self.options.arguments.append(f'--user-data-dir={temp_dir.name}')",Prepares the user data directory if necessary.,"def _setup_user_dir ( self ) : if '--user-data-dir' not in [ arg . split ( '=' ) [ 0 ] for arg in self . options . arguments ] : # For all browsers, use a temporary directory temp_dir = self . _temp_directory_manager . create_temp_dir ( ) self . options . arguments . append ( f'--user-data-dir={temp_dir.name}' )",Prepares the user data directory if necessary.
/ag2/autogen/mcp/mcp_proxy/security.py,__post_init__,"def __post_init__(
        self,
    ) -> ""BaseSecurity"":  # dataclasses uses __post_init__ instead of model_validator
        """"""Validate the in_value based on the type.""""""
        valid_in_values = {
            ""apiKey"": [""header"", ""query"", ""cookie""],
            ""http"": [""bearer"", ""basic""],
            ""oauth2"": [""bearer""],
            ""openIdConnect"": [""bearer""],
            ""mutualTLS"": [""tls""],
            ""unsupported"": [""unsupported""],
        }
        if self.in_value not in valid_in_values[self.type]:
            raise ValueError(f""Invalid in_value '{self.in_value}' for type '{self.type}'"")
        return self","def __post_init__(
        self,
    ) -> ""BaseSecurity"":  # dataclasses uses __post_init__ instead of model_validator
        """"""Validate the in_value based on the type.""""""
        valid_in_values = {
            ""apiKey"": [""header"", ""query"", ""cookie""],
            ""http"": [""bearer"", ""basic""],
            ""oauth2"": [""bearer""],
            ""openIdConnect"": [""bearer""],
            ""mutualTLS"": [""tls""],
            ""unsupported"": [""unsupported""],
        }
        if self.in_value not in valid_in_values[self.type]:
            raise ValueError(f""Invalid in_value '{self.in_value}' for type '{self.type}'"")
        return self",Validate the in_value based on the type.,Validate the in_value based on the type.,"def __post_init__(
        self,
    ) -> ""BaseSecurity"":  # dataclasses uses __post_init__ instead of model_validator
        
        valid_in_values = {
            ""apiKey"": [""header"", ""query"", ""cookie""],
            ""http"": [""bearer"", ""basic""],
            ""oauth2"": [""bearer""],
            ""openIdConnect"": [""bearer""],
            ""mutualTLS"": [""tls""],
            ""unsupported"": [""unsupported""],
        }
        if self.in_value not in valid_in_values[self.type]:
            raise ValueError(f""Invalid in_value '{self.in_value}' for type '{self.type}'"")
        return self",Validate the in_value based on the type.,"def __post_init__ ( self , ) -> ""BaseSecurity"" : # dataclasses uses __post_init__ instead of model_validator valid_in_values = { ""apiKey"" : [ ""header"" , ""query"" , ""cookie"" ] , ""http"" : [ ""bearer"" , ""basic"" ] , ""oauth2"" : [ ""bearer"" ] , ""openIdConnect"" : [ ""bearer"" ] , ""mutualTLS"" : [ ""tls"" ] , ""unsupported"" : [ ""unsupported"" ] , } if self . in_value not in valid_in_values [ self . type ] : raise ValueError ( f""Invalid in_value '{self.in_value}' for type '{self.type}'"" ) return self",Validate the in_value based on the type.
/fastmcp/src/fastmcp/prompts/prompt.py,to_mcp_prompt,"def to_mcp_prompt(self, **overrides: Any) -> MCPPrompt:
        """"""Convert the prompt to an MCP prompt.""""""
        arguments = [
            MCPPromptArgument(
                name=arg.name,
                description=arg.description,
                required=arg.required,
            )
            for arg in self.arguments or []
        ]
        kwargs = {
            ""name"": self.name,
            ""description"": self.description,
            ""arguments"": arguments,
        }
        return MCPPrompt(**kwargs | overrides)","def to_mcp_prompt(self, **overrides: Any) -> MCPPrompt:
        """"""Convert the prompt to an MCP prompt.""""""
        arguments = [
            MCPPromptArgument(
                name=arg.name,
                description=arg.description,
                required=arg.required,
            )
            for arg in self.arguments or []
        ]
        kwargs = {
            ""name"": self.name,
            ""description"": self.description,
            ""arguments"": arguments,
        }
        return MCPPrompt(**kwargs | overrides)",Convert the prompt to an MCP prompt.,Convert the prompt to an MCP prompt.,"def to_mcp_prompt(self, **overrides: Any) -> MCPPrompt:
        
        arguments = [
            MCPPromptArgument(
                name=arg.name,
                description=arg.description,
                required=arg.required,
            )
            for arg in self.arguments or []
        ]
        kwargs = {
            ""name"": self.name,
            ""description"": self.description,
            ""arguments"": arguments,
        }
        return MCPPrompt(**kwargs | overrides)",Convert the prompt to an MCP prompt.,"def to_mcp_prompt ( self , ** overrides : Any ) -> MCPPrompt : arguments = [ MCPPromptArgument ( name = arg . name , description = arg . description , required = arg . required , ) for arg in self . arguments or [ ] ] kwargs = { ""name"" : self . name , ""description"" : self . description , ""arguments"" : arguments , } return MCPPrompt ( ** kwargs | overrides )",Convert the prompt to an MCP prompt.
/ag2/autogen/agentchat/group/context_variables.py,__getitem__,"def __getitem__(self, key: str) -> Any:
        """"""Get a value using dictionary syntax: context[key]""""""
        try:
            return self.data[key]
        except KeyError:
            raise KeyError(f""Context variable '{key}' not found"")","def __getitem__(self, key: str) -> Any:
        """"""Get a value using dictionary syntax: context[key]""""""
        try:
            return self.data[key]
        except KeyError:
            raise KeyError(f""Context variable '{key}' not found"")",Get a value using dictionary syntax: context[key],Get a value using dictionary syntax: context[key],"def __getitem__(self, key: str) -> Any:
        
        try:
            return self.data[key]
        except KeyError:
            raise KeyError(f""Context variable '{key}' not found"")",Get a value using dictionary syntax: context[key],"def __getitem__ ( self , key : str ) -> Any : try : return self . data [ key ] except KeyError : raise KeyError ( f""Context variable '{key}' not found"" )",Get a value using dictionary syntax: context[key]
/alphafold3/src/alphafold3/data/msa_store.py,__call__,"def __call__(
      self, query_sequence: str, chain_polymer_type: str
  ) -> tuple[msa.Msa, MsaErrors]:
    """"""Returns an MSA containing just the query sequence, never errors.""""""
    if chain_polymer_type != self._chain_polymer_type:
      raise ValueError(
          f'EmptyMsaProvider of type {self._chain_polymer_type} called with '
          f'sequence of {chain_polymer_type=}, {query_sequence=}.'
      )
    return (
        msa.Msa.from_empty(
            query_sequence=query_sequence,
            chain_poly_type=self._chain_polymer_type,
        ),
        (),
    )","def __call__(
      self, query_sequence: str, chain_polymer_type: str
  ) -> tuple[msa.Msa, MsaErrors]:
    """"""Returns an MSA containing just the query sequence, never errors.""""""
    if chain_polymer_type != self._chain_polymer_type:
      raise ValueError(
          f'EmptyMsaProvider of type {self._chain_polymer_type} called with '
          f'sequence of {chain_polymer_type=}, {query_sequence=}.'
      )
    return (
        msa.Msa.from_empty(
            query_sequence=query_sequence,
            chain_poly_type=self._chain_polymer_type,
        ),
        (),
    )","Returns an MSA containing just the query sequence, never errors.","Returns an MSA containing just the query sequence, never errors.","def __call__(
      self, query_sequence: str, chain_polymer_type: str
  ) -> tuple[msa.Msa, MsaErrors]:
    
    if chain_polymer_type != self._chain_polymer_type:
      raise ValueError(
          f'EmptyMsaProvider of type {self._chain_polymer_type} called with '
          f'sequence of {chain_polymer_type=}, {query_sequence=}.'
      )
    return (
        msa.Msa.from_empty(
            query_sequence=query_sequence,
            chain_poly_type=self._chain_polymer_type,
        ),
        (),
    )","Returns an MSA containing just the query sequence, never errors.","def __call__ ( self , query_sequence : str , chain_polymer_type : str ) -> tuple [ msa . Msa , MsaErrors ] : if chain_polymer_type != self . _chain_polymer_type : raise ValueError ( f'EmptyMsaProvider of type {self._chain_polymer_type} called with ' f'sequence of {chain_polymer_type=}, {query_sequence=}.' ) return ( msa . Msa . from_empty ( query_sequence = query_sequence , chain_poly_type = self . _chain_polymer_type , ) , ( ) , )","Returns an MSA containing just the query sequence, never errors."
/agenticSeek/sources/tools/searxSearch.py,interpreter_feedback,"def interpreter_feedback(self, output: str) -> str:
        """"""
        Feedback of web search to agent.
        """"""
        if self.execution_failure_check(output):
            return f""Web search failed: {output}""
        return f""Web search result:\n{output}""","def interpreter_feedback(self, output: str) -> str:
        """"""
        Feedback of web search to agent.
        """"""
        if self.execution_failure_check(output):
            return f""Web search failed: {output}""
        return f""Web search result:\n{output}""",Feedback of web search to agent.,Feedback of web search to agent.,"def interpreter_feedback(self, output: str) -> str:
        
        if self.execution_failure_check(output):
            return f""Web search failed: {output}""
        return f""Web search result:\n{output}""",Feedback of web search to agent.,"def interpreter_feedback ( self , output : str ) -> str : if self . execution_failure_check ( output ) : return f""Web search failed: {output}"" return f""Web search result:\n{output}""",Feedback of web search to agent.
/Second-Me/lpm_kernel/api/domains/trainprocess/trainprocess_service.py,map_your_entity_network,"def map_your_entity_network(self)->bool:
        """"""Map entity network using notes and basic info""""""
        try:
            # Mark step as in progress
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.IN_PROGRESS)
            logger.info(""Starting entity network mapping..."")
        
            # Get or prepare L2 data
            self._prepare_l2_data()

            l2_generator = L2Generator(
                data_path=os.path.join(os.getcwd(), ""resources"")
            )
            l2_generator.data_preprocess(self.l2_data[""notes""], self.l2_data[""basic_info""])
            
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.COMPLETED)
            logger.info(""Entity network mapping completed successfully"")
            return True
            
        except Exception as e:
            logger.error(f""Map entity network failed: {str(e)}"")
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.FAILED)
            self._cleanup_resources()
            return False","def map_your_entity_network(self)->bool:
        """"""Map entity network using notes and basic info""""""
        try:
            # Mark step as in progress
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.IN_PROGRESS)
            logger.info(""Starting entity network mapping..."")
        
            # Get or prepare L2 data
            self._prepare_l2_data()

            l2_generator = L2Generator(
                data_path=os.path.join(os.getcwd(), ""resources"")
            )
            l2_generator.data_preprocess(self.l2_data[""notes""], self.l2_data[""basic_info""])
            
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.COMPLETED)
            logger.info(""Entity network mapping completed successfully"")
            return True
            
        except Exception as e:
            logger.error(f""Map entity network failed: {str(e)}"")
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.FAILED)
            self._cleanup_resources()
            return False",Map entity network using notes and basic info,Map entity network using notes and basic info,"def map_your_entity_network(self)->bool:
        
        try:
            # Mark step as in progress
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.IN_PROGRESS)
            logger.info(""Starting entity network mapping..."")
        
            # Get or prepare L2 data
            self._prepare_l2_data()

            l2_generator = L2Generator(
                data_path=os.path.join(os.getcwd(), ""resources"")
            )
            l2_generator.data_preprocess(self.l2_data[""notes""], self.l2_data[""basic_info""])
            
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.COMPLETED)
            logger.info(""Entity network mapping completed successfully"")
            return True
            
        except Exception as e:
            logger.error(f""Map entity network failed: {str(e)}"")
            self.progress.mark_step_status(ProcessStep.MAP_ENTITY_NETWORK, Status.FAILED)
            self._cleanup_resources()
            return False",Map entity network using notes and basic info,"def map_your_entity_network ( self ) -> bool : try : # Mark step as in progress self . progress . mark_step_status ( ProcessStep . MAP_ENTITY_NETWORK , Status . IN_PROGRESS ) logger . info ( ""Starting entity network mapping..."" ) # Get or prepare L2 data self . _prepare_l2_data ( ) l2_generator = L2Generator ( data_path = os . path . join ( os . getcwd ( ) , ""resources"" ) ) l2_generator . data_preprocess ( self . l2_data [ ""notes"" ] , self . l2_data [ ""basic_info"" ] ) self . progress . mark_step_status ( ProcessStep . MAP_ENTITY_NETWORK , Status . COMPLETED ) logger . info ( ""Entity network mapping completed successfully"" ) return True except Exception as e : logger . error ( f""Map entity network failed: {str(e)}"" ) self . progress . mark_step_status ( ProcessStep . MAP_ENTITY_NETWORK , Status . FAILED ) self . _cleanup_resources ( ) return False",Map entity network using notes and basic info
/verl/verl/third_party/vllm/vllm_v_0_5_4/spmd_gpu_executor.py,initialize_cache,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        """"""Initialize the KV cache in all workers.""""""

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")","def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        """"""Initialize the KV cache in all workers.""""""

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")",Initialize the KV cache in all workers.,Initialize the KV cache in all workers.,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")",Initialize the KV cache in all workers.,"def initialize_cache ( self , num_gpu_blocks : int , num_cpu_blocks : int ) -> None : # NOTE: We log here to avoid multiple logs when number of workers is # greater than one. We could log in the engine, but not all executors # have GPUs. logger . info ( ""# GPU blocks: %d, # CPU blocks: %d"" , num_gpu_blocks , num_cpu_blocks ) self . cache_config . num_gpu_blocks = num_gpu_blocks self . cache_config . num_cpu_blocks = num_cpu_blocks if torch . distributed . get_rank ( ) == 0 : print ( f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"" ) self . worker . initialize_cache ( num_gpu_blocks = num_gpu_blocks , num_cpu_blocks = num_cpu_blocks ) if torch . distributed . get_rank ( ) == 0 : print ( f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"" )",Initialize the KV cache in all workers.
/mcp/src/amazon-keyspaces-mcp-server/awslabs/amazon_keyspaces_mcp_server/client.py,_build_service_characteristics,"def _build_service_characteristics(self) -> Dict[str, Any]:
        """"""Build service characteristics for Amazon Keyspaces.""""""
        characteristics: Dict[str, Any] = {
            'write_throughput_limitation': 'Amazon Keyspaces has specific throughput characteristics that differ from self-managed Cassandra',
            'implementation_notes': 'The service architecture imposes a 1MB item size limit and throughput constraints different from open-source Cassandra',
        }

        response_guidance = {
            'do_not_mention': ['DynamoDB', 'underlying implementation', 'AWS storage layer'],
            'preferred_terminology': [
                'Keyspaces architecture',
                'managed service design',
                'AWS distributed systems',
            ],
        }

        characteristics['response_guidance'] = response_guidance

        return characteristics","def _build_service_characteristics(self) -> Dict[str, Any]:
        """"""Build service characteristics for Amazon Keyspaces.""""""
        characteristics: Dict[str, Any] = {
            'write_throughput_limitation': 'Amazon Keyspaces has specific throughput characteristics that differ from self-managed Cassandra',
            'implementation_notes': 'The service architecture imposes a 1MB item size limit and throughput constraints different from open-source Cassandra',
        }

        response_guidance = {
            'do_not_mention': ['DynamoDB', 'underlying implementation', 'AWS storage layer'],
            'preferred_terminology': [
                'Keyspaces architecture',
                'managed service design',
                'AWS distributed systems',
            ],
        }

        characteristics['response_guidance'] = response_guidance

        return characteristics",Build service characteristics for Amazon Keyspaces.,Build service characteristics for Amazon Keyspaces.,"def _build_service_characteristics(self) -> Dict[str, Any]:
        
        characteristics: Dict[str, Any] = {
            'write_throughput_limitation': 'Amazon Keyspaces has specific throughput characteristics that differ from self-managed Cassandra',
            'implementation_notes': 'The service architecture imposes a 1MB item size limit and throughput constraints different from open-source Cassandra',
        }

        response_guidance = {
            'do_not_mention': ['DynamoDB', 'underlying implementation', 'AWS storage layer'],
            'preferred_terminology': [
                'Keyspaces architecture',
                'managed service design',
                'AWS distributed systems',
            ],
        }

        characteristics['response_guidance'] = response_guidance

        return characteristics",Build service characteristics for Amazon Keyspaces.,"def _build_service_characteristics ( self ) -> Dict [ str , Any ] : characteristics : Dict [ str , Any ] = { 'write_throughput_limitation' : 'Amazon Keyspaces has specific throughput characteristics that differ from self-managed Cassandra' , 'implementation_notes' : 'The service architecture imposes a 1MB item size limit and throughput constraints different from open-source Cassandra' , } response_guidance = { 'do_not_mention' : [ 'DynamoDB' , 'underlying implementation' , 'AWS storage layer' ] , 'preferred_terminology' : [ 'Keyspaces architecture' , 'managed service design' , 'AWS distributed systems' , ] , } characteristics [ 'response_guidance' ] = response_guidance return characteristics",Build service characteristics for Amazon Keyspaces.
/nv-ingest/client/src/nv_ingest_client/primitives/tasks/split.py,__str__,"def __str__(self) -> str:
        """"""
        Returns a string with the object's config and run time state
        """"""
        info = """"
        info += ""Split Task:\n""
        info += f""  tokenizer: {self._tokenizer}\n""
        info += f""  chunk_size: {self._chunk_size}\n""
        info += f""  chunk_overlap: {self._chunk_overlap}\n""
        for key, value in self._params.items():
            info += f""  {key}: {value}\n""
        return info","def __str__(self) -> str:
        """"""
        Returns a string with the object's config and run time state
        """"""
        info = """"
        info += ""Split Task:\n""
        info += f""  tokenizer: {self._tokenizer}\n""
        info += f""  chunk_size: {self._chunk_size}\n""
        info += f""  chunk_overlap: {self._chunk_overlap}\n""
        for key, value in self._params.items():
            info += f""  {key}: {value}\n""
        return info",Returns a string with the object's config and run time state,Returns a string with the object's config and run time state,"def __str__(self) -> str:
        
        info = """"
        info += ""Split Task:\n""
        info += f""  tokenizer: {self._tokenizer}\n""
        info += f""  chunk_size: {self._chunk_size}\n""
        info += f""  chunk_overlap: {self._chunk_overlap}\n""
        for key, value in self._params.items():
            info += f""  {key}: {value}\n""
        return info",Returns a string with the object's config and run time state,"def __str__ ( self ) -> str : info = """" info += ""Split Task:\n"" info += f""  tokenizer: {self._tokenizer}\n"" info += f""  chunk_size: {self._chunk_size}\n"" info += f""  chunk_overlap: {self._chunk_overlap}\n"" for key , value in self . _params . items ( ) : info += f""  {key}: {value}\n"" return info",Returns a string with the object's config and run time state
/Scrapling/scrapling/engines/camo.py,_get_camoufox_options,"def _get_camoufox_options(self):
        """"""Return consistent browser options dictionary for both sync and async methods""""""
        return {
            ""geoip"": self.geoip,
            ""proxy"": self.proxy,
            ""enable_cache"": True,
            ""addons"": self.addons,
            ""exclude_addons"": [] if self.disable_ads else [DefaultAddons.UBO],
            ""headless"": self.headless,
            ""humanize"": self.humanize,
            ""i_know_what_im_doing"": True,  # To turn warnings off with the user configurations
            ""allow_webgl"": self.allow_webgl,
            ""block_webrtc"": self.block_webrtc,
            ""block_images"": self.block_images,  # Careful! it makes some websites doesn't finish loading at all like stackoverflow even in headful
            ""os"": None if self.os_randomize else get_os_name(),
            **self.additional_arguments
        }","def _get_camoufox_options(self):
        """"""Return consistent browser options dictionary for both sync and async methods""""""
        return {
            ""geoip"": self.geoip,
            ""proxy"": self.proxy,
            ""enable_cache"": True,
            ""addons"": self.addons,
            ""exclude_addons"": [] if self.disable_ads else [DefaultAddons.UBO],
            ""headless"": self.headless,
            ""humanize"": self.humanize,
            ""i_know_what_im_doing"": True,  # To turn warnings off with the user configurations
            ""allow_webgl"": self.allow_webgl,
            ""block_webrtc"": self.block_webrtc,
            ""block_images"": self.block_images,  # Careful! it makes some websites doesn't finish loading at all like stackoverflow even in headful
            ""os"": None if self.os_randomize else get_os_name(),
            **self.additional_arguments
        }",Return consistent browser options dictionary for both sync and async methods,Return consistent browser options dictionary for both sync and async methods,"def _get_camoufox_options(self):
        
        return {
            ""geoip"": self.geoip,
            ""proxy"": self.proxy,
            ""enable_cache"": True,
            ""addons"": self.addons,
            ""exclude_addons"": [] if self.disable_ads else [DefaultAddons.UBO],
            ""headless"": self.headless,
            ""humanize"": self.humanize,
            ""i_know_what_im_doing"": True,  # To turn warnings off with the user configurations
            ""allow_webgl"": self.allow_webgl,
            ""block_webrtc"": self.block_webrtc,
            ""block_images"": self.block_images,  # Careful! it makes some websites doesn't finish loading at all like stackoverflow even in headful
            ""os"": None if self.os_randomize else get_os_name(),
            **self.additional_arguments
        }",Return consistent browser options dictionary for both sync and async methods,"def _get_camoufox_options ( self ) : return { ""geoip"" : self . geoip , ""proxy"" : self . proxy , ""enable_cache"" : True , ""addons"" : self . addons , ""exclude_addons"" : [ ] if self . disable_ads else [ DefaultAddons . UBO ] , ""headless"" : self . headless , ""humanize"" : self . humanize , ""i_know_what_im_doing"" : True , # To turn warnings off with the user configurations ""allow_webgl"" : self . allow_webgl , ""block_webrtc"" : self . block_webrtc , ""block_images"" : self . block_images , # Careful! it makes some websites doesn't finish loading at all like stackoverflow even in headful ""os"" : None if self . os_randomize else get_os_name ( ) , ** self . additional_arguments }",Return consistent browser options dictionary for both sync and async methods
/nexa-sdk/nexa/gguf/sd/stable_diffusion.py,validate_dimensions,"def validate_dimensions(dimension: Union[int, float], attribute_name: str) -> int:
    """"""Dimensions must be a multiple of 64 otherwise a GGML_ASSERT error is encountered.""""""
    dimension = int(dimension)
    if dimension <= 0 or dimension % 64 != 0:
        raise ValueError(f""The '{attribute_name}' must be a multiple of 64."")
    return dimension","def validate_dimensions(dimension: Union[int, float], attribute_name: str) -> int:
    """"""Dimensions must be a multiple of 64 otherwise a GGML_ASSERT error is encountered.""""""
    dimension = int(dimension)
    if dimension <= 0 or dimension % 64 != 0:
        raise ValueError(f""The '{attribute_name}' must be a multiple of 64."")
    return dimension",Dimensions must be a multiple of 64 otherwise a GGML_ASSERT error is encountered.,Dimensions must be a multiple of 64 otherwise a GGML_ASSERT error is encountered.,"def validate_dimensions(dimension: Union[int, float], attribute_name: str) -> int:
    
    dimension = int(dimension)
    if dimension <= 0 or dimension % 64 != 0:
        raise ValueError(f""The '{attribute_name}' must be a multiple of 64."")
    return dimension",Dimensions must be a multiple of 64 otherwise a GGML_ASSERT error is encountered.,"def validate_dimensions ( dimension : Union [ int , float ] , attribute_name : str ) -> int : dimension = int ( dimension ) if dimension <= 0 or dimension % 64 != 0 : raise ValueError ( f""The '{attribute_name}' must be a multiple of 64."" ) return dimension",Dimensions must be a multiple of 64 otherwise a GGML_ASSERT error is encountered.
/adk-python/tests/unittests/code_executors/test_code_executor_context.py,context_with_data,"def context_with_data() -> CodeExecutorContext:
  """"""Fixture for a CodeExecutorContext with some pre-populated data.""""""
  state_data = {
      ""_code_execution_context"": {
          ""execution_session_id"": ""session123"",
          ""processed_input_files"": [""file1.csv"", ""file2.txt""],
      },
      ""_code_executor_input_files"": [
          {""name"": ""input1.txt"", ""content"": ""YQ=="", ""mime_type"": ""text/plain""}
      ],
      ""_code_executor_error_counts"": {""invocationA"": 2},
  }
  state = State(state_data, {})
  return CodeExecutorContext(state)","def context_with_data() -> CodeExecutorContext:
  """"""Fixture for a CodeExecutorContext with some pre-populated data.""""""
  state_data = {
      ""_code_execution_context"": {
          ""execution_session_id"": ""session123"",
          ""processed_input_files"": [""file1.csv"", ""file2.txt""],
      },
      ""_code_executor_input_files"": [
          {""name"": ""input1.txt"", ""content"": ""YQ=="", ""mime_type"": ""text/plain""}
      ],
      ""_code_executor_error_counts"": {""invocationA"": 2},
  }
  state = State(state_data, {})
  return CodeExecutorContext(state)",Fixture for a CodeExecutorContext with some pre-populated data.,Fixture for a CodeExecutorContext with some pre-populated data.,"def context_with_data() -> CodeExecutorContext:
  
  state_data = {
      ""_code_execution_context"": {
          ""execution_session_id"": ""session123"",
          ""processed_input_files"": [""file1.csv"", ""file2.txt""],
      },
      ""_code_executor_input_files"": [
          {""name"": ""input1.txt"", ""content"": ""YQ=="", ""mime_type"": ""text/plain""}
      ],
      ""_code_executor_error_counts"": {""invocationA"": 2},
  }
  state = State(state_data, {})
  return CodeExecutorContext(state)",Fixture for a CodeExecutorContext with some pre-populated data.,"def context_with_data ( ) -> CodeExecutorContext : state_data = { ""_code_execution_context"" : { ""execution_session_id"" : ""session123"" , ""processed_input_files"" : [ ""file1.csv"" , ""file2.txt"" ] , } , ""_code_executor_input_files"" : [ { ""name"" : ""input1.txt"" , ""content"" : ""YQ=="" , ""mime_type"" : ""text/plain"" } ] , ""_code_executor_error_counts"" : { ""invocationA"" : 2 } , } state = State ( state_data , { } ) return CodeExecutorContext ( state )",Fixture for a CodeExecutorContext with some pre-populated data.
/potpie/app/modules/intelligence/tools/web_tools/github_add_pr_comment.py,_format_comment_body,"def _format_comment_body(self, comment: GitHubPRComment) -> str:
        """"""Format a comment body with code snippet and suggestion if provided.""""""
        body = comment.comment_body

        # Add code snippet reference if provided
        if comment.code_snippet:
            body += f""\n\n```\n{comment.code_snippet}\n```""

        # Add suggestion if provided
        if comment.suggestion:
            body += f""\n\n```suggestion\n{comment.suggestion}\n```""

        return body","def _format_comment_body(self, comment: GitHubPRComment) -> str:
        """"""Format a comment body with code snippet and suggestion if provided.""""""
        body = comment.comment_body

        # Add code snippet reference if provided
        if comment.code_snippet:
            body += f""\n\n```\n{comment.code_snippet}\n```""

        # Add suggestion if provided
        if comment.suggestion:
            body += f""\n\n```suggestion\n{comment.suggestion}\n```""

        return body",Format a comment body with code snippet and suggestion if provided.,Format a comment body with code snippet and suggestion if provided.,"def _format_comment_body(self, comment: GitHubPRComment) -> str:
        
        body = comment.comment_body

        # Add code snippet reference if provided
        if comment.code_snippet:
            body += f""\n\n```\n{comment.code_snippet}\n```""

        # Add suggestion if provided
        if comment.suggestion:
            body += f""\n\n```suggestion\n{comment.suggestion}\n```""

        return body",Format a comment body with code snippet and suggestion if provided.,"def _format_comment_body ( self , comment : GitHubPRComment ) -> str : body = comment . comment_body # Add code snippet reference if provided if comment . code_snippet : body += f""\n\n```\n{comment.code_snippet}\n```"" # Add suggestion if provided if comment . suggestion : body += f""\n\n```suggestion\n{comment.suggestion}\n```"" return body",Format a comment body with code snippet and suggestion if provided.
/morphik-core/ee/services/connectors/zotero_connector.py,_save_credentials,"def _save_credentials(self) -> None:
        """"""Save credentials to file.""""""
        if not self.credentials:
            logger.error(f""Attempted to save null credentials for user {self.user_morphik_id}."")
            return

        creds_path = self._get_user_credentials_path()
        try:
            with open(creds_path, ""w"") as creds_file:
                json.dump(self.credentials, creds_file)
            logger.info(f""Successfully saved Zotero credentials for user {self.user_morphik_id}"")
        except Exception as e:
            logger.error(f""Failed to save Zotero credentials for user {self.user_morphik_id}: {e}"")","def _save_credentials(self) -> None:
        """"""Save credentials to file.""""""
        if not self.credentials:
            logger.error(f""Attempted to save null credentials for user {self.user_morphik_id}."")
            return

        creds_path = self._get_user_credentials_path()
        try:
            with open(creds_path, ""w"") as creds_file:
                json.dump(self.credentials, creds_file)
            logger.info(f""Successfully saved Zotero credentials for user {self.user_morphik_id}"")
        except Exception as e:
            logger.error(f""Failed to save Zotero credentials for user {self.user_morphik_id}: {e}"")",Save credentials to file.,Save credentials to file.,"def _save_credentials(self) -> None:
        
        if not self.credentials:
            logger.error(f""Attempted to save null credentials for user {self.user_morphik_id}."")
            return

        creds_path = self._get_user_credentials_path()
        try:
            with open(creds_path, ""w"") as creds_file:
                json.dump(self.credentials, creds_file)
            logger.info(f""Successfully saved Zotero credentials for user {self.user_morphik_id}"")
        except Exception as e:
            logger.error(f""Failed to save Zotero credentials for user {self.user_morphik_id}: {e}"")",Save credentials to file.,"def _save_credentials ( self ) -> None : if not self . credentials : logger . error ( f""Attempted to save null credentials for user {self.user_morphik_id}."" ) return creds_path = self . _get_user_credentials_path ( ) try : with open ( creds_path , ""w"" ) as creds_file : json . dump ( self . credentials , creds_file ) logger . info ( f""Successfully saved Zotero credentials for user {self.user_morphik_id}"" ) except Exception as e : logger . error ( f""Failed to save Zotero credentials for user {self.user_morphik_id}: {e}"" )",Save credentials to file.
/ai-hedge-fund/src/utils/llm.py,extract_json_from_response,"def extract_json_from_response(content: str) -> dict | None:
    """"""Extracts JSON from markdown-formatted response.""""""
    try:
        json_start = content.find(""```json"")
        if json_start != -1:
            json_text = content[json_start + 7 :]  # Skip past ```json
            json_end = json_text.find(""```"")
            if json_end != -1:
                json_text = json_text[:json_end].strip()
                return json.loads(json_text)
    except Exception as e:
        print(f""Error extracting JSON from response: {e}"")
    return None","def extract_json_from_response(content: str) -> dict | None:
    """"""Extracts JSON from markdown-formatted response.""""""
    try:
        json_start = content.find(""```json"")
        if json_start != -1:
            json_text = content[json_start + 7 :]  # Skip past ```json
            json_end = json_text.find(""```"")
            if json_end != -1:
                json_text = json_text[:json_end].strip()
                return json.loads(json_text)
    except Exception as e:
        print(f""Error extracting JSON from response: {e}"")
    return None",Extracts JSON from markdown-formatted response.,Extracts JSON from markdown-formatted response.,"def extract_json_from_response(content: str) -> dict | None:
    
    try:
        json_start = content.find(""```json"")
        if json_start != -1:
            json_text = content[json_start + 7 :]  # Skip past ```json
            json_end = json_text.find(""```"")
            if json_end != -1:
                json_text = json_text[:json_end].strip()
                return json.loads(json_text)
    except Exception as e:
        print(f""Error extracting JSON from response: {e}"")
    return None",Extracts JSON from markdown-formatted response.,"def extract_json_from_response ( content : str ) -> dict | None : try : json_start = content . find ( ""```json"" ) if json_start != - 1 : json_text = content [ json_start + 7 : ] # Skip past ```json json_end = json_text . find ( ""```"" ) if json_end != - 1 : json_text = json_text [ : json_end ] . strip ( ) return json . loads ( json_text ) except Exception as e : print ( f""Error extracting JSON from response: {e}"" ) return None",Extracts JSON from markdown-formatted response.
/olmocr/olmocr/train/hf/warc_parser.py,extract_target_uri_s3,"def extract_target_uri_s3(bucket, key, s3_client, head_bytes=1048576):
    """"""
    Retrieves the first head_bytes bytes (1 MB by default) from the S3 object using a range request,
    and extracts the first response record's target URI from the HTTP headers.
    """"""
    target_uri = None
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key, Range=f""bytes=0-{head_bytes-1}"")
        stream = response[""Body""]
        for record in ArchiveIterator(stream):
            for name, value in record.rec_headers.headers:
                if name == ""WARC-Target-URI"":
                    target_uri = value
                    break
            if target_uri:
                break  # Only use the first valid response record
    except Exception as e:
        tqdm.write(f""Error processing s3://{bucket}/{key}: {e}"")
    return target_uri","def extract_target_uri_s3(bucket, key, s3_client, head_bytes=1048576):
    """"""
    Retrieves the first head_bytes bytes (1 MB by default) from the S3 object using a range request,
    and extracts the first response record's target URI from the HTTP headers.
    """"""
    target_uri = None
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key, Range=f""bytes=0-{head_bytes-1}"")
        stream = response[""Body""]
        for record in ArchiveIterator(stream):
            for name, value in record.rec_headers.headers:
                if name == ""WARC-Target-URI"":
                    target_uri = value
                    break
            if target_uri:
                break  # Only use the first valid response record
    except Exception as e:
    return target_uri","Retrieves the first head_bytes bytes (1 MB by default) from the S3 object using a range request,
and extracts the first response record's target URI from the HTTP headers.","Retrieves the first head_bytes bytes (1 MB by default) from the S3 object using a range request, and extracts the first response record's target URI from the HTTP headers.","def extract_target_uri_s3(bucket, key, s3_client, head_bytes=1048576):
    
    target_uri = None
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key, Range=f""bytes=0-{head_bytes-1}"")
        stream = response[""Body""]
        for record in ArchiveIterator(stream):
            for name, value in record.rec_headers.headers:
                if name == ""WARC-Target-URI"":
                    target_uri = value
                    break
            if target_uri:
                break  # Only use the first valid response record
    except Exception as e:
    return target_uri","Retrieves the first head_bytes bytes (1 MB by default) from the S3 object using a range request, and extracts the first response record's target URI from the HTTP headers.","def extract_target_uri_s3 ( bucket , key , s3_client , head_bytes = 1048576 ) : target_uri = None try : response = s3_client . get_object ( Bucket = bucket , Key = key , Range = f""bytes=0-{head_bytes-1}"" ) stream = response [ ""Body"" ] for record in ArchiveIterator ( stream ) : for name , value in record . rec_headers . headers : if name == ""WARC-Target-URI"" : target_uri = value break if target_uri : break # Only use the first valid response record except Exception as e : return target_uri","Retrieves the first head_bytes bytes (1 MB by default) from the S3 object using a range request, and extracts the first response record's target URI from the HTTP headers."
/openai-agents-python/examples/voice/streamed/my_workflow.py,get_weather,"def get_weather(city: str) -> str:
    """"""Get the weather for a given city.""""""
    print(f""[debug] get_weather called with city: {city}"")
    choices = [""sunny"", ""cloudy"", ""rainy"", ""snowy""]
    return f""The weather in {city} is {random.choice(choices)}.""","def get_weather(city: str) -> str:
    """"""Get the weather for a given city.""""""
    print(f""[debug] get_weather called with city: {city}"")
    choices = [""sunny"", ""cloudy"", ""rainy"", ""snowy""]
    return f""The weather in {city} is {random.choice(choices)}.""",Get the weather for a given city.,Get the weather for a given city.,"def get_weather(city: str) -> str:
    
    print(f""[debug] get_weather called with city: {city}"")
    choices = [""sunny"", ""cloudy"", ""rainy"", ""snowy""]
    return f""The weather in {city} is {random.choice(choices)}.""",Get the weather for a given city.,"def get_weather ( city : str ) -> str : print ( f""[debug] get_weather called with city: {city}"" ) choices = [ ""sunny"" , ""cloudy"" , ""rainy"" , ""snowy"" ] return f""The weather in {city} is {random.choice(choices)}.""",Get the weather for a given city.
/local-deep-research/examples/optimization/multi_benchmark_speed_demo.py,evaluate,"def evaluate(self, system_config, num_examples=1, output_dir=None):
        """"""Simulate benchmark evaluation with predefined scores.""""""
        print(f""[SIM] Running {self.name} benchmark simulation..."")
        print(f""[SIM] System config: {system_config}"")

        # Return simulated results
        return {
            ""quality_score"": self.quality_score,
            ""speed_score"": self.speed_score,
            ""component_timing"": {
                ""search"": 0.5,
                ""processing"": 0.3,
                ""llm"": 1.2,
                ""total"": 2.0,
            },
            ""resource_usage"": {""memory_mb"": 500, ""cpu_percent"": 30},
        }","def evaluate(self, system_config, num_examples=1, output_dir=None):
        """"""Simulate benchmark evaluation with predefined scores.""""""
        print(f""[SIM] Running {self.name} benchmark simulation..."")
        print(f""[SIM] System config: {system_config}"")

        # Return simulated results
        return {
            ""quality_score"": self.quality_score,
            ""speed_score"": self.speed_score,
            ""component_timing"": {
                ""search"": 0.5,
                ""processing"": 0.3,
                ""llm"": 1.2,
                ""total"": 2.0,
            },
            ""resource_usage"": {""memory_mb"": 500, ""cpu_percent"": 30},
        }",Simulate benchmark evaluation with predefined scores.,Simulate benchmark evaluation with predefined scores.,"def evaluate(self, system_config, num_examples=1, output_dir=None):
        
        print(f""[SIM] Running {self.name} benchmark simulation..."")
        print(f""[SIM] System config: {system_config}"")

        # Return simulated results
        return {
            ""quality_score"": self.quality_score,
            ""speed_score"": self.speed_score,
            ""component_timing"": {
                ""search"": 0.5,
                ""processing"": 0.3,
                ""llm"": 1.2,
                ""total"": 2.0,
            },
            ""resource_usage"": {""memory_mb"": 500, ""cpu_percent"": 30},
        }",Simulate benchmark evaluation with predefined scores.,"def evaluate ( self , system_config , num_examples = 1 , output_dir = None ) : print ( f""[SIM] Running {self.name} benchmark simulation..."" ) print ( f""[SIM] System config: {system_config}"" ) # Return simulated results return { ""quality_score"" : self . quality_score , ""speed_score"" : self . speed_score , ""component_timing"" : { ""search"" : 0.5 , ""processing"" : 0.3 , ""llm"" : 1.2 , ""total"" : 2.0 , } , ""resource_usage"" : { ""memory_mb"" : 500 , ""cpu_percent"" : 30 } , }",Simulate benchmark evaluation with predefined scores.
/OpenManus/app/logger.py,define_log_level,"def define_log_level(print_level=""INFO"", logfile_level=""DEBUG"", name: str = None):
    """"""Adjust the log level to above level""""""
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime(""%Y%m%d%H%M%S"")
    log_name = (
        f""{name}_{formatted_date}"" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()
    _logger.add(sys.stderr, level=print_level)
    _logger.add(PROJECT_ROOT / f""logs/{log_name}.log"", level=logfile_level)
    return _logger","def define_log_level(print_level=""INFO"", logfile_level=""DEBUG"", name: str = None):
    """"""Adjust the log level to above level""""""
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime(""%Y%m%d%H%M%S"")
    log_name = (
        f""{name}_{formatted_date}"" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()
    _logger.add(sys.stderr, level=print_level)
    _logger.add(PROJECT_ROOT / f""logs/{log_name}.log"", level=logfile_level)
    return _logger",Adjust the log level to above level,Adjust the log level to above level,"def define_log_level(print_level=""INFO"", logfile_level=""DEBUG"", name: str = None):
    
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime(""%Y%m%d%H%M%S"")
    log_name = (
        f""{name}_{formatted_date}"" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()
    _logger.add(sys.stderr, level=print_level)
    _logger.add(PROJECT_ROOT / f""logs/{log_name}.log"", level=logfile_level)
    return _logger",Adjust the log level to above level,"def define_log_level ( print_level = ""INFO"" , logfile_level = ""DEBUG"" , name : str = None ) : global _print_level _print_level = print_level current_date = datetime . now ( ) formatted_date = current_date . strftime ( ""%Y%m%d%H%M%S"" ) log_name = ( f""{name}_{formatted_date}"" if name else formatted_date ) # name a log with prefix name _logger . remove ( ) _logger . add ( sys . stderr , level = print_level ) _logger . add ( PROJECT_ROOT / f""logs/{log_name}.log"" , level = logfile_level ) return _logger",Adjust the log level to above level
/ai-hedge-fund/src/agents/technicals.py,normalize_pandas,"def normalize_pandas(obj):
    """"""Convert pandas Series/DataFrames to primitive Python types""""""
    if isinstance(obj, pd.Series):
        return obj.tolist()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict(""records"")
    elif isinstance(obj, dict):
        return {k: normalize_pandas(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [normalize_pandas(item) for item in obj]
    return obj","def normalize_pandas(obj):
    """"""Convert pandas Series/DataFrames to primitive Python types""""""
    if isinstance(obj, pd.Series):
        return obj.tolist()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict(""records"")
    elif isinstance(obj, dict):
        return {k: normalize_pandas(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [normalize_pandas(item) for item in obj]
    return obj",Convert pandas Series/DataFrames to primitive Python types,Convert pandas Series/DataFrames to primitive Python types,"def normalize_pandas(obj):
    
    if isinstance(obj, pd.Series):
        return obj.tolist()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict(""records"")
    elif isinstance(obj, dict):
        return {k: normalize_pandas(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [normalize_pandas(item) for item in obj]
    return obj",Convert pandas Series/DataFrames to primitive Python types,"def normalize_pandas ( obj ) : if isinstance ( obj , pd . Series ) : return obj . tolist ( ) elif isinstance ( obj , pd . DataFrame ) : return obj . to_dict ( ""records"" ) elif isinstance ( obj , dict ) : return { k : normalize_pandas ( v ) for k , v in obj . items ( ) } elif isinstance ( obj , ( list , tuple ) ) : return [ normalize_pandas ( item ) for item in obj ] return obj",Convert pandas Series/DataFrames to primitive Python types
/Second-Me/lpm_kernel/api/domains/kernel2/services/advanced_chat_service.py,format_solution,"def format_solution(self, solution: str) -> str:
        """"""Format the final solution""""""
        logger.info(""Starting solution formatting phase..."")
        logger.info(f""Formatting solution of length {len(solution)} characters..."")
        
        chat_request = ChatRequest(
            message=solution,
            system_prompt="""",  # Will be set by strategy
            temperature=0.3  # Lower temperature for more consistent formatting
        )
        
        logger.info(""Calling chat service with SolutionFormatterStrategy..."")
        response = chat_service.chat(
            request=chat_request,
            strategy_chain=[BasePromptStrategy, SolutionFormatterStrategy],
            stream=False,
            json_response=False
        )
        
        formatted_solution = response.choices[0].message.content
        logger.info(f""Formatting completed. Result length: {len(formatted_solution)} characters"")
        logger.info(f""First 100 characters of formatted solution: {formatted_solution[:100]}..."")
        return formatted_solution","def format_solution(self, solution: str) -> str:
        """"""Format the final solution""""""
        logger.info(""Starting solution formatting phase..."")
        logger.info(f""Formatting solution of length {len(solution)} characters..."")
        
        chat_request = ChatRequest(
            message=solution,
            system_prompt="""",  # Will be set by strategy
            temperature=0.3  # Lower temperature for more consistent formatting
        )
        
        logger.info(""Calling chat service with SolutionFormatterStrategy..."")
        response = chat_service.chat(
            request=chat_request,
            strategy_chain=[BasePromptStrategy, SolutionFormatterStrategy],
            stream=False,
            json_response=False
        )
        
        formatted_solution = response.choices[0].message.content
        logger.info(f""Formatting completed. Result length: {len(formatted_solution)} characters"")
        logger.info(f""First 100 characters of formatted solution: {formatted_solution[:100]}..."")
        return formatted_solution",Format the final solution,Format the final solution,"def format_solution(self, solution: str) -> str:
        
        logger.info(""Starting solution formatting phase..."")
        logger.info(f""Formatting solution of length {len(solution)} characters..."")
        
        chat_request = ChatRequest(
            message=solution,
            system_prompt="""",  # Will be set by strategy
            temperature=0.3  # Lower temperature for more consistent formatting
        )
        
        logger.info(""Calling chat service with SolutionFormatterStrategy..."")
        response = chat_service.chat(
            request=chat_request,
            strategy_chain=[BasePromptStrategy, SolutionFormatterStrategy],
            stream=False,
            json_response=False
        )
        
        formatted_solution = response.choices[0].message.content
        logger.info(f""Formatting completed. Result length: {len(formatted_solution)} characters"")
        logger.info(f""First 100 characters of formatted solution: {formatted_solution[:100]}..."")
        return formatted_solution",Format the final solution,"def format_solution ( self , solution : str ) -> str : logger . info ( ""Starting solution formatting phase..."" ) logger . info ( f""Formatting solution of length {len(solution)} characters..."" ) chat_request = ChatRequest ( message = solution , system_prompt = """" , # Will be set by strategy temperature = 0.3 # Lower temperature for more consistent formatting ) logger . info ( ""Calling chat service with SolutionFormatterStrategy..."" ) response = chat_service . chat ( request = chat_request , strategy_chain = [ BasePromptStrategy , SolutionFormatterStrategy ] , stream = False , json_response = False ) formatted_solution = response . choices [ 0 ] . message . content logger . info ( f""Formatting completed. Result length: {len(formatted_solution)} characters"" ) logger . info ( f""First 100 characters of formatted solution: {formatted_solution[:100]}..."" ) return formatted_solution",Format the final solution
/potpie/app/modules/auth/api_key_service.py,get_client_and_project,"def get_client_and_project():
        """"""Get Secret Manager client and project ID based on environment.""""""
        is_dev_mode = os.getenv(""isDevelopmentMode"", ""enabled"") == ""enabled""
        if is_dev_mode:
            return None, None

        project_id = os.environ.get(""GCP_PROJECT"")
        if not project_id:
            raise HTTPException(
                status_code=500, detail=""GCP_PROJECT environment variable is not set""
            )

        try:
            client = secretmanager.SecretManagerServiceClient()
            return client, project_id
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f""Failed to initialize Secret Manager client: {str(e)}"",
            )","def get_client_and_project():
        """"""Get Secret Manager client and project ID based on environment.""""""
        is_dev_mode = os.getenv(""isDevelopmentMode"", ""enabled"") == ""enabled""
        if is_dev_mode:
            return None, None

        project_id = os.environ.get(""GCP_PROJECT"")
        if not project_id:
            raise HTTPException(
                status_code=500, detail=""GCP_PROJECT environment variable is not set""
            )

        try:
            client = secretmanager.SecretManagerServiceClient()
            return client, project_id
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f""Failed to initialize Secret Manager client: {str(e)}"",
            )",Get Secret Manager client and project ID based on environment.,Get Secret Manager client and project ID based on environment.,"def get_client_and_project():
        
        is_dev_mode = os.getenv(""isDevelopmentMode"", ""enabled"") == ""enabled""
        if is_dev_mode:
            return None, None

        project_id = os.environ.get(""GCP_PROJECT"")
        if not project_id:
            raise HTTPException(
                status_code=500, detail=""GCP_PROJECT environment variable is not set""
            )

        try:
            client = secretmanager.SecretManagerServiceClient()
            return client, project_id
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f""Failed to initialize Secret Manager client: {str(e)}"",
            )",Get Secret Manager client and project ID based on environment.,"def get_client_and_project ( ) : is_dev_mode = os . getenv ( ""isDevelopmentMode"" , ""enabled"" ) == ""enabled"" if is_dev_mode : return None , None project_id = os . environ . get ( ""GCP_PROJECT"" ) if not project_id : raise HTTPException ( status_code = 500 , detail = ""GCP_PROJECT environment variable is not set"" ) try : client = secretmanager . SecretManagerServiceClient ( ) return client , project_id except Exception as e : raise HTTPException ( status_code = 500 , detail = f""Failed to initialize Secret Manager client: {str(e)}"" , )",Get Secret Manager client and project ID based on environment.
/local-deep-research/src/local_deep_research/benchmarks/datasets/custom_dataset_template.py,get_dataset_info,"def get_dataset_info(cls) -> Dict[str, str]:
        """"""Get basic information about the dataset.""""""
        return {
            ""id"": ""custom"",  # Unique identifier for the dataset
            ""name"": ""Custom Dataset"",  # Human-readable name
            ""description"": ""Template for a custom benchmark dataset"",  # Description
            ""url"": cls.get_default_dataset_path(),  # Default URL or path
        }","def get_dataset_info(cls) -> Dict[str, str]:
        """"""Get basic information about the dataset.""""""
        return {
            ""id"": ""custom"",  # Unique identifier for the dataset
            ""name"": ""Custom Dataset"",  # Human-readable name
            ""description"": ""Template for a custom benchmark dataset"",  # Description
            ""url"": cls.get_default_dataset_path(),  # Default URL or path
        }",Get basic information about the dataset.,Get basic information about the dataset.,"def get_dataset_info(cls) -> Dict[str, str]:
        
        return {
            ""id"": ""custom"",  # Unique identifier for the dataset
            ""name"": ""Custom Dataset"",  # Human-readable name
            ""description"": ""Template for a custom benchmark dataset"",  # Description
            ""url"": cls.get_default_dataset_path(),  # Default URL or path
        }",Get basic information about the dataset.,"def get_dataset_info ( cls ) -> Dict [ str , str ] : return { ""id"" : ""custom"" , # Unique identifier for the dataset ""name"" : ""Custom Dataset"" , # Human-readable name ""description"" : ""Template for a custom benchmark dataset"" , # Description ""url"" : cls . get_default_dataset_path ( ) , # Default URL or path }",Get basic information about the dataset.
/alphafold3/src/alphafold3/model/feat_batch.py,as_data_dict,"def as_data_dict(self) -> features.BatchDict:
    """"""Converts batch object to dictionary.""""""
    output = {
        **self.msa.as_data_dict(),
        **self.templates.as_data_dict(),
        **self.token_features.as_data_dict(),
        **self.ref_structure.as_data_dict(),
        **self.predicted_structure_info.as_data_dict(),
        **self.polymer_ligand_bond_info.as_data_dict(),
        **self.ligand_ligand_bond_info.as_data_dict(),
        **self.pseudo_beta_info.as_data_dict(),
        **self.atom_cross_att.as_data_dict(),
        **self.convert_model_output.as_data_dict(),
        **self.frames.as_data_dict(),
    }
    return output","def as_data_dict(self) -> features.BatchDict:
    """"""Converts batch object to dictionary.""""""
    output = {
        **self.msa.as_data_dict(),
        **self.templates.as_data_dict(),
        **self.token_features.as_data_dict(),
        **self.ref_structure.as_data_dict(),
        **self.predicted_structure_info.as_data_dict(),
        **self.polymer_ligand_bond_info.as_data_dict(),
        **self.ligand_ligand_bond_info.as_data_dict(),
        **self.pseudo_beta_info.as_data_dict(),
        **self.atom_cross_att.as_data_dict(),
        **self.convert_model_output.as_data_dict(),
        **self.frames.as_data_dict(),
    }
    return output",Converts batch object to dictionary.,Converts batch object to dictionary.,"def as_data_dict(self) -> features.BatchDict:
    
    output = {
        **self.msa.as_data_dict(),
        **self.templates.as_data_dict(),
        **self.token_features.as_data_dict(),
        **self.ref_structure.as_data_dict(),
        **self.predicted_structure_info.as_data_dict(),
        **self.polymer_ligand_bond_info.as_data_dict(),
        **self.ligand_ligand_bond_info.as_data_dict(),
        **self.pseudo_beta_info.as_data_dict(),
        **self.atom_cross_att.as_data_dict(),
        **self.convert_model_output.as_data_dict(),
        **self.frames.as_data_dict(),
    }
    return output",Converts batch object to dictionary.,"def as_data_dict ( self ) -> features . BatchDict : output = { ** self . msa . as_data_dict ( ) , ** self . templates . as_data_dict ( ) , ** self . token_features . as_data_dict ( ) , ** self . ref_structure . as_data_dict ( ) , ** self . predicted_structure_info . as_data_dict ( ) , ** self . polymer_ligand_bond_info . as_data_dict ( ) , ** self . ligand_ligand_bond_info . as_data_dict ( ) , ** self . pseudo_beta_info . as_data_dict ( ) , ** self . atom_cross_att . as_data_dict ( ) , ** self . convert_model_output . as_data_dict ( ) , ** self . frames . as_data_dict ( ) , } return output",Converts batch object to dictionary.
/NLWeb/code/core/fastTrack.py,is_fastTrack_eligible,"def is_fastTrack_eligible(self):
        """"""Check if query is eligible for fast track processing""""""
        if (self.handler.context_url != ''):
            logger.debug(""Fast track not eligible: context_url present"")
            return False
        if (len(self.handler.prev_queries) > 0):
            logger.debug(f""Fast track not eligible: {len(self.handler.prev_queries)} previous queries present"")
            return False
        logger.info(""Query is eligible for fast track"")
        return True","def is_fastTrack_eligible(self):
        """"""Check if query is eligible for fast track processing""""""
        if (self.handler.context_url != ''):
            logger.debug(""Fast track not eligible: context_url present"")
            return False
        if (len(self.handler.prev_queries) > 0):
            logger.debug(f""Fast track not eligible: {len(self.handler.prev_queries)} previous queries present"")
            return False
        logger.info(""Query is eligible for fast track"")
        return True",Check if query is eligible for fast track processing,Check if query is eligible for fast track processing,"def is_fastTrack_eligible(self):
        
        if (self.handler.context_url != ''):
            logger.debug(""Fast track not eligible: context_url present"")
            return False
        if (len(self.handler.prev_queries) > 0):
            logger.debug(f""Fast track not eligible: {len(self.handler.prev_queries)} previous queries present"")
            return False
        logger.info(""Query is eligible for fast track"")
        return True",Check if query is eligible for fast track processing,"def is_fastTrack_eligible ( self ) : if ( self . handler . context_url != '' ) : logger . debug ( ""Fast track not eligible: context_url present"" ) return False if ( len ( self . handler . prev_queries ) > 0 ) : logger . debug ( f""Fast track not eligible: {len(self.handler.prev_queries)} previous queries present"" ) return False logger . info ( ""Query is eligible for fast track"" ) return True",Check if query is eligible for fast track processing
/browser-use/tests/extraction_test.py,get_price_per_token,"def get_price_per_token(model: str) -> float:
		""""""Get the price per token for a specified model.

		@todo: move to utils, use a package or sth
		""""""
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]","def get_price_per_token(model: str) -> float:
		""""""Get the price per token for a specified model.

		@todo: move to utils, use a package or sth
		""""""
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]","Get the price per token for a specified model.

@todo: move to utils, use a package or sth",Get the price per token for a specified model.,"def get_price_per_token(model: str) -> float:
		
		prices = {
			'gpt-4o': 2.5 / 1e6,
			'gpt-4o-mini': 0.15 / 1e6,
		}
		return prices[model]",Get the price per token for a specified model.,"def get_price_per_token ( model : str ) -> float : prices = { 'gpt-4o' : 2.5 / 1e6 , 'gpt-4o-mini' : 0.15 / 1e6 , } return prices [ model ]",Get the price per token for a specified model.
/open_deep_research/src/open_deep_research/multi_agent.py,get_research_tools,"def get_research_tools(config: RunnableConfig):
    """"""Get research tools based on configuration""""""
    search_tool = get_search_tool(config)
    tool_list = [search_tool, Section]
    return tool_list, {tool.name: tool for tool in tool_list}","def get_research_tools(config: RunnableConfig):
    """"""Get research tools based on configuration""""""
    search_tool = get_search_tool(config)
    tool_list = [search_tool, Section]
    return tool_list, {tool.name: tool for tool in tool_list}",Get research tools based on configuration,Get research tools based on configuration,"def get_research_tools(config: RunnableConfig):
    
    search_tool = get_search_tool(config)
    tool_list = [search_tool, Section]
    return tool_list, {tool.name: tool for tool in tool_list}",Get research tools based on configuration,"def get_research_tools ( config : RunnableConfig ) : search_tool = get_search_tool ( config ) tool_list = [ search_tool , Section ] return tool_list , { tool . name : tool for tool in tool_list }",Get research tools based on configuration
/local-deep-research/src/local_deep_research/advanced_search_system/strategies/iterdrag_strategy.py,_generate_subqueries,"def _generate_subqueries(
        self, query: str, initial_results: List[Dict], current_knowledge: str
    ) -> List[str]:
        """"""Generate sub-queries based on initial search results and the main query.""""""
        try:
            # Format context for question generation
            context = f""""""Current Query: {query}
Current Date: {datetime.now().strftime('%Y-%m-%d')}
Past Questions: {self.questions_by_iteration}
Current Knowledge: {current_knowledge}

Initial Search Results:
{json.dumps(initial_results, indent=2)}""""""

            # Generate sub-queries using the question generator
            return self.question_generator.generate_questions(
                query, context, int(get_db_setting(""search.questions_per_iteration""))
            )
        except Exception:
            logger.exception(""Error generating sub-queries"")
            return []","def _generate_subqueries(
        self, query: str, initial_results: List[Dict], current_knowledge: str
    ) -> List[str]:
        """"""Generate sub-queries based on initial search results and the main query.""""""
        try:
            # Format context for question generation
            context = f""""""Current Query: {query}
Current Date: {datetime.now().strftime('%Y-%m-%d')}
Past Questions: {self.questions_by_iteration}
Current Knowledge: {current_knowledge}

Initial Search Results:
{json.dumps(initial_results, indent=2)}""""""

            # Generate sub-queries using the question generator
            return self.question_generator.generate_questions(
                query, context, int(get_db_setting(""search.questions_per_iteration""))
            )
        except Exception:
            logger.exception(""Error generating sub-queries"")
            return []",Generate sub-queries based on initial search results and the main query.,Generate sub-queries based on initial search results and the main query.,"def _generate_subqueries(
        self, query: str, initial_results: List[Dict], current_knowledge: str
    ) -> List[str]:
        
        try:
            # Format context for question generation
            context = f

            # Generate sub-queries using the question generator
            return self.question_generator.generate_questions(
                query, context, int(get_db_setting(""search.questions_per_iteration""))
            )
        except Exception:
            logger.exception(""Error generating sub-queries"")
            return []",Generate sub-queries based on initial search results and the main query.,"def _generate_subqueries ( self , query : str , initial_results : List [ Dict ] , current_knowledge : str ) -> List [ str ] : try : # Format context for question generation context = f # Generate sub-queries using the question generator return self . question_generator . generate_questions ( query , context , int ( get_db_setting ( ""search.questions_per_iteration"" ) ) ) except Exception : logger . exception ( ""Error generating sub-queries"" ) return [ ]",Generate sub-queries based on initial search results and the main query.
/OpenManus-RL/openmanus_rl/agentgym/agentenv-gaia/gaia/config.py,load_server_config,"def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        """"""Load MCP server configuration from JSON file""""""
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if not config_file:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig(
                        type=server_config[""type""],
                        url=server_config.get(""url""),
                        command=server_config.get(""command""),
                        args=server_config.get(""args"", []),
                    )
                return servers
        except Exception as e:
            raise ValueError(f""Failed to load MCP server config: {e}"")","def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        """"""Load MCP server configuration from JSON file""""""
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if not config_file:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig(
                        type=server_config[""type""],
                        url=server_config.get(""url""),
                        command=server_config.get(""command""),
                        args=server_config.get(""args"", []),
                    )
                return servers
        except Exception as e:
            raise ValueError(f""Failed to load MCP server config: {e}"")",Load MCP server configuration from JSON file,Load MCP server configuration from JSON file,"def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if not config_file:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig(
                        type=server_config[""type""],
                        url=server_config.get(""url""),
                        command=server_config.get(""command""),
                        args=server_config.get(""args"", []),
                    )
                return servers
        except Exception as e:
            raise ValueError(f""Failed to load MCP server config: {e}"")",Load MCP server configuration from JSON file,"def load_server_config ( cls ) -> Dict [ str , MCPServerConfig ] : config_path = PROJECT_ROOT / ""config"" / ""mcp.json"" try : config_file = config_path if config_path . exists ( ) else None if not config_file : return { } with config_file . open ( ) as f : data = json . load ( f ) servers = { } for server_id , server_config in data . get ( ""mcpServers"" , { } ) . items ( ) : servers [ server_id ] = MCPServerConfig ( type = server_config [ ""type"" ] , url = server_config . get ( ""url"" ) , command = server_config . get ( ""command"" ) , args = server_config . get ( ""args"" , [ ] ) , ) return servers except Exception as e : raise ValueError ( f""Failed to load MCP server config: {e}"" )",Load MCP server configuration from JSON file
/local-deep-research/src/local_deep_research/web/routes/settings_routes.py,api_get_types,"def api_get_types():
    """"""Get all setting types""""""
    try:
        # Get all setting types
        types = [t.value for t in SettingType]
        return jsonify({""types"": types})
    except Exception as e:
        logger.exception(""Error getting types"")
        return jsonify({""error"": str(e)}), 500","def api_get_types():
    """"""Get all setting types""""""
    try:
        # Get all setting types
        types = [t.value for t in SettingType]
        return jsonify({""types"": types})
    except Exception as e:
        logger.exception(""Error getting types"")
        return jsonify({""error"": str(e)}), 500",Get all setting types,Get all setting types,"def api_get_types():
    
    try:
        # Get all setting types
        types = [t.value for t in SettingType]
        return jsonify({""types"": types})
    except Exception as e:
        logger.exception(""Error getting types"")
        return jsonify({""error"": str(e)}), 500",Get all setting types,"def api_get_types ( ) : try : # Get all setting types types = [ t . value for t in SettingType ] return jsonify ( { ""types"" : types } ) except Exception as e : logger . exception ( ""Error getting types"" ) return jsonify ( { ""error"" : str ( e ) } ) , 500",Get all setting types
/echomimic_v2/src/models/unet_2d_condition.py,set_default_attn_processor,"def set_default_attn_processor(self):
        """"""
        Disables custom attention processors and sets the default attention implementation.
        """"""
        if all(
            proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnAddedKVProcessor()
        elif all(
            proc.__class__ in CROSS_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnProcessor()
        else:
            raise ValueError(
                f""Cannot call `set_default_attn_processor` when attention processors are of type {next(iter(self.attn_processors.values()))}""
            )

        self.set_attn_processor(processor, _remove_lora=True)","def set_default_attn_processor(self):
        """"""
        Disables custom attention processors and sets the default attention implementation.
        """"""
        if all(
            proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnAddedKVProcessor()
        elif all(
            proc.__class__ in CROSS_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnProcessor()
        else:
            raise ValueError(
                f""Cannot call `set_default_attn_processor` when attention processors are of type {next(iter(self.attn_processors.values()))}""
            )

        self.set_attn_processor(processor, _remove_lora=True)",Disables custom attention processors and sets the default attention implementation.,Disables custom attention processors and sets the default attention implementation.,"def set_default_attn_processor(self):
        
        if all(
            proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnAddedKVProcessor()
        elif all(
            proc.__class__ in CROSS_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnProcessor()
        else:
            raise ValueError(
                f""Cannot call `set_default_attn_processor` when attention processors are of type {next(iter(self.attn_processors.values()))}""
            )

        self.set_attn_processor(processor, _remove_lora=True)",Disables custom attention processors and sets the default attention implementation.,"def set_default_attn_processor ( self ) : if all ( proc . __class__ in ADDED_KV_ATTENTION_PROCESSORS for proc in self . attn_processors . values ( ) ) : processor = AttnAddedKVProcessor ( ) elif all ( proc . __class__ in CROSS_ATTENTION_PROCESSORS for proc in self . attn_processors . values ( ) ) : processor = AttnProcessor ( ) else : raise ValueError ( f""Cannot call `set_default_attn_processor` when attention processors are of type {next(iter(self.attn_processors.values()))}"" ) self . set_attn_processor ( processor , _remove_lora = True )",Disables custom attention processors and sets the default attention implementation.
/nv-ingest/client/src/nv_ingest_client/primitives/jobs/job_state.py,state,"def state(self, value: JobStateEnum) -> None:
        """"""Sets the current state of the job with transition constraints.""""""
        if self._state in _TERMINAL_STATES:
            logger.error(f""Attempt to change state from {self._state.name} to {value.name} denied."")
            raise ValueError(f""Cannot change state from {self._state.name} to {value.name}."")
        if value.value < self._state.value:
            logger.error(f""Invalid state transition attempt from {self._state.name} to {value.name}."")
            raise ValueError(f""State can only transition forward, from {self._state.name} to {value.name} not allowed."")
        self._state = value","def state(self, value: JobStateEnum) -> None:
        """"""Sets the current state of the job with transition constraints.""""""
        if self._state in _TERMINAL_STATES:
            logger.error(f""Attempt to change state from {self._state.name} to {value.name} denied."")
            raise ValueError(f""Cannot change state from {self._state.name} to {value.name}."")
        if value.value < self._state.value:
            logger.error(f""Invalid state transition attempt from {self._state.name} to {value.name}."")
            raise ValueError(f""State can only transition forward, from {self._state.name} to {value.name} not allowed."")
        self._state = value",Sets the current state of the job with transition constraints.,Sets the current state of the job with transition constraints.,"def state(self, value: JobStateEnum) -> None:
        
        if self._state in _TERMINAL_STATES:
            logger.error(f""Attempt to change state from {self._state.name} to {value.name} denied."")
            raise ValueError(f""Cannot change state from {self._state.name} to {value.name}."")
        if value.value < self._state.value:
            logger.error(f""Invalid state transition attempt from {self._state.name} to {value.name}."")
            raise ValueError(f""State can only transition forward, from {self._state.name} to {value.name} not allowed."")
        self._state = value",Sets the current state of the job with transition constraints.,"def state ( self , value : JobStateEnum ) -> None : if self . _state in _TERMINAL_STATES : logger . error ( f""Attempt to change state from {self._state.name} to {value.name} denied."" ) raise ValueError ( f""Cannot change state from {self._state.name} to {value.name}."" ) if value . value < self . _state . value : logger . error ( f""Invalid state transition attempt from {self._state.name} to {value.name}."" ) raise ValueError ( f""State can only transition forward, from {self._state.name} to {value.name} not allowed."" ) self . _state = value",Sets the current state of the job with transition constraints.
/Scrapling/scrapling/core/storage_adaptors.py,_get_hash,"def _get_hash(identifier: str) -> str:
        """"""If you want to hash identifier in your storage system, use this safer""""""
        identifier = identifier.lower().strip()
        if isinstance(identifier, str):
            # Hash functions have to take bytes
            identifier = identifier.encode('utf-8')

        hash_value = sha256(identifier).hexdigest()
        return f""{hash_value}_{len(identifier)}""","def _get_hash(identifier: str) -> str:
        """"""If you want to hash identifier in your storage system, use this safer""""""
        identifier = identifier.lower().strip()
        if isinstance(identifier, str):
            # Hash functions have to take bytes
            identifier = identifier.encode('utf-8')

        hash_value = sha256(identifier).hexdigest()
        return f""{hash_value}_{len(identifier)}""","If you want to hash identifier in your storage system, use this safer","If you want to hash identifier in your storage system, use this safer","def _get_hash(identifier: str) -> str:
        
        identifier = identifier.lower().strip()
        if isinstance(identifier, str):
            # Hash functions have to take bytes
            identifier = identifier.encode('utf-8')

        hash_value = sha256(identifier).hexdigest()
        return f""{hash_value}_{len(identifier)}""","If you want to hash identifier in your storage system, use this safer","def _get_hash ( identifier : str ) -> str : identifier = identifier . lower ( ) . strip ( ) if isinstance ( identifier , str ) : # Hash functions have to take bytes identifier = identifier . encode ( 'utf-8' ) hash_value = sha256 ( identifier ) . hexdigest ( ) return f""{hash_value}_{len(identifier)}""","If you want to hash identifier in your storage system, use this safer"
/optillm/optillm/cepo/cepo.py,extract_question_only,"def extract_question_only(task: str) -> str:
    """"""We noticed that sometimes if the task includes specific formatting instructions, they may interfere with the reasoning flow. This
    is a temporary workaround to extract the question only from the task. Work in progress.
    """"""
    question_only = task.replace('\n## Question: \n\n', '')
    question_only = question_only.replace('\n\n\n## Instruction \n\nPlease answer this question by first reasoning and then providing your answer.\nPresent your reasoning and solution in the following json format. \nPlease show your final answer in the `answer` field, e.g.,`""answer"": ""42""`.\n\n```json\n{\n    ""reasoning"": ""___"",\n    ""answer"": ""___""\n}\n```\n', '')
    return question_only","def extract_question_only(task: str) -> str:
    """"""We noticed that sometimes if the task includes specific formatting instructions, they may interfere with the reasoning flow. This
    is a temporary workaround to extract the question only from the task. Work in progress.
    """"""
    question_only = task.replace('\n## Question: \n\n', '')
    question_only = question_only.replace('\n\n\n## Instruction \n\nPlease answer this question by first reasoning and then providing your answer.\nPresent your reasoning and solution in the following json format. \nPlease show your final answer in the `answer` field, e.g.,`""answer"": ""42""`.\n\n```json\n{\n    ""reasoning"": ""___"",\n    ""answer"": ""___""\n}\n```\n', '')
    return question_only","We noticed that sometimes if the task includes specific formatting instructions, they may interfere with the reasoning flow. This
is a temporary workaround to extract the question only from the task. Work in progress.","We noticed that sometimes if the task includes specific formatting instructions, they may interfere with the reasoning flow.","def extract_question_only(task: str) -> str:
    
    question_only = task.replace('\n## Question: \n\n', '')
    question_only = question_only.replace('\n\n\n## Instruction \n\nPlease answer this question by first reasoning and then providing your answer.\nPresent your reasoning and solution in the following json format. \nPlease show your final answer in the `answer` field, e.g.,`""answer"": ""42""`.\n\n```json\n{\n    ""reasoning"": ""___"",\n    ""answer"": ""___""\n}\n```\n', '')
    return question_only","We noticed that sometimes if the task includes specific formatting instructions, they may interfere with the reasoning flow.","def extract_question_only ( task : str ) -> str : question_only = task . replace ( '\n## Question: \n\n' , '' ) question_only = question_only . replace ( '\n\n\n## Instruction \n\nPlease answer this question by first reasoning and then providing your answer.\nPresent your reasoning and solution in the following json format. \nPlease show your final answer in the `answer` field, e.g.,`""answer"": ""42""`.\n\n```json\n{\n    ""reasoning"": ""___"",\n    ""answer"": ""___""\n}\n```\n' , '' ) return question_only","We noticed that sometimes if the task includes specific formatting instructions, they may interfere with the reasoning flow."
/ag2/autogen/oai/ollama.py,is_valid_tool_call_item,"def is_valid_tool_call_item(call_item: dict) -> bool:
    """"""Check that a dictionary item has at least 'name', optionally 'arguments' and no other keys to match a tool call JSON""""""
    if ""name"" not in call_item or not isinstance(call_item[""name""], str):
        return False

    if set(call_item.keys()) - {""name"", ""arguments""}:  # noqa: SIM103
        return False

    return True","def is_valid_tool_call_item(call_item: dict) -> bool:
    """"""Check that a dictionary item has at least 'name', optionally 'arguments' and no other keys to match a tool call JSON""""""
    if ""name"" not in call_item or not isinstance(call_item[""name""], str):
        return False

    if set(call_item.keys()) - {""name"", ""arguments""}:  # noqa: SIM103
        return False

    return True","Check that a dictionary item has at least 'name', optionally 'arguments' and no other keys to match a tool call JSON","Check that a dictionary item has at least 'name', optionally 'arguments' and no other keys to match a tool call JSON","def is_valid_tool_call_item(call_item: dict) -> bool:
    
    if ""name"" not in call_item or not isinstance(call_item[""name""], str):
        return False

    if set(call_item.keys()) - {""name"", ""arguments""}:  # noqa: SIM103
        return False

    return True","Check that a dictionary item has at least 'name', optionally 'arguments' and no other keys to match a tool call JSON","def is_valid_tool_call_item ( call_item : dict ) -> bool : if ""name"" not in call_item or not isinstance ( call_item [ ""name"" ] , str ) : return False if set ( call_item . keys ( ) ) - { ""name"" , ""arguments"" } : # noqa: SIM103 return False return True","Check that a dictionary item has at least 'name', optionally 'arguments' and no other keys to match a tool call JSON"
/verl/verl/single_controller/base/decorator.py,update_dispatch_mode,"def update_dispatch_mode(dispatch_mode, dispatch_fn, collect_fn):
    """"""
    Update the dispatch mode.
    """"""
    _check_dispatch_mode(dispatch_mode)
    assert dispatch_mode in DISPATCH_MODE_FN_REGISTRY, f""dispatch_mode {dispatch_mode} not found""
    DISPATCH_MODE_FN_REGISTRY[dispatch_mode] = {""dispatch_fn"": dispatch_fn, ""collect_fn"": collect_fn}","def update_dispatch_mode(dispatch_mode, dispatch_fn, collect_fn):
    """"""
    Update the dispatch mode.
    """"""
    _check_dispatch_mode(dispatch_mode)
    assert dispatch_mode in DISPATCH_MODE_FN_REGISTRY, f""dispatch_mode {dispatch_mode} not found""
    DISPATCH_MODE_FN_REGISTRY[dispatch_mode] = {""dispatch_fn"": dispatch_fn, ""collect_fn"": collect_fn}",Update the dispatch mode.,Update the dispatch mode.,"def update_dispatch_mode(dispatch_mode, dispatch_fn, collect_fn):
    
    _check_dispatch_mode(dispatch_mode)
    assert dispatch_mode in DISPATCH_MODE_FN_REGISTRY, f""dispatch_mode {dispatch_mode} not found""
    DISPATCH_MODE_FN_REGISTRY[dispatch_mode] = {""dispatch_fn"": dispatch_fn, ""collect_fn"": collect_fn}",Update the dispatch mode.,"def update_dispatch_mode ( dispatch_mode , dispatch_fn , collect_fn ) : _check_dispatch_mode ( dispatch_mode ) assert dispatch_mode in DISPATCH_MODE_FN_REGISTRY , f""dispatch_mode {dispatch_mode} not found"" DISPATCH_MODE_FN_REGISTRY [ dispatch_mode ] = { ""dispatch_fn"" : dispatch_fn , ""collect_fn"" : collect_fn }",Update the dispatch mode.
/cua/libs/agent/agent/providers/anthropic/loop.py,_handle_content,"def _handle_content(self, content):
        """"""Handle content updates from the assistant.""""""
        if content.get(""type"") == ""text"":
            text = content.get(""text"", """")
            if text == ""<DONE>"":
                return
            logger.info(f""Assistant: {text}"")","def _handle_content(self, content):
        """"""Handle content updates from the assistant.""""""
        if content.get(""type"") == ""text"":
            text = content.get(""text"", """")
            if text == ""<DONE>"":
                return
            logger.info(f""Assistant: {text}"")",Handle content updates from the assistant.,Handle content updates from the assistant.,"def _handle_content(self, content):
        
        if content.get(""type"") == ""text"":
            text = content.get(""text"", """")
            if text == ""<DONE>"":
                return
            logger.info(f""Assistant: {text}"")",Handle content updates from the assistant.,"def _handle_content ( self , content ) : if content . get ( ""type"" ) == ""text"" : text = content . get ( ""text"" , """" ) if text == ""<DONE>"" : return logger . info ( f""Assistant: {text}"" )",Handle content updates from the assistant.
/python-sdk/src/mcp/server/fastmcp/prompts/prompt_manager.py,add_prompt,"def add_prompt(self, prompt: Prompt) -> Prompt:
        """"""Add a prompt to the manager.""""""
        logger.debug(f""Adding prompt: {prompt.name}"")
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing
        self._prompts[prompt.name] = prompt
        return prompt","def add_prompt(self, prompt: Prompt) -> Prompt:
        """"""Add a prompt to the manager.""""""
        logger.debug(f""Adding prompt: {prompt.name}"")
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing
        self._prompts[prompt.name] = prompt
        return prompt",Add a prompt to the manager.,Add a prompt to the manager.,"def add_prompt(self, prompt: Prompt) -> Prompt:
        
        logger.debug(f""Adding prompt: {prompt.name}"")
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing
        self._prompts[prompt.name] = prompt
        return prompt",Add a prompt to the manager.,"def add_prompt ( self , prompt : Prompt ) -> Prompt : logger . debug ( f""Adding prompt: {prompt.name}"" ) existing = self . _prompts . get ( prompt . name ) if existing : if self . warn_on_duplicate_prompts : logger . warning ( f""Prompt already exists: {prompt.name}"" ) return existing self . _prompts [ prompt . name ] = prompt return prompt",Add a prompt to the manager.
/local-deep-research/src/local_deep_research/benchmarks/optimization/optuna_optimizer.py,_create_visualizations,"def _create_visualizations(self):
        """"""Create and save comprehensive visualizations of the optimization results.""""""
        if not PLOTTING_AVAILABLE:
            logger.warning(""Matplotlib not available, skipping visualization creation"")
            return

        if not self.study or len(self.study.trials) < 2:
            logger.warning(""Not enough trials to create visualizations"")
            return

        # Create directory for visualizations
        viz_dir = os.path.join(self.output_dir, ""visualizations"")
        os.makedirs(viz_dir, exist_ok=True)

        # Create Optuna visualizations
        self._create_optuna_visualizations(viz_dir)

        # Create custom visualizations
        self._create_custom_visualizations(viz_dir)

        logger.info(f""Visualizations saved to {viz_dir}"")","def _create_visualizations(self):
        """"""Create and save comprehensive visualizations of the optimization results.""""""
        if not PLOTTING_AVAILABLE:
            logger.warning(""Matplotlib not available, skipping visualization creation"")
            return

        if not self.study or len(self.study.trials) < 2:
            logger.warning(""Not enough trials to create visualizations"")
            return

        # Create directory for visualizations
        viz_dir = os.path.join(self.output_dir, ""visualizations"")
        os.makedirs(viz_dir, exist_ok=True)

        # Create Optuna visualizations
        self._create_optuna_visualizations(viz_dir)

        # Create custom visualizations
        self._create_custom_visualizations(viz_dir)

        logger.info(f""Visualizations saved to {viz_dir}"")",Create and save comprehensive visualizations of the optimization results.,Create and save comprehensive visualizations of the optimization results.,"def _create_visualizations(self):
        
        if not PLOTTING_AVAILABLE:
            logger.warning(""Matplotlib not available, skipping visualization creation"")
            return

        if not self.study or len(self.study.trials) < 2:
            logger.warning(""Not enough trials to create visualizations"")
            return

        # Create directory for visualizations
        viz_dir = os.path.join(self.output_dir, ""visualizations"")
        os.makedirs(viz_dir, exist_ok=True)

        # Create Optuna visualizations
        self._create_optuna_visualizations(viz_dir)

        # Create custom visualizations
        self._create_custom_visualizations(viz_dir)

        logger.info(f""Visualizations saved to {viz_dir}"")",Create and save comprehensive visualizations of the optimization results.,"def _create_visualizations ( self ) : if not PLOTTING_AVAILABLE : logger . warning ( ""Matplotlib not available, skipping visualization creation"" ) return if not self . study or len ( self . study . trials ) < 2 : logger . warning ( ""Not enough trials to create visualizations"" ) return # Create directory for visualizations viz_dir = os . path . join ( self . output_dir , ""visualizations"" ) os . makedirs ( viz_dir , exist_ok = True ) # Create Optuna visualizations self . _create_optuna_visualizations ( viz_dir ) # Create custom visualizations self . _create_custom_visualizations ( viz_dir ) logger . info ( f""Visualizations saved to {viz_dir}"" )",Create and save comprehensive visualizations of the optimization results.
/mcp-agent/src/mcp_agent/workflows/embedding/embedding_base.py,compute_similarity_scores,"def compute_similarity_scores(
    embedding_a: FloatArray, embedding_b: FloatArray
) -> Dict[str, float]:
    """"""
    Compute different similarity metrics between embeddings
    """"""
    # Reshape for sklearn's cosine_similarity
    a_emb = embedding_a.reshape(1, -1)
    b_emb = embedding_b.reshape(1, -1)

    cosine_sim = float(cosine_similarity(a_emb, b_emb)[0, 0])

    # Could add other similarity metrics here
    return {
        ""cosine"": cosine_sim,
        # ""euclidean"": float(euclidean_similarity),
        # ""dot_product"": float(dot_product)
    }","def compute_similarity_scores(
    embedding_a: FloatArray, embedding_b: FloatArray
) -> Dict[str, float]:
    """"""
    Compute different similarity metrics between embeddings
    """"""
    # Reshape for sklearn's cosine_similarity
    a_emb = embedding_a.reshape(1, -1)
    b_emb = embedding_b.reshape(1, -1)

    cosine_sim = float(cosine_similarity(a_emb, b_emb)[0, 0])

    # Could add other similarity metrics here
    return {
        ""cosine"": cosine_sim,
        # ""euclidean"": float(euclidean_similarity),
        # ""dot_product"": float(dot_product)
    }",Compute different similarity metrics between embeddings,Compute different similarity metrics between embeddings,"def compute_similarity_scores(
    embedding_a: FloatArray, embedding_b: FloatArray
) -> Dict[str, float]:
    
    # Reshape for sklearn's cosine_similarity
    a_emb = embedding_a.reshape(1, -1)
    b_emb = embedding_b.reshape(1, -1)

    cosine_sim = float(cosine_similarity(a_emb, b_emb)[0, 0])

    # Could add other similarity metrics here
    return {
        ""cosine"": cosine_sim,
        # ""euclidean"": float(euclidean_similarity),
        # ""dot_product"": float(dot_product)
    }",Compute different similarity metrics between embeddings,"def compute_similarity_scores ( embedding_a : FloatArray , embedding_b : FloatArray ) -> Dict [ str , float ] : # Reshape for sklearn's cosine_similarity a_emb = embedding_a . reshape ( 1 , - 1 ) b_emb = embedding_b . reshape ( 1 , - 1 ) cosine_sim = float ( cosine_similarity ( a_emb , b_emb ) [ 0 , 0 ] ) # Could add other similarity metrics here return { ""cosine"" : cosine_sim , # ""euclidean"": float(euclidean_similarity), # ""dot_product"": float(dot_product) }",Compute different similarity metrics between embeddings
/Kokoro-FastAPI/ui/lib/files.py,delete_all_output_files,"def delete_all_output_files() -> bool:
    """"""Delete all audio files from the outputs directory. Returns True if successful.""""""
    try:
        for filename in os.listdir(OUTPUTS_DIR):
            if any(filename.endswith(ext) for ext in AUDIO_FORMATS):
                file_path = os.path.join(OUTPUTS_DIR, filename)
                os.remove(file_path)
        return True
    except Exception as e:
        print(f""Error deleting output files: {e}"")
        return False","def delete_all_output_files() -> bool:
    """"""Delete all audio files from the outputs directory. Returns True if successful.""""""
    try:
        for filename in os.listdir(OUTPUTS_DIR):
            if any(filename.endswith(ext) for ext in AUDIO_FORMATS):
                file_path = os.path.join(OUTPUTS_DIR, filename)
                os.remove(file_path)
        return True
    except Exception as e:
        print(f""Error deleting output files: {e}"")
        return False",Delete all audio files from the outputs directory. Returns True if successful.,Delete all audio files from the outputs directory.,"def delete_all_output_files() -> bool:
    
    try:
        for filename in os.listdir(OUTPUTS_DIR):
            if any(filename.endswith(ext) for ext in AUDIO_FORMATS):
                file_path = os.path.join(OUTPUTS_DIR, filename)
                os.remove(file_path)
        return True
    except Exception as e:
        print(f""Error deleting output files: {e}"")
        return False",Delete all audio files from the outputs directory.,"def delete_all_output_files ( ) -> bool : try : for filename in os . listdir ( OUTPUTS_DIR ) : if any ( filename . endswith ( ext ) for ext in AUDIO_FORMATS ) : file_path = os . path . join ( OUTPUTS_DIR , filename ) os . remove ( file_path ) return True except Exception as e : print ( f""Error deleting output files: {e}"" ) return False",Delete all audio files from the outputs directory.
/adk-samples/python/agents/personalized-shopping/personalized_shopping/shared_libraries/web_agent_site/utils.py,setup_logger,"def setup_logger(session_id, user_log_dir):
    """"""Creates a log file and logging object for the corresponding session ID""""""
    logger = logging.getLogger(session_id)
    formatter = logging.Formatter(""%(message)s"")
    file_handler = logging.FileHandler(user_log_dir / f""{session_id}.jsonl"", mode=""w"")
    file_handler.setFormatter(formatter)
    logger.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    return logger","def setup_logger(session_id, user_log_dir):
    """"""Creates a log file and logging object for the corresponding session ID""""""
    logger = logging.getLogger(session_id)
    formatter = logging.Formatter(""%(message)s"")
    file_handler = logging.FileHandler(user_log_dir / f""{session_id}.jsonl"", mode=""w"")
    file_handler.setFormatter(formatter)
    logger.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    return logger",Creates a log file and logging object for the corresponding session ID,Creates a log file and logging object for the corresponding session ID,"def setup_logger(session_id, user_log_dir):
    
    logger = logging.getLogger(session_id)
    formatter = logging.Formatter(""%(message)s"")
    file_handler = logging.FileHandler(user_log_dir / f""{session_id}.jsonl"", mode=""w"")
    file_handler.setFormatter(formatter)
    logger.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    return logger",Creates a log file and logging object for the corresponding session ID,"def setup_logger ( session_id , user_log_dir ) : logger = logging . getLogger ( session_id ) formatter = logging . Formatter ( ""%(message)s"" ) file_handler = logging . FileHandler ( user_log_dir / f""{session_id}.jsonl"" , mode = ""w"" ) file_handler . setFormatter ( formatter ) logger . setLevel ( logging . INFO ) logger . addHandler ( file_handler ) return logger",Creates a log file and logging object for the corresponding session ID
/OpenManus-RL/openmanus_rl/agentgym/agentenv-webarena/webarena/agent/prompts/to_json.py,run,"def run() -> None:
    """"""Convert all python files in agent/prompts to json files in agent/prompts/jsons

    Python files are easiser to edit
    """"""
    for p_file in glob.glob(f""agent/prompts/raw/*.py""):
        # import the file as a module
        base_name = os.path.basename(p_file).replace("".py"", """")
        module = importlib.import_module(f""agent.prompts.raw.{base_name}"")
        prompt = module.prompt
        # save the prompt as a json file
        os.makedirs(""agent/prompts/jsons"", exist_ok=True)
        with open(f""agent/prompts/jsons/{base_name}.json"", ""w+"") as f:
            json.dump(prompt, f, indent=2)
    print(f""Done convert python files to json"")","def run() -> None:
    """"""Convert all python files in agent/prompts to json files in agent/prompts/jsons

    Python files are easiser to edit
    """"""
    for p_file in glob.glob(f""agent/prompts/raw/*.py""):
        # import the file as a module
        base_name = os.path.basename(p_file).replace("".py"", """")
        module = importlib.import_module(f""agent.prompts.raw.{base_name}"")
        prompt = module.prompt
        # save the prompt as a json file
        os.makedirs(""agent/prompts/jsons"", exist_ok=True)
        with open(f""agent/prompts/jsons/{base_name}.json"", ""w+"") as f:
            json.dump(prompt, f, indent=2)
    print(f""Done convert python files to json"")","Convert all python files in agent/prompts to json files in agent/prompts/jsons

Python files are easiser to edit",Convert all python files in agent/prompts to json files in agent/prompts/jsons,"def run() -> None:
    
    for p_file in glob.glob(f""agent/prompts/raw/*.py""):
        # import the file as a module
        base_name = os.path.basename(p_file).replace("".py"", """")
        module = importlib.import_module(f""agent.prompts.raw.{base_name}"")
        prompt = module.prompt
        # save the prompt as a json file
        os.makedirs(""agent/prompts/jsons"", exist_ok=True)
        with open(f""agent/prompts/jsons/{base_name}.json"", ""w+"") as f:
            json.dump(prompt, f, indent=2)
    print(f""Done convert python files to json"")",Convert all python files in agent/prompts to json files in agent/prompts/jsons,"def run ( ) -> None : for p_file in glob . glob ( f""agent/prompts/raw/*.py"" ) : # import the file as a module base_name = os . path . basename ( p_file ) . replace ( "".py"" , """" ) module = importlib . import_module ( f""agent.prompts.raw.{base_name}"" ) prompt = module . prompt # save the prompt as a json file os . makedirs ( ""agent/prompts/jsons"" , exist_ok = True ) with open ( f""agent/prompts/jsons/{base_name}.json"" , ""w+"" ) as f : json . dump ( prompt , f , indent = 2 ) print ( f""Done convert python files to json"" )",Convert all python files in agent/prompts to json files in agent/prompts/jsons
/preswald/preswald/interfaces/components.py,text,"def text(markdown_str: str, size: float = 1.0, component_id: str | None = None, **kwargs) -> ComponentReturn:
    """"""Create a text/markdown component.""""""
    component = {
        ""type"": ""text"",
        ""id"": component_id,
        ""markdown"": markdown_str,
        ""value"": markdown_str,
        ""size"": size,
    }

    logger.info(f""[text] ID = {component_id}, content = {markdown_str}"")
    return ComponentReturn(markdown_str, component)","def text(markdown_str: str, size: float = 1.0, component_id: str | None = None, **kwargs) -> ComponentReturn:
    """"""Create a text/markdown component.""""""
    component = {
        ""type"": ""text"",
        ""id"": component_id,
        ""markdown"": markdown_str,
        ""value"": markdown_str,
        ""size"": size,
    }

    logger.info(f""[text] ID = {component_id}, content = {markdown_str}"")
    return ComponentReturn(markdown_str, component)",Create a text/markdown component.,Create a text/markdown component.,"def text(markdown_str: str, size: float = 1.0, component_id: str | None = None, **kwargs) -> ComponentReturn:
    
    component = {
        ""type"": ""text"",
        ""id"": component_id,
        ""markdown"": markdown_str,
        ""value"": markdown_str,
        ""size"": size,
    }

    logger.info(f""[text] ID = {component_id}, content = {markdown_str}"")
    return ComponentReturn(markdown_str, component)",Create a text/markdown component.,"def text ( markdown_str : str , size : float = 1.0 , component_id : str | None = None , ** kwargs ) -> ComponentReturn : component = { ""type"" : ""text"" , ""id"" : component_id , ""markdown"" : markdown_str , ""value"" : markdown_str , ""size"" : size , } logger . info ( f""[text] ID = {component_id}, content = {markdown_str}"" ) return ComponentReturn ( markdown_str , component )",Create a text/markdown component.
/deep-searcher/tests/loader/web_crawler/test_firecrawl_crawler.py,setUp,"def setUp(self):
        """"""Set up test fixtures.""""""
        # Patch the environment variable
        self.env_patcher = patch.dict('os.environ', {'FIRECRAWL_API_KEY': 'fake-api-key'})
        self.env_patcher.start()
        
        # Create a mock for the FirecrawlApp
        self.firecrawl_app_patcher = patch('deepsearcher.loader.web_crawler.firecrawl_crawler.FirecrawlApp')
        self.mock_firecrawl_app = self.firecrawl_app_patcher.start()
        
        # Set up mock instances
        self.mock_app_instance = MagicMock()
        self.mock_firecrawl_app.return_value = self.mock_app_instance
        
        # Create the crawler
        self.crawler = FireCrawlCrawler()","def setUp(self):
        """"""Set up test fixtures.""""""
        # Patch the environment variable
        self.env_patcher = patch.dict('os.environ', {'FIRECRAWL_API_KEY': 'fake-api-key'})
        self.env_patcher.start()
        
        # Create a mock for the FirecrawlApp
        self.firecrawl_app_patcher = patch('deepsearcher.loader.web_crawler.firecrawl_crawler.FirecrawlApp')
        self.mock_firecrawl_app = self.firecrawl_app_patcher.start()
        
        # Set up mock instances
        self.mock_app_instance = MagicMock()
        self.mock_firecrawl_app.return_value = self.mock_app_instance
        
        # Create the crawler
        self.crawler = FireCrawlCrawler()",Set up test fixtures.,Set up test fixtures.,"def setUp(self):
        
        # Patch the environment variable
        self.env_patcher = patch.dict('os.environ', {'FIRECRAWL_API_KEY': 'fake-api-key'})
        self.env_patcher.start()
        
        # Create a mock for the FirecrawlApp
        self.firecrawl_app_patcher = patch('deepsearcher.loader.web_crawler.firecrawl_crawler.FirecrawlApp')
        self.mock_firecrawl_app = self.firecrawl_app_patcher.start()
        
        # Set up mock instances
        self.mock_app_instance = MagicMock()
        self.mock_firecrawl_app.return_value = self.mock_app_instance
        
        # Create the crawler
        self.crawler = FireCrawlCrawler()",Set up test fixtures.,"def setUp ( self ) : # Patch the environment variable self . env_patcher = patch . dict ( 'os.environ' , { 'FIRECRAWL_API_KEY' : 'fake-api-key' } ) self . env_patcher . start ( ) # Create a mock for the FirecrawlApp self . firecrawl_app_patcher = patch ( 'deepsearcher.loader.web_crawler.firecrawl_crawler.FirecrawlApp' ) self . mock_firecrawl_app = self . firecrawl_app_patcher . start ( ) # Set up mock instances self . mock_app_instance = MagicMock ( ) self . mock_firecrawl_app . return_value = self . mock_app_instance # Create the crawler self . crawler = FireCrawlCrawler ( )",Set up test fixtures.
/local-deep-research/src/local_deep_research/web_search_engines/engines/search_engine_searxng.py,_respect_rate_limit,"def _respect_rate_limit(self):
        """"""Apply self-imposed rate limiting between requests""""""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if time_since_last_request < self.delay_between_requests:
            wait_time = self.delay_between_requests - time_since_last_request
            logger.info(f""Rate limiting: waiting {wait_time:.2f} seconds"")
            time.sleep(wait_time)

        self.last_request_time = time.time()","def _respect_rate_limit(self):
        """"""Apply self-imposed rate limiting between requests""""""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if time_since_last_request < self.delay_between_requests:
            wait_time = self.delay_between_requests - time_since_last_request
            logger.info(f""Rate limiting: waiting {wait_time:.2f} seconds"")
            time.sleep(wait_time)

        self.last_request_time = time.time()",Apply self-imposed rate limiting between requests,Apply self-imposed rate limiting between requests,"def _respect_rate_limit(self):
        
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if time_since_last_request < self.delay_between_requests:
            wait_time = self.delay_between_requests - time_since_last_request
            logger.info(f""Rate limiting: waiting {wait_time:.2f} seconds"")
            time.sleep(wait_time)

        self.last_request_time = time.time()",Apply self-imposed rate limiting between requests,"def _respect_rate_limit ( self ) : current_time = time . time ( ) time_since_last_request = current_time - self . last_request_time if time_since_last_request < self . delay_between_requests : wait_time = self . delay_between_requests - time_since_last_request logger . info ( f""Rate limiting: waiting {wait_time:.2f} seconds"" ) time . sleep ( wait_time ) self . last_request_time = time . time ( )",Apply self-imposed rate limiting between requests
/magentic-ui/src/magentic_ui/backend/database/schema_manager.py,_cleanup_existing_alembic,"def _cleanup_existing_alembic(self) -> None:
        """"""
        Completely remove existing Alembic configuration including versions.
        For fresh initialization, we don't need to preserve anything.
        """"""
        # logger.info(""Cleaning up existing Alembic configuration..."")

        # Remove entire alembic directory if it exists
        if self.alembic_dir.exists():
            import shutil

            shutil.rmtree(self.alembic_dir)
            logger.info(f""Removed alembic directory: {self.alembic_dir}"")

        # Remove alembic.ini if it exists
        if self.alembic_ini_path.exists():
            self.alembic_ini_path.unlink()
            logger.info(""Removed alembic.ini"")","def _cleanup_existing_alembic(self) -> None:
        """"""
        Completely remove existing Alembic configuration including versions.
        For fresh initialization, we don't need to preserve anything.
        """"""
        # logger.info(""Cleaning up existing Alembic configuration..."")

        # Remove entire alembic directory if it exists
        if self.alembic_dir.exists():
            import shutil

            shutil.rmtree(self.alembic_dir)
            logger.info(f""Removed alembic directory: {self.alembic_dir}"")

        # Remove alembic.ini if it exists
        if self.alembic_ini_path.exists():
            self.alembic_ini_path.unlink()
            logger.info(""Removed alembic.ini"")","Completely remove existing Alembic configuration including versions.
For fresh initialization, we don't need to preserve anything.",Completely remove existing Alembic configuration including versions.,"def _cleanup_existing_alembic(self) -> None:
        
        # logger.info(""Cleaning up existing Alembic configuration..."")

        # Remove entire alembic directory if it exists
        if self.alembic_dir.exists():
            import shutil

            shutil.rmtree(self.alembic_dir)
            logger.info(f""Removed alembic directory: {self.alembic_dir}"")

        # Remove alembic.ini if it exists
        if self.alembic_ini_path.exists():
            self.alembic_ini_path.unlink()
            logger.info(""Removed alembic.ini"")",Completely remove existing Alembic configuration including versions.,"def _cleanup_existing_alembic ( self ) -> None : # logger.info(""Cleaning up existing Alembic configuration..."") # Remove entire alembic directory if it exists if self . alembic_dir . exists ( ) : import shutil shutil . rmtree ( self . alembic_dir ) logger . info ( f""Removed alembic directory: {self.alembic_dir}"" ) # Remove alembic.ini if it exists if self . alembic_ini_path . exists ( ) : self . alembic_ini_path . unlink ( ) logger . info ( ""Removed alembic.ini"" )",Completely remove existing Alembic configuration including versions.
/PocketFlow/cookbook/pocketflow-google-calendar/utils/google_calendar.py,create_custom_calendar,"def create_custom_calendar(calendar_name, description=""""):
    """"""Creates a new custom calendar in Google Calendar.""""""
    service = get_calendar_service()
    
    calendar = {
        'summary': calendar_name,
        'description': description,
        'timeZone': TIMEZONE
    }

    created_calendar = service.calendars().insert(body=calendar).execute()
    return created_calendar","def create_custom_calendar(calendar_name, description=""""):
    """"""Creates a new custom calendar in Google Calendar.""""""
    service = get_calendar_service()
    
    calendar = {
        'summary': calendar_name,
        'description': description,
        'timeZone': TIMEZONE
    }

    created_calendar = service.calendars().insert(body=calendar).execute()
    return created_calendar",Creates a new custom calendar in Google Calendar.,Creates a new custom calendar in Google Calendar.,"def create_custom_calendar(calendar_name, description=""""):
    
    service = get_calendar_service()
    
    calendar = {
        'summary': calendar_name,
        'description': description,
        'timeZone': TIMEZONE
    }

    created_calendar = service.calendars().insert(body=calendar).execute()
    return created_calendar",Creates a new custom calendar in Google Calendar.,"def create_custom_calendar ( calendar_name , description = """" ) : service = get_calendar_service ( ) calendar = { 'summary' : calendar_name , 'description' : description , 'timeZone' : TIMEZONE } created_calendar = service . calendars ( ) . insert ( body = calendar ) . execute ( ) return created_calendar",Creates a new custom calendar in Google Calendar.
/ag2/autogen/oai/client.py,_separate_create_config,"def _separate_create_config(self, config: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        """"""Separate the config into create_config and extra_kwargs.""""""
        create_config = {k: v for k, v in config.items() if k not in self.extra_kwargs}
        extra_kwargs = {k: v for k, v in config.items() if k in self.extra_kwargs}
        return create_config, extra_kwargs","def _separate_create_config(self, config: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        """"""Separate the config into create_config and extra_kwargs.""""""
        create_config = {k: v for k, v in config.items() if k not in self.extra_kwargs}
        extra_kwargs = {k: v for k, v in config.items() if k in self.extra_kwargs}
        return create_config, extra_kwargs",Separate the config into create_config and extra_kwargs.,Separate the config into create_config and extra_kwargs.,"def _separate_create_config(self, config: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        
        create_config = {k: v for k, v in config.items() if k not in self.extra_kwargs}
        extra_kwargs = {k: v for k, v in config.items() if k in self.extra_kwargs}
        return create_config, extra_kwargs",Separate the config into create_config and extra_kwargs.,"def _separate_create_config ( self , config : dict [ str , Any ] ) -> tuple [ dict [ str , Any ] , dict [ str , Any ] ] : create_config = { k : v for k , v in config . items ( ) if k not in self . extra_kwargs } extra_kwargs = { k : v for k , v in config . items ( ) if k in self . extra_kwargs } return create_config , extra_kwargs",Separate the config into create_config and extra_kwargs.
/mcp-agent/src/mcp_agent/workflows/orchestrator/orchestrator.py,_format_server_info,"def _format_server_info(self, server_name: str) -> str:
        """"""Format server information for display to planners""""""
        server_config = self.server_registry.get_server_config(server_name)
        server_str = f""Server Name: {server_name}""
        if not server_config:
            return server_str

        description = server_config.description
        if description:
            server_str = f""{server_str}\nDescription: {description}""

        return server_str","def _format_server_info(self, server_name: str) -> str:
        """"""Format server information for display to planners""""""
        server_config = self.server_registry.get_server_config(server_name)
        server_str = f""Server Name: {server_name}""
        if not server_config:
            return server_str

        description = server_config.description
        if description:
            server_str = f""{server_str}\nDescription: {description}""

        return server_str",Format server information for display to planners,Format server information for display to planners,"def _format_server_info(self, server_name: str) -> str:
        
        server_config = self.server_registry.get_server_config(server_name)
        server_str = f""Server Name: {server_name}""
        if not server_config:
            return server_str

        description = server_config.description
        if description:
            server_str = f""{server_str}\nDescription: {description}""

        return server_str",Format server information for display to planners,"def _format_server_info ( self , server_name : str ) -> str : server_config = self . server_registry . get_server_config ( server_name ) server_str = f""Server Name: {server_name}"" if not server_config : return server_str description = server_config . description if description : server_str = f""{server_str}\nDescription: {description}"" return server_str",Format server information for display to planners
/optillm/scripts/gen_optillmbench.py,format_question,"def format_question(category: str, question: str, answer: str) -> Dict[str, Any]:
    """"""Format a question for the benchmark dataset""""""
    # Basic sanity checks
    if not question or not answer:
        raise ValueError(f""Empty question or answer in {category}"")
        
    return {
        ""id"": f""{category}_{random.getrandbits(32):08x}"",
        ""category"": category,
        ""question"": clean_text(question),
        ""answer"": clean_text(answer),
        ""metadata"": {
            ""source"": SOURCES[category][""name""],
            ""type"": category,
            ""difficulty"": ""challenging""  # All examples are chosen to be challenging
        }
    }","def format_question(category: str, question: str, answer: str) -> Dict[str, Any]:
    """"""Format a question for the benchmark dataset""""""
    # Basic sanity checks
    if not question or not answer:
        raise ValueError(f""Empty question or answer in {category}"")
        
    return {
        ""id"": f""{category}_{random.getrandbits(32):08x}"",
        ""category"": category,
        ""question"": clean_text(question),
        ""answer"": clean_text(answer),
        ""metadata"": {
            ""source"": SOURCES[category][""name""],
            ""type"": category,
            ""difficulty"": ""challenging""  # All examples are chosen to be challenging
        }
    }",Format a question for the benchmark dataset,Format a question for the benchmark dataset,"def format_question(category: str, question: str, answer: str) -> Dict[str, Any]:
    
    # Basic sanity checks
    if not question or not answer:
        raise ValueError(f""Empty question or answer in {category}"")
        
    return {
        ""id"": f""{category}_{random.getrandbits(32):08x}"",
        ""category"": category,
        ""question"": clean_text(question),
        ""answer"": clean_text(answer),
        ""metadata"": {
            ""source"": SOURCES[category][""name""],
            ""type"": category,
            ""difficulty"": ""challenging""  # All examples are chosen to be challenging
        }
    }",Format a question for the benchmark dataset,"def format_question ( category : str , question : str , answer : str ) -> Dict [ str , Any ] : # Basic sanity checks if not question or not answer : raise ValueError ( f""Empty question or answer in {category}"" ) return { ""id"" : f""{category}_{random.getrandbits(32):08x}"" , ""category"" : category , ""question"" : clean_text ( question ) , ""answer"" : clean_text ( answer ) , ""metadata"" : { ""source"" : SOURCES [ category ] [ ""name"" ] , ""type"" : category , ""difficulty"" : ""challenging"" # All examples are chosen to be challenging } }",Format a question for the benchmark dataset
/fastmcp/src/fastmcp/tools/tool_manager.py,get_tool,"def get_tool(self, key: str) -> Tool:
        """"""Get tool by key.""""""
        if key in self._tools:
            return self._tools[key]
        raise NotFoundError(f""Unknown tool: {key}"")","def get_tool(self, key: str) -> Tool:
        """"""Get tool by key.""""""
        if key in self._tools:
            return self._tools[key]
        raise NotFoundError(f""Unknown tool: {key}"")",Get tool by key.,Get tool by key.,"def get_tool(self, key: str) -> Tool:
        
        if key in self._tools:
            return self._tools[key]
        raise NotFoundError(f""Unknown tool: {key}"")",Get tool by key.,"def get_tool ( self , key : str ) -> Tool : if key in self . _tools : return self . _tools [ key ] raise NotFoundError ( f""Unknown tool: {key}"" )",Get tool by key.
/airweave/backend/airweave/platform/sync/async_helpers.py,stable_serialize,"def stable_serialize(obj: Any) -> Any:
    """"""Serialize object in a stable way for hashing.""""""
    if isinstance(obj, dict):
        return {k: stable_serialize(v) for k, v in sorted(obj.items())}
    elif isinstance(obj, (list, tuple)):
        return [stable_serialize(x) for x in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:
        return str(obj)","def stable_serialize(obj: Any) -> Any:
    """"""Serialize object in a stable way for hashing.""""""
    if isinstance(obj, dict):
        return {k: stable_serialize(v) for k, v in sorted(obj.items())}
    elif isinstance(obj, (list, tuple)):
        return [stable_serialize(x) for x in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:
        return str(obj)",Serialize object in a stable way for hashing.,Serialize object in a stable way for hashing.,"def stable_serialize(obj: Any) -> Any:
    
    if isinstance(obj, dict):
        return {k: stable_serialize(v) for k, v in sorted(obj.items())}
    elif isinstance(obj, (list, tuple)):
        return [stable_serialize(x) for x in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:
        return str(obj)",Serialize object in a stable way for hashing.,"def stable_serialize ( obj : Any ) -> Any : if isinstance ( obj , dict ) : return { k : stable_serialize ( v ) for k , v in sorted ( obj . items ( ) ) } elif isinstance ( obj , ( list , tuple ) ) : return [ stable_serialize ( x ) for x in obj ] elif isinstance ( obj , ( str , int , float , bool , type ( None ) ) ) : return obj else : return str ( obj )",Serialize object in a stable way for hashing.
/web-ui/src/controller/custom_controller.py,register_mcp_tools,"def register_mcp_tools(self):
        """"""
        Register the MCP tools used by this controller.
        """"""
        if self.mcp_client:
            for server_name in self.mcp_client.server_name_to_tools:
                for tool in self.mcp_client.server_name_to_tools[server_name]:
                    tool_name = f""mcp.{server_name}.{tool.name}""
                    self.registry.registry.actions[tool_name] = RegisteredAction(
                        name=tool_name,
                        description=tool.description,
                        function=tool,
                        param_model=create_tool_param_model(tool),
                    )
                    logger.info(f""Add mcp tool: {tool_name}"")
                logger.debug(
                    f""Registered {len(self.mcp_client.server_name_to_tools[server_name])} mcp tools for {server_name}"")
        else:
            logger.warning(f""MCP client not started."")","def register_mcp_tools(self):
        """"""
        Register the MCP tools used by this controller.
        """"""
        if self.mcp_client:
            for server_name in self.mcp_client.server_name_to_tools:
                for tool in self.mcp_client.server_name_to_tools[server_name]:
                    tool_name = f""mcp.{server_name}.{tool.name}""
                    self.registry.registry.actions[tool_name] = RegisteredAction(
                        name=tool_name,
                        description=tool.description,
                        function=tool,
                        param_model=create_tool_param_model(tool),
                    )
                    logger.info(f""Add mcp tool: {tool_name}"")
                logger.debug(
                    f""Registered {len(self.mcp_client.server_name_to_tools[server_name])} mcp tools for {server_name}"")
        else:
            logger.warning(f""MCP client not started."")",Register the MCP tools used by this controller.,Register the MCP tools used by this controller.,"def register_mcp_tools(self):
        
        if self.mcp_client:
            for server_name in self.mcp_client.server_name_to_tools:
                for tool in self.mcp_client.server_name_to_tools[server_name]:
                    tool_name = f""mcp.{server_name}.{tool.name}""
                    self.registry.registry.actions[tool_name] = RegisteredAction(
                        name=tool_name,
                        description=tool.description,
                        function=tool,
                        param_model=create_tool_param_model(tool),
                    )
                    logger.info(f""Add mcp tool: {tool_name}"")
                logger.debug(
                    f""Registered {len(self.mcp_client.server_name_to_tools[server_name])} mcp tools for {server_name}"")
        else:
            logger.warning(f""MCP client not started."")",Register the MCP tools used by this controller.,"def register_mcp_tools ( self ) : if self . mcp_client : for server_name in self . mcp_client . server_name_to_tools : for tool in self . mcp_client . server_name_to_tools [ server_name ] : tool_name = f""mcp.{server_name}.{tool.name}"" self . registry . registry . actions [ tool_name ] = RegisteredAction ( name = tool_name , description = tool . description , function = tool , param_model = create_tool_param_model ( tool ) , ) logger . info ( f""Add mcp tool: {tool_name}"" ) logger . debug ( f""Registered {len(self.mcp_client.server_name_to_tools[server_name])} mcp tools for {server_name}"" ) else : logger . warning ( f""MCP client not started."" )",Register the MCP tools used by this controller.
/VideoCaptioner/app/core/bk_asr/asr_data.py,_ms_to_ass_ts,"def _ms_to_ass_ts(ms: int) -> str:
        """"""Convert milliseconds to ASS timestamp format (H:MM:SS.cc)""""""
        total_seconds, milliseconds = divmod(ms, 1000)
        minutes, seconds = divmod(total_seconds, 60)
        hours, minutes = divmod(minutes, 60)
        centiseconds = int(milliseconds / 10)
        return f""{int(hours):01}:{int(minutes):02}:{int(seconds):02}.{centiseconds:02}""","def _ms_to_ass_ts(ms: int) -> str:
        """"""Convert milliseconds to ASS timestamp format (H:MM:SS.cc)""""""
        total_seconds, milliseconds = divmod(ms, 1000)
        minutes, seconds = divmod(total_seconds, 60)
        hours, minutes = divmod(minutes, 60)
        centiseconds = int(milliseconds / 10)
        return f""{int(hours):01}:{int(minutes):02}:{int(seconds):02}.{centiseconds:02}""",Convert milliseconds to ASS timestamp format (H:MM:SS.cc),Convert milliseconds to ASS timestamp format (H:MM:SS.cc),"def _ms_to_ass_ts(ms: int) -> str:
        
        total_seconds, milliseconds = divmod(ms, 1000)
        minutes, seconds = divmod(total_seconds, 60)
        hours, minutes = divmod(minutes, 60)
        centiseconds = int(milliseconds / 10)
        return f""{int(hours):01}:{int(minutes):02}:{int(seconds):02}.{centiseconds:02}""",Convert milliseconds to ASS timestamp format (H:MM:SS.cc),"def _ms_to_ass_ts ( ms : int ) -> str : total_seconds , milliseconds = divmod ( ms , 1000 ) minutes , seconds = divmod ( total_seconds , 60 ) hours , minutes = divmod ( minutes , 60 ) centiseconds = int ( milliseconds / 10 ) return f""{int(hours):01}:{int(minutes):02}:{int(seconds):02}.{centiseconds:02}""",Convert milliseconds to ASS timestamp format (H:MM:SS.cc)
/nexa-sdk/nexa/gguf/outetts/interface.py,get_model_config,"def get_model_config(version: str):
    """"""
    Retrieve the configuration for a given model version.
    """"""
    if version not in MODEL_CONFIGS:
        raise ValueError(f""Unsupported model version '{version}'. Supported versions are: {list(MODEL_CONFIGS.keys())}"")
    return MODEL_CONFIGS[version]","def get_model_config(version: str):
    """"""
    Retrieve the configuration for a given model version.
    """"""
    if version not in MODEL_CONFIGS:
        raise ValueError(f""Unsupported model version '{version}'. Supported versions are: {list(MODEL_CONFIGS.keys())}"")
    return MODEL_CONFIGS[version]",Retrieve the configuration for a given model version.,Retrieve the configuration for a given model version.,"def get_model_config(version: str):
    
    if version not in MODEL_CONFIGS:
        raise ValueError(f""Unsupported model version '{version}'. Supported versions are: {list(MODEL_CONFIGS.keys())}"")
    return MODEL_CONFIGS[version]",Retrieve the configuration for a given model version.,"def get_model_config ( version : str ) : if version not in MODEL_CONFIGS : raise ValueError ( f""Unsupported model version '{version}'. Supported versions are: {list(MODEL_CONFIGS.keys())}"" ) return MODEL_CONFIGS [ version ]",Retrieve the configuration for a given model version.
/NLWeb/code/retrieval/snowflake_client.py,get_cortex_search_service,"def get_cortex_search_service(cfg: RetrievalProviderConfig) -> Tuple[str,str,str]:
    """"""
    Retrieve the Cortex Search Service (database, schema, service) to use from the configuration, or raise an error.
    """"""
    if not cfg:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake configuration"")
    index_name = cfg.index_name
    if not index_name:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake Cortex Search Service, is SNOWFLAKE_CORTEX_SEARCH_SERVICE set?"")
    parts = index_name.split(""."")
    if len(parts) != 3:
        raise snowflake.ConfigurationError(f""Invalid SNOWFLAKE_CORTEX_SEARCH_SERVICE, expected format:<database>.<schema>.<service>, got {index_name}"")
    return (parts[0], parts[1], parts[2])","def get_cortex_search_service(cfg: RetrievalProviderConfig) -> Tuple[str,str,str]:
    """"""
    Retrieve the Cortex Search Service (database, schema, service) to use from the configuration, or raise an error.
    """"""
    if not cfg:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake configuration"")
    index_name = cfg.index_name
    if not index_name:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake Cortex Search Service, is SNOWFLAKE_CORTEX_SEARCH_SERVICE set?"")
    parts = index_name.split(""."")
    if len(parts) != 3:
        raise snowflake.ConfigurationError(f""Invalid SNOWFLAKE_CORTEX_SEARCH_SERVICE, expected format:<database>.<schema>.<service>, got {index_name}"")
    return (parts[0], parts[1], parts[2])","Retrieve the Cortex Search Service (database, schema, service) to use from the configuration, or raise an error.","Retrieve the Cortex Search Service (database, schema, service) to use from the configuration, or raise an error.","def get_cortex_search_service(cfg: RetrievalProviderConfig) -> Tuple[str,str,str]:
    
    if not cfg:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake configuration"")
    index_name = cfg.index_name
    if not index_name:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake Cortex Search Service, is SNOWFLAKE_CORTEX_SEARCH_SERVICE set?"")
    parts = index_name.split(""."")
    if len(parts) != 3:
        raise snowflake.ConfigurationError(f""Invalid SNOWFLAKE_CORTEX_SEARCH_SERVICE, expected format:<database>.<schema>.<service>, got {index_name}"")
    return (parts[0], parts[1], parts[2])","Retrieve the Cortex Search Service (database, schema, service) to use from the configuration, or raise an error.","def get_cortex_search_service ( cfg : RetrievalProviderConfig ) -> Tuple [ str , str , str ] : if not cfg : raise snowflake . ConfigurationError ( ""Unable to determine Snowflake configuration"" ) index_name = cfg . index_name if not index_name : raise snowflake . ConfigurationError ( ""Unable to determine Snowflake Cortex Search Service, is SNOWFLAKE_CORTEX_SEARCH_SERVICE set?"" ) parts = index_name . split ( ""."" ) if len ( parts ) != 3 : raise snowflake . ConfigurationError ( f""Invalid SNOWFLAKE_CORTEX_SEARCH_SERVICE, expected format:<database>.<schema>.<service>, got {index_name}"" ) return ( parts [ 0 ] , parts [ 1 ] , parts [ 2 ] )","Retrieve the Cortex Search Service (database, schema, service) to use from the configuration, or raise an error."
/LightRAG/lightrag/kg/faiss_impl.py,_save_faiss_index,"def _save_faiss_index(self):
        """"""
        Save the current Faiss index + metadata to disk so it can persist across runs.
        """"""
        faiss.write_index(self._index, self._faiss_index_file)

        # Save metadata dict to JSON. Convert all keys to strings for JSON storage.
        # _id_to_meta is { int: { '__id__': doc_id, '__vector__': [float,...], ... } }
        # We'll keep the int -> dict, but JSON requires string keys.
        serializable_dict = {}
        for fid, meta in self._id_to_meta.items():
            serializable_dict[str(fid)] = meta

        with open(self._meta_file, ""w"", encoding=""utf-8"") as f:
            json.dump(serializable_dict, f)","def _save_faiss_index(self):
        """"""
        Save the current Faiss index + metadata to disk so it can persist across runs.
        """"""
        faiss.write_index(self._index, self._faiss_index_file)

        # Save metadata dict to JSON. Convert all keys to strings for JSON storage.
        # _id_to_meta is { int: { '__id__': doc_id, '__vector__': [float,...], ... } }
        # We'll keep the int -> dict, but JSON requires string keys.
        serializable_dict = {}
        for fid, meta in self._id_to_meta.items():
            serializable_dict[str(fid)] = meta

        with open(self._meta_file, ""w"", encoding=""utf-8"") as f:
            json.dump(serializable_dict, f)",Save the current Faiss index + metadata to disk so it can persist across runs.,Save the current Faiss index + metadata to disk so it can persist across runs.,"def _save_faiss_index(self):
        
        faiss.write_index(self._index, self._faiss_index_file)

        # Save metadata dict to JSON. Convert all keys to strings for JSON storage.
        # _id_to_meta is { int: { '__id__': doc_id, '__vector__': [float,...], ... } }
        # We'll keep the int -> dict, but JSON requires string keys.
        serializable_dict = {}
        for fid, meta in self._id_to_meta.items():
            serializable_dict[str(fid)] = meta

        with open(self._meta_file, ""w"", encoding=""utf-8"") as f:
            json.dump(serializable_dict, f)",Save the current Faiss index + metadata to disk so it can persist across runs.,"def _save_faiss_index ( self ) : faiss . write_index ( self . _index , self . _faiss_index_file ) # Save metadata dict to JSON. Convert all keys to strings for JSON storage. # _id_to_meta is { int: { '__id__': doc_id, '__vector__': [float,...], ... } } # We'll keep the int -> dict, but JSON requires string keys. serializable_dict = { } for fid , meta in self . _id_to_meta . items ( ) : serializable_dict [ str ( fid ) ] = meta with open ( self . _meta_file , ""w"" , encoding = ""utf-8"" ) as f : json . dump ( serializable_dict , f )",Save the current Faiss index + metadata to disk so it can persist across runs.
/ragaai-catalyst/ragaai_catalyst/dataset.py,_jsonl_to_csv,"def _jsonl_to_csv(self, jsonl_file, csv_file):
        """"""Convert a JSONL file to a CSV file.""""""
        with open(jsonl_file, 'r', encoding='utf-8') as infile:
            data = [json.loads(line) for line in infile]
        
        if not data:
            print(""Empty JSONL file."")
            return
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as outfile:
            writer = csv.DictWriter(outfile, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        
        print(f""Converted {jsonl_file} to {csv_file}"")","def _jsonl_to_csv(self, jsonl_file, csv_file):
        """"""Convert a JSONL file to a CSV file.""""""
        with open(jsonl_file, 'r', encoding='utf-8') as infile:
            data = [json.loads(line) for line in infile]
        
        if not data:
            print(""Empty JSONL file."")
            return
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as outfile:
            writer = csv.DictWriter(outfile, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        
        print(f""Converted {jsonl_file} to {csv_file}"")",Convert a JSONL file to a CSV file.,Convert a JSONL file to a CSV file.,"def _jsonl_to_csv(self, jsonl_file, csv_file):
        
        with open(jsonl_file, 'r', encoding='utf-8') as infile:
            data = [json.loads(line) for line in infile]
        
        if not data:
            print(""Empty JSONL file."")
            return
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as outfile:
            writer = csv.DictWriter(outfile, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        
        print(f""Converted {jsonl_file} to {csv_file}"")",Convert a JSONL file to a CSV file.,"def _jsonl_to_csv ( self , jsonl_file , csv_file ) : with open ( jsonl_file , 'r' , encoding = 'utf-8' ) as infile : data = [ json . loads ( line ) for line in infile ] if not data : print ( ""Empty JSONL file."" ) return with open ( csv_file , 'w' , newline = '' , encoding = 'utf-8' ) as outfile : writer = csv . DictWriter ( outfile , fieldnames = data [ 0 ] . keys ( ) ) writer . writeheader ( ) writer . writerows ( data ) print ( f""Converted {jsonl_file} to {csv_file}"" )",Convert a JSONL file to a CSV file.
/ragaai-catalyst/ragaai_catalyst/tracers/tracer.py,recursive_mask_values,"def recursive_mask_values(obj, parent_key=None):
            """"""Apply masking to all values in nested structure.""""""
            if isinstance(obj, dict):
                return {k: recursive_mask_values(v, k) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [recursive_mask_values(item, parent_key) for item in obj]
            elif isinstance(obj, str):
                # List of keys that should NOT be masked
                excluded_keys = {
                    'start_time', 'end_time', 'name', 'id', 
                    'hash_id', 'parent_id', 'source_hash_id',
                    'cost', 'type', 'feedback', 'error', 'ctx','telemetry.sdk.version',
                    'telemetry.sdk.language','service.name'
                }
                # Apply masking only if the key is NOT in the excluded list
                if parent_key and parent_key.lower() not in excluded_keys:
                    return masking_func(obj)
                return obj
            else:
                return obj","def recursive_mask_values(obj, parent_key=None):
            """"""Apply masking to all values in nested structure.""""""
            if isinstance(obj, dict):
                return {k: recursive_mask_values(v, k) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [recursive_mask_values(item, parent_key) for item in obj]
            elif isinstance(obj, str):
                # List of keys that should NOT be masked
                excluded_keys = {
                    'start_time', 'end_time', 'name', 'id', 
                    'hash_id', 'parent_id', 'source_hash_id',
                    'cost', 'type', 'feedback', 'error', 'ctx','telemetry.sdk.version',
                    'telemetry.sdk.language','service.name'
                }
                # Apply masking only if the key is NOT in the excluded list
                if parent_key and parent_key.lower() not in excluded_keys:
                    return masking_func(obj)
                return obj
            else:
                return obj",Apply masking to all values in nested structure.,Apply masking to all values in nested structure.,"def recursive_mask_values(obj, parent_key=None):
            
            if isinstance(obj, dict):
                return {k: recursive_mask_values(v, k) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [recursive_mask_values(item, parent_key) for item in obj]
            elif isinstance(obj, str):
                # List of keys that should NOT be masked
                excluded_keys = {
                    'start_time', 'end_time', 'name', 'id', 
                    'hash_id', 'parent_id', 'source_hash_id',
                    'cost', 'type', 'feedback', 'error', 'ctx','telemetry.sdk.version',
                    'telemetry.sdk.language','service.name'
                }
                # Apply masking only if the key is NOT in the excluded list
                if parent_key and parent_key.lower() not in excluded_keys:
                    return masking_func(obj)
                return obj
            else:
                return obj",Apply masking to all values in nested structure.,"def recursive_mask_values ( obj , parent_key = None ) : if isinstance ( obj , dict ) : return { k : recursive_mask_values ( v , k ) for k , v in obj . items ( ) } elif isinstance ( obj , list ) : return [ recursive_mask_values ( item , parent_key ) for item in obj ] elif isinstance ( obj , str ) : # List of keys that should NOT be masked excluded_keys = { 'start_time' , 'end_time' , 'name' , 'id' , 'hash_id' , 'parent_id' , 'source_hash_id' , 'cost' , 'type' , 'feedback' , 'error' , 'ctx' , 'telemetry.sdk.version' , 'telemetry.sdk.language' , 'service.name' } # Apply masking only if the key is NOT in the excluded list if parent_key and parent_key . lower ( ) not in excluded_keys : return masking_func ( obj ) return obj else : return obj",Apply masking to all values in nested structure.
/verl/verl/utils/import_utils.py,deprecated,"def deprecated(replacement: str = """"):
    """"""Decorator to mark APIs as deprecated.""""""
    import functools
    import warnings

    def decorator(func):
        qualified_name = _get_qualified_name(func)
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            msg = f""Warning: API '{qualified_name}' is deprecated.""
            if replacement:
                msg += f"" Please use '{replacement}' instead.""
            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        return wrapped
    return decorator","def deprecated(replacement: str = """"):
    """"""Decorator to mark APIs as deprecated.""""""
    import functools
    import warnings

    def decorator(func):
        qualified_name = _get_qualified_name(func)
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            msg = f""Warning: API '{qualified_name}' is deprecated.""
            if replacement:
                msg += f"" Please use '{replacement}' instead.""
            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        return wrapped
    return decorator",Decorator to mark APIs as deprecated.,Decorator to mark APIs as deprecated.,"def deprecated(replacement: str = """"):
    
    import functools
    import warnings

    def decorator(func):
        qualified_name = _get_qualified_name(func)
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            msg = f""Warning: API '{qualified_name}' is deprecated.""
            if replacement:
                msg += f"" Please use '{replacement}' instead.""
            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        return wrapped
    return decorator",Decorator to mark APIs as deprecated.,"def deprecated ( replacement : str = """" ) : import functools import warnings def decorator ( func ) : qualified_name = _get_qualified_name ( func ) @ functools . wraps ( func ) def wrapped ( * args , ** kwargs ) : msg = f""Warning: API '{qualified_name}' is deprecated."" if replacement : msg += f"" Please use '{replacement}' instead."" warnings . warn ( msg , category = DeprecationWarning , stacklevel = 2 ) return func ( * args , ** kwargs ) return wrapped return decorator",Decorator to mark APIs as deprecated.
/preswald/preswald/browser/virtual_service.py,__init__,"def __init__(self):
        """"""Initialize the service with browser-compatible components""""""
        super().__init__()

        # Client connections (virtual)
        self.websocket_connections = {}

        # In browser, set dummy branding manager
        self.branding_manager = type(
            ""DummyBrandingManager"",
            (),
            {
                ""static_dir"": """",
                ""get_branding_config"": lambda *args: {
                    ""name"": ""Preswald"",
                    ""favicon"": ""/favicon.ico"",
                },
            },
        )()

        # Register with JavaScript
        self._register_js_handlers()","def __init__(self):
        """"""Initialize the service with browser-compatible components""""""
        super().__init__()

        # Client connections (virtual)
        self.websocket_connections = {}

        # In browser, set dummy branding manager
        self.branding_manager = type(
            ""DummyBrandingManager"",
            (),
            {
                ""static_dir"": """",
                ""get_branding_config"": lambda *args: {
                    ""name"": ""Preswald"",
                    ""favicon"": ""/favicon.ico"",
                },
            },
        )()

        # Register with JavaScript
        self._register_js_handlers()",Initialize the service with browser-compatible components,Initialize the service with browser-compatible components,"def __init__(self):
        
        super().__init__()

        # Client connections (virtual)
        self.websocket_connections = {}

        # In browser, set dummy branding manager
        self.branding_manager = type(
            ""DummyBrandingManager"",
            (),
            {
                ""static_dir"": """",
                ""get_branding_config"": lambda *args: {
                    ""name"": ""Preswald"",
                    ""favicon"": ""/favicon.ico"",
                },
            },
        )()

        # Register with JavaScript
        self._register_js_handlers()",Initialize the service with browser-compatible components,"def __init__ ( self ) : super ( ) . __init__ ( ) # Client connections (virtual) self . websocket_connections = { } # In browser, set dummy branding manager self . branding_manager = type ( ""DummyBrandingManager"" , ( ) , { ""static_dir"" : """" , ""get_branding_config"" : lambda * args : { ""name"" : ""Preswald"" , ""favicon"" : ""/favicon.ico"" , } , } , ) ( ) # Register with JavaScript self . _register_js_handlers ( )",Initialize the service with browser-compatible components
/local-deep-research/src/local_deep_research/web_search_engines/engines/search_engine_wayback.py,_format_timestamp,"def _format_timestamp(self, timestamp: str) -> str:
        """"""Format Wayback Machine timestamp into readable date""""""
        if len(timestamp) < 14:
            return timestamp

        try:
            year = timestamp[0:4]
            month = timestamp[4:6]
            day = timestamp[6:8]
            hour = timestamp[8:10]
            minute = timestamp[10:12]
            second = timestamp[12:14]
            return f""{year}-{month}-{day} {hour}:{minute}:{second}""
        except Exception:
            return timestamp","def _format_timestamp(self, timestamp: str) -> str:
        """"""Format Wayback Machine timestamp into readable date""""""
        if len(timestamp) < 14:
            return timestamp

        try:
            year = timestamp[0:4]
            month = timestamp[4:6]
            day = timestamp[6:8]
            hour = timestamp[8:10]
            minute = timestamp[10:12]
            second = timestamp[12:14]
            return f""{year}-{month}-{day} {hour}:{minute}:{second}""
        except Exception:
            return timestamp",Format Wayback Machine timestamp into readable date,Format Wayback Machine timestamp into readable date,"def _format_timestamp(self, timestamp: str) -> str:
        
        if len(timestamp) < 14:
            return timestamp

        try:
            year = timestamp[0:4]
            month = timestamp[4:6]
            day = timestamp[6:8]
            hour = timestamp[8:10]
            minute = timestamp[10:12]
            second = timestamp[12:14]
            return f""{year}-{month}-{day} {hour}:{minute}:{second}""
        except Exception:
            return timestamp",Format Wayback Machine timestamp into readable date,"def _format_timestamp ( self , timestamp : str ) -> str : if len ( timestamp ) < 14 : return timestamp try : year = timestamp [ 0 : 4 ] month = timestamp [ 4 : 6 ] day = timestamp [ 6 : 8 ] hour = timestamp [ 8 : 10 ] minute = timestamp [ 10 : 12 ] second = timestamp [ 12 : 14 ] return f""{year}-{month}-{day} {hour}:{minute}:{second}"" except Exception : return timestamp",Format Wayback Machine timestamp into readable date
/intentkit/app/admin/generator/skill_processor.py,get_skill_states,"def get_skill_states(skill_category: str) -> Set[str]:
    """"""Get the actual skill states for a given skill category by importing its module.""""""
    if skill_category in _skill_states_cache:
        return _skill_states_cache[skill_category]

    try:
        # Import the skill category module
        skill_module = importlib.import_module(f""skills.{skill_category}"")

        # Look for the SkillStates TypedDict class
        if hasattr(skill_module, ""SkillStates""):
            skill_states_class = getattr(skill_module, ""SkillStates"")
            # Get the annotations which contain the state names
            if hasattr(skill_states_class, ""__annotations__""):
                states = set(skill_states_class.__annotations__.keys())
                _skill_states_cache[skill_category] = states
                return states

        logger.warning(f""Could not find SkillStates for {skill_category}"")
        return set()

    except ImportError as e:
        logger.warning(f""Could not import skill category {skill_category}: {e}"")
        return set()","def get_skill_states(skill_category: str) -> Set[str]:
    """"""Get the actual skill states for a given skill category by importing its module.""""""
    if skill_category in _skill_states_cache:
        return _skill_states_cache[skill_category]

    try:
        # Import the skill category module
        skill_module = importlib.import_module(f""skills.{skill_category}"")

        # Look for the SkillStates TypedDict class
        if hasattr(skill_module, ""SkillStates""):
            skill_states_class = getattr(skill_module, ""SkillStates"")
            # Get the annotations which contain the state names
            if hasattr(skill_states_class, ""__annotations__""):
                states = set(skill_states_class.__annotations__.keys())
                _skill_states_cache[skill_category] = states
                return states

        logger.warning(f""Could not find SkillStates for {skill_category}"")
        return set()

    except ImportError as e:
        logger.warning(f""Could not import skill category {skill_category}: {e}"")
        return set()",Get the actual skill states for a given skill category by importing its module.,Get the actual skill states for a given skill category by importing its module.,"def get_skill_states(skill_category: str) -> Set[str]:
    
    if skill_category in _skill_states_cache:
        return _skill_states_cache[skill_category]

    try:
        # Import the skill category module
        skill_module = importlib.import_module(f""skills.{skill_category}"")

        # Look for the SkillStates TypedDict class
        if hasattr(skill_module, ""SkillStates""):
            skill_states_class = getattr(skill_module, ""SkillStates"")
            # Get the annotations which contain the state names
            if hasattr(skill_states_class, ""__annotations__""):
                states = set(skill_states_class.__annotations__.keys())
                _skill_states_cache[skill_category] = states
                return states

        logger.warning(f""Could not find SkillStates for {skill_category}"")
        return set()

    except ImportError as e:
        logger.warning(f""Could not import skill category {skill_category}: {e}"")
        return set()",Get the actual skill states for a given skill category by importing its module.,"def get_skill_states ( skill_category : str ) -> Set [ str ] : if skill_category in _skill_states_cache : return _skill_states_cache [ skill_category ] try : # Import the skill category module skill_module = importlib . import_module ( f""skills.{skill_category}"" ) # Look for the SkillStates TypedDict class if hasattr ( skill_module , ""SkillStates"" ) : skill_states_class = getattr ( skill_module , ""SkillStates"" ) # Get the annotations which contain the state names if hasattr ( skill_states_class , ""__annotations__"" ) : states = set ( skill_states_class . __annotations__ . keys ( ) ) _skill_states_cache [ skill_category ] = states return states logger . warning ( f""Could not find SkillStates for {skill_category}"" ) return set ( ) except ImportError as e : logger . warning ( f""Could not import skill category {skill_category}: {e}"" ) return set ( )",Get the actual skill states for a given skill category by importing its module.
/local-deep-research/src/local_deep_research/search_system.py,_progress_callback,"def _progress_callback(self, message: str, progress: int, metadata: dict) -> None:
        """"""Handle progress updates from the strategy.""""""
        logger.info(f""Progress: {progress}% - {message}"")
        if hasattr(self, ""progress_callback""):
            self.progress_callback(message, progress, metadata)","def _progress_callback(self, message: str, progress: int, metadata: dict) -> None:
        """"""Handle progress updates from the strategy.""""""
        logger.info(f""Progress: {progress}% - {message}"")
        if hasattr(self, ""progress_callback""):
            self.progress_callback(message, progress, metadata)",Handle progress updates from the strategy.,Handle progress updates from the strategy.,"def _progress_callback(self, message: str, progress: int, metadata: dict) -> None:
        
        logger.info(f""Progress: {progress}% - {message}"")
        if hasattr(self, ""progress_callback""):
            self.progress_callback(message, progress, metadata)",Handle progress updates from the strategy.,"def _progress_callback ( self , message : str , progress : int , metadata : dict ) -> None : logger . info ( f""Progress: {progress}% - {message}"" ) if hasattr ( self , ""progress_callback"" ) : self . progress_callback ( message , progress , metadata )",Handle progress updates from the strategy.
/local-deep-research/examples/benchmarks/claude_grading/benchmark.py,custom_get_evaluation_llm,"def custom_get_evaluation_llm(custom_config=None):
            """"""
            Override that uses the local get_llm with database access.
            """"""
            if custom_config is None:
                custom_config = evaluation_config

            print(f""Getting evaluation LLM with config: {custom_config}"")
            return get_llm(**custom_config)","def custom_get_evaluation_llm(custom_config=None):
            """"""
            Override that uses the local get_llm with database access.
            """"""
            if custom_config is None:
                custom_config = evaluation_config

            print(f""Getting evaluation LLM with config: {custom_config}"")
            return get_llm(**custom_config)",Override that uses the local get_llm with database access.,Override that uses the local get_llm with database access.,"def custom_get_evaluation_llm(custom_config=None):
            
            if custom_config is None:
                custom_config = evaluation_config

            print(f""Getting evaluation LLM with config: {custom_config}"")
            return get_llm(**custom_config)",Override that uses the local get_llm with database access.,"def custom_get_evaluation_llm ( custom_config = None ) : if custom_config is None : custom_config = evaluation_config print ( f""Getting evaluation LLM with config: {custom_config}"" ) return get_llm ( ** custom_config )",Override that uses the local get_llm with database access.
/Second-Me/lpm_kernel/models/memory.py,to_dict,"def to_dict(self):
        """"""Convert to dictionary, including document_id""""""
        result = {
            ""id"": self.id,
            ""name"": self.name,
            ""type"": self.type,
            ""path"": self.path,
            ""created_at"": self.created_at.isoformat() if self.created_at else None,
            ""meta_data"": self.meta_data,
        }
        if self.document_id:
            result[""document_id""] = self.document_id
        return result","def to_dict(self):
        """"""Convert to dictionary, including document_id""""""
        result = {
            ""id"": self.id,
            ""name"": self.name,
            ""type"": self.type,
            ""path"": self.path,
            ""created_at"": self.created_at.isoformat() if self.created_at else None,
            ""meta_data"": self.meta_data,
        }
        if self.document_id:
            result[""document_id""] = self.document_id
        return result","Convert to dictionary, including document_id","Convert to dictionary, including document_id","def to_dict(self):
        
        result = {
            ""id"": self.id,
            ""name"": self.name,
            ""type"": self.type,
            ""path"": self.path,
            ""created_at"": self.created_at.isoformat() if self.created_at else None,
            ""meta_data"": self.meta_data,
        }
        if self.document_id:
            result[""document_id""] = self.document_id
        return result","Convert to dictionary, including document_id","def to_dict ( self ) : result = { ""id"" : self . id , ""name"" : self . name , ""type"" : self . type , ""path"" : self . path , ""created_at"" : self . created_at . isoformat ( ) if self . created_at else None , ""meta_data"" : self . meta_data , } if self . document_id : result [ ""document_id"" ] = self . document_id return result","Convert to dictionary, including document_id"
/fastmcp/src/fastmcp/contrib/mcp_mixin/mcp_mixin.py,mcp_tool,"def mcp_tool(
    name: str | None = None,
    description: str | None = None,
    tags: set[str] | None = None,
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """"""Decorator to mark a method as an MCP tool for later registration.""""""

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        call_args = {
            ""name"": name or func.__name__,
            ""description"": description,
            ""tags"": tags,
        }
        call_args = {k: v for k, v in call_args.items() if v is not None}
        setattr(func, _MCP_REGISTRATION_TOOL_ATTR, call_args)
        return func

    return decorator","def mcp_tool(
    name: str | None = None,
    description: str | None = None,
    tags: set[str] | None = None,
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """"""Decorator to mark a method as an MCP tool for later registration.""""""

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        call_args = {
            ""name"": name or func.__name__,
            ""description"": description,
            ""tags"": tags,
        }
        call_args = {k: v for k, v in call_args.items() if v is not None}
        setattr(func, _MCP_REGISTRATION_TOOL_ATTR, call_args)
        return func

    return decorator",Decorator to mark a method as an MCP tool for later registration.,Decorator to mark a method as an MCP tool for later registration.,"def mcp_tool(
    name: str | None = None,
    description: str | None = None,
    tags: set[str] | None = None,
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        call_args = {
            ""name"": name or func.__name__,
            ""description"": description,
            ""tags"": tags,
        }
        call_args = {k: v for k, v in call_args.items() if v is not None}
        setattr(func, _MCP_REGISTRATION_TOOL_ATTR, call_args)
        return func

    return decorator",Decorator to mark a method as an MCP tool for later registration.,"def mcp_tool ( name : str | None = None , description : str | None = None , tags : set [ str ] | None = None , ) -> Callable [ [ Callable [ ... , Any ] ] , Callable [ ... , Any ] ] : def decorator ( func : Callable [ ... , Any ] ) -> Callable [ ... , Any ] : call_args = { ""name"" : name or func . __name__ , ""description"" : description , ""tags"" : tags , } call_args = { k : v for k , v in call_args . items ( ) if v is not None } setattr ( func , _MCP_REGISTRATION_TOOL_ATTR , call_args ) return func return decorator",Decorator to mark a method as an MCP tool for later registration.
/nv-ingest/src/nv_ingest/api/v1/ingest.py,trace_id_to_uuid,"def trace_id_to_uuid(trace_id: str) -> str:
    """"""Convert a 32-character OpenTelemetry trace ID to a UUID-like format.""""""
    trace_id = str(trace.format_trace_id(trace_id))
    if len(trace_id) != 32:
        raise ValueError(""Trace ID must be a 32-character hexadecimal string"")
    return f""{trace_id[:8]}-{trace_id[8:12]}-{trace_id[12:16]}-{trace_id[16:20]}-{trace_id[20:]}""","def trace_id_to_uuid(trace_id: str) -> str:
    """"""Convert a 32-character OpenTelemetry trace ID to a UUID-like format.""""""
    trace_id = str(trace.format_trace_id(trace_id))
    if len(trace_id) != 32:
        raise ValueError(""Trace ID must be a 32-character hexadecimal string"")
    return f""{trace_id[:8]}-{trace_id[8:12]}-{trace_id[12:16]}-{trace_id[16:20]}-{trace_id[20:]}""",Convert a 32-character OpenTelemetry trace ID to a UUID-like format.,Convert a 32-character OpenTelemetry trace ID to a UUID-like format.,"def trace_id_to_uuid(trace_id: str) -> str:
    
    trace_id = str(trace.format_trace_id(trace_id))
    if len(trace_id) != 32:
        raise ValueError(""Trace ID must be a 32-character hexadecimal string"")
    return f""{trace_id[:8]}-{trace_id[8:12]}-{trace_id[12:16]}-{trace_id[16:20]}-{trace_id[20:]}""",Convert a 32-character OpenTelemetry trace ID to a UUID-like format.,"def trace_id_to_uuid ( trace_id : str ) -> str : trace_id = str ( trace . format_trace_id ( trace_id ) ) if len ( trace_id ) != 32 : raise ValueError ( ""Trace ID must be a 32-character hexadecimal string"" ) return f""{trace_id[:8]}-{trace_id[8:12]}-{trace_id[12:16]}-{trace_id[16:20]}-{trace_id[20:]}""",Convert a 32-character OpenTelemetry trace ID to a UUID-like format.
/NLWeb/code/retrieval/azure_search_client.py,_get_endpoint_config,"def _get_endpoint_config(self):
        """"""Get the Azure Search endpoint configuration from CONFIG""""""
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is an Azure AI Search endpoint
        if endpoint_config.db_type != ""azure_ai_search"":
            error_msg = f""Endpoint {self.endpoint_name} is not an Azure AI Search endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config","def _get_endpoint_config(self):
        """"""Get the Azure Search endpoint configuration from CONFIG""""""
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is an Azure AI Search endpoint
        if endpoint_config.db_type != ""azure_ai_search"":
            error_msg = f""Endpoint {self.endpoint_name} is not an Azure AI Search endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config",Get the Azure Search endpoint configuration from CONFIG,Get the Azure Search endpoint configuration from CONFIG,"def _get_endpoint_config(self):
        
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is an Azure AI Search endpoint
        if endpoint_config.db_type != ""azure_ai_search"":
            error_msg = f""Endpoint {self.endpoint_name} is not an Azure AI Search endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config",Get the Azure Search endpoint configuration from CONFIG,"def _get_endpoint_config ( self ) : endpoint_config = CONFIG . retrieval_endpoints . get ( self . endpoint_name ) if not endpoint_config : error_msg = f""No configuration found for endpoint {self.endpoint_name}"" logger . error ( error_msg ) raise ValueError ( error_msg ) # Verify this is an Azure AI Search endpoint if endpoint_config . db_type != ""azure_ai_search"" : error_msg = f""Endpoint {self.endpoint_name} is not an Azure AI Search endpoint (type: {endpoint_config.db_type})"" logger . error ( error_msg ) raise ValueError ( error_msg ) return endpoint_config",Get the Azure Search endpoint configuration from CONFIG
/olmocr/scripts/chatgpt_tag_dolmadocs_v1.py,save_results,"def save_results(results, output_dir):
    """"""Save the full results to a JSON file for analysis.""""""
    output_path = Path(output_dir) / ""rich_autoscan_results.json""

    # Convert results to serializable format
    serializable_results = []
    for result in results:
        if result is None:
            continue
        serializable_results.append(result)

    with open(output_path, ""w"") as f:
        json.dump(serializable_results, f, indent=2, default=lambda o: o.value if isinstance(o, Enum) else o)

    print(f""Results saved to {output_path}"")","def save_results(results, output_dir):
    """"""Save the full results to a JSON file for analysis.""""""
    output_path = Path(output_dir) / ""rich_autoscan_results.json""

    # Convert results to serializable format
    serializable_results = []
    for result in results:
        if result is None:
            continue
        serializable_results.append(result)

    with open(output_path, ""w"") as f:
        json.dump(serializable_results, f, indent=2, default=lambda o: o.value if isinstance(o, Enum) else o)

    print(f""Results saved to {output_path}"")",Save the full results to a JSON file for analysis.,Save the full results to a JSON file for analysis.,"def save_results(results, output_dir):
    
    output_path = Path(output_dir) / ""rich_autoscan_results.json""

    # Convert results to serializable format
    serializable_results = []
    for result in results:
        if result is None:
            continue
        serializable_results.append(result)

    with open(output_path, ""w"") as f:
        json.dump(serializable_results, f, indent=2, default=lambda o: o.value if isinstance(o, Enum) else o)

    print(f""Results saved to {output_path}"")",Save the full results to a JSON file for analysis.,"def save_results ( results , output_dir ) : output_path = Path ( output_dir ) / ""rich_autoscan_results.json"" # Convert results to serializable format serializable_results = [ ] for result in results : if result is None : continue serializable_results . append ( result ) with open ( output_path , ""w"" ) as f : json . dump ( serializable_results , f , indent = 2 , default = lambda o : o . value if isinstance ( o , Enum ) else o ) print ( f""Results saved to {output_path}"" )",Save the full results to a JSON file for analysis.
/fastmcp/tests/server/openapi/test_openapi_path_parameters.py,mock_client,"def mock_client():
    """"""Create a mock httpx.AsyncClient.""""""
    client = AsyncMock(spec=httpx.AsyncClient)
    # Set up a mock response
    mock_response = MagicMock()
    mock_response.json.return_value = {""result"": ""success""}
    mock_response.raise_for_status.return_value = None
    client.request.return_value = mock_response
    return client","def mock_client():
    """"""Create a mock httpx.AsyncClient.""""""
    client = AsyncMock(spec=httpx.AsyncClient)
    # Set up a mock response
    mock_response = MagicMock()
    mock_response.json.return_value = {""result"": ""success""}
    mock_response.raise_for_status.return_value = None
    client.request.return_value = mock_response
    return client",Create a mock httpx.AsyncClient.,Create a mock httpx.AsyncClient.,"def mock_client():
    
    client = AsyncMock(spec=httpx.AsyncClient)
    # Set up a mock response
    mock_response = MagicMock()
    mock_response.json.return_value = {""result"": ""success""}
    mock_response.raise_for_status.return_value = None
    client.request.return_value = mock_response
    return client",Create a mock httpx.AsyncClient.,"def mock_client ( ) : client = AsyncMock ( spec = httpx . AsyncClient ) # Set up a mock response mock_response = MagicMock ( ) mock_response . json . return_value = { ""result"" : ""success"" } mock_response . raise_for_status . return_value = None client . request . return_value = mock_response return client",Create a mock httpx.AsyncClient.
/mcp-agent/examples/usecases/reliable_conversation/src/utils/logging.py,log_workflow_step,"def log_workflow_step(
    logger, conversation_id: str, step: str, details: Optional[Dict[str, Any]] = None
):
    """"""Log workflow execution steps for debugging""""""
    log_data = {
        ""conversation_id"": conversation_id,
        ""workflow_step"": step,
        **(details or {}),
    }
    logger.debug(f""Workflow step: {step}"", data=log_data)","def log_workflow_step(
    logger, conversation_id: str, step: str, details: Optional[Dict[str, Any]] = None
):
    """"""Log workflow execution steps for debugging""""""
    log_data = {
        ""conversation_id"": conversation_id,
        ""workflow_step"": step,
        **(details or {}),
    }
    logger.debug(f""Workflow step: {step}"", data=log_data)",Log workflow execution steps for debugging,Log workflow execution steps for debugging,"def log_workflow_step(
    logger, conversation_id: str, step: str, details: Optional[Dict[str, Any]] = None
):
    
    log_data = {
        ""conversation_id"": conversation_id,
        ""workflow_step"": step,
        **(details or {}),
    }
    logger.debug(f""Workflow step: {step}"", data=log_data)",Log workflow execution steps for debugging,"def log_workflow_step ( logger , conversation_id : str , step : str , details : Optional [ Dict [ str , Any ] ] = None ) : log_data = { ""conversation_id"" : conversation_id , ""workflow_step"" : step , ** ( details or { } ) , } logger . debug ( f""Workflow step: {step}"" , data = log_data )",Log workflow execution steps for debugging
/aci/backend/aci/common/embeddings.py,generate_app_embedding,"def generate_app_embedding(
    app: AppEmbeddingFields,
    openai_client: OpenAI,
    embedding_model: str,
    embedding_dimension: int,
) -> list[float]:
    """"""
    Generate embedding for app.
    TODO: what else should be included or not in the embedding?
    """"""
    logger.debug(f""Generating embedding for app: {app.name}..."")
    # generate app embeddings based on app config's name, display_name, provider, description, categories
    text_for_embedding = app.model_dump_json()
    logger.debug(f""Text for app embedding: {text_for_embedding}"")
    return generate_embedding(
        openai_client, embedding_model, embedding_dimension, text_for_embedding
    )","def generate_app_embedding(
    app: AppEmbeddingFields,
    openai_client: OpenAI,
    embedding_model: str,
    embedding_dimension: int,
) -> list[float]:
    """"""
    Generate embedding for app.
    TODO: what else should be included or not in the embedding?
    """"""
    logger.debug(f""Generating embedding for app: {app.name}..."")
    # generate app embeddings based on app config's name, display_name, provider, description, categories
    text_for_embedding = app.model_dump_json()
    logger.debug(f""Text for app embedding: {text_for_embedding}"")
    return generate_embedding(
        openai_client, embedding_model, embedding_dimension, text_for_embedding
    )","Generate embedding for app.
TODO: what else should be included or not in the embedding?",Generate embedding for app.,"def generate_app_embedding(
    app: AppEmbeddingFields,
    openai_client: OpenAI,
    embedding_model: str,
    embedding_dimension: int,
) -> list[float]:
    
    logger.debug(f""Generating embedding for app: {app.name}..."")
    # generate app embeddings based on app config's name, display_name, provider, description, categories
    text_for_embedding = app.model_dump_json()
    logger.debug(f""Text for app embedding: {text_for_embedding}"")
    return generate_embedding(
        openai_client, embedding_model, embedding_dimension, text_for_embedding
    )",Generate embedding for app.,"def generate_app_embedding ( app : AppEmbeddingFields , openai_client : OpenAI , embedding_model : str , embedding_dimension : int , ) -> list [ float ] : logger . debug ( f""Generating embedding for app: {app.name}..."" ) # generate app embeddings based on app config's name, display_name, provider, description, categories text_for_embedding = app . model_dump_json ( ) logger . debug ( f""Text for app embedding: {text_for_embedding}"" ) return generate_embedding ( openai_client , embedding_model , embedding_dimension , text_for_embedding )",Generate embedding for app.
/Scrapling/scrapling/engines/toolbelt/fingerprints.py,get_os_name,"def get_os_name() -> Union[str, None]:
    """"""Get the current OS name in the same format needed for browserforge

    :return: Current OS name or `None` otherwise
    """"""
    #
    os_name = platform.system()
    return {
        'Linux': 'linux',
        'Darwin': 'macos',
        'Windows': 'windows',
        # For the future? because why not
        'iOS': 'ios',
    }.get(os_name)","def get_os_name() -> Union[str, None]:
    """"""Get the current OS name in the same format needed for browserforge

    :return: Current OS name or `None` otherwise
    """"""
    #
    os_name = platform.system()
    return {
        'Linux': 'linux',
        'Darwin': 'macos',
        'Windows': 'windows',
        # For the future? because why not
        'iOS': 'ios',
    }.get(os_name)","Get the current OS name in the same format needed for browserforge

:return: Current OS name or `None` otherwise",Get the current OS name in the same format needed for browserforge :return: Current OS name or `None` otherwise,"def get_os_name() -> Union[str, None]:
    
    #
    os_name = platform.system()
    return {
        'Linux': 'linux',
        'Darwin': 'macos',
        'Windows': 'windows',
        # For the future? because why not
        'iOS': 'ios',
    }.get(os_name)",Get the current OS name in the same format needed for browserforge :return: Current OS name or `None` otherwise,"def get_os_name ( ) -> Union [ str , None ] : # os_name = platform . system ( ) return { 'Linux' : 'linux' , 'Darwin' : 'macos' , 'Windows' : 'windows' , # For the future? because why not 'iOS' : 'ios' , } . get ( os_name )",Get the current OS name in the same format needed for browserforge :return: Current OS name or `None` otherwise
/Hunyuan3D-2/hy3dgen/shapegen/utils.py,__exit__,"def __exit__(self, exc_type, exc_value, exc_tb):
        """"""Context manager exit: stop timing and log results.""""""
        if os.environ.get('HY3DGEN_DEBUG', '0') == '1':
            self.end.record()
            torch.cuda.synchronize()
            self.time = self.start.elapsed_time(self.end)
            if self.name is not None:
                logger.info(f'{self.name} takes {self.time} ms')","def __exit__(self, exc_type, exc_value, exc_tb):
        """"""Context manager exit: stop timing and log results.""""""
        if os.environ.get('HY3DGEN_DEBUG', '0') == '1':
            self.end.record()
            torch.cuda.synchronize()
            self.time = self.start.elapsed_time(self.end)
            if self.name is not None:
                logger.info(f'{self.name} takes {self.time} ms')",Context manager exit: stop timing and log results.,Context manager exit: stop timing and log results.,"def __exit__(self, exc_type, exc_value, exc_tb):
        
        if os.environ.get('HY3DGEN_DEBUG', '0') == '1':
            self.end.record()
            torch.cuda.synchronize()
            self.time = self.start.elapsed_time(self.end)
            if self.name is not None:
                logger.info(f'{self.name} takes {self.time} ms')",Context manager exit: stop timing and log results.,"def __exit__ ( self , exc_type , exc_value , exc_tb ) : if os . environ . get ( 'HY3DGEN_DEBUG' , '0' ) == '1' : self . end . record ( ) torch . cuda . synchronize ( ) self . time = self . start . elapsed_time ( self . end ) if self . name is not None : logger . info ( f'{self.name} takes {self.time} ms' )",Context manager exit: stop timing and log results.
/cua/libs/agent/agent/core/experiment.py,create_turn_dir,"def create_turn_dir(self) -> None:
        """"""Create a new directory for the current turn.""""""
        if not self.run_dir:
            logger.warning(""Cannot create turn directory: run_dir not set"")
            return

        # Increment turn counter
        self.turn_count += 1

        # Create turn directory with padded number
        turn_name = f""turn_{self.turn_count:03d}""
        self.current_turn_dir = os.path.join(self.run_dir, turn_name)
        os.makedirs(self.current_turn_dir, exist_ok=True)
        logger.info(f""Created turn directory: {self.current_turn_dir}"")","def create_turn_dir(self) -> None:
        """"""Create a new directory for the current turn.""""""
        if not self.run_dir:
            logger.warning(""Cannot create turn directory: run_dir not set"")
            return

        # Increment turn counter
        self.turn_count += 1

        # Create turn directory with padded number
        turn_name = f""turn_{self.turn_count:03d}""
        self.current_turn_dir = os.path.join(self.run_dir, turn_name)
        os.makedirs(self.current_turn_dir, exist_ok=True)
        logger.info(f""Created turn directory: {self.current_turn_dir}"")",Create a new directory for the current turn.,Create a new directory for the current turn.,"def create_turn_dir(self) -> None:
        
        if not self.run_dir:
            logger.warning(""Cannot create turn directory: run_dir not set"")
            return

        # Increment turn counter
        self.turn_count += 1

        # Create turn directory with padded number
        turn_name = f""turn_{self.turn_count:03d}""
        self.current_turn_dir = os.path.join(self.run_dir, turn_name)
        os.makedirs(self.current_turn_dir, exist_ok=True)
        logger.info(f""Created turn directory: {self.current_turn_dir}"")",Create a new directory for the current turn.,"def create_turn_dir ( self ) -> None : if not self . run_dir : logger . warning ( ""Cannot create turn directory: run_dir not set"" ) return # Increment turn counter self . turn_count += 1 # Create turn directory with padded number turn_name = f""turn_{self.turn_count:03d}"" self . current_turn_dir = os . path . join ( self . run_dir , turn_name ) os . makedirs ( self . current_turn_dir , exist_ok = True ) logger . info ( f""Created turn directory: {self.current_turn_dir}"" )",Create a new directory for the current turn.
/markitdown/packages/markitdown/src/markitdown/_stream_info.py,copy_and_update,"def copy_and_update(self, *args, **kwargs):
        """"""Copy the StreamInfo object and update it with the given StreamInfo
        instance and/or other keyword arguments.""""""
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if len(kwargs) > 0:
            new_info.update(kwargs)

        return StreamInfo(**new_info)","def copy_and_update(self, *args, **kwargs):
        """"""Copy the StreamInfo object and update it with the given StreamInfo
        instance and/or other keyword arguments.""""""
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if len(kwargs) > 0:
            new_info.update(kwargs)

        return StreamInfo(**new_info)","Copy the StreamInfo object and update it with the given StreamInfo
instance and/or other keyword arguments.",Copy the StreamInfo object and update it with the given StreamInfo instance and/or other keyword arguments.,"def copy_and_update(self, *args, **kwargs):
        
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if len(kwargs) > 0:
            new_info.update(kwargs)

        return StreamInfo(**new_info)",Copy the StreamInfo object and update it with the given StreamInfo instance and/or other keyword arguments.,"def copy_and_update ( self , * args , ** kwargs ) : new_info = asdict ( self ) for si in args : assert isinstance ( si , StreamInfo ) new_info . update ( { k : v for k , v in asdict ( si ) . items ( ) if v is not None } ) if len ( kwargs ) > 0 : new_info . update ( kwargs ) return StreamInfo ( ** new_info )",Copy the StreamInfo object and update it with the given StreamInfo instance and/or other keyword arguments.
/alphafold3/src/alphafold3/model/post_processing.py,write_output,"def write_output(
    inference_result: model.InferenceResult,
    output_dir: os.PathLike[str] | str,
    terms_of_use: str | None = None,
    name: str | None = None,
) -> None:
  """"""Writes processed inference result to a directory.""""""
  processed_result = post_process_inference_result(inference_result)

  prefix = f'{name}_' if name is not None else ''

  with open(os.path.join(output_dir, f'{prefix}model.cif'), 'wb') as f:
    f.write(processed_result.cif)

  with open(
      os.path.join(output_dir, f'{prefix}summary_confidences.json'), 'wb'
  ) as f:
    f.write(processed_result.structure_confidence_summary_json)

  with open(os.path.join(output_dir, f'{prefix}confidences.json'), 'wb') as f:
    f.write(processed_result.structure_full_data_json)

  if terms_of_use is not None:
    with open(os.path.join(output_dir, 'TERMS_OF_USE.md'), 'wt') as f:
      f.write(terms_of_use)","def write_output(
    inference_result: model.InferenceResult,
    output_dir: os.PathLike[str] | str,
    terms_of_use: str | None = None,
    name: str | None = None,
) -> None:
  """"""Writes processed inference result to a directory.""""""
  processed_result = post_process_inference_result(inference_result)

  prefix = f'{name}_' if name is not None else ''

  with open(os.path.join(output_dir, f'{prefix}model.cif'), 'wb') as f:
    f.write(processed_result.cif)

  with open(
      os.path.join(output_dir, f'{prefix}summary_confidences.json'), 'wb'
  ) as f:
    f.write(processed_result.structure_confidence_summary_json)

  with open(os.path.join(output_dir, f'{prefix}confidences.json'), 'wb') as f:
    f.write(processed_result.structure_full_data_json)

  if terms_of_use is not None:
    with open(os.path.join(output_dir, 'TERMS_OF_USE.md'), 'wt') as f:
      f.write(terms_of_use)",Writes processed inference result to a directory.,Writes processed inference result to a directory.,"def write_output(
    inference_result: model.InferenceResult,
    output_dir: os.PathLike[str] | str,
    terms_of_use: str | None = None,
    name: str | None = None,
) -> None:
  
  processed_result = post_process_inference_result(inference_result)

  prefix = f'{name}_' if name is not None else ''

  with open(os.path.join(output_dir, f'{prefix}model.cif'), 'wb') as f:
    f.write(processed_result.cif)

  with open(
      os.path.join(output_dir, f'{prefix}summary_confidences.json'), 'wb'
  ) as f:
    f.write(processed_result.structure_confidence_summary_json)

  with open(os.path.join(output_dir, f'{prefix}confidences.json'), 'wb') as f:
    f.write(processed_result.structure_full_data_json)

  if terms_of_use is not None:
    with open(os.path.join(output_dir, 'TERMS_OF_USE.md'), 'wt') as f:
      f.write(terms_of_use)",Writes processed inference result to a directory.,"def write_output ( inference_result : model . InferenceResult , output_dir : os . PathLike [ str ] | str , terms_of_use : str | None = None , name : str | None = None , ) -> None : processed_result = post_process_inference_result ( inference_result ) prefix = f'{name}_' if name is not None else '' with open ( os . path . join ( output_dir , f'{prefix}model.cif' ) , 'wb' ) as f : f . write ( processed_result . cif ) with open ( os . path . join ( output_dir , f'{prefix}summary_confidences.json' ) , 'wb' ) as f : f . write ( processed_result . structure_confidence_summary_json ) with open ( os . path . join ( output_dir , f'{prefix}confidences.json' ) , 'wb' ) as f : f . write ( processed_result . structure_full_data_json ) if terms_of_use is not None : with open ( os . path . join ( output_dir , 'TERMS_OF_USE.md' ) , 'wt' ) as f : f . write ( terms_of_use )",Writes processed inference result to a directory.
/Scrapling/scrapling/core/translator.py,xpath_attr_functional_pseudo_element,"def xpath_attr_functional_pseudo_element(
            xpath: OriginalXPathExpr, function: FunctionalPseudoElement
    ) -> XPathExpr:
        """"""Support selecting attribute values using ::attr() pseudo-element""""""
        if function.argument_types() not in ([""STRING""], [""IDENT""]):
            raise ExpressionError(
                f""Expected a single string or ident for ::attr(), got {function.arguments!r}""
            )
        return XPathExpr.from_xpath(xpath, attribute=function.arguments[0].value)","def xpath_attr_functional_pseudo_element(
            xpath: OriginalXPathExpr, function: FunctionalPseudoElement
    ) -> XPathExpr:
        """"""Support selecting attribute values using ::attr() pseudo-element""""""
        if function.argument_types() not in ([""STRING""], [""IDENT""]):
            raise ExpressionError(
                f""Expected a single string or ident for ::attr(), got {function.arguments!r}""
            )
        return XPathExpr.from_xpath(xpath, attribute=function.arguments[0].value)",Support selecting attribute values using ::attr() pseudo-element,Support selecting attribute values using ::attr() pseudo-element,"def xpath_attr_functional_pseudo_element(
            xpath: OriginalXPathExpr, function: FunctionalPseudoElement
    ) -> XPathExpr:
        
        if function.argument_types() not in ([""STRING""], [""IDENT""]):
            raise ExpressionError(
                f""Expected a single string or ident for ::attr(), got {function.arguments!r}""
            )
        return XPathExpr.from_xpath(xpath, attribute=function.arguments[0].value)",Support selecting attribute values using ::attr() pseudo-element,"def xpath_attr_functional_pseudo_element ( xpath : OriginalXPathExpr , function : FunctionalPseudoElement ) -> XPathExpr : if function . argument_types ( ) not in ( [ ""STRING"" ] , [ ""IDENT"" ] ) : raise ExpressionError ( f""Expected a single string or ident for ::attr(), got {function.arguments!r}"" ) return XPathExpr . from_xpath ( xpath , attribute = function . arguments [ 0 ] . value )",Support selecting attribute values using ::attr() pseudo-element
/verl/verl/utils/device.py,get_torch_device,"def get_torch_device() -> any:
    """"""Return the corresponding torch attribute based on the device type string.
    Returns:
        module: The corresponding torch device namespace, or torch.cuda if not found.
    """"""
    device_name = get_device_name()
    try:
        return getattr(torch, device_name)
    except AttributeError:
        logger.warning(f""Device namespace '{device_name}' not found in torch, try to load torch.cuda."")
        return torch.cuda","def get_torch_device() -> any:
    """"""Return the corresponding torch attribute based on the device type string.
    Returns:
        module: The corresponding torch device namespace, or torch.cuda if not found.
    """"""
    device_name = get_device_name()
    try:
        return getattr(torch, device_name)
    except AttributeError:
        logger.warning(f""Device namespace '{device_name}' not found in torch, try to load torch.cuda."")
        return torch.cuda","Return the corresponding torch attribute based on the device type string.
Returns:
    module: The corresponding torch device namespace, or torch.cuda if not found.",Return the corresponding torch attribute based on the device type string.,"def get_torch_device() -> any:
    
    device_name = get_device_name()
    try:
        return getattr(torch, device_name)
    except AttributeError:
        logger.warning(f""Device namespace '{device_name}' not found in torch, try to load torch.cuda."")
        return torch.cuda",Return the corresponding torch attribute based on the device type string.,"def get_torch_device ( ) -> any : device_name = get_device_name ( ) try : return getattr ( torch , device_name ) except AttributeError : logger . warning ( f""Device namespace '{device_name}' not found in torch, try to load torch.cuda."" ) return torch . cuda",Return the corresponding torch attribute based on the device type string.
/OpenManus-RL/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py,initialize_cache,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        """"""Initialize the KV cache in all workers.""""""

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )","def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        """"""Initialize the KV cache in all workers.""""""

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )",Initialize the KV cache in all workers.,Initialize the KV cache in all workers.,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )",Initialize the KV cache in all workers.,"def initialize_cache ( self , num_gpu_blocks : int , num_cpu_blocks : int ) -> None : # NOTE: We log here to avoid multiple logs when number of workers is # greater than one. We could log in the engine, but not all executors # have GPUs. logger . info ( ""# GPU blocks: %d, # CPU blocks: %d"" , num_gpu_blocks , num_cpu_blocks ) self . cache_config . num_gpu_blocks = num_gpu_blocks self . cache_config . num_cpu_blocks = num_cpu_blocks if torch . distributed . get_rank ( ) == 0 : print ( f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"" ) self . worker . initialize_cache ( num_gpu_blocks = num_gpu_blocks , num_cpu_blocks = num_cpu_blocks ) if torch . distributed . get_rank ( ) == 0 : print ( f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"" )",Initialize the KV cache in all workers.
/nv-ingest/client/src/nv_ingest_client/primitives/tasks/filter.py,__str__,"def __str__(self) -> str:
        """"""
        Returns a string with the object's config and run time state
        """"""
        info = """"
        info += ""Filter Task:\n""
        info += f""  content_type: {self._content_type}\n""
        info += f""  min_size: {self._min_size}\n""
        info += f""  max_aspect_ratio: {self._max_aspect_ratio}\n""
        info += f""  min_aspect_ratio: {self._min_aspect_ratio}\n""
        info += f""  filter: {self._filter}\n""
        return info","def __str__(self) -> str:
        """"""
        Returns a string with the object's config and run time state
        """"""
        info = """"
        info += ""Filter Task:\n""
        info += f""  content_type: {self._content_type}\n""
        info += f""  min_size: {self._min_size}\n""
        info += f""  max_aspect_ratio: {self._max_aspect_ratio}\n""
        info += f""  min_aspect_ratio: {self._min_aspect_ratio}\n""
        info += f""  filter: {self._filter}\n""
        return info",Returns a string with the object's config and run time state,Returns a string with the object's config and run time state,"def __str__(self) -> str:
        
        info = """"
        info += ""Filter Task:\n""
        info += f""  content_type: {self._content_type}\n""
        info += f""  min_size: {self._min_size}\n""
        info += f""  max_aspect_ratio: {self._max_aspect_ratio}\n""
        info += f""  min_aspect_ratio: {self._min_aspect_ratio}\n""
        info += f""  filter: {self._filter}\n""
        return info",Returns a string with the object's config and run time state,"def __str__ ( self ) -> str : info = """" info += ""Filter Task:\n"" info += f""  content_type: {self._content_type}\n"" info += f""  min_size: {self._min_size}\n"" info += f""  max_aspect_ratio: {self._max_aspect_ratio}\n"" info += f""  min_aspect_ratio: {self._min_aspect_ratio}\n"" info += f""  filter: {self._filter}\n"" return info",Returns a string with the object's config and run time state
/fast-graphrag/fast_graphrag/_types.py,to_dict,"def to_dict(self) -> Dict[str, Any]:
    """"""Convert the query response to a dictionary.""""""
    return {
      ""response"": self.response,
      ""context"": {
        ""entities"": [(e.to_dict(e, include_fields=e.F_TO_CONTEXT), float(s)) for e, s in self.context.entities],
        ""relations"": [(r.to_dict(r, include_fields=r.F_TO_CONTEXT), float(s)) for r, s in self.context.relations],
        ""chunks"": [(c.to_dict(c, include_fields=c.F_TO_CONTEXT), float(s)) for c, s in self.context.chunks],
      },
    }","def to_dict(self) -> Dict[str, Any]:
    """"""Convert the query response to a dictionary.""""""
    return {
      ""response"": self.response,
      ""context"": {
        ""entities"": [(e.to_dict(e, include_fields=e.F_TO_CONTEXT), float(s)) for e, s in self.context.entities],
        ""relations"": [(r.to_dict(r, include_fields=r.F_TO_CONTEXT), float(s)) for r, s in self.context.relations],
        ""chunks"": [(c.to_dict(c, include_fields=c.F_TO_CONTEXT), float(s)) for c, s in self.context.chunks],
      },
    }",Convert the query response to a dictionary.,Convert the query response to a dictionary.,"def to_dict(self) -> Dict[str, Any]:
    
    return {
      ""response"": self.response,
      ""context"": {
        ""entities"": [(e.to_dict(e, include_fields=e.F_TO_CONTEXT), float(s)) for e, s in self.context.entities],
        ""relations"": [(r.to_dict(r, include_fields=r.F_TO_CONTEXT), float(s)) for r, s in self.context.relations],
        ""chunks"": [(c.to_dict(c, include_fields=c.F_TO_CONTEXT), float(s)) for c, s in self.context.chunks],
      },
    }",Convert the query response to a dictionary.,"def to_dict ( self ) -> Dict [ str , Any ] : return { ""response"" : self . response , ""context"" : { ""entities"" : [ ( e . to_dict ( e , include_fields = e . F_TO_CONTEXT ) , float ( s ) ) for e , s in self . context . entities ] , ""relations"" : [ ( r . to_dict ( r , include_fields = r . F_TO_CONTEXT ) , float ( s ) ) for r , s in self . context . relations ] , ""chunks"" : [ ( c . to_dict ( c , include_fields = c . F_TO_CONTEXT ) , float ( s ) ) for c , s in self . context . chunks ] , } , }",Convert the query response to a dictionary.
/olmocr/olmocr/bench/scripts/url_matcher.py,extract_urls_from_jsonl,"def extract_urls_from_jsonl(file_path):
    """"""Extract URLs from a JSONL file.""""""
    urls = set()
    url_to_data = {}
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if ""url"" in data and data[""url""]:
                    url = data[""url""]
                    urls.add(url)
                    # Store minimal context for each URL
                    url_to_data[url] = {""id"": data.get(""id"", """"), ""type"": data.get(""type"", """"), ""page"": data.get(""page"", """")}
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON from line in {file_path}"")
                continue
    return urls, url_to_data","def extract_urls_from_jsonl(file_path):
    """"""Extract URLs from a JSONL file.""""""
    urls = set()
    url_to_data = {}
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if ""url"" in data and data[""url""]:
                    url = data[""url""]
                    urls.add(url)
                    # Store minimal context for each URL
                    url_to_data[url] = {""id"": data.get(""id"", """"), ""type"": data.get(""type"", """"), ""page"": data.get(""page"", """")}
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON from line in {file_path}"")
                continue
    return urls, url_to_data",Extract URLs from a JSONL file.,Extract URLs from a JSONL file.,"def extract_urls_from_jsonl(file_path):
    
    urls = set()
    url_to_data = {}
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if ""url"" in data and data[""url""]:
                    url = data[""url""]
                    urls.add(url)
                    # Store minimal context for each URL
                    url_to_data[url] = {""id"": data.get(""id"", """"), ""type"": data.get(""type"", """"), ""page"": data.get(""page"", """")}
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON from line in {file_path}"")
                continue
    return urls, url_to_data",Extract URLs from a JSONL file.,"def extract_urls_from_jsonl ( file_path ) : urls = set ( ) url_to_data = { } with open ( file_path , ""r"" , encoding = ""utf-8"" ) as f : for line in f : try : data = json . loads ( line . strip ( ) ) if ""url"" in data and data [ ""url"" ] : url = data [ ""url"" ] urls . add ( url ) # Store minimal context for each URL url_to_data [ url ] = { ""id"" : data . get ( ""id"" , """" ) , ""type"" : data . get ( ""type"" , """" ) , ""page"" : data . get ( ""page"" , """" ) } except json . JSONDecodeError : print ( f""Warning: Could not parse JSON from line in {file_path}"" ) continue return urls , url_to_data",Extract URLs from a JSONL file.
/Kokoro-FastAPI/ui/lib/api.py,get_status_html,"def get_status_html(is_available: bool) -> str:
    """"""Generate HTML for status indicator.""""""
    color = ""green"" if is_available else ""red""
    status = ""Available"" if is_available else ""Unavailable""
    return f""""""
        <div style=""display: flex; align-items: center; gap: 8px;"">
            <div style=""width: 12px; height: 12px; border-radius: 50%; background-color: {color};""></div>
            <span>TTS Service: {status}</span>
        </div>
    """"""","def get_status_html(is_available: bool) -> str:
    """"""Generate HTML for status indicator.""""""
    color = ""green"" if is_available else ""red""
    status = ""Available"" if is_available else ""Unavailable""
    return f""""""
        <div style=""display: flex; align-items: center; gap: 8px;"">
            <div style=""width: 12px; height: 12px; border-radius: 50%; background-color: {color};""></div>
            <span>TTS Service: {status}</span>
        </div>
    """"""",Generate HTML for status indicator.,Generate HTML for status indicator.,"def get_status_html(is_available: bool) -> str:
    
    color = ""green"" if is_available else ""red""
    status = ""Available"" if is_available else ""Unavailable""
    return f",Generate HTML for status indicator.,"def get_status_html ( is_available : bool ) -> str : color = ""green"" if is_available else ""red"" status = ""Available"" if is_available else ""Unavailable"" return f",Generate HTML for status indicator.
/NLWeb/code/webserver/WebServer.py,get_port,"def get_port():
    """"""Get the port to listen on, using config or environment.""""""
    if 'PORT' in os.environ:
        port = int(os.environ['PORT'])
        print(f""Using PORT from environment variable: {port}"")
        return port
    elif 'WEBSITE_SITE_NAME' in os.environ:
        # Running in Azure App Service
        print(""Running in Azure App Service, using default port 8000"")
        return 8000  # Azure will redirect requests to this port
    else:
        # Use configured port
        print(f""Using configured port {CONFIG.port}"")
        return CONFIG.port","def get_port():
    """"""Get the port to listen on, using config or environment.""""""
    if 'PORT' in os.environ:
        port = int(os.environ['PORT'])
        print(f""Using PORT from environment variable: {port}"")
        return port
    elif 'WEBSITE_SITE_NAME' in os.environ:
        # Running in Azure App Service
        print(""Running in Azure App Service, using default port 8000"")
        return 8000  # Azure will redirect requests to this port
    else:
        # Use configured port
        print(f""Using configured port {CONFIG.port}"")
        return CONFIG.port","Get the port to listen on, using config or environment.","Get the port to listen on, using config or environment.","def get_port():
    
    if 'PORT' in os.environ:
        port = int(os.environ['PORT'])
        print(f""Using PORT from environment variable: {port}"")
        return port
    elif 'WEBSITE_SITE_NAME' in os.environ:
        # Running in Azure App Service
        print(""Running in Azure App Service, using default port 8000"")
        return 8000  # Azure will redirect requests to this port
    else:
        # Use configured port
        print(f""Using configured port {CONFIG.port}"")
        return CONFIG.port","Get the port to listen on, using config or environment.","def get_port ( ) : if 'PORT' in os . environ : port = int ( os . environ [ 'PORT' ] ) print ( f""Using PORT from environment variable: {port}"" ) return port elif 'WEBSITE_SITE_NAME' in os . environ : # Running in Azure App Service print ( ""Running in Azure App Service, using default port 8000"" ) return 8000 # Azure will redirect requests to this port else : # Use configured port print ( f""Using configured port {CONFIG.port}"" ) return CONFIG . port","Get the port to listen on, using config or environment."
/olmocr/olmocr/bench/scripts/difference_viewer.py,parse_rules_file,"def parse_rules_file(file_path):
    """"""Parse the rules file and organize rules by PDF.""""""
    pdf_rules = defaultdict(list)

    with open(file_path, ""r"") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rule = json.loads(line)
                if ""pdf"" in rule:
                    pdf_rules[rule[""pdf""]].append(rule)
            except json.JSONDecodeError:
                print(f""Warning: Could not parse line as JSON: {line}"")

    return pdf_rules","def parse_rules_file(file_path):
    """"""Parse the rules file and organize rules by PDF.""""""
    pdf_rules = defaultdict(list)

    with open(file_path, ""r"") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rule = json.loads(line)
                if ""pdf"" in rule:
                    pdf_rules[rule[""pdf""]].append(rule)
            except json.JSONDecodeError:
                print(f""Warning: Could not parse line as JSON: {line}"")

    return pdf_rules",Parse the rules file and organize rules by PDF.,Parse the rules file and organize rules by PDF.,"def parse_rules_file(file_path):
    
    pdf_rules = defaultdict(list)

    with open(file_path, ""r"") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rule = json.loads(line)
                if ""pdf"" in rule:
                    pdf_rules[rule[""pdf""]].append(rule)
            except json.JSONDecodeError:
                print(f""Warning: Could not parse line as JSON: {line}"")

    return pdf_rules",Parse the rules file and organize rules by PDF.,"def parse_rules_file ( file_path ) : pdf_rules = defaultdict ( list ) with open ( file_path , ""r"" ) as f : for line in f : line = line . strip ( ) if not line : continue try : rule = json . loads ( line ) if ""pdf"" in rule : pdf_rules [ rule [ ""pdf"" ] ] . append ( rule ) except json . JSONDecodeError : print ( f""Warning: Could not parse line as JSON: {line}"" ) return pdf_rules",Parse the rules file and organize rules by PDF.
/owl/owl/utils/document_toolkit.py,_download_file,"def _download_file(self, url: str):
        r""""""Download a file from a URL and save it to the cache directory.""""""
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            file_name = url.split(""/"")[-1]

            file_path = os.path.join(self.cache_dir, file_name)

            with open(file_path, ""wb"") as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)

            return file_path

        except requests.exceptions.RequestException as e:
            print(f""Error downloading the file: {e}"")","def _download_file(self, url: str):
        r""""""Download a file from a URL and save it to the cache directory.""""""
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            file_name = url.split(""/"")[-1]

            file_path = os.path.join(self.cache_dir, file_name)

            with open(file_path, ""wb"") as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)

            return file_path

        except requests.exceptions.RequestException as e:
            print(f""Error downloading the file: {e}"")",Download a file from a URL and save it to the cache directory.,Download a file from a URL and save it to the cache directory.,"def _download_file(self, url: str):
        
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            file_name = url.split(""/"")[-1]

            file_path = os.path.join(self.cache_dir, file_name)

            with open(file_path, ""wb"") as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)

            return file_path

        except requests.exceptions.RequestException as e:
            print(f""Error downloading the file: {e}"")",Download a file from a URL and save it to the cache directory.,"def _download_file ( self , url : str ) : try : response = requests . get ( url , stream = True ) response . raise_for_status ( ) file_name = url . split ( ""/"" ) [ - 1 ] file_path = os . path . join ( self . cache_dir , file_name ) with open ( file_path , ""wb"" ) as file : for chunk in response . iter_content ( chunk_size = 8192 ) : file . write ( chunk ) return file_path except requests . exceptions . RequestException as e : print ( f""Error downloading the file: {e}"" )",Download a file from a URL and save it to the cache directory.
/EasyR1/verl/single_controller/base/decorator.py,get_predefined_execute_fn,"def get_predefined_execute_fn(execute_mode: Execute):
    """"""
    Note that here we only asks execute_all and execute_rank_zero to be implemented
    Leave the choice of how these two functions handle argument 'blocking' to users
    """"""
    predefined_execute_mode_fn = {
        Execute.ALL: {""execute_fn_name"": ""execute_all""},
        Execute.RANK_ZERO: {""execute_fn_name"": ""execute_rank_zero""},
    }
    return predefined_execute_mode_fn[execute_mode]","def get_predefined_execute_fn(execute_mode: Execute):
    """"""
    Note that here we only asks execute_all and execute_rank_zero to be implemented
    Leave the choice of how these two functions handle argument 'blocking' to users
    """"""
    predefined_execute_mode_fn = {
        Execute.ALL: {""execute_fn_name"": ""execute_all""},
        Execute.RANK_ZERO: {""execute_fn_name"": ""execute_rank_zero""},
    }
    return predefined_execute_mode_fn[execute_mode]","Note that here we only asks execute_all and execute_rank_zero to be implemented
Leave the choice of how these two functions handle argument 'blocking' to users",Note that here we only asks execute_all and execute_rank_zero to be implemented,"def get_predefined_execute_fn(execute_mode: Execute):
    
    predefined_execute_mode_fn = {
        Execute.ALL: {""execute_fn_name"": ""execute_all""},
        Execute.RANK_ZERO: {""execute_fn_name"": ""execute_rank_zero""},
    }
    return predefined_execute_mode_fn[execute_mode]",Note that here we only asks execute_all and execute_rank_zero to be implemented,"def get_predefined_execute_fn ( execute_mode : Execute ) : predefined_execute_mode_fn = { Execute . ALL : { ""execute_fn_name"" : ""execute_all"" } , Execute . RANK_ZERO : { ""execute_fn_name"" : ""execute_rank_zero"" } , } return predefined_execute_mode_fn [ execute_mode ]",Note that here we only asks execute_all and execute_rank_zero to be implemented
/airweave/backend/airweave/api/auth.py,__init__,"def __init__(self):
            """"""Initialize the mock Auth0 instance.""""""
            self.domain = ""mock-domain.auth0.com""
            self.audience = ""https://mock-api/""
            self.algorithms = [""RS256""]
            self.jwks = {""keys"": []}
            self.auth0_user_model = Auth0User","def __init__(self):
            """"""Initialize the mock Auth0 instance.""""""
            self.domain = ""mock-domain.auth0.com""
            self.algorithms = [""RS256""]
            self.jwks = {""keys"": []}
            self.auth0_user_model = Auth0User",Initialize the mock Auth0 instance.,Initialize the mock Auth0 instance.,"def __init__(self):
            
            self.domain = ""mock-domain.auth0.com""
            self.algorithms = [""RS256""]
            self.jwks = {""keys"": []}
            self.auth0_user_model = Auth0User",Initialize the mock Auth0 instance.,"def __init__ ( self ) : self . domain = ""mock-domain.auth0.com"" self . algorithms = [ ""RS256"" ] self . jwks = { ""keys"" : [ ] } self . auth0_user_model = Auth0User",Initialize the mock Auth0 instance.
/NLWeb/code/tools/qdrant_load.py,recreate_collection,"def recreate_collection(collection_name, vector_size):
    """"""Recreate a collection in Qdrant""""""
    if client.collection_exists(collection_name):
        print(f""Dropping existing collection '{collection_name}'"")
        client.delete_collection(collection_name)

    print(f""Creating collection '{collection_name}' with vector size {vector_size}"")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
    )","def recreate_collection(collection_name, vector_size):
    """"""Recreate a collection in Qdrant""""""
    if client.collection_exists(collection_name):
        print(f""Dropping existing collection '{collection_name}'"")
        client.delete_collection(collection_name)

    print(f""Creating collection '{collection_name}' with vector size {vector_size}"")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
    )",Recreate a collection in Qdrant,Recreate a collection in Qdrant,"def recreate_collection(collection_name, vector_size):
    
    if client.collection_exists(collection_name):
        print(f""Dropping existing collection '{collection_name}'"")
        client.delete_collection(collection_name)

    print(f""Creating collection '{collection_name}' with vector size {vector_size}"")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
    )",Recreate a collection in Qdrant,"def recreate_collection ( collection_name , vector_size ) : if client . collection_exists ( collection_name ) : print ( f""Dropping existing collection '{collection_name}'"" ) client . delete_collection ( collection_name ) print ( f""Creating collection '{collection_name}' with vector size {vector_size}"" ) client . create_collection ( collection_name = collection_name , vectors_config = VectorParams ( size = vector_size , distance = Distance . COSINE ) , )",Recreate a collection in Qdrant
/KAG/kag/solver/executor/retriever/local_knowledge_base/kag_retriever/kag_hybrid_executor.py,to_dict,"def to_dict(self):
        """"""Convert response to dictionary format""""""
        return {
            ""retrieved_task"": self.retrieved_task,
            ""sub_question"": [item.to_dict() for item in self.sub_retrieved_set],
            ""graph_data"": (
                [str(spo) for spo in self.graph_data.get_all_spo()]
                if self.graph_data
                else []
            ),
            ""chunk_datas"": [item.to_dict() for item in self.chunk_datas],
            ""summary"": self.summary,
        }","def to_dict(self):
        """"""Convert response to dictionary format""""""
        return {
            ""retrieved_task"": self.retrieved_task,
            ""sub_question"": [item.to_dict() for item in self.sub_retrieved_set],
            ""graph_data"": (
                [str(spo) for spo in self.graph_data.get_all_spo()]
                if self.graph_data
                else []
            ),
            ""chunk_datas"": [item.to_dict() for item in self.chunk_datas],
            ""summary"": self.summary,
        }",Convert response to dictionary format,Convert response to dictionary format,"def to_dict(self):
        
        return {
            ""retrieved_task"": self.retrieved_task,
            ""sub_question"": [item.to_dict() for item in self.sub_retrieved_set],
            ""graph_data"": (
                [str(spo) for spo in self.graph_data.get_all_spo()]
                if self.graph_data
                else []
            ),
            ""chunk_datas"": [item.to_dict() for item in self.chunk_datas],
            ""summary"": self.summary,
        }",Convert response to dictionary format,"def to_dict ( self ) : return { ""retrieved_task"" : self . retrieved_task , ""sub_question"" : [ item . to_dict ( ) for item in self . sub_retrieved_set ] , ""graph_data"" : ( [ str ( spo ) for spo in self . graph_data . get_all_spo ( ) ] if self . graph_data else [ ] ) , ""chunk_datas"" : [ item . to_dict ( ) for item in self . chunk_datas ] , ""summary"" : self . summary , }",Convert response to dictionary format
/smolagents/examples/multiple_tools.py,get_random_fact,"def get_random_fact() -> str:
    """"""
    Fetches a random fact from the ""uselessfacts.jsph.pl"" API.
    Returns:
        str: A string containing the random fact or an error message if the request fails.
    """"""
    url = ""https://uselessfacts.jsph.pl/random.json?language=en""

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()

        return f""Random Fact: {data['text']}""

    except requests.exceptions.RequestException as e:
        return f""Error fetching random fact: {str(e)}""","def get_random_fact() -> str:
    """"""
    Fetches a random fact from the ""uselessfacts.jsph.pl"" API.
    Returns:
        str: A string containing the random fact or an error message if the request fails.
    """"""

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()

        return f""Random Fact: {data['text']}""

    except requests.exceptions.RequestException as e:
        return f""Error fetching random fact: {str(e)}""","Fetches a random fact from the ""uselessfacts.jsph.pl"" API.
Returns:
    str: A string containing the random fact or an error message if the request fails.","Fetches a random fact from the ""uselessfacts.jsph.pl"" API.","def get_random_fact() -> str:
    

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()

        return f""Random Fact: {data['text']}""

    except requests.exceptions.RequestException as e:
        return f""Error fetching random fact: {str(e)}""","Fetches a random fact from the ""uselessfacts.jsph.pl"" API.","def get_random_fact ( ) -> str : try : response = requests . get ( url ) response . raise_for_status ( ) data = response . json ( ) return f""Random Fact: {data['text']}"" except requests . exceptions . RequestException as e : return f""Error fetching random fact: {str(e)}""","Fetches a random fact from the ""uselessfacts.jsph.pl"" API."
/nv-ingest/api/src/nv_ingest_api/internal/primitives/nim/nim_client.py,_fetch_max_batch_size,"def _fetch_max_batch_size(self, model_name, model_version: str = """") -> int:
        """"""Fetch the maximum batch size from the Triton model configuration in a thread-safe manner.""""""
        if model_name in self._max_batch_sizes:
            return self._max_batch_sizes[model_name]

        with self._lock:
            # Double check, just in case another thread set the value while we were waiting
            if model_name in self._max_batch_sizes:
                return self._max_batch_sizes[model_name]

            if not self._grpc_endpoint:
                self._max_batch_sizes[model_name] = 1
                return 1

            try:
                client = self.client if self.client else grpcclient.InferenceServerClient(url=self._grpc_endpoint)
                model_config = client.get_model_config(model_name=model_name, model_version=model_version)
                self._max_batch_sizes[model_name] = model_config.config.max_batch_size
                logger.debug(f""Max batch size for model '{model_name}': {self._max_batch_sizes[model_name]}"")
            except Exception as e:
                self._max_batch_sizes[model_name] = 1
                logger.warning(f""Failed to retrieve max batch size: {e}, defaulting to 1"")

            return self._max_batch_sizes[model_name]","def _fetch_max_batch_size(self, model_name, model_version: str = """") -> int:
        """"""Fetch the maximum batch size from the Triton model configuration in a thread-safe manner.""""""
        if model_name in self._max_batch_sizes:
            return self._max_batch_sizes[model_name]

        with self._lock:
            # Double check, just in case another thread set the value while we were waiting
            if model_name in self._max_batch_sizes:
                return self._max_batch_sizes[model_name]

            if not self._grpc_endpoint:
                self._max_batch_sizes[model_name] = 1
                return 1

            try:
                client = self.client if self.client else grpcclient.InferenceServerClient(url=self._grpc_endpoint)
                model_config = client.get_model_config(model_name=model_name, model_version=model_version)
                self._max_batch_sizes[model_name] = model_config.config.max_batch_size
                logger.debug(f""Max batch size for model '{model_name}': {self._max_batch_sizes[model_name]}"")
            except Exception as e:
                self._max_batch_sizes[model_name] = 1
                logger.warning(f""Failed to retrieve max batch size: {e}, defaulting to 1"")

            return self._max_batch_sizes[model_name]",Fetch the maximum batch size from the Triton model configuration in a thread-safe manner.,Fetch the maximum batch size from the Triton model configuration in a thread-safe manner.,"def _fetch_max_batch_size(self, model_name, model_version: str = """") -> int:
        
        if model_name in self._max_batch_sizes:
            return self._max_batch_sizes[model_name]

        with self._lock:
            # Double check, just in case another thread set the value while we were waiting
            if model_name in self._max_batch_sizes:
                return self._max_batch_sizes[model_name]

            if not self._grpc_endpoint:
                self._max_batch_sizes[model_name] = 1
                return 1

            try:
                client = self.client if self.client else grpcclient.InferenceServerClient(url=self._grpc_endpoint)
                model_config = client.get_model_config(model_name=model_name, model_version=model_version)
                self._max_batch_sizes[model_name] = model_config.config.max_batch_size
                logger.debug(f""Max batch size for model '{model_name}': {self._max_batch_sizes[model_name]}"")
            except Exception as e:
                self._max_batch_sizes[model_name] = 1
                logger.warning(f""Failed to retrieve max batch size: {e}, defaulting to 1"")

            return self._max_batch_sizes[model_name]",Fetch the maximum batch size from the Triton model configuration in a thread-safe manner.,"def _fetch_max_batch_size ( self , model_name , model_version : str = """" ) -> int : if model_name in self . _max_batch_sizes : return self . _max_batch_sizes [ model_name ] with self . _lock : # Double check, just in case another thread set the value while we were waiting if model_name in self . _max_batch_sizes : return self . _max_batch_sizes [ model_name ] if not self . _grpc_endpoint : self . _max_batch_sizes [ model_name ] = 1 return 1 try : client = self . client if self . client else grpcclient . InferenceServerClient ( url = self . _grpc_endpoint ) model_config = client . get_model_config ( model_name = model_name , model_version = model_version ) self . _max_batch_sizes [ model_name ] = model_config . config . max_batch_size logger . debug ( f""Max batch size for model '{model_name}': {self._max_batch_sizes[model_name]}"" ) except Exception as e : self . _max_batch_sizes [ model_name ] = 1 logger . warning ( f""Failed to retrieve max batch size: {e}, defaulting to 1"" ) return self . _max_batch_sizes [ model_name ]",Fetch the maximum batch size from the Triton model configuration in a thread-safe manner.
/mcp-agent/tests/workflows/llm/test_augmented_llm_bedrock.py,create_text_response,"def create_text_response(text, stop_reason=""end_turn"", usage=None):
        """"""
        Creates a text response for testing.
        """"""
        return {
            ""output"": {
                ""message"": {
                    ""role"": ""assistant"",
                    ""content"": [{""text"": text}],
                },
            },
            ""stopReason"": stop_reason,
            ""usage"": usage
            or {
                ""inputTokens"": 150,
                ""outputTokens"": 100,
                ""totalTokens"": 250,
            },
        }","def create_text_response(text, stop_reason=""end_turn"", usage=None):
        """"""
        Creates a text response for testing.
        """"""
        return {
            ""output"": {
                ""message"": {
                    ""role"": ""assistant"",
                    ""content"": [{""text"": text}],
                },
            },
            ""stopReason"": stop_reason,
            ""usage"": usage
            or {
                ""inputTokens"": 150,
                ""outputTokens"": 100,
                ""totalTokens"": 250,
            },
        }",Creates a text response for testing.,Creates a text response for testing.,"def create_text_response(text, stop_reason=""end_turn"", usage=None):
        
        return {
            ""output"": {
                ""message"": {
                    ""role"": ""assistant"",
                    ""content"": [{""text"": text}],
                },
            },
            ""stopReason"": stop_reason,
            ""usage"": usage
            or {
                ""inputTokens"": 150,
                ""outputTokens"": 100,
                ""totalTokens"": 250,
            },
        }",Creates a text response for testing.,"def create_text_response ( text , stop_reason = ""end_turn"" , usage = None ) : return { ""output"" : { ""message"" : { ""role"" : ""assistant"" , ""content"" : [ { ""text"" : text } ] , } , } , ""stopReason"" : stop_reason , ""usage"" : usage or { ""inputTokens"" : 150 , ""outputTokens"" : 100 , ""totalTokens"" : 250 , } , }",Creates a text response for testing.
/Second-Me/lpm_kernel/models/l1.py,from_dict,"def from_dict(cls, data: dict) -> ""L1GenerationResult"":
        """"""Create instance from dictionary""""""
        return cls(
            bio=data.get(""bio""),
            clusters=data.get(""clusters"", {""clusterList"": []}),
            chunk_topics=data.get(""chunk_topics"", {}),
            generate_time=datetime.fromisoformat(data[""generate_time""])
            if ""generate_time"" in data
            else datetime.now(),
        )","def from_dict(cls, data: dict) -> ""L1GenerationResult"":
        """"""Create instance from dictionary""""""
        return cls(
            bio=data.get(""bio""),
            clusters=data.get(""clusters"", {""clusterList"": []}),
            chunk_topics=data.get(""chunk_topics"", {}),
            generate_time=datetime.fromisoformat(data[""generate_time""])
            if ""generate_time"" in data
            else datetime.now(),
        )",Create instance from dictionary,Create instance from dictionary,"def from_dict(cls, data: dict) -> ""L1GenerationResult"":
        
        return cls(
            bio=data.get(""bio""),
            clusters=data.get(""clusters"", {""clusterList"": []}),
            chunk_topics=data.get(""chunk_topics"", {}),
            generate_time=datetime.fromisoformat(data[""generate_time""])
            if ""generate_time"" in data
            else datetime.now(),
        )",Create instance from dictionary,"def from_dict ( cls , data : dict ) -> ""L1GenerationResult"" : return cls ( bio = data . get ( ""bio"" ) , clusters = data . get ( ""clusters"" , { ""clusterList"" : [ ] } ) , chunk_topics = data . get ( ""chunk_topics"" , { } ) , generate_time = datetime . fromisoformat ( data [ ""generate_time"" ] ) if ""generate_time"" in data else datetime . now ( ) , )",Create instance from dictionary
/magentic-ui/src/magentic_ui/teams/orchestrator/_utils.py,extract_json_from_string,"def extract_json_from_string(s: str) -> Optional[Any]:
    """"""
    Searches for a JSON object within the string and returns the loaded JSON if found, otherwise returns None.
    """"""
    # Regex to find JSON objects (greedy, matches first { to last })
    match = re.search(r""\{.*\}"", s, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None","def extract_json_from_string(s: str) -> Optional[Any]:
    """"""
    Searches for a JSON object within the string and returns the loaded JSON if found, otherwise returns None.
    """"""
    # Regex to find JSON objects (greedy, matches first { to last })
    match = re.search(r""\{.*\}"", s, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None","Searches for a JSON object within the string and returns the loaded JSON if found, otherwise returns None.","Searches for a JSON object within the string and returns the loaded JSON if found, otherwise returns None.","def extract_json_from_string(s: str) -> Optional[Any]:
    
    # Regex to find JSON objects (greedy, matches first { to last })
    match = re.search(r""\{.*\}"", s, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None","Searches for a JSON object within the string and returns the loaded JSON if found, otherwise returns None.","def extract_json_from_string ( s : str ) -> Optional [ Any ] : # Regex to find JSON objects (greedy, matches first { to last }) match = re . search ( r""\{.*\}"" , s , re . DOTALL ) if match : json_str = match . group ( 0 ) try : return json . loads ( json_str ) except json . JSONDecodeError : return None return None","Searches for a JSON object within the string and returns the loaded JSON if found, otherwise returns None."
/ag2/autogen/oai/openai_utils.py,create_gpt_vector_store,"def create_gpt_vector_store(client: ""OpenAI"", name: str, fild_ids: list[str]) -> Any:
    """"""Create a openai vector store for gpt assistant""""""
    try:
        vector_store = client.vector_stores.create(name=name)
    except Exception as e:
        raise AttributeError(f""Failed to create vector store, please install the latest OpenAI python package: {e}"")

    # poll the status of the file batch for completion.
    batch = client.vector_stores.file_batches.create_and_poll(vector_store_id=vector_store.id, file_ids=fild_ids)

    if batch.status == ""in_progress"":
        time.sleep(1)
        logging.debug(f""file batch status: {batch.file_counts}"")
        batch = client.vector_stores.file_batches.poll(vector_store_id=vector_store.id, batch_id=batch.id)

    if batch.status == ""completed"":
        return vector_store

    raise ValueError(f""Failed to upload files to vector store {vector_store.id}:{batch.status}"")","def create_gpt_vector_store(client: ""OpenAI"", name: str, fild_ids: list[str]) -> Any:
    """"""Create a openai vector store for gpt assistant""""""
    try:
        vector_store = client.vector_stores.create(name=name)
    except Exception as e:
        raise AttributeError(f""Failed to create vector store, please install the latest OpenAI python package: {e}"")

    # poll the status of the file batch for completion.
    batch = client.vector_stores.file_batches.create_and_poll(vector_store_id=vector_store.id, file_ids=fild_ids)

    if batch.status == ""in_progress"":
        time.sleep(1)
        logging.debug(f""file batch status: {batch.file_counts}"")
        batch = client.vector_stores.file_batches.poll(vector_store_id=vector_store.id, batch_id=batch.id)

    if batch.status == ""completed"":
        return vector_store

    raise ValueError(f""Failed to upload files to vector store {vector_store.id}:{batch.status}"")",Create a openai vector store for gpt assistant,Create a openai vector store for gpt assistant,"def create_gpt_vector_store(client: ""OpenAI"", name: str, fild_ids: list[str]) -> Any:
    
    try:
        vector_store = client.vector_stores.create(name=name)
    except Exception as e:
        raise AttributeError(f""Failed to create vector store, please install the latest OpenAI python package: {e}"")

    # poll the status of the file batch for completion.
    batch = client.vector_stores.file_batches.create_and_poll(vector_store_id=vector_store.id, file_ids=fild_ids)

    if batch.status == ""in_progress"":
        time.sleep(1)
        logging.debug(f""file batch status: {batch.file_counts}"")
        batch = client.vector_stores.file_batches.poll(vector_store_id=vector_store.id, batch_id=batch.id)

    if batch.status == ""completed"":
        return vector_store

    raise ValueError(f""Failed to upload files to vector store {vector_store.id}:{batch.status}"")",Create a openai vector store for gpt assistant,"def create_gpt_vector_store ( client : ""OpenAI"" , name : str , fild_ids : list [ str ] ) -> Any : try : vector_store = client . vector_stores . create ( name = name ) except Exception as e : raise AttributeError ( f""Failed to create vector store, please install the latest OpenAI python package: {e}"" ) # poll the status of the file batch for completion. batch = client . vector_stores . file_batches . create_and_poll ( vector_store_id = vector_store . id , file_ids = fild_ids ) if batch . status == ""in_progress"" : time . sleep ( 1 ) logging . debug ( f""file batch status: {batch.file_counts}"" ) batch = client . vector_stores . file_batches . poll ( vector_store_id = vector_store . id , batch_id = batch . id ) if batch . status == ""completed"" : return vector_store raise ValueError ( f""Failed to upload files to vector store {vector_store.id}:{batch.status}"" )",Create a openai vector store for gpt assistant
/potpie/app/modules/intelligence/provider/llm_config.py,get_config_for_model,"def get_config_for_model(model_string: str) -> Dict[str, Any]:
    """"""Get configuration for a specific model, with fallback to defaults.""""""
    if model_string in MODEL_CONFIG_MAP:
        return MODEL_CONFIG_MAP[model_string]
    # If model not found, use default configuration based on provider
    provider, _ = parse_model_string(model_string)
    return {
        ""provider"": provider,
        ""default_params"": {""temperature"": 0.3},
    }","def get_config_for_model(model_string: str) -> Dict[str, Any]:
    """"""Get configuration for a specific model, with fallback to defaults.""""""
    if model_string in MODEL_CONFIG_MAP:
        return MODEL_CONFIG_MAP[model_string]
    # If model not found, use default configuration based on provider
    provider, _ = parse_model_string(model_string)
    return {
        ""provider"": provider,
        ""default_params"": {""temperature"": 0.3},
    }","Get configuration for a specific model, with fallback to defaults.","Get configuration for a specific model, with fallback to defaults.","def get_config_for_model(model_string: str) -> Dict[str, Any]:
    
    if model_string in MODEL_CONFIG_MAP:
        return MODEL_CONFIG_MAP[model_string]
    # If model not found, use default configuration based on provider
    provider, _ = parse_model_string(model_string)
    return {
        ""provider"": provider,
        ""default_params"": {""temperature"": 0.3},
    }","Get configuration for a specific model, with fallback to defaults.","def get_config_for_model ( model_string : str ) -> Dict [ str , Any ] : if model_string in MODEL_CONFIG_MAP : return MODEL_CONFIG_MAP [ model_string ] # If model not found, use default configuration based on provider provider , _ = parse_model_string ( model_string ) return { ""provider"" : provider , ""default_params"" : { ""temperature"" : 0.3 } , }","Get configuration for a specific model, with fallback to defaults."
/ag2/autogen/agentchat/contrib/capabilities/generate_images.py,_validate_resolution_format,"def _validate_resolution_format(resolution: str):
    """"""Checks if a string is in a valid resolution format (e.g., ""1024x768"").""""""
    pattern = r""^\d+x\d+$""  # Matches a pattern of digits, ""x"", and digits
    matched_resolution = re.match(pattern, resolution)
    if matched_resolution is None:
        raise ValueError(f""Invalid resolution format: {resolution}"")","def _validate_resolution_format(resolution: str):
    """"""Checks if a string is in a valid resolution format (e.g., ""1024x768"").""""""
    pattern = r""^\d+x\d+$""  # Matches a pattern of digits, ""x"", and digits
    matched_resolution = re.match(pattern, resolution)
    if matched_resolution is None:
        raise ValueError(f""Invalid resolution format: {resolution}"")","Checks if a string is in a valid resolution format (e.g., ""1024x768"").","Checks if a string is in a valid resolution format (e.g., ""1024x768"").","def _validate_resolution_format(resolution: str):
    
    pattern = r""^\d+x\d+$""  # Matches a pattern of digits, ""x"", and digits
    matched_resolution = re.match(pattern, resolution)
    if matched_resolution is None:
        raise ValueError(f""Invalid resolution format: {resolution}"")","Checks if a string is in a valid resolution format (e.g., ""1024x768"").","def _validate_resolution_format ( resolution : str ) : pattern = r""^\d+x\d+$"" # Matches a pattern of digits, ""x"", and digits matched_resolution = re . match ( pattern , resolution ) if matched_resolution is None : raise ValueError ( f""Invalid resolution format: {resolution}"" )","Checks if a string is in a valid resolution format (e.g., ""1024x768"")."
/fastmcp/tests/server/test_tool_exclude_args.py,create_item,"def create_item(
        name: str, value: int, state: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        """"""Create a new item.""""""
        if state:
            # state was read
            pass
        return {""name"": name, ""value"": value}","def create_item(
        name: str, value: int, state: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        """"""Create a new item.""""""
        if state:
            # state was read
            pass
        return {""name"": name, ""value"": value}",Create a new item.,Create a new item.,"def create_item(
        name: str, value: int, state: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        
        if state:
            # state was read
            pass
        return {""name"": name, ""value"": value}",Create a new item.,"def create_item ( name : str , value : int , state : dict [ str , Any ] | None = None ) -> dict [ str , Any ] : if state : # state was read pass return { ""name"" : name , ""value"" : value }",Create a new item.
/Sana/diffusion/model/timestep_sampler.py,create_named_schedule_sampler,"def create_named_schedule_sampler(name, diffusion):
    """"""
    Create a ScheduleSampler from a library of pre-defined samplers.
    :param name: the name of the sampler.
    :param diffusion: the diffusion object to sample for.
    """"""
    if name == ""uniform"":
        return UniformSampler(diffusion)
    elif name == ""loss-second-moment"":
        return LossSecondMomentResampler(diffusion)
    else:
        raise NotImplementedError(f""unknown schedule sampler: {name}"")","def create_named_schedule_sampler(name, diffusion):
    """"""
    Create a ScheduleSampler from a library of pre-defined samplers.
    :param name: the name of the sampler.
    :param diffusion: the diffusion object to sample for.
    """"""
    if name == ""uniform"":
        return UniformSampler(diffusion)
    elif name == ""loss-second-moment"":
        return LossSecondMomentResampler(diffusion)
    else:
        raise NotImplementedError(f""unknown schedule sampler: {name}"")","Create a ScheduleSampler from a library of pre-defined samplers.
:param name: the name of the sampler.
:param diffusion: the diffusion object to sample for.",Create a ScheduleSampler from a library of pre-defined samplers.,"def create_named_schedule_sampler(name, diffusion):
    
    if name == ""uniform"":
        return UniformSampler(diffusion)
    elif name == ""loss-second-moment"":
        return LossSecondMomentResampler(diffusion)
    else:
        raise NotImplementedError(f""unknown schedule sampler: {name}"")",Create a ScheduleSampler from a library of pre-defined samplers.,"def create_named_schedule_sampler ( name , diffusion ) : if name == ""uniform"" : return UniformSampler ( diffusion ) elif name == ""loss-second-moment"" : return LossSecondMomentResampler ( diffusion ) else : raise NotImplementedError ( f""unknown schedule sampler: {name}"" )",Create a ScheduleSampler from a library of pre-defined samplers.
/browser-use/examples/browser/window_sizing.py,validate_window_size,"def validate_window_size(configured: dict[str, Any], actual: dict[str, Any]) -> None:
	""""""Compare configured window size with actual size and report differences""""""
	# Allow for small differences due to browser chrome, scrollbars, etc.
	width_diff = abs(configured['width'] - actual['width'])
	height_diff = abs(configured['height'] - actual['height'])

	# Tolerance of 5% or 20px, whichever is greater
	width_tolerance = max(configured['width'] * 0.05, 20)
	height_tolerance = max(configured['height'] * 0.05, 20)

	if width_diff > width_tolerance or height_diff > height_tolerance:
		print(f'⚠️  WARNING: Significant difference between expected and actual page size! ±{width_diff}x{height_diff}px')
		raise Exception('Window size validation failed')
	else:
		print('✅ Window size validation passed: actual size matches configured size within tolerance')","def validate_window_size(configured: dict[str, Any], actual: dict[str, Any]) -> None:
	""""""Compare configured window size with actual size and report differences""""""
	# Allow for small differences due to browser chrome, scrollbars, etc.
	width_diff = abs(configured['width'] - actual['width'])
	height_diff = abs(configured['height'] - actual['height'])

	# Tolerance of 5% or 20px, whichever is greater
	width_tolerance = max(configured['width'] * 0.05, 20)
	height_tolerance = max(configured['height'] * 0.05, 20)

	if width_diff > width_tolerance or height_diff > height_tolerance:
		print(f'⚠️  WARNING: Significant difference between expected and actual page size! ±{width_diff}x{height_diff}px')
		raise Exception('Window size validation failed')
	else:
		print('✅ Window size validation passed: actual size matches configured size within tolerance')",Compare configured window size with actual size and report differences,Compare configured window size with actual size and report differences,"def validate_window_size(configured: dict[str, Any], actual: dict[str, Any]) -> None:
	
	# Allow for small differences due to browser chrome, scrollbars, etc.
	width_diff = abs(configured['width'] - actual['width'])
	height_diff = abs(configured['height'] - actual['height'])

	# Tolerance of 5% or 20px, whichever is greater
	width_tolerance = max(configured['width'] * 0.05, 20)
	height_tolerance = max(configured['height'] * 0.05, 20)

	if width_diff > width_tolerance or height_diff > height_tolerance:
		print(f'⚠️  WARNING: Significant difference between expected and actual page size! ±{width_diff}x{height_diff}px')
		raise Exception('Window size validation failed')
	else:
		print('✅ Window size validation passed: actual size matches configured size within tolerance')",Compare configured window size with actual size and report differences,"def validate_window_size ( configured : dict [ str , Any ] , actual : dict [ str , Any ] ) -> None : # Allow for small differences due to browser chrome, scrollbars, etc. width_diff = abs ( configured [ 'width' ] - actual [ 'width' ] ) height_diff = abs ( configured [ 'height' ] - actual [ 'height' ] ) # Tolerance of 5% or 20px, whichever is greater width_tolerance = max ( configured [ 'width' ] * 0.05 , 20 ) height_tolerance = max ( configured [ 'height' ] * 0.05 , 20 ) if width_diff > width_tolerance or height_diff > height_tolerance : print ( f'⚠️  WARNING: Significant difference between expected and actual page size! ±{width_diff}x{height_diff}px' ) raise Exception ( 'Window size validation failed' ) else : print ( '✅ Window size validation passed: actual size matches configured size within tolerance' )",Compare configured window size with actual size and report differences
/ACE-Step/acestep/pipeline_ace_step.py,cleanup_memory,"def cleanup_memory(self):
        """"""Clean up GPU and CPU memory to prevent VRAM overflow during multiple generations.""""""
        # Clear CUDA cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

            # Log memory usage if in verbose mode
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            logger.info(f""GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved"")

        # Collect Python garbage
        import gc
        gc.collect()","def cleanup_memory(self):
        """"""Clean up GPU and CPU memory to prevent VRAM overflow during multiple generations.""""""
        # Clear CUDA cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

            # Log memory usage if in verbose mode
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            logger.info(f""GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved"")

        # Collect Python garbage
        import gc
        gc.collect()",Clean up GPU and CPU memory to prevent VRAM overflow during multiple generations.,Clean up GPU and CPU memory to prevent VRAM overflow during multiple generations.,"def cleanup_memory(self):
        
        # Clear CUDA cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

            # Log memory usage if in verbose mode
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            logger.info(f""GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved"")

        # Collect Python garbage
        import gc
        gc.collect()",Clean up GPU and CPU memory to prevent VRAM overflow during multiple generations.,"def cleanup_memory ( self ) : # Clear CUDA cache if torch . cuda . is_available ( ) : torch . cuda . empty_cache ( ) # Log memory usage if in verbose mode allocated = torch . cuda . memory_allocated ( ) / ( 1024 ** 3 ) reserved = torch . cuda . memory_reserved ( ) / ( 1024 ** 3 ) logger . info ( f""GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved"" ) # Collect Python garbage import gc gc . collect ( )",Clean up GPU and CPU memory to prevent VRAM overflow during multiple generations.
/verl/verl/utils/fs.py,copy_to_shm,"def copy_to_shm(src:str):
    """"""
        Load the model into   /dev/shm   to make the process of loading the model multiple times more efficient.
    """"""
    shm_model_root = '/dev/shm/verl-cache/'
    src_abs = os.path.abspath(os.path.normpath(src))
    dest = os.path.join(shm_model_root, hashlib.md5(src_abs.encode('utf-8')).hexdigest())
    os.makedirs(dest, exist_ok=True)
    dest = os.path.join(dest, os.path.basename(src_abs))
    if os.path.exists(dest) and verify_copy(src, dest):
        # inform user and depends on him
        print(f""[WARNING]: The memory model path {dest} already exists. If it is not you want, please clear it and restart the task."")
    else:
        if os.path.isdir(src):
            shutil.copytree(src, dest, symlinks=False, dirs_exist_ok=True)
        else:
            shutil.copy2(src, dest)
    return dest","def copy_to_shm(src:str):
    """"""
        Load the model into   /dev/shm   to make the process of loading the model multiple times more efficient.
    """"""
    shm_model_root = '/dev/shm/verl-cache/'
    src_abs = os.path.abspath(os.path.normpath(src))
    dest = os.path.join(shm_model_root, hashlib.md5(src_abs.encode('utf-8')).hexdigest())
    os.makedirs(dest, exist_ok=True)
    dest = os.path.join(dest, os.path.basename(src_abs))
    if os.path.exists(dest) and verify_copy(src, dest):
        # inform user and depends on him
        print(f""[WARNING]: The memory model path {dest} already exists. If it is not you want, please clear it and restart the task."")
    else:
        if os.path.isdir(src):
            shutil.copytree(src, dest, symlinks=False, dirs_exist_ok=True)
        else:
            shutil.copy2(src, dest)
    return dest",Load the model into   /dev/shm   to make the process of loading the model multiple times more efficient.,Load the model into   /dev/shm   to make the process of loading the model multiple times more efficient.,"def copy_to_shm(src:str):
    
    shm_model_root = '/dev/shm/verl-cache/'
    src_abs = os.path.abspath(os.path.normpath(src))
    dest = os.path.join(shm_model_root, hashlib.md5(src_abs.encode('utf-8')).hexdigest())
    os.makedirs(dest, exist_ok=True)
    dest = os.path.join(dest, os.path.basename(src_abs))
    if os.path.exists(dest) and verify_copy(src, dest):
        # inform user and depends on him
        print(f""[WARNING]: The memory model path {dest} already exists. If it is not you want, please clear it and restart the task."")
    else:
        if os.path.isdir(src):
            shutil.copytree(src, dest, symlinks=False, dirs_exist_ok=True)
        else:
            shutil.copy2(src, dest)
    return dest",Load the model into   /dev/shm   to make the process of loading the model multiple times more efficient.,"def copy_to_shm ( src : str ) : shm_model_root = '/dev/shm/verl-cache/' src_abs = os . path . abspath ( os . path . normpath ( src ) ) dest = os . path . join ( shm_model_root , hashlib . md5 ( src_abs . encode ( 'utf-8' ) ) . hexdigest ( ) ) os . makedirs ( dest , exist_ok = True ) dest = os . path . join ( dest , os . path . basename ( src_abs ) ) if os . path . exists ( dest ) and verify_copy ( src , dest ) : # inform user and depends on him print ( f""[WARNING]: The memory model path {dest} already exists. If it is not you want, please clear it and restart the task."" ) else : if os . path . isdir ( src ) : shutil . copytree ( src , dest , symlinks = False , dirs_exist_ok = True ) else : shutil . copy2 ( src , dest ) return dest",Load the model into /dev/shm to make the process of loading the model multiple times more efficient.
/ai-hedge-fund/src/main.py,parse_hedge_fund_response,"def parse_hedge_fund_response(response):
    """"""Parses a JSON string and returns a dictionary.""""""
    try:
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f""JSON decoding error: {e}\nResponse: {repr(response)}"")
        return None
    except TypeError as e:
        print(f""Invalid response type (expected string, got {type(response).__name__}): {e}"")
        return None
    except Exception as e:
        print(f""Unexpected error while parsing response: {e}\nResponse: {repr(response)}"")
        return None","def parse_hedge_fund_response(response):
    """"""Parses a JSON string and returns a dictionary.""""""
    try:
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f""JSON decoding error: {e}\nResponse: {repr(response)}"")
        return None
    except TypeError as e:
        print(f""Invalid response type (expected string, got {type(response).__name__}): {e}"")
        return None
    except Exception as e:
        print(f""Unexpected error while parsing response: {e}\nResponse: {repr(response)}"")
        return None",Parses a JSON string and returns a dictionary.,Parses a JSON string and returns a dictionary.,"def parse_hedge_fund_response(response):
    
    try:
        return json.loads(response)
    except json.JSONDecodeError as e:
        print(f""JSON decoding error: {e}\nResponse: {repr(response)}"")
        return None
    except TypeError as e:
        print(f""Invalid response type (expected string, got {type(response).__name__}): {e}"")
        return None
    except Exception as e:
        print(f""Unexpected error while parsing response: {e}\nResponse: {repr(response)}"")
        return None",Parses a JSON string and returns a dictionary.,"def parse_hedge_fund_response ( response ) : try : return json . loads ( response ) except json . JSONDecodeError as e : print ( f""JSON decoding error: {e}\nResponse: {repr(response)}"" ) return None except TypeError as e : print ( f""Invalid response type (expected string, got {type(response).__name__}): {e}"" ) return None except Exception as e : print ( f""Unexpected error while parsing response: {e}\nResponse: {repr(response)}"" ) return None",Parses a JSON string and returns a dictionary.
/nexa-sdk/nexa/gguf/outetts/wav_tokenizer/decoder/pretrained.py,from_pretrained,"def from_pretrained(self, repo_id: str) -> ""Vocos"":
        """"""
        Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.
        """"""
        config_path = hf_hub_download(repo_id=repo_id, filename=""config.yaml"")
        model_path = hf_hub_download(repo_id=repo_id, filename=""pytorch_model.bin"")
        model = self.from_hparams(config_path)
        state_dict = torch.load(model_path, map_location=""cpu"")
        if isinstance(model.feature_extractor, EncodecFeatures):
            encodec_parameters = {
                ""feature_extractor.encodec."" + key: value
                for key, value in model.feature_extractor.encodec.state_dict().items()
            }
            state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model","def from_pretrained(self, repo_id: str) -> ""Vocos"":
        """"""
        Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.
        """"""
        config_path = hf_hub_download(repo_id=repo_id, filename=""config.yaml"")
        model_path = hf_hub_download(repo_id=repo_id, filename=""pytorch_model.bin"")
        model = self.from_hparams(config_path)
        state_dict = torch.load(model_path, map_location=""cpu"")
        if isinstance(model.feature_extractor, EncodecFeatures):
            encodec_parameters = {
                ""feature_extractor.encodec."" + key: value
                for key, value in model.feature_extractor.encodec.state_dict().items()
            }
            state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model",Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.,Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.,"def from_pretrained(self, repo_id: str) -> ""Vocos"":
        
        config_path = hf_hub_download(repo_id=repo_id, filename=""config.yaml"")
        model_path = hf_hub_download(repo_id=repo_id, filename=""pytorch_model.bin"")
        model = self.from_hparams(config_path)
        state_dict = torch.load(model_path, map_location=""cpu"")
        if isinstance(model.feature_extractor, EncodecFeatures):
            encodec_parameters = {
                ""feature_extractor.encodec."" + key: value
                for key, value in model.feature_extractor.encodec.state_dict().items()
            }
            state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model",Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.,"def from_pretrained ( self , repo_id : str ) -> ""Vocos"" : config_path = hf_hub_download ( repo_id = repo_id , filename = ""config.yaml"" ) model_path = hf_hub_download ( repo_id = repo_id , filename = ""pytorch_model.bin"" ) model = self . from_hparams ( config_path ) state_dict = torch . load ( model_path , map_location = ""cpu"" ) if isinstance ( model . feature_extractor , EncodecFeatures ) : encodec_parameters = { ""feature_extractor.encodec."" + key : value for key , value in model . feature_extractor . encodec . state_dict ( ) . items ( ) } state_dict . update ( encodec_parameters ) model . load_state_dict ( state_dict ) model . eval ( ) return model",Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.
/alphafold3/src/alphafold3/model/mmcif_metadata.py,add_legal_comment,"def add_legal_comment(cif: str) -> str:
  """"""Adds legal comment at the top of the mmCIF.""""""
  # fmt: off
  # pylint: disable=line-too-long
  comment = (
      '# By using this file you agree to the legally binding terms of use found at\n'
      f'# {_LICENSE_URL}.\n'
      '# To request access to the AlphaFold 3 model parameters, follow the process set\n'
      '# out at https://github.com/google-deepmind/alphafold3. You may only use these if\n'
      '# received directly from Google. Use is subject to terms of use available at\n'
      '# https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md.'
  )
  # pylint: enable=line-too-long
  # fmt: on
  return f'{comment}\n{cif}'","def add_legal_comment(cif: str) -> str:
  """"""Adds legal comment at the top of the mmCIF.""""""
  # fmt: off
  # pylint: disable=line-too-long
  comment = (
      '# By using this file you agree to the legally binding terms of use found at\n'
      f'# {_LICENSE_URL}.\n'
      '# To request access to the AlphaFold 3 model parameters, follow the process set\n'
      '# received directly from Google. Use is subject to terms of use available at\n'
  )
  # pylint: enable=line-too-long
  # fmt: on
  return f'{comment}\n{cif}'",Adds legal comment at the top of the mmCIF.,Adds legal comment at the top of the mmCIF.,"def add_legal_comment(cif: str) -> str:
  
  # fmt: off
  # pylint: disable=line-too-long
  comment = (
      '# By using this file you agree to the legally binding terms of use found at\n'
      f'# {_LICENSE_URL}.\n'
      '# To request access to the AlphaFold 3 model parameters, follow the process set\n'
      '# received directly from Google. Use is subject to terms of use available at\n'
  )
  # pylint: enable=line-too-long
  # fmt: on
  return f'{comment}\n{cif}'",Adds legal comment at the top of the mmCIF.,"def add_legal_comment ( cif : str ) -> str : # fmt: off # pylint: disable=line-too-long comment = ( '# By using this file you agree to the legally binding terms of use found at\n' f'# {_LICENSE_URL}.\n' '# To request access to the AlphaFold 3 model parameters, follow the process set\n' '# received directly from Google. Use is subject to terms of use available at\n' ) # pylint: enable=line-too-long # fmt: on return f'{comment}\n{cif}'",Adds legal comment at the top of the mmCIF.
/airweave/backend/airweave/platform/sources/outlook_calendar.py,_parse_datetime_field,"def _parse_datetime_field(self, dt_obj: Optional[Dict]) -> Optional[datetime]:
        """"""Parse datetime from Microsoft Graph API format.""""""
        if not dt_obj or not dt_obj.get(""dateTime""):
            return None
        try:
            dt_str = dt_obj[""dateTime""]
            if ""T"" in dt_str:
                # Handle timezone info
                if dt_str.endswith(""Z""):
                    dt_str = dt_str.replace(""Z"", ""+00:00"")
                elif ""+"" not in dt_str and ""-"" not in dt_str[-6:]:
                    # If no timezone info, assume UTC
                    dt_str += ""+00:00""
                return datetime.fromisoformat(dt_str)
        except (ValueError, TypeError) as e:
            logger.warning(f""Error parsing datetime: {str(e)}"")
        return None","def _parse_datetime_field(self, dt_obj: Optional[Dict]) -> Optional[datetime]:
        """"""Parse datetime from Microsoft Graph API format.""""""
        if not dt_obj or not dt_obj.get(""dateTime""):
            return None
        try:
            dt_str = dt_obj[""dateTime""]
            if ""T"" in dt_str:
                # Handle timezone info
                if dt_str.endswith(""Z""):
                    dt_str = dt_str.replace(""Z"", ""+00:00"")
                elif ""+"" not in dt_str and ""-"" not in dt_str[-6:]:
                    # If no timezone info, assume UTC
                    dt_str += ""+00:00""
                return datetime.fromisoformat(dt_str)
        except (ValueError, TypeError) as e:
            logger.warning(f""Error parsing datetime: {str(e)}"")
        return None",Parse datetime from Microsoft Graph API format.,Parse datetime from Microsoft Graph API format.,"def _parse_datetime_field(self, dt_obj: Optional[Dict]) -> Optional[datetime]:
        
        if not dt_obj or not dt_obj.get(""dateTime""):
            return None
        try:
            dt_str = dt_obj[""dateTime""]
            if ""T"" in dt_str:
                # Handle timezone info
                if dt_str.endswith(""Z""):
                    dt_str = dt_str.replace(""Z"", ""+00:00"")
                elif ""+"" not in dt_str and ""-"" not in dt_str[-6:]:
                    # If no timezone info, assume UTC
                    dt_str += ""+00:00""
                return datetime.fromisoformat(dt_str)
        except (ValueError, TypeError) as e:
            logger.warning(f""Error parsing datetime: {str(e)}"")
        return None",Parse datetime from Microsoft Graph API format.,"def _parse_datetime_field ( self , dt_obj : Optional [ Dict ] ) -> Optional [ datetime ] : if not dt_obj or not dt_obj . get ( ""dateTime"" ) : return None try : dt_str = dt_obj [ ""dateTime"" ] if ""T"" in dt_str : # Handle timezone info if dt_str . endswith ( ""Z"" ) : dt_str = dt_str . replace ( ""Z"" , ""+00:00"" ) elif ""+"" not in dt_str and ""-"" not in dt_str [ - 6 : ] : # If no timezone info, assume UTC dt_str += ""+00:00"" return datetime . fromisoformat ( dt_str ) except ( ValueError , TypeError ) as e : logger . warning ( f""Error parsing datetime: {str(e)}"" ) return None",Parse datetime from Microsoft Graph API format.
/NLWeb/code/retrieval/opensearch_client.py,_get_auth_headers,"def _get_auth_headers(self) -> Dict[str, str]:
        """"""
        Get authentication headers for OpenSearch requests.
        Supports both basic auth (username:password) and API key authentication.
        """"""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json""
        }
        
        if ':' in self.credentials:
            # Basic authentication (username:password)
            encoded_credentials = base64.b64encode(self.credentials.encode()).decode()
            headers[""Authorization""] = f""Basic {encoded_credentials}""
        else:
            # API key authentication
            headers[""Authorization""] = f""Bearer {self.credentials}""
        
        return headers","def _get_auth_headers(self) -> Dict[str, str]:
        """"""
        Get authentication headers for OpenSearch requests.
        Supports both basic auth (username:password) and API key authentication.
        """"""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json""
        }
        
        if ':' in self.credentials:
            # Basic authentication (username:password)
            encoded_credentials = base64.b64encode(self.credentials.encode()).decode()
            headers[""Authorization""] = f""Basic {encoded_credentials}""
        else:
            # API key authentication
            headers[""Authorization""] = f""Bearer {self.credentials}""
        
        return headers","Get authentication headers for OpenSearch requests.
Supports both basic auth (username:password) and API key authentication.",Get authentication headers for OpenSearch requests.,"def _get_auth_headers(self) -> Dict[str, str]:
        
        headers = {
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json""
        }
        
        if ':' in self.credentials:
            # Basic authentication (username:password)
            encoded_credentials = base64.b64encode(self.credentials.encode()).decode()
            headers[""Authorization""] = f""Basic {encoded_credentials}""
        else:
            # API key authentication
            headers[""Authorization""] = f""Bearer {self.credentials}""
        
        return headers",Get authentication headers for OpenSearch requests.,"def _get_auth_headers ( self ) -> Dict [ str , str ] : headers = { ""Content-Type"" : ""application/json"" , ""Accept"" : ""application/json"" } if ':' in self . credentials : # Basic authentication (username:password) encoded_credentials = base64 . b64encode ( self . credentials . encode ( ) ) . decode ( ) headers [ ""Authorization"" ] = f""Basic {encoded_credentials}"" else : # API key authentication headers [ ""Authorization"" ] = f""Bearer {self.credentials}"" return headers",Get authentication headers for OpenSearch requests.
/Kokoro-FastAPI/api/src/main.py,setup_logger,"def setup_logger():
    """"""Configure loguru logger with custom formatting""""""
    config = {
        ""handlers"": [
            {
                ""sink"": sys.stdout,
                ""format"": ""<fg #2E8B57>{time:hh:mm:ss A}</fg #2E8B57> | ""
                ""{level: <8} | ""
                ""<fg #4169E1>{module}:{line}</fg #4169E1> | ""
                ""{message}"",
                ""colorize"": True,
                ""level"": ""DEBUG"",
            },
        ],
    }
    logger.remove()
    logger.configure(**config)
    logger.level(""ERROR"", color=""<red>"")","def setup_logger():
    """"""Configure loguru logger with custom formatting""""""
    config = {
        ""handlers"": [
            {
                ""sink"": sys.stdout,
                ""format"": ""<fg #2E8B57>{time:hh:mm:ss A}</fg #2E8B57> | ""
                ""{level: <8} | ""
                ""<fg #4169E1>{module}:{line}</fg #4169E1> | ""
                ""{message}"",
                ""colorize"": True,
                ""level"": ""DEBUG"",
            },
        ],
    }
    logger.remove()
    logger.configure(**config)
    logger.level(""ERROR"", color=""<red>"")",Configure loguru logger with custom formatting,Configure loguru logger with custom formatting,"def setup_logger():
    
    config = {
        ""handlers"": [
            {
                ""sink"": sys.stdout,
                ""format"": ""<fg #2E8B57>{time:hh:mm:ss A}</fg #2E8B57> | ""
                ""{level: <8} | ""
                ""<fg #4169E1>{module}:{line}</fg #4169E1> | ""
                ""{message}"",
                ""colorize"": True,
                ""level"": ""DEBUG"",
            },
        ],
    }
    logger.remove()
    logger.configure(**config)
    logger.level(""ERROR"", color=""<red>"")",Configure loguru logger with custom formatting,"def setup_logger ( ) : config = { ""handlers"" : [ { ""sink"" : sys . stdout , ""format"" : ""<fg #2E8B57>{time:hh:mm:ss A}</fg #2E8B57> | "" ""{level: <8} | "" ""<fg #4169E1>{module}:{line}</fg #4169E1> | "" ""{message}"" , ""colorize"" : True , ""level"" : ""DEBUG"" , } , ] , } logger . remove ( ) logger . configure ( ** config ) logger . level ( ""ERROR"" , color = ""<red>"" )",Configure loguru logger with custom formatting
/nv-ingest/client/src/nv_ingest_client/primitives/tasks/infographic_extraction.py,to_dict,"def to_dict(self) -> Dict:
        """"""
        Convert to a dict for submission to redis
        """"""

        task_properties = {
            ""params"": {},
        }

        return {""type"": ""infographic_data_extract"", ""task_properties"": task_properties}","def to_dict(self) -> Dict:
        """"""
        Convert to a dict for submission to redis
        """"""

        task_properties = {
            ""params"": {},
        }

        return {""type"": ""infographic_data_extract"", ""task_properties"": task_properties}",Convert to a dict for submission to redis,Convert to a dict for submission to redis,"def to_dict(self) -> Dict:
        

        task_properties = {
            ""params"": {},
        }

        return {""type"": ""infographic_data_extract"", ""task_properties"": task_properties}",Convert to a dict for submission to redis,"def to_dict ( self ) -> Dict : task_properties = { ""params"" : { } , } return { ""type"" : ""infographic_data_extract"" , ""task_properties"" : task_properties }",Convert to a dict for submission to redis
/mcp/src/cdk-mcp-server/awslabs/cdk_mcp_server/data/lambda_powertools_loader.py,get_topic_map,"def get_topic_map() -> Dict[str, str]:
    """"""Get a dictionary mapping topic names to their descriptions.""""""
    return {
        'index': 'Overview and table of contents',
        'logging': 'Structured logging implementation',
        'tracing': 'Tracing implementation',
        'metrics': 'Metrics implementation',
        'cdk': 'CDK integration patterns',
        'dependencies': 'Dependencies management',
        'insights': 'Lambda Insights integration',
        'bedrock': 'Bedrock Agent integration',
    }","def get_topic_map() -> Dict[str, str]:
    """"""Get a dictionary mapping topic names to their descriptions.""""""
    return {
        'index': 'Overview and table of contents',
        'logging': 'Structured logging implementation',
        'tracing': 'Tracing implementation',
        'metrics': 'Metrics implementation',
        'cdk': 'CDK integration patterns',
        'dependencies': 'Dependencies management',
        'insights': 'Lambda Insights integration',
        'bedrock': 'Bedrock Agent integration',
    }",Get a dictionary mapping topic names to their descriptions.,Get a dictionary mapping topic names to their descriptions.,"def get_topic_map() -> Dict[str, str]:
    
    return {
        'index': 'Overview and table of contents',
        'logging': 'Structured logging implementation',
        'tracing': 'Tracing implementation',
        'metrics': 'Metrics implementation',
        'cdk': 'CDK integration patterns',
        'dependencies': 'Dependencies management',
        'insights': 'Lambda Insights integration',
        'bedrock': 'Bedrock Agent integration',
    }",Get a dictionary mapping topic names to their descriptions.,"def get_topic_map ( ) -> Dict [ str , str ] : return { 'index' : 'Overview and table of contents' , 'logging' : 'Structured logging implementation' , 'tracing' : 'Tracing implementation' , 'metrics' : 'Metrics implementation' , 'cdk' : 'CDK integration patterns' , 'dependencies' : 'Dependencies management' , 'insights' : 'Lambda Insights integration' , 'bedrock' : 'Bedrock Agent integration' , }",Get a dictionary mapping topic names to their descriptions.
/Agent-S/gui_agents/s1/aci/windowsagentarena/GroundingAgent.py,hotkey,"def hotkey(self, keys: List):
        """"""Press a hotkey combination
        Args:
            keys:List the keys to press in combination in a list format (e.g. ['ctrl', 'c'])
        """"""
        # add quotes around the keys
        keys = [f""'{key}'"" for key in keys]
        return f""import pyautogui; pyautogui.hotkey({', '.join(keys)})""","def hotkey(self, keys: List):
        """"""Press a hotkey combination
        Args:
            keys:List the keys to press in combination in a list format (e.g. ['ctrl', 'c'])
        """"""
        # add quotes around the keys
        keys = [f""'{key}'"" for key in keys]
        return f""import pyautogui; pyautogui.hotkey({', '.join(keys)})""","Press a hotkey combination
Args:
    keys:List the keys to press in combination in a list format (e.g. ['ctrl', 'c'])",Press a hotkey combination,"def hotkey(self, keys: List):
        
        # add quotes around the keys
        keys = [f""'{key}'"" for key in keys]
        return f""import pyautogui; pyautogui.hotkey({', '.join(keys)})""",Press a hotkey combination,"def hotkey ( self , keys : List ) : # add quotes around the keys keys = [ f""'{key}'"" for key in keys ] return f""import pyautogui; pyautogui.hotkey({', '.join(keys)})""",Press a hotkey combination
/mcp/src/documentdb-mcp-server/awslabs/documentdb_mcp_server/connection_tools.py,close_idle_connections,"def close_idle_connections(cls) -> None:
        """"""Close connections that have been idle for longer than the timeout.""""""
        now = datetime.now()
        idle_threshold = now - timedelta(minutes=cls._idle_timeout)

        idle_connections = [
            conn_id
            for conn_id, info in cls._connections.items()
            if info.last_used < idle_threshold
        ]

        for conn_id in idle_connections:
            logger.info(f'Closing idle DocumentDB connection {conn_id}')
            cls._connections[conn_id].client.close()
            del cls._connections[conn_id]","def close_idle_connections(cls) -> None:
        """"""Close connections that have been idle for longer than the timeout.""""""
        now = datetime.now()
        idle_threshold = now - timedelta(minutes=cls._idle_timeout)

        idle_connections = [
            conn_id
            for conn_id, info in cls._connections.items()
            if info.last_used < idle_threshold
        ]

        for conn_id in idle_connections:
            logger.info(f'Closing idle DocumentDB connection {conn_id}')
            cls._connections[conn_id].client.close()
            del cls._connections[conn_id]",Close connections that have been idle for longer than the timeout.,Close connections that have been idle for longer than the timeout.,"def close_idle_connections(cls) -> None:
        
        now = datetime.now()
        idle_threshold = now - timedelta(minutes=cls._idle_timeout)

        idle_connections = [
            conn_id
            for conn_id, info in cls._connections.items()
            if info.last_used < idle_threshold
        ]

        for conn_id in idle_connections:
            logger.info(f'Closing idle DocumentDB connection {conn_id}')
            cls._connections[conn_id].client.close()
            del cls._connections[conn_id]",Close connections that have been idle for longer than the timeout.,"def close_idle_connections ( cls ) -> None : now = datetime . now ( ) idle_threshold = now - timedelta ( minutes = cls . _idle_timeout ) idle_connections = [ conn_id for conn_id , info in cls . _connections . items ( ) if info . last_used < idle_threshold ] for conn_id in idle_connections : logger . info ( f'Closing idle DocumentDB connection {conn_id}' ) cls . _connections [ conn_id ] . client . close ( ) del cls . _connections [ conn_id ]",Close connections that have been idle for longer than the timeout.
/smolagents/src/smolagents/monitoring.py,build_agent_tree,"def build_agent_tree(parent_tree, agent_obj):
            """"""Recursively builds the agent tree.""""""
            parent_tree.add(create_tools_section(agent_obj.tools))

            if agent_obj.managed_agents:
                agents_branch = parent_tree.add(""🤖 [italic #1E90FF]Managed agents:"")
                for name, managed_agent in agent_obj.managed_agents.items():
                    agent_tree = agents_branch.add(get_agent_headline(managed_agent, name))
                    if managed_agent.__class__.__name__ == ""CodeAgent"":
                        agent_tree.add(
                            f""✅ [italic #1E90FF]Authorized imports:[/italic #1E90FF] {managed_agent.additional_authorized_imports}""
                        )
                    agent_tree.add(f""📝 [italic #1E90FF]Description:[/italic #1E90FF] {managed_agent.description}"")
                    build_agent_tree(agent_tree, managed_agent)","def build_agent_tree(parent_tree, agent_obj):
            """"""Recursively builds the agent tree.""""""
            parent_tree.add(create_tools_section(agent_obj.tools))

            if agent_obj.managed_agents:
                agents_branch = parent_tree.add(""🤖 [italic #1E90FF]Managed agents:"")
                for name, managed_agent in agent_obj.managed_agents.items():
                    agent_tree = agents_branch.add(get_agent_headline(managed_agent, name))
                    if managed_agent.__class__.__name__ == ""CodeAgent"":
                        agent_tree.add(
                            f""✅ [italic #1E90FF]Authorized imports:[/italic #1E90FF] {managed_agent.additional_authorized_imports}""
                        )
                    agent_tree.add(f""📝 [italic #1E90FF]Description:[/italic #1E90FF] {managed_agent.description}"")
                    build_agent_tree(agent_tree, managed_agent)",Recursively builds the agent tree.,Recursively builds the agent tree.,"def build_agent_tree(parent_tree, agent_obj):
            
            parent_tree.add(create_tools_section(agent_obj.tools))

            if agent_obj.managed_agents:
                agents_branch = parent_tree.add(""🤖 [italic #1E90FF]Managed agents:"")
                for name, managed_agent in agent_obj.managed_agents.items():
                    agent_tree = agents_branch.add(get_agent_headline(managed_agent, name))
                    if managed_agent.__class__.__name__ == ""CodeAgent"":
                        agent_tree.add(
                            f""✅ [italic #1E90FF]Authorized imports:[/italic #1E90FF] {managed_agent.additional_authorized_imports}""
                        )
                    agent_tree.add(f""📝 [italic #1E90FF]Description:[/italic #1E90FF] {managed_agent.description}"")
                    build_agent_tree(agent_tree, managed_agent)",Recursively builds the agent tree.,"def build_agent_tree ( parent_tree , agent_obj ) : parent_tree . add ( create_tools_section ( agent_obj . tools ) ) if agent_obj . managed_agents : agents_branch = parent_tree . add ( ""🤖 [italic #1E90FF]Managed agents:"" ) for name , managed_agent in agent_obj . managed_agents . items ( ) : agent_tree = agents_branch . add ( get_agent_headline ( managed_agent , name ) ) if managed_agent . __class__ . __name__ == ""CodeAgent"" : agent_tree . add ( f""✅ [italic #1E90FF]Authorized imports:[/italic #1E90FF] {managed_agent.additional_authorized_imports}"" ) agent_tree . add ( f""📝 [italic #1E90FF]Description:[/italic #1E90FF] {managed_agent.description}"" ) build_agent_tree ( agent_tree , managed_agent )",Recursively builds the agent tree.
/OpenManus-RL/verl/utils/memory_buffer.py,get_weight_buffer_meta_from_module,"def get_weight_buffer_meta_from_module(module: nn.Module) -> Dict[str, Dict]:
    """"""
    Return a dictionary containing name to a shape and dtype.
    """"""
    weight_buffer_meta = {}
    for name, param in sorted(module.named_parameters()):
        weight_buffer_meta[name] = {'shape': param.shape, 'dtype': param.dtype}
    return weight_buffer_meta","def get_weight_buffer_meta_from_module(module: nn.Module) -> Dict[str, Dict]:
    """"""
    Return a dictionary containing name to a shape and dtype.
    """"""
    weight_buffer_meta = {}
    for name, param in sorted(module.named_parameters()):
        weight_buffer_meta[name] = {'shape': param.shape, 'dtype': param.dtype}
    return weight_buffer_meta",Return a dictionary containing name to a shape and dtype.,Return a dictionary containing name to a shape and dtype.,"def get_weight_buffer_meta_from_module(module: nn.Module) -> Dict[str, Dict]:
    
    weight_buffer_meta = {}
    for name, param in sorted(module.named_parameters()):
        weight_buffer_meta[name] = {'shape': param.shape, 'dtype': param.dtype}
    return weight_buffer_meta",Return a dictionary containing name to a shape and dtype.,"def get_weight_buffer_meta_from_module ( module : nn . Module ) -> Dict [ str , Dict ] : weight_buffer_meta = { } for name , param in sorted ( module . named_parameters ( ) ) : weight_buffer_meta [ name ] = { 'shape' : param . shape , 'dtype' : param . dtype } return weight_buffer_meta",Return a dictionary containing name to a shape and dtype.
/web-ui/src/webui/components/deep_research_agent_tab.py,_read_file_safe,"def _read_file_safe(file_path: str) -> Optional[str]:
    """"""Safely read a file, returning None if it doesn't exist or on error.""""""
    if not os.path.exists(file_path):
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f""Error reading file {file_path}: {e}"")
        return None","def _read_file_safe(file_path: str) -> Optional[str]:
    """"""Safely read a file, returning None if it doesn't exist or on error.""""""
    if not os.path.exists(file_path):
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f""Error reading file {file_path}: {e}"")
        return None","Safely read a file, returning None if it doesn't exist or on error.","Safely read a file, returning None if it doesn't exist or on error.","def _read_file_safe(file_path: str) -> Optional[str]:
    
    if not os.path.exists(file_path):
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f""Error reading file {file_path}: {e}"")
        return None","Safely read a file, returning None if it doesn't exist or on error.","def _read_file_safe ( file_path : str ) -> Optional [ str ] : if not os . path . exists ( file_path ) : return None try : with open ( file_path , 'r' , encoding = 'utf-8' ) as f : return f . read ( ) except Exception as e : logger . error ( f""Error reading file {file_path}: {e}"" ) return None","Safely read a file, returning None if it doesn't exist or on error."
/python-sdk/src/mcp/client/session_group.py,__init__,"def __init__(
        self,
        exit_stack: contextlib.AsyncExitStack | None = None,
        component_name_hook: _ComponentNameHook | None = None,
    ) -> None:
        """"""Initializes the MCP client.""""""

        self._tools = {}
        self._resources = {}
        self._prompts = {}

        self._sessions = {}
        self._tool_to_session = {}
        if exit_stack is None:
            self._exit_stack = contextlib.AsyncExitStack()
            self._owns_exit_stack = True
        else:
            self._exit_stack = exit_stack
            self._owns_exit_stack = False
        self._session_exit_stacks = {}
        self._component_name_hook = component_name_hook","def __init__(
        self,
        exit_stack: contextlib.AsyncExitStack | None = None,
        component_name_hook: _ComponentNameHook | None = None,
    ) -> None:
        """"""Initializes the MCP client.""""""

        self._tools = {}
        self._resources = {}
        self._prompts = {}

        self._sessions = {}
        self._tool_to_session = {}
        if exit_stack is None:
            self._exit_stack = contextlib.AsyncExitStack()
            self._owns_exit_stack = True
        else:
            self._exit_stack = exit_stack
            self._owns_exit_stack = False
        self._session_exit_stacks = {}
        self._component_name_hook = component_name_hook",Initializes the MCP client.,Initializes the MCP client.,"def __init__(
        self,
        exit_stack: contextlib.AsyncExitStack | None = None,
        component_name_hook: _ComponentNameHook | None = None,
    ) -> None:
        

        self._tools = {}
        self._resources = {}
        self._prompts = {}

        self._sessions = {}
        self._tool_to_session = {}
        if exit_stack is None:
            self._exit_stack = contextlib.AsyncExitStack()
            self._owns_exit_stack = True
        else:
            self._exit_stack = exit_stack
            self._owns_exit_stack = False
        self._session_exit_stacks = {}
        self._component_name_hook = component_name_hook",Initializes the MCP client.,"def __init__ ( self , exit_stack : contextlib . AsyncExitStack | None = None , component_name_hook : _ComponentNameHook | None = None , ) -> None : self . _tools = { } self . _resources = { } self . _prompts = { } self . _sessions = { } self . _tool_to_session = { } if exit_stack is None : self . _exit_stack = contextlib . AsyncExitStack ( ) self . _owns_exit_stack = True else : self . _exit_stack = exit_stack self . _owns_exit_stack = False self . _session_exit_stacks = { } self . _component_name_hook = component_name_hook",Initializes the MCP client.
/cursor-free-vip/oauth_auth.py,_fix_chrome_permissions,"def _fix_chrome_permissions(self, user_data_dir):
        """"""Fix permissions for Chrome user data directory""""""
        try:
            if sys.platform == 'darwin':  # macOS
                import subprocess
                import pwd
                
                # Get current user
                current_user = pwd.getpwuid(os.getuid()).pw_name
                
                # Fix permissions for Chrome directory
                chrome_dir = os.path.expanduser('~/Library/Application Support/Google/Chrome')
                if os.path.exists(chrome_dir):
                    subprocess.run(['chmod', '-R', 'u+rwX', chrome_dir])
                    subprocess.run(['chown', '-R', f'{current_user}:staff', chrome_dir])
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('oauth.chrome_permissions_fixed') if self.translator else 'Fixed Chrome user data directory permissions'}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.YELLOW}{EMOJI['WARNING']} {self.translator.get('oauth.chrome_permissions_fix_failed', error=str(e)) if self.translator else f'Failed to fix Chrome permissions: {str(e)}'}{Style.RESET_ALL}"")","def _fix_chrome_permissions(self, user_data_dir):
        """"""Fix permissions for Chrome user data directory""""""
        try:
            if sys.platform == 'darwin':  # macOS
                import subprocess
                import pwd
                
                # Get current user
                current_user = pwd.getpwuid(os.getuid()).pw_name
                
                # Fix permissions for Chrome directory
                chrome_dir = os.path.expanduser('~/Library/Application Support/Google/Chrome')
                if os.path.exists(chrome_dir):
                    subprocess.run(['chmod', '-R', 'u+rwX', chrome_dir])
                    subprocess.run(['chown', '-R', f'{current_user}:staff', chrome_dir])
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('oauth.chrome_permissions_fixed') if self.translator else 'Fixed Chrome user data directory permissions'}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.YELLOW}{EMOJI['WARNING']} {self.translator.get('oauth.chrome_permissions_fix_failed', error=str(e)) if self.translator else f'Failed to fix Chrome permissions: {str(e)}'}{Style.RESET_ALL}"")",Fix permissions for Chrome user data directory,Fix permissions for Chrome user data directory,"def _fix_chrome_permissions(self, user_data_dir):
        
        try:
            if sys.platform == 'darwin':  # macOS
                import subprocess
                import pwd
                
                # Get current user
                current_user = pwd.getpwuid(os.getuid()).pw_name
                
                # Fix permissions for Chrome directory
                chrome_dir = os.path.expanduser('~/Library/Application Support/Google/Chrome')
                if os.path.exists(chrome_dir):
                    subprocess.run(['chmod', '-R', 'u+rwX', chrome_dir])
                    subprocess.run(['chown', '-R', f'{current_user}:staff', chrome_dir])
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('oauth.chrome_permissions_fixed') if self.translator else 'Fixed Chrome user data directory permissions'}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.YELLOW}{EMOJI['WARNING']} {self.translator.get('oauth.chrome_permissions_fix_failed', error=str(e)) if self.translator else f'Failed to fix Chrome permissions: {str(e)}'}{Style.RESET_ALL}"")",Fix permissions for Chrome user data directory,"def _fix_chrome_permissions ( self , user_data_dir ) : try : if sys . platform == 'darwin' : # macOS import subprocess import pwd # Get current user current_user = pwd . getpwuid ( os . getuid ( ) ) . pw_name # Fix permissions for Chrome directory chrome_dir = os . path . expanduser ( '~/Library/Application Support/Google/Chrome' ) if os . path . exists ( chrome_dir ) : subprocess . run ( [ 'chmod' , '-R' , 'u+rwX' , chrome_dir ] ) subprocess . run ( [ 'chown' , '-R' , f'{current_user}:staff' , chrome_dir ] ) print ( f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('oauth.chrome_permissions_fixed') if self.translator else 'Fixed Chrome user data directory permissions'}{Style.RESET_ALL}"" ) except Exception as e : print ( f""{Fore.YELLOW}{EMOJI['WARNING']} {self.translator.get('oauth.chrome_permissions_fix_failed', error=str(e)) if self.translator else f'Failed to fix Chrome permissions: {str(e)}'}{Style.RESET_ALL}"" )",Fix permissions for Chrome user data directory
/Sana/diffusion/model/model_growth_utils.py,init_progressive,"def init_progressive(self, noise_scale=0.01):
        """"""progressive init strategy (with noise)""""""
        self._copy_non_transformer_params()

        # copy pretrained layers
        for i in range(self.pretrained_layers):
            for key in self.pretrained_state:
                if f""blocks.{i}."" in key:
                    new_key = key.replace(f""blocks.{i}."", f""blocks.{i}."")
                    self.target_state[new_key] = self.pretrained_state[key]

        # progressive init new layers
        for i in range(self.pretrained_layers, self.target_layers):
            prev_layer = i - 1
            for key in self.target_state:
                if f""blocks.{i}."" in key:
                    prev_key = key.replace(f""blocks.{i}."", f""blocks.{prev_layer}."")
                    # add random noise
                    noise = torch.randn_like(self.target_state[prev_key]) * noise_scale
                    self.target_state[key] = self.target_state[prev_key] + noise

        self.target_model.load_state_dict(self.target_state)
        return self.target_model","def init_progressive(self, noise_scale=0.01):
        """"""progressive init strategy (with noise)""""""
        self._copy_non_transformer_params()

        # copy pretrained layers
        for i in range(self.pretrained_layers):
            for key in self.pretrained_state:
                if f""blocks.{i}."" in key:
                    new_key = key.replace(f""blocks.{i}."", f""blocks.{i}."")
                    self.target_state[new_key] = self.pretrained_state[key]

        # progressive init new layers
        for i in range(self.pretrained_layers, self.target_layers):
            prev_layer = i - 1
            for key in self.target_state:
                if f""blocks.{i}."" in key:
                    prev_key = key.replace(f""blocks.{i}."", f""blocks.{prev_layer}."")
                    # add random noise
                    noise = torch.randn_like(self.target_state[prev_key]) * noise_scale
                    self.target_state[key] = self.target_state[prev_key] + noise

        self.target_model.load_state_dict(self.target_state)
        return self.target_model",progressive init strategy (with noise),progressive init strategy (with noise),"def init_progressive(self, noise_scale=0.01):
        
        self._copy_non_transformer_params()

        # copy pretrained layers
        for i in range(self.pretrained_layers):
            for key in self.pretrained_state:
                if f""blocks.{i}."" in key:
                    new_key = key.replace(f""blocks.{i}."", f""blocks.{i}."")
                    self.target_state[new_key] = self.pretrained_state[key]

        # progressive init new layers
        for i in range(self.pretrained_layers, self.target_layers):
            prev_layer = i - 1
            for key in self.target_state:
                if f""blocks.{i}."" in key:
                    prev_key = key.replace(f""blocks.{i}."", f""blocks.{prev_layer}."")
                    # add random noise
                    noise = torch.randn_like(self.target_state[prev_key]) * noise_scale
                    self.target_state[key] = self.target_state[prev_key] + noise

        self.target_model.load_state_dict(self.target_state)
        return self.target_model",progressive init strategy (with noise),"def init_progressive ( self , noise_scale = 0.01 ) : self . _copy_non_transformer_params ( ) # copy pretrained layers for i in range ( self . pretrained_layers ) : for key in self . pretrained_state : if f""blocks.{i}."" in key : new_key = key . replace ( f""blocks.{i}."" , f""blocks.{i}."" ) self . target_state [ new_key ] = self . pretrained_state [ key ] # progressive init new layers for i in range ( self . pretrained_layers , self . target_layers ) : prev_layer = i - 1 for key in self . target_state : if f""blocks.{i}."" in key : prev_key = key . replace ( f""blocks.{i}."" , f""blocks.{prev_layer}."" ) # add random noise noise = torch . randn_like ( self . target_state [ prev_key ] ) * noise_scale self . target_state [ key ] = self . target_state [ prev_key ] + noise self . target_model . load_state_dict ( self . target_state ) return self . target_model",progressive init strategy (with noise)
/mcp-agent/src/mcp_agent/workflows/evaluator_optimizer/evaluator_optimizer.py,_build_refinement_prompt,"def _build_refinement_prompt(
        self,
        original_request: str,
        current_response: str,
        feedback: EvaluationResult,
        iteration: int,
    ) -> str:
        """"""Build the refinement prompt for the optimizer""""""
        return f""""""
        Improve your previous response based on the evaluation feedback.
        
        Original Request: {original_request}
        
        Previous Response (Iteration {iteration + 1}): 
        {current_response}
        
        Quality Rating: {feedback.rating}
        Feedback: {feedback.feedback}
        Areas to Focus On: {"", "".join(feedback.focus_areas)}
        
        Generate an improved version addressing the feedback while maintaining accuracy and relevance.
        """"""","def _build_refinement_prompt(
        self,
        original_request: str,
        current_response: str,
        feedback: EvaluationResult,
        iteration: int,
    ) -> str:
        """"""Build the refinement prompt for the optimizer""""""
        return f""""""
        Improve your previous response based on the evaluation feedback.
        
        Original Request: {original_request}
        
        Previous Response (Iteration {iteration + 1}): 
        {current_response}
        
        Quality Rating: {feedback.rating}
        Feedback: {feedback.feedback}
        Areas to Focus On: {"", "".join(feedback.focus_areas)}
        
        Generate an improved version addressing the feedback while maintaining accuracy and relevance.
        """"""",Build the refinement prompt for the optimizer,Build the refinement prompt for the optimizer,"def _build_refinement_prompt(
        self,
        original_request: str,
        current_response: str,
        feedback: EvaluationResult,
        iteration: int,
    ) -> str:
        
        return f",Build the refinement prompt for the optimizer,"def _build_refinement_prompt ( self , original_request : str , current_response : str , feedback : EvaluationResult , iteration : int , ) -> str : return f",Build the refinement prompt for the optimizer
/optillm/optillm/plugins/coc_plugin.py,extract_code_blocks,"def extract_code_blocks(text: str) -> List[str]:
    """"""Extract Python code blocks from text.""""""
    pattern = r'```python\s*(.*?)\s*```'
    matches = re.findall(pattern, text, re.DOTALL)
    blocks = [m.strip() for m in matches]
    logger.info(f""Extracted {len(blocks)} code blocks"")
    for i, block in enumerate(blocks):
        logger.info(f""Code block {i+1}:\n{block}"")
    return blocks","def extract_code_blocks(text: str) -> List[str]:
    """"""Extract Python code blocks from text.""""""
    pattern = r'```python\s*(.*?)\s*```'
    matches = re.findall(pattern, text, re.DOTALL)
    blocks = [m.strip() for m in matches]
    logger.info(f""Extracted {len(blocks)} code blocks"")
    for i, block in enumerate(blocks):
        logger.info(f""Code block {i+1}:\n{block}"")
    return blocks",Extract Python code blocks from text.,Extract Python code blocks from text.,"def extract_code_blocks(text: str) -> List[str]:
    
    pattern = r'```python\s*(.*?)\s*```'
    matches = re.findall(pattern, text, re.DOTALL)
    blocks = [m.strip() for m in matches]
    logger.info(f""Extracted {len(blocks)} code blocks"")
    for i, block in enumerate(blocks):
        logger.info(f""Code block {i+1}:\n{block}"")
    return blocks",Extract Python code blocks from text.,"def extract_code_blocks ( text : str ) -> List [ str ] : pattern = r'```python\s*(.*?)\s*```' matches = re . findall ( pattern , text , re . DOTALL ) blocks = [ m . strip ( ) for m in matches ] logger . info ( f""Extracted {len(blocks)} code blocks"" ) for i , block in enumerate ( blocks ) : logger . info ( f""Code block {i+1}:\n{block}"" ) return blocks",Extract Python code blocks from text.
/LightRAG/lightrag/api/lightrag_server.py,check_and_install_dependencies,"def check_and_install_dependencies():
    """"""Check and install required dependencies""""""
    required_packages = [
        ""uvicorn"",
        ""tiktoken"",
        ""fastapi"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")","def check_and_install_dependencies():
    """"""Check and install required dependencies""""""
    required_packages = [
        ""uvicorn"",
        ""tiktoken"",
        ""fastapi"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")",Check and install required dependencies,Check and install required dependencies,"def check_and_install_dependencies():
    
    required_packages = [
        ""uvicorn"",
        ""tiktoken"",
        ""fastapi"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")",Check and install required dependencies,"def check_and_install_dependencies ( ) : required_packages = [ ""uvicorn"" , ""tiktoken"" , ""fastapi"" , # Add other required packages here ] for package in required_packages : if not pm . is_installed ( package ) : print ( f""Installing {package}..."" ) pm . install ( package ) print ( f""{package} installed successfully"" )",Check and install required dependencies
/ai-hedge-fund/src/agents/michael_burry.py,_analyze_contrarian_sentiment,"def _analyze_contrarian_sentiment(news):
    """"""Very rough gauge: a wall of recent negative headlines can be a *positive* for a contrarian.""""""

    max_score = 1
    score = 0
    details: list[str] = []

    if not news:
        details.append(""No recent news"")
        return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}

    # Count negative sentiment articles
    sentiment_negative_count = sum(
        1 for n in news if n.sentiment and n.sentiment.lower() in [""negative"", ""bearish""]
    )
    
    if sentiment_negative_count >= 5:
        score += 1  # The more hated, the better (assuming fundamentals hold up)
        details.append(f""{sentiment_negative_count} negative headlines (contrarian opportunity)"")
    else:
        details.append(""Limited negative press"")

    return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}","def _analyze_contrarian_sentiment(news):
    """"""Very rough gauge: a wall of recent negative headlines can be a *positive* for a contrarian.""""""

    max_score = 1
    score = 0
    details: list[str] = []

    if not news:
        details.append(""No recent news"")
        return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}

    # Count negative sentiment articles
    sentiment_negative_count = sum(
        1 for n in news if n.sentiment and n.sentiment.lower() in [""negative"", ""bearish""]
    )
    
    if sentiment_negative_count >= 5:
        score += 1  # The more hated, the better (assuming fundamentals hold up)
        details.append(f""{sentiment_negative_count} negative headlines (contrarian opportunity)"")
    else:
        details.append(""Limited negative press"")

    return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}",Very rough gauge: a wall of recent negative headlines can be a *positive* for a contrarian.,Very rough gauge: a wall of recent negative headlines can be a *positive* for a contrarian.,"def _analyze_contrarian_sentiment(news):
    

    max_score = 1
    score = 0
    details: list[str] = []

    if not news:
        details.append(""No recent news"")
        return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}

    # Count negative sentiment articles
    sentiment_negative_count = sum(
        1 for n in news if n.sentiment and n.sentiment.lower() in [""negative"", ""bearish""]
    )
    
    if sentiment_negative_count >= 5:
        score += 1  # The more hated, the better (assuming fundamentals hold up)
        details.append(f""{sentiment_negative_count} negative headlines (contrarian opportunity)"")
    else:
        details.append(""Limited negative press"")

    return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}",Very rough gauge: a wall of recent negative headlines can be a *positive* for a contrarian.,"def _analyze_contrarian_sentiment ( news ) : max_score = 1 score = 0 details : list [ str ] = [ ] if not news : details . append ( ""No recent news"" ) return { ""score"" : score , ""max_score"" : max_score , ""details"" : ""; "" . join ( details ) } # Count negative sentiment articles sentiment_negative_count = sum ( 1 for n in news if n . sentiment and n . sentiment . lower ( ) in [ ""negative"" , ""bearish"" ] ) if sentiment_negative_count >= 5 : score += 1 # The more hated, the better (assuming fundamentals hold up) details . append ( f""{sentiment_negative_count} negative headlines (contrarian opportunity)"" ) else : details . append ( ""Limited negative press"" ) return { ""score"" : score , ""max_score"" : max_score , ""details"" : ""; "" . join ( details ) }",Very rough gauge: a wall of recent negative headlines can be a *positive* for a contrarian.
/nexa-sdk/nexa/gguf/llama/llama_grammar.py,opt_repetitions,"def opt_repetitions(up_to_n, prefix_with_sep=False):
        """"""
        - n=4, no sep:             '(a (a (a (a)?)?)?)?'
        - n=4, sep=',', prefix:    '("","" a ("","" a ("","" a ("","" a)?)?)?)?'
        - n=4, sep=',', no prefix: '(a ("","" a ("","" a ("","" a)?)?)?)?'
        """"""

        content = (
            f""{separator_rule} {item_rule}""
            if prefix_with_sep and separator_rule
            else item_rule
        )
        if up_to_n == 0:
            return """"
        elif up_to_n == 1:
            return f""({content})?""
        elif separator_rule and not prefix_with_sep:
            return f""({content} {opt_repetitions(up_to_n - 1, prefix_with_sep=True)})?""
        else:
            return (f""({content} "" * up_to_n).rstrip() + ("")?"" * up_to_n)","def opt_repetitions(up_to_n, prefix_with_sep=False):
        """"""
        - n=4, no sep:             '(a (a (a (a)?)?)?)?'
        - n=4, sep=',', prefix:    '("","" a ("","" a ("","" a ("","" a)?)?)?)?'
        - n=4, sep=',', no prefix: '(a ("","" a ("","" a ("","" a)?)?)?)?'
        """"""

        content = (
            f""{separator_rule} {item_rule}""
            if prefix_with_sep and separator_rule
            else item_rule
        )
        if up_to_n == 0:
            return """"
        elif up_to_n == 1:
            return f""({content})?""
        elif separator_rule and not prefix_with_sep:
            return f""({content} {opt_repetitions(up_to_n - 1, prefix_with_sep=True)})?""
        else:
            return (f""({content} "" * up_to_n).rstrip() + ("")?"" * up_to_n)","- n=4, no sep:             '(a (a (a (a)?)?)?)?'
- n=4, sep=',', prefix:    '("","" a ("","" a ("","" a ("","" a)?)?)?)?'
- n=4, sep=',', no prefix: '(a ("","" a ("","" a ("","" a)?)?)?)?'","n=4, no sep:             '(a (a (a (a)?)?)?)?' - n=4, sep=',', prefix:    '("","" a ("","" a ("","" a ("","" a)?)?)?)?' - n=4, sep=',', no prefix: '(a ("","" a ("","" a ("","" a)?)?)?)?'","def opt_repetitions(up_to_n, prefix_with_sep=False):
        

        content = (
            f""{separator_rule} {item_rule}""
            if prefix_with_sep and separator_rule
            else item_rule
        )
        if up_to_n == 0:
            return """"
        elif up_to_n == 1:
            return f""({content})?""
        elif separator_rule and not prefix_with_sep:
            return f""({content} {opt_repetitions(up_to_n - 1, prefix_with_sep=True)})?""
        else:
            return (f""({content} "" * up_to_n).rstrip() + ("")?"" * up_to_n)","n=4, no sep:             '(a (a (a (a)?)?)?)?' - n=4, sep=',', prefix:    '("","" a ("","" a ("","" a ("","" a)?)?)?)?' - n=4, sep=',', no prefix: '(a ("","" a ("","" a ("","" a)?)?)?)?'","def opt_repetitions ( up_to_n , prefix_with_sep = False ) : content = ( f""{separator_rule} {item_rule}"" if prefix_with_sep and separator_rule else item_rule ) if up_to_n == 0 : return """" elif up_to_n == 1 : return f""({content})?"" elif separator_rule and not prefix_with_sep : return f""({content} {opt_repetitions(up_to_n - 1, prefix_with_sep=True)})?"" else : return ( f""({content} "" * up_to_n ) . rstrip ( ) + ( "")?"" * up_to_n )","n=4, no sep: '(a (a (a (a)?)?)?)?' - n=4, sep=',', prefix: '("","" a ("","" a ("","" a ("","" a)?)?)?)?' - n=4, sep=',', no prefix: '(a ("","" a ("","" a ("","" a)?)?)?)?'"
/ragaai-catalyst/ragaai_catalyst/tracers/agentic_tracing/utils/zip_list_of_unique_files.py,comment_magic_commands,"def comment_magic_commands(script_content: str) -> str:
    """"""Comment out magic commands, shell commands, and direct execution commands in the script content.""""""
    lines = script_content.splitlines()
    commented_lines = []
    for line in lines:
        # Check for magic commands, shell commands, or direct execution commands
        if re.match(r'^\s*(!|%|pip|apt-get|curl|conda)', line.strip()):
            commented_lines.append(f""# {line}"")  # Comment the line
        else:
            commented_lines.append(line)  # Keep the line unchanged
    return ""\n"".join(commented_lines)","def comment_magic_commands(script_content: str) -> str:
    """"""Comment out magic commands, shell commands, and direct execution commands in the script content.""""""
    lines = script_content.splitlines()
    commented_lines = []
    for line in lines:
        # Check for magic commands, shell commands, or direct execution commands
        if re.match(r'^\s*(!|%|pip|apt-get|curl|conda)', line.strip()):
            commented_lines.append(f""# {line}"")  # Comment the line
        else:
            commented_lines.append(line)  # Keep the line unchanged
    return ""\n"".join(commented_lines)","Comment out magic commands, shell commands, and direct execution commands in the script content.","Comment out magic commands, shell commands, and direct execution commands in the script content.","def comment_magic_commands(script_content: str) -> str:
    
    lines = script_content.splitlines()
    commented_lines = []
    for line in lines:
        # Check for magic commands, shell commands, or direct execution commands
        if re.match(r'^\s*(!|%|pip|apt-get|curl|conda)', line.strip()):
            commented_lines.append(f""# {line}"")  # Comment the line
        else:
            commented_lines.append(line)  # Keep the line unchanged
    return ""\n"".join(commented_lines)","Comment out magic commands, shell commands, and direct execution commands in the script content.","def comment_magic_commands ( script_content : str ) -> str : lines = script_content . splitlines ( ) commented_lines = [ ] for line in lines : # Check for magic commands, shell commands, or direct execution commands if re . match ( r'^\s*(!|%|pip|apt-get|curl|conda)' , line . strip ( ) ) : commented_lines . append ( f""# {line}"" ) # Comment the line else : commented_lines . append ( line ) # Keep the line unchanged return ""\n"" . join ( commented_lines )","Comment out magic commands, shell commands, and direct execution commands in the script content."
/olmocr/olmocr/train/core/cli.py,to_native_types,"def to_native_types(obj: Any, resolve: bool = True, throw_on_missing: bool = True, enum_to_str: bool = True) -> Any:
    """"""Converts an OmegaConf object to native types (dicts, lists, etc.)""""""

    # convert dataclass to structured config
    if hasattr(obj, ""to_dict""):
        # huggingface objects have a to_dict method, we prefer that
        obj = obj.to_dict()
    elif is_dataclass(obj):
        # we go through structured config instead and hope for the best
        obj = om.to_container(obj)

    if isinstance(obj, DictConfig) or isinstance(obj, ListConfig):
        obj = om.to_container(obj, resolve=resolve, throw_on_missing=throw_on_missing, enum_to_str=enum_to_str)

    if isinstance(obj, dict):
        return {k: to_native_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [to_native_types(v) for v in obj]
    else:
        return obj","def to_native_types(obj: Any, resolve: bool = True, throw_on_missing: bool = True, enum_to_str: bool = True) -> Any:
    """"""Converts an OmegaConf object to native types (dicts, lists, etc.)""""""

    # convert dataclass to structured config
    if hasattr(obj, ""to_dict""):
        # huggingface objects have a to_dict method, we prefer that
        obj = obj.to_dict()
    elif is_dataclass(obj):
        # we go through structured config instead and hope for the best
        obj = om.to_container(obj)

    if isinstance(obj, DictConfig) or isinstance(obj, ListConfig):
        obj = om.to_container(obj, resolve=resolve, throw_on_missing=throw_on_missing, enum_to_str=enum_to_str)

    if isinstance(obj, dict):
        return {k: to_native_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [to_native_types(v) for v in obj]
    else:
        return obj","Converts an OmegaConf object to native types (dicts, lists, etc.)","Converts an OmegaConf object to native types (dicts, lists, etc.)","def to_native_types(obj: Any, resolve: bool = True, throw_on_missing: bool = True, enum_to_str: bool = True) -> Any:
    

    # convert dataclass to structured config
    if hasattr(obj, ""to_dict""):
        # huggingface objects have a to_dict method, we prefer that
        obj = obj.to_dict()
    elif is_dataclass(obj):
        # we go through structured config instead and hope for the best
        obj = om.to_container(obj)

    if isinstance(obj, DictConfig) or isinstance(obj, ListConfig):
        obj = om.to_container(obj, resolve=resolve, throw_on_missing=throw_on_missing, enum_to_str=enum_to_str)

    if isinstance(obj, dict):
        return {k: to_native_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [to_native_types(v) for v in obj]
    else:
        return obj","Converts an OmegaConf object to native types (dicts, lists, etc.)","def to_native_types ( obj : Any , resolve : bool = True , throw_on_missing : bool = True , enum_to_str : bool = True ) -> Any : # convert dataclass to structured config if hasattr ( obj , ""to_dict"" ) : # huggingface objects have a to_dict method, we prefer that obj = obj . to_dict ( ) elif is_dataclass ( obj ) : # we go through structured config instead and hope for the best obj = om . to_container ( obj ) if isinstance ( obj , DictConfig ) or isinstance ( obj , ListConfig ) : obj = om . to_container ( obj , resolve = resolve , throw_on_missing = throw_on_missing , enum_to_str = enum_to_str ) if isinstance ( obj , dict ) : return { k : to_native_types ( v ) for k , v in obj . items ( ) } elif isinstance ( obj , list ) : return [ to_native_types ( v ) for v in obj ] else : return obj","Converts an OmegaConf object to native types (dicts, lists, etc.)"
/ragaai-catalyst/ragaai_catalyst/guardrails_manager.py,list_fail_condition,"def list_fail_condition(self):
        """"""
        List all fail conditions for the current project's deployments.
        
        :return: A list of fail conditions.
        """"""
        payload = {}
        headers = {
                'Authorization': f'Bearer {os.getenv(""RAGAAI_CATALYST_TOKEN"")}',
                'X-Project-Id': str(self.project_id)
                }
        response = requests.request(""GET"", f""{self.base_url}/guardrail/deployment/configurations"", headers=headers, data=payload, timeout=self.timeout)
        return response.json()[""data""]","def list_fail_condition(self):
        """"""
        List all fail conditions for the current project's deployments.
        
        :return: A list of fail conditions.
        """"""
        payload = {}
        headers = {
                'Authorization': f'Bearer {os.getenv(""RAGAAI_CATALYST_TOKEN"")}',
                'X-Project-Id': str(self.project_id)
                }
        response = requests.request(""GET"", f""{self.base_url}/guardrail/deployment/configurations"", headers=headers, data=payload, timeout=self.timeout)
        return response.json()[""data""]","List all fail conditions for the current project's deployments.

:return: A list of fail conditions.",List all fail conditions for the current project's deployments.,"def list_fail_condition(self):
        
        payload = {}
        headers = {
                'Authorization': f'Bearer {os.getenv(""RAGAAI_CATALYST_TOKEN"")}',
                'X-Project-Id': str(self.project_id)
                }
        response = requests.request(""GET"", f""{self.base_url}/guardrail/deployment/configurations"", headers=headers, data=payload, timeout=self.timeout)
        return response.json()[""data""]",List all fail conditions for the current project's deployments.,"def list_fail_condition ( self ) : payload = { } headers = { 'Authorization' : f'Bearer {os.getenv(""RAGAAI_CATALYST_TOKEN"")}' , 'X-Project-Id' : str ( self . project_id ) } response = requests . request ( ""GET"" , f""{self.base_url}/guardrail/deployment/configurations"" , headers = headers , data = payload , timeout = self . timeout ) return response . json ( ) [ ""data"" ]",List all fail conditions for the current project's deployments.
/mcp/src/mysql-mcp-server/awslabs/mysql_mcp_server/mutable_sql_detector.py,detect_mutating_keywords,"def detect_mutating_keywords(sql: str) -> list[str]:
    """"""Return a list of mutating keywords found in the SQL (excluding comments).""""""
    matched = []

    if DDL_REGEX.search(sql):
        matched.append('DDL')

    if PERMISSION_REGEX.search(sql):
        matched.append('PERMISSION')

    if SYSTEM_REGEX.search(sql):
        matched.append('SYSTEM')

    # Match individual keywords from MUTATING_KEYWORDS
    keyword_matches = MUTATING_PATTERN.findall(sql)
    if keyword_matches:
        # Deduplicate and normalize casing
        matched.extend(sorted({k.upper() for k in keyword_matches}))

    return matched","def detect_mutating_keywords(sql: str) -> list[str]:
    """"""Return a list of mutating keywords found in the SQL (excluding comments).""""""
    matched = []

    if DDL_REGEX.search(sql):
        matched.append('DDL')

    if PERMISSION_REGEX.search(sql):
        matched.append('PERMISSION')

    if SYSTEM_REGEX.search(sql):
        matched.append('SYSTEM')

    # Match individual keywords from MUTATING_KEYWORDS
    keyword_matches = MUTATING_PATTERN.findall(sql)
    if keyword_matches:
        # Deduplicate and normalize casing
        matched.extend(sorted({k.upper() for k in keyword_matches}))

    return matched",Return a list of mutating keywords found in the SQL (excluding comments).,Return a list of mutating keywords found in the SQL (excluding comments).,"def detect_mutating_keywords(sql: str) -> list[str]:
    
    matched = []

    if DDL_REGEX.search(sql):
        matched.append('DDL')

    if PERMISSION_REGEX.search(sql):
        matched.append('PERMISSION')

    if SYSTEM_REGEX.search(sql):
        matched.append('SYSTEM')

    # Match individual keywords from MUTATING_KEYWORDS
    keyword_matches = MUTATING_PATTERN.findall(sql)
    if keyword_matches:
        # Deduplicate and normalize casing
        matched.extend(sorted({k.upper() for k in keyword_matches}))

    return matched",Return a list of mutating keywords found in the SQL (excluding comments).,def detect_mutating_keywords ( sql : str ) -> list [ str ] : matched = [ ] if DDL_REGEX . search ( sql ) : matched . append ( 'DDL' ) if PERMISSION_REGEX . search ( sql ) : matched . append ( 'PERMISSION' ) if SYSTEM_REGEX . search ( sql ) : matched . append ( 'SYSTEM' ) # Match individual keywords from MUTATING_KEYWORDS keyword_matches = MUTATING_PATTERN . findall ( sql ) if keyword_matches : # Deduplicate and normalize casing matched . extend ( sorted ( { k . upper ( ) for k in keyword_matches } ) ) return matched,Return a list of mutating keywords found in the SQL (excluding comments).
/mcp-agent/src/mcp_agent/tracing/file_span_exporter.py,_get_trace_filename,"def _get_trace_filename(self) -> str:
        """"""Generate a trace filename based on the path settings.""""""
        path_pattern = self.path_settings.path_pattern
        unique_id_type = self.path_settings.unique_id

        if unique_id_type == ""session_id"":
            unique_id = self.session_id
        elif unique_id_type == ""timestamp"":
            now = datetime.now()
            time_format = self.path_settings.timestamp_format
            unique_id = now.strftime(time_format)
        else:
            raise ValueError(
                f""Invalid unique_id type: {unique_id_type}. Expected 'session_id' or 'timestamp'.""
            )

        return path_pattern.replace(""{unique_id}"", unique_id)","def _get_trace_filename(self) -> str:
        """"""Generate a trace filename based on the path settings.""""""
        path_pattern = self.path_settings.path_pattern
        unique_id_type = self.path_settings.unique_id

        if unique_id_type == ""session_id"":
            unique_id = self.session_id
        elif unique_id_type == ""timestamp"":
            now = datetime.now()
            time_format = self.path_settings.timestamp_format
            unique_id = now.strftime(time_format)
        else:
            raise ValueError(
                f""Invalid unique_id type: {unique_id_type}. Expected 'session_id' or 'timestamp'.""
            )

        return path_pattern.replace(""{unique_id}"", unique_id)",Generate a trace filename based on the path settings.,Generate a trace filename based on the path settings.,"def _get_trace_filename(self) -> str:
        
        path_pattern = self.path_settings.path_pattern
        unique_id_type = self.path_settings.unique_id

        if unique_id_type == ""session_id"":
            unique_id = self.session_id
        elif unique_id_type == ""timestamp"":
            now = datetime.now()
            time_format = self.path_settings.timestamp_format
            unique_id = now.strftime(time_format)
        else:
            raise ValueError(
                f""Invalid unique_id type: {unique_id_type}. Expected 'session_id' or 'timestamp'.""
            )

        return path_pattern.replace(""{unique_id}"", unique_id)",Generate a trace filename based on the path settings.,"def _get_trace_filename ( self ) -> str : path_pattern = self . path_settings . path_pattern unique_id_type = self . path_settings . unique_id if unique_id_type == ""session_id"" : unique_id = self . session_id elif unique_id_type == ""timestamp"" : now = datetime . now ( ) time_format = self . path_settings . timestamp_format unique_id = now . strftime ( time_format ) else : raise ValueError ( f""Invalid unique_id type: {unique_id_type}. Expected 'session_id' or 'timestamp'."" ) return path_pattern . replace ( ""{unique_id}"" , unique_id )",Generate a trace filename based on the path settings.
/browser-use/browser_use/agent/prompts.py,_load_prompt_template,"def _load_prompt_template(self) -> None:
		""""""Load the prompt template from the markdown file.""""""
		try:
			# This works both in development and when installed as a package
			with importlib.resources.files('browser_use.agent').joinpath('system_prompt.md').open('r') as f:
				self.prompt_template = f.read()
		except Exception as e:
			raise RuntimeError(f'Failed to load system prompt template: {e}')","def _load_prompt_template(self) -> None:
		""""""Load the prompt template from the markdown file.""""""
		try:
			# This works both in development and when installed as a package
			with importlib.resources.files('browser_use.agent').joinpath('system_prompt.md').open('r') as f:
				self.prompt_template = f.read()
		except Exception as e:
			raise RuntimeError(f'Failed to load system prompt template: {e}')",Load the prompt template from the markdown file.,Load the prompt template from the markdown file.,"def _load_prompt_template(self) -> None:
		
		try:
			# This works both in development and when installed as a package
			with importlib.resources.files('browser_use.agent').joinpath('system_prompt.md').open('r') as f:
				self.prompt_template = f.read()
		except Exception as e:
			raise RuntimeError(f'Failed to load system prompt template: {e}')",Load the prompt template from the markdown file.,def _load_prompt_template ( self ) -> None : try : # This works both in development and when installed as a package with importlib . resources . files ( 'browser_use.agent' ) . joinpath ( 'system_prompt.md' ) . open ( 'r' ) as f : self . prompt_template = f . read ( ) except Exception as e : raise RuntimeError ( f'Failed to load system prompt template: {e}' ),Load the prompt template from the markdown file.
/browser-use/browser_use/agent/views.py,model_dump,"def model_dump(self, **kwargs) -> dict[str, Any]:
		""""""Custom serialization that properly uses AgentHistory's model_dump""""""
		return {
			'history': [h.model_dump(**kwargs) for h in self.history],
		}","def model_dump(self, **kwargs) -> dict[str, Any]:
		""""""Custom serialization that properly uses AgentHistory's model_dump""""""
		return {
			'history': [h.model_dump(**kwargs) for h in self.history],
		}",Custom serialization that properly uses AgentHistory's model_dump,Custom serialization that properly uses AgentHistory's model_dump,"def model_dump(self, **kwargs) -> dict[str, Any]:
		
		return {
			'history': [h.model_dump(**kwargs) for h in self.history],
		}",Custom serialization that properly uses AgentHistory's model_dump,"def model_dump ( self , ** kwargs ) -> dict [ str , Any ] : return { 'history' : [ h . model_dump ( ** kwargs ) for h in self . history ] , }",Custom serialization that properly uses AgentHistory's model_dump
/morphik-core/utils/printer.py,should_ignore_directory,"def should_ignore_directory(dirname: str) -> bool:
    """"""Check if directory should be ignored.""""""
    ignore_dirs = {
        ""venv"",
        ""env"",
        "".venv"",
        ""virtualenv"",
        ""__pycache__"",
        "".pytest_cache"",
        "".mypy_cache"",
        "".tox"",
        "".git"",
        ""build"",
        ""dist"",
        ""node_modules"",
        "".next"",
        ""storage"",
    }
    return dirname in ignore_dirs","def should_ignore_directory(dirname: str) -> bool:
    """"""Check if directory should be ignored.""""""
    ignore_dirs = {
        ""venv"",
        ""env"",
        "".venv"",
        ""virtualenv"",
        ""__pycache__"",
        "".pytest_cache"",
        "".mypy_cache"",
        "".tox"",
        "".git"",
        ""build"",
        ""dist"",
        ""node_modules"",
        "".next"",
        ""storage"",
    }
    return dirname in ignore_dirs",Check if directory should be ignored.,Check if directory should be ignored.,"def should_ignore_directory(dirname: str) -> bool:
    
    ignore_dirs = {
        ""venv"",
        ""env"",
        "".venv"",
        ""virtualenv"",
        ""__pycache__"",
        "".pytest_cache"",
        "".mypy_cache"",
        "".tox"",
        "".git"",
        ""build"",
        ""dist"",
        ""node_modules"",
        "".next"",
        ""storage"",
    }
    return dirname in ignore_dirs",Check if directory should be ignored.,"def should_ignore_directory ( dirname : str ) -> bool : ignore_dirs = { ""venv"" , ""env"" , "".venv"" , ""virtualenv"" , ""__pycache__"" , "".pytest_cache"" , "".mypy_cache"" , "".tox"" , "".git"" , ""build"" , ""dist"" , ""node_modules"" , "".next"" , ""storage"" , } return dirname in ignore_dirs",Check if directory should be ignored.
/ClearerVoice-Studio/train/speech_super_resolution/dataloader/dataloader.py,read_audio,"def read_audio(file_path):
    """"""
    Use AudioSegment to load audio from all supported audio input format
    """"""
    
    try:
        audio = AudioSegment.from_file(file_path)
        return audio
    except Exception as e:
        print(f""Error loading file: {e}"")
        return None","def read_audio(file_path):
    """"""
    Use AudioSegment to load audio from all supported audio input format
    """"""
    
    try:
        audio = AudioSegment.from_file(file_path)
        return audio
    except Exception as e:
        print(f""Error loading file: {e}"")
        return None",Use AudioSegment to load audio from all supported audio input format,Use AudioSegment to load audio from all supported audio input format,"def read_audio(file_path):
    
    
    try:
        audio = AudioSegment.from_file(file_path)
        return audio
    except Exception as e:
        print(f""Error loading file: {e}"")
        return None",Use AudioSegment to load audio from all supported audio input format,"def read_audio ( file_path ) : try : audio = AudioSegment . from_file ( file_path ) return audio except Exception as e : print ( f""Error loading file: {e}"" ) return None",Use AudioSegment to load audio from all supported audio input format
/cua/libs/pylume/pylume/client.py,print_curl,"def print_curl(self, method: str, path: str, data: Optional[Dict[str, Any]] = None) -> None:
        """"""Print equivalent curl command for debugging.""""""
        curl_cmd = f""""""curl -X {method} \\
  '{self.base_url}{path}'""""""
        
        if data:
            curl_cmd += f"" \\\n  -H 'Content-Type: application/json' \\\n  -d '{json.dumps(data)}'""
        
        print(""\nEquivalent curl command:"")
        print(curl_cmd)
        print()","def print_curl(self, method: str, path: str, data: Optional[Dict[str, Any]] = None) -> None:
        """"""Print equivalent curl command for debugging.""""""
        curl_cmd = f""""""curl -X {method} \\
  '{self.base_url}{path}'""""""
        
        if data:
            curl_cmd += f"" \\\n  -H 'Content-Type: application/json' \\\n  -d '{json.dumps(data)}'""
        
        print(""\nEquivalent curl command:"")
        print(curl_cmd)
        print()",Print equivalent curl command for debugging.,Print equivalent curl command for debugging.,"def print_curl(self, method: str, path: str, data: Optional[Dict[str, Any]] = None) -> None:
        
        curl_cmd = f
        
        if data:
            curl_cmd += f"" \\\n  -H 'Content-Type: application/json' \\\n  -d '{json.dumps(data)}'""
        
        print(""\nEquivalent curl command:"")
        print(curl_cmd)
        print()",Print equivalent curl command for debugging.,"def print_curl ( self , method : str , path : str , data : Optional [ Dict [ str , Any ] ] = None ) -> None : curl_cmd = f if data : curl_cmd += f"" \\\n  -H 'Content-Type: application/json' \\\n  -d '{json.dumps(data)}'"" print ( ""\nEquivalent curl command:"" ) print ( curl_cmd ) print ( )",Print equivalent curl command for debugging.
/airweave/backend/airweave/platform/file_handling/conversion/converters/img_converter.py,_log_available_capabilities,"def _log_available_capabilities(self):
        """"""Log which conversion capabilities are available.""""""
        capabilities = []
        if self.mistral_client:
            capabilities.append(""Mistral OCR"")
        if self.exiftool_available:
            capabilities.append(""Exiftool metadata extraction"")
        if self.openai_client:
            capabilities.append(""OpenAI image description"")

        if capabilities:
            logger.info(f""Image converter initialized with: {', '.join(capabilities)}"")
        else:
            logger.warning(""Image converter initialized without any processing capabilities"")","def _log_available_capabilities(self):
        """"""Log which conversion capabilities are available.""""""
        capabilities = []
        if self.mistral_client:
            capabilities.append(""Mistral OCR"")
        if self.exiftool_available:
            capabilities.append(""Exiftool metadata extraction"")
        if self.openai_client:
            capabilities.append(""OpenAI image description"")

        if capabilities:
            logger.info(f""Image converter initialized with: {', '.join(capabilities)}"")
        else:
            logger.warning(""Image converter initialized without any processing capabilities"")",Log which conversion capabilities are available.,Log which conversion capabilities are available.,"def _log_available_capabilities(self):
        
        capabilities = []
        if self.mistral_client:
            capabilities.append(""Mistral OCR"")
        if self.exiftool_available:
            capabilities.append(""Exiftool metadata extraction"")
        if self.openai_client:
            capabilities.append(""OpenAI image description"")

        if capabilities:
            logger.info(f""Image converter initialized with: {', '.join(capabilities)}"")
        else:
            logger.warning(""Image converter initialized without any processing capabilities"")",Log which conversion capabilities are available.,"def _log_available_capabilities ( self ) : capabilities = [ ] if self . mistral_client : capabilities . append ( ""Mistral OCR"" ) if self . exiftool_available : capabilities . append ( ""Exiftool metadata extraction"" ) if self . openai_client : capabilities . append ( ""OpenAI image description"" ) if capabilities : logger . info ( f""Image converter initialized with: {', '.join(capabilities)}"" ) else : logger . warning ( ""Image converter initialized without any processing capabilities"" )",Log which conversion capabilities are available.
/Kokoro-FastAPI/api/src/routers/openai_compatible.py,load_openai_mappings,"def load_openai_mappings() -> Dict:
    """"""Load OpenAI voice and model mappings from JSON""""""
    api_dir = os.path.dirname(os.path.dirname(__file__))
    mapping_path = os.path.join(api_dir, ""core"", ""openai_mappings.json"")
    try:
        with open(mapping_path, ""r"") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f""Failed to load OpenAI mappings: {e}"")
        return {""models"": {}, ""voices"": {}}","def load_openai_mappings() -> Dict:
    """"""Load OpenAI voice and model mappings from JSON""""""
    api_dir = os.path.dirname(os.path.dirname(__file__))
    mapping_path = os.path.join(api_dir, ""core"", ""openai_mappings.json"")
    try:
        with open(mapping_path, ""r"") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f""Failed to load OpenAI mappings: {e}"")
        return {""models"": {}, ""voices"": {}}",Load OpenAI voice and model mappings from JSON,Load OpenAI voice and model mappings from JSON,"def load_openai_mappings() -> Dict:
    
    api_dir = os.path.dirname(os.path.dirname(__file__))
    mapping_path = os.path.join(api_dir, ""core"", ""openai_mappings.json"")
    try:
        with open(mapping_path, ""r"") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f""Failed to load OpenAI mappings: {e}"")
        return {""models"": {}, ""voices"": {}}",Load OpenAI voice and model mappings from JSON,"def load_openai_mappings ( ) -> Dict : api_dir = os . path . dirname ( os . path . dirname ( __file__ ) ) mapping_path = os . path . join ( api_dir , ""core"" , ""openai_mappings.json"" ) try : with open ( mapping_path , ""r"" ) as f : return json . load ( f ) except Exception as e : logger . error ( f""Failed to load OpenAI mappings: {e}"" ) return { ""models"" : { } , ""voices"" : { } }",Load OpenAI voice and model mappings from JSON
/adk-python/src/google/adk/tools/openapi_tool/openapi_spec_parser/operation_parser.py,get_pydoc_string,"def get_pydoc_string(self) -> str:
    """"""Returns the generated PyDoc string.""""""
    pydoc_params = [param.to_pydoc_string() for param in self._params]
    pydoc_description = (
        self._operation.summary or self._operation.description or ''
    )
    pydoc_return = PydocHelper.generate_return_doc(
        self._operation.responses or {}
    )
    pydoc_arg_list = chr(10).join(
        f'        {param_doc}' for param_doc in pydoc_params
    )
    return dedent(f""""""
        \""\""\""{pydoc_description}

        Args:
        {pydoc_arg_list}

        {pydoc_return}
        \""\""\""
            """""").strip()","def get_pydoc_string(self) -> str:
    """"""Returns the generated PyDoc string.""""""
    pydoc_params = [param.to_pydoc_string() for param in self._params]
    pydoc_description = (
        self._operation.summary or self._operation.description or ''
    )
    pydoc_return = PydocHelper.generate_return_doc(
        self._operation.responses or {}
    )
    pydoc_arg_list = chr(10).join(
        f'        {param_doc}' for param_doc in pydoc_params
    )
    return dedent(f""""""
        \""\""\""{pydoc_description}

        Args:
        {pydoc_arg_list}

        {pydoc_return}
        \""\""\""
            """""").strip()",Returns the generated PyDoc string.,Returns the generated PyDoc string.,"def get_pydoc_string(self) -> str:
    
    pydoc_params = [param.to_pydoc_string() for param in self._params]
    pydoc_description = (
        self._operation.summary or self._operation.description or ''
    )
    pydoc_return = PydocHelper.generate_return_doc(
        self._operation.responses or {}
    )
    pydoc_arg_list = chr(10).join(
        f'        {param_doc}' for param_doc in pydoc_params
    )
    return dedent(f).strip()",Returns the generated PyDoc string.,def get_pydoc_string ( self ) -> str : pydoc_params = [ param . to_pydoc_string ( ) for param in self . _params ] pydoc_description = ( self . _operation . summary or self . _operation . description or '' ) pydoc_return = PydocHelper . generate_return_doc ( self . _operation . responses or { } ) pydoc_arg_list = chr ( 10 ) . join ( f'        {param_doc}' for param_doc in pydoc_params ) return dedent ( f ) . strip ( ),Returns the generated PyDoc string.
/mcp-use/mcp_use/managers/tools/get_active_server.py,_run,"def _run(self, **kwargs) -> str:
        """"""Get the currently active MCP server.""""""
        if not self.server_manager.active_server:
            return (
                ""No MCP server is currently active. ""
                ""Use connect_to_mcp_server to connect to a server.""
            )
        return f""Currently active MCP server: {self.server_manager.active_server}""","def _run(self, **kwargs) -> str:
        """"""Get the currently active MCP server.""""""
        if not self.server_manager.active_server:
            return (
                ""No MCP server is currently active. ""
                ""Use connect_to_mcp_server to connect to a server.""
            )
        return f""Currently active MCP server: {self.server_manager.active_server}""",Get the currently active MCP server.,Get the currently active MCP server.,"def _run(self, **kwargs) -> str:
        
        if not self.server_manager.active_server:
            return (
                ""No MCP server is currently active. ""
                ""Use connect_to_mcp_server to connect to a server.""
            )
        return f""Currently active MCP server: {self.server_manager.active_server}""",Get the currently active MCP server.,"def _run ( self , ** kwargs ) -> str : if not self . server_manager . active_server : return ( ""No MCP server is currently active. "" ""Use connect_to_mcp_server to connect to a server."" ) return f""Currently active MCP server: {self.server_manager.active_server}""",Get the currently active MCP server.
/nexa-sdk/nexa/siglip/nexa_siglip_server.py,load_images_from_directory,"def load_images_from_directory(image_dir, valid_extensions=('.jpg', '.jpeg', '.png', '.webp')):
    """"""Load images from directory""""""
    images_dict = {}

    if not os.path.exists(image_dir):
        raise ValueError(f""Directory {image_dir} does not exist"")

    for filename in os.listdir(image_dir):
        if filename.lower().endswith(valid_extensions):
            image_path = os.path.join(image_dir, filename)
            try:
                image = Image.open(image_path).convert(""RGB"")
                images_dict[image_path] = image
            except Exception as e:
                print(f""Failed to load image {filename}: {str(e)}"")

    if not images_dict:
        raise ValueError(f""No valid image files found in {image_dir}"")

    return images_dict","def load_images_from_directory(image_dir, valid_extensions=('.jpg', '.jpeg', '.png', '.webp')):
    """"""Load images from directory""""""
    images_dict = {}

    if not os.path.exists(image_dir):
        raise ValueError(f""Directory {image_dir} does not exist"")

    for filename in os.listdir(image_dir):
        if filename.lower().endswith(valid_extensions):
            image_path = os.path.join(image_dir, filename)
            try:
                image = Image.open(image_path).convert(""RGB"")
                images_dict[image_path] = image
            except Exception as e:
                print(f""Failed to load image {filename}: {str(e)}"")

    if not images_dict:
        raise ValueError(f""No valid image files found in {image_dir}"")

    return images_dict",Load images from directory,Load images from directory,"def load_images_from_directory(image_dir, valid_extensions=('.jpg', '.jpeg', '.png', '.webp')):
    
    images_dict = {}

    if not os.path.exists(image_dir):
        raise ValueError(f""Directory {image_dir} does not exist"")

    for filename in os.listdir(image_dir):
        if filename.lower().endswith(valid_extensions):
            image_path = os.path.join(image_dir, filename)
            try:
                image = Image.open(image_path).convert(""RGB"")
                images_dict[image_path] = image
            except Exception as e:
                print(f""Failed to load image {filename}: {str(e)}"")

    if not images_dict:
        raise ValueError(f""No valid image files found in {image_dir}"")

    return images_dict",Load images from directory,"def load_images_from_directory ( image_dir , valid_extensions = ( '.jpg' , '.jpeg' , '.png' , '.webp' ) ) : images_dict = { } if not os . path . exists ( image_dir ) : raise ValueError ( f""Directory {image_dir} does not exist"" ) for filename in os . listdir ( image_dir ) : if filename . lower ( ) . endswith ( valid_extensions ) : image_path = os . path . join ( image_dir , filename ) try : image = Image . open ( image_path ) . convert ( ""RGB"" ) images_dict [ image_path ] = image except Exception as e : print ( f""Failed to load image {filename}: {str(e)}"" ) if not images_dict : raise ValueError ( f""No valid image files found in {image_dir}"" ) return images_dict",Load images from directory
/morphik-core/core/tests/unit/test_multivector.py,get_sample_document_chunks,"def get_sample_document_chunks(num_chunks=3, num_vectors=3, dim=128):
    """"""Create sample document chunks for testing""""""
    chunks = []
    for i in range(num_chunks):
        embeddings = get_sample_embeddings(num_vectors, dim)
        chunk = DocumentChunk(
            document_id=f""doc_{i}"",
            content=f""Test content {i}"",
            embedding=embeddings,
            chunk_number=i,
            metadata={""test_key"": f""test_value_{i}""},
        )
        chunks.append(chunk)
    return chunks","def get_sample_document_chunks(num_chunks=3, num_vectors=3, dim=128):
    """"""Create sample document chunks for testing""""""
    chunks = []
    for i in range(num_chunks):
        embeddings = get_sample_embeddings(num_vectors, dim)
        chunk = DocumentChunk(
            document_id=f""doc_{i}"",
            content=f""Test content {i}"",
            embedding=embeddings,
            chunk_number=i,
            metadata={""test_key"": f""test_value_{i}""},
        )
        chunks.append(chunk)
    return chunks",Create sample document chunks for testing,Create sample document chunks for testing,"def get_sample_document_chunks(num_chunks=3, num_vectors=3, dim=128):
    
    chunks = []
    for i in range(num_chunks):
        embeddings = get_sample_embeddings(num_vectors, dim)
        chunk = DocumentChunk(
            document_id=f""doc_{i}"",
            content=f""Test content {i}"",
            embedding=embeddings,
            chunk_number=i,
            metadata={""test_key"": f""test_value_{i}""},
        )
        chunks.append(chunk)
    return chunks",Create sample document chunks for testing,"def get_sample_document_chunks ( num_chunks = 3 , num_vectors = 3 , dim = 128 ) : chunks = [ ] for i in range ( num_chunks ) : embeddings = get_sample_embeddings ( num_vectors , dim ) chunk = DocumentChunk ( document_id = f""doc_{i}"" , content = f""Test content {i}"" , embedding = embeddings , chunk_number = i , metadata = { ""test_key"" : f""test_value_{i}"" } , ) chunks . append ( chunk ) return chunks",Create sample document chunks for testing
/preswald/preswald/engine/base_service.py,_update_component_states,"def _update_component_states(self, states: dict[str, Any]):
        """"""Update internal state dictionary with cleaned component values.""""""
        with self._lock:
            logger.info(""[STATE] Updating states"")
            for component_id, new_value in states.items():
                old_value = self._component_states.get(component_id)

                cleaned_new_value = clean_nan_values(new_value)
                cleaned_old_value = clean_nan_values(old_value)

                if cleaned_old_value != cleaned_new_value:
                    self._component_states[component_id] = cleaned_new_value
                    logger.info(f""[STATE] State changed for {component_id=}"")
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f""[STATE]  - {cleaned_old_value=}\n  - {cleaned_new_value=}"")","def _update_component_states(self, states: dict[str, Any]):
        """"""Update internal state dictionary with cleaned component values.""""""
        with self._lock:
            logger.info(""[STATE] Updating states"")
            for component_id, new_value in states.items():
                old_value = self._component_states.get(component_id)

                cleaned_new_value = clean_nan_values(new_value)
                cleaned_old_value = clean_nan_values(old_value)

                if cleaned_old_value != cleaned_new_value:
                    self._component_states[component_id] = cleaned_new_value
                    logger.info(f""[STATE] State changed for {component_id=}"")
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f""[STATE]  - {cleaned_old_value=}\n  - {cleaned_new_value=}"")",Update internal state dictionary with cleaned component values.,Update internal state dictionary with cleaned component values.,"def _update_component_states(self, states: dict[str, Any]):
        
        with self._lock:
            logger.info(""[STATE] Updating states"")
            for component_id, new_value in states.items():
                old_value = self._component_states.get(component_id)

                cleaned_new_value = clean_nan_values(new_value)
                cleaned_old_value = clean_nan_values(old_value)

                if cleaned_old_value != cleaned_new_value:
                    self._component_states[component_id] = cleaned_new_value
                    logger.info(f""[STATE] State changed for {component_id=}"")
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f""[STATE]  - {cleaned_old_value=}\n  - {cleaned_new_value=}"")",Update internal state dictionary with cleaned component values.,"def _update_component_states ( self , states : dict [ str , Any ] ) : with self . _lock : logger . info ( ""[STATE] Updating states"" ) for component_id , new_value in states . items ( ) : old_value = self . _component_states . get ( component_id ) cleaned_new_value = clean_nan_values ( new_value ) cleaned_old_value = clean_nan_values ( old_value ) if cleaned_old_value != cleaned_new_value : self . _component_states [ component_id ] = cleaned_new_value logger . info ( f""[STATE] State changed for {component_id=}"" ) if logger . isEnabledFor ( logging . DEBUG ) : logger . debug ( f""[STATE]  - {cleaned_old_value=}\n  - {cleaned_new_value=}"" )",Update internal state dictionary with cleaned component values.
/ragaai-catalyst/tests/examples/smolagents/most_upvoted_paper/most_upvoted_paper.py,main,"def main(model_name: str = ""gpt-4o-mini"", provider: str = ""openai""):
    """"""Initialize and run the paper summarization agent.""""""
    model = LiteLLMModel(
        model_id=f""{provider}/{model_name}"",
        api_key=os.environ.get(""OPENAI_API_KEY""),
    )
    
    agent = CodeAgent(
        tools=[
            get_hugging_face_top_daily_paper,
            get_paper_id_by_title,
            download_paper_by_id,
            read_pdf_file
        ],
        model=model,
        add_base_tools=True
    )

    agent.run(
        ""Summarize today's top paper on Hugging Face daily papers by reading it.""
    )","def main(model_name: str = ""gpt-4o-mini"", provider: str = ""openai""):
    """"""Initialize and run the paper summarization agent.""""""
    model = LiteLLMModel(
        model_id=f""{provider}/{model_name}"",
        api_key=os.environ.get(""OPENAI_API_KEY""),
    )
    
    agent = CodeAgent(
        tools=[
            get_hugging_face_top_daily_paper,
            get_paper_id_by_title,
            download_paper_by_id,
            read_pdf_file
        ],
        model=model,
        add_base_tools=True
    )

    agent.run(
        ""Summarize today's top paper on Hugging Face daily papers by reading it.""
    )",Initialize and run the paper summarization agent.,Initialize and run the paper summarization agent.,"def main(model_name: str = ""gpt-4o-mini"", provider: str = ""openai""):
    
    model = LiteLLMModel(
        model_id=f""{provider}/{model_name}"",
        api_key=os.environ.get(""OPENAI_API_KEY""),
    )
    
    agent = CodeAgent(
        tools=[
            get_hugging_face_top_daily_paper,
            get_paper_id_by_title,
            download_paper_by_id,
            read_pdf_file
        ],
        model=model,
        add_base_tools=True
    )

    agent.run(
        ""Summarize today's top paper on Hugging Face daily papers by reading it.""
    )",Initialize and run the paper summarization agent.,"def main ( model_name : str = ""gpt-4o-mini"" , provider : str = ""openai"" ) : model = LiteLLMModel ( model_id = f""{provider}/{model_name}"" , api_key = os . environ . get ( ""OPENAI_API_KEY"" ) , ) agent = CodeAgent ( tools = [ get_hugging_face_top_daily_paper , get_paper_id_by_title , download_paper_by_id , read_pdf_file ] , model = model , add_base_tools = True ) agent . run ( ""Summarize today's top paper on Hugging Face daily papers by reading it."" )",Initialize and run the paper summarization agent.
/Qwen2-VL/qwen-vl-finetune/qwenvl/train/train_qwen.py,safe_save_model_for_hf_trainer,"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    """"""Collects the state dict and dump to disk.""""""

    if trainer.deepspeed:
        torch.cuda.synchronize()
        trainer.save_model(output_dir)
        return

    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)","def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    """"""Collects the state dict and dump to disk.""""""

    if trainer.deepspeed:
        torch.cuda.synchronize()
        trainer.save_model(output_dir)
        return

    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,Collects the state dict and dump to disk.,"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    

    if trainer.deepspeed:
        torch.cuda.synchronize()
        trainer.save_model(output_dir)
        return

    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,"def safe_save_model_for_hf_trainer ( trainer : transformers . Trainer , output_dir : str ) : if trainer . deepspeed : torch . cuda . synchronize ( ) trainer . save_model ( output_dir ) return state_dict = trainer . model . state_dict ( ) if trainer . args . should_save : cpu_state_dict = { key : value . cpu ( ) for key , value in state_dict . items ( ) } del state_dict trainer . _save ( output_dir , state_dict = cpu_state_dict )",Collects the state dict and dump to disk.
/nexa-sdk/nexa/eval/nexa_perf/nexa_backend.py,load_model,"def load_model(self) -> None:
        """"""
        Load the model from the given model path (normally GGUF, GGML)
        """"""
        # TODO: add mps (apple metal) support, currently cant benchmark mps device accurately for energy
        if self.config.device == ""cuda"" or self.config.device == ""mps"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""gpu"", **self.config.model_kwargs)
        elif self.config.device == ""cpu"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""cpu"", **self.config.model_kwargs)
        else:
            raise ValueError(f""Invalid device: {self.config.device}"")
        
        self.pretrained_model = nexa_model.model","def load_model(self) -> None:
        """"""
        Load the model from the given model path (normally GGUF, GGML)
        """"""
        # TODO: add mps (apple metal) support, currently cant benchmark mps device accurately for energy
        if self.config.device == ""cuda"" or self.config.device == ""mps"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""gpu"", **self.config.model_kwargs)
        elif self.config.device == ""cpu"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""cpu"", **self.config.model_kwargs)
        else:
            raise ValueError(f""Invalid device: {self.config.device}"")
        
        self.pretrained_model = nexa_model.model","Load the model from the given model path (normally GGUF, GGML)","Load the model from the given model path (normally GGUF, GGML)","def load_model(self) -> None:
        
        # TODO: add mps (apple metal) support, currently cant benchmark mps device accurately for energy
        if self.config.device == ""cuda"" or self.config.device == ""mps"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""gpu"", **self.config.model_kwargs)
        elif self.config.device == ""cpu"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""cpu"", **self.config.model_kwargs)
        else:
            raise ValueError(f""Invalid device: {self.config.device}"")
        
        self.pretrained_model = nexa_model.model","Load the model from the given model path (normally GGUF, GGML)","def load_model ( self ) -> None : # TODO: add mps (apple metal) support, currently cant benchmark mps device accurately for energy if self . config . device == ""cuda"" or self . config . device == ""mps"" : nexa_model = NexaTextInference ( model_path = self . config . model , device = ""gpu"" , ** self . config . model_kwargs ) elif self . config . device == ""cpu"" : nexa_model = NexaTextInference ( model_path = self . config . model , device = ""cpu"" , ** self . config . model_kwargs ) else : raise ValueError ( f""Invalid device: {self.config.device}"" ) self . pretrained_model = nexa_model . model","Load the model from the given model path (normally GGUF, GGML)"
/Agent-S/gui_agents/s2/agents/grounding.py,switch_applications,"def switch_applications(self, app_code):
        """"""Switch to a different application that is already open
        Args:
            app_code:str the code name of the application to switch to from the provided list of open applications
        """"""
        if self.platform == ""darwin"":
            return f""import pyautogui; import time; pyautogui.hotkey('command', 'space', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""
        elif self.platform == ""linux"":
            return UBUNTU_APP_SETUP.replace(""APP_NAME"", app_code)
        elif self.platform == ""windows"":
            return f""import pyautogui; import time; pyautogui.hotkey('win', 'd', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""","def switch_applications(self, app_code):
        """"""Switch to a different application that is already open
        Args:
            app_code:str the code name of the application to switch to from the provided list of open applications
        """"""
        if self.platform == ""darwin"":
            return f""import pyautogui; import time; pyautogui.hotkey('command', 'space', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""
        elif self.platform == ""linux"":
            return UBUNTU_APP_SETUP.replace(""APP_NAME"", app_code)
        elif self.platform == ""windows"":
            return f""import pyautogui; import time; pyautogui.hotkey('win', 'd', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""","Switch to a different application that is already open
Args:
    app_code:str the code name of the application to switch to from the provided list of open applications",Switch to a different application that is already open,"def switch_applications(self, app_code):
        
        if self.platform == ""darwin"":
            return f""import pyautogui; import time; pyautogui.hotkey('command', 'space', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""
        elif self.platform == ""linux"":
            return UBUNTU_APP_SETUP.replace(""APP_NAME"", app_code)
        elif self.platform == ""windows"":
            return f""import pyautogui; import time; pyautogui.hotkey('win', 'd', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""",Switch to a different application that is already open,"def switch_applications ( self , app_code ) : if self . platform == ""darwin"" : return f""import pyautogui; import time; pyautogui.hotkey('command', 'space', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)"" elif self . platform == ""linux"" : return UBUNTU_APP_SETUP . replace ( ""APP_NAME"" , app_code ) elif self . platform == ""windows"" : return f""import pyautogui; import time; pyautogui.hotkey('win', 'd', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""",Switch to a different application that is already open
/ai-hedge-fund/src/data/cache.py,_merge_data,"def _merge_data(self, existing: list[dict] | None, new_data: list[dict], key_field: str) -> list[dict]:
        """"""Merge existing and new data, avoiding duplicates based on a key field.""""""
        if not existing:
            return new_data

        # Create a set of existing keys for O(1) lookup
        existing_keys = {item[key_field] for item in existing}

        # Only add items that don't exist yet
        merged = existing.copy()
        merged.extend([item for item in new_data if item[key_field] not in existing_keys])
        return merged","def _merge_data(self, existing: list[dict] | None, new_data: list[dict], key_field: str) -> list[dict]:
        """"""Merge existing and new data, avoiding duplicates based on a key field.""""""
        if not existing:
            return new_data

        # Create a set of existing keys for O(1) lookup
        existing_keys = {item[key_field] for item in existing}

        # Only add items that don't exist yet
        merged = existing.copy()
        merged.extend([item for item in new_data if item[key_field] not in existing_keys])
        return merged","Merge existing and new data, avoiding duplicates based on a key field.","Merge existing and new data, avoiding duplicates based on a key field.","def _merge_data(self, existing: list[dict] | None, new_data: list[dict], key_field: str) -> list[dict]:
        
        if not existing:
            return new_data

        # Create a set of existing keys for O(1) lookup
        existing_keys = {item[key_field] for item in existing}

        # Only add items that don't exist yet
        merged = existing.copy()
        merged.extend([item for item in new_data if item[key_field] not in existing_keys])
        return merged","Merge existing and new data, avoiding duplicates based on a key field.","def _merge_data ( self , existing : list [ dict ] | None , new_data : list [ dict ] , key_field : str ) -> list [ dict ] : if not existing : return new_data # Create a set of existing keys for O(1) lookup existing_keys = { item [ key_field ] for item in existing } # Only add items that don't exist yet merged = existing . copy ( ) merged . extend ( [ item for item in new_data if item [ key_field ] not in existing_keys ] ) return merged","Merge existing and new data, avoiding duplicates based on a key field."
/open-r1/src/open_r1/utils/hub.py,check_hub_revision_exists,"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    """"""Checks if a given Hub revision exists.""""""
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id,
                    revision=training_args.hub_model_revision,
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )","def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    """"""Checks if a given Hub revision exists.""""""
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id,
                    revision=training_args.hub_model_revision,
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )",Checks if a given Hub revision exists.,Checks if a given Hub revision exists.,"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id,
                    revision=training_args.hub_model_revision,
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )",Checks if a given Hub revision exists.,"def check_hub_revision_exists ( training_args : SFTConfig | GRPOConfig ) : if repo_exists ( training_args . hub_model_id ) : if training_args . push_to_hub_revision is True : # First check if the revision exists revisions = [ rev . name for rev in list_repo_refs ( training_args . hub_model_id ) . branches ] # If the revision exists, we next check it has a README file if training_args . hub_model_revision in revisions : repo_files = list_repo_files ( repo_id = training_args . hub_model_id , revision = training_args . hub_model_revision , ) if ""README.md"" in repo_files and training_args . overwrite_hub_revision is False : raise ValueError ( f""Revision {training_args.hub_model_revision} already exists. "" ""Use --overwrite_hub_revision to overwrite it."" )",Checks if a given Hub revision exists.
/adk-python/src/google/adk/sessions/base_session_service.py,__update_session_state,"def __update_session_state(self, session: Session, event: Event) -> None:
    """"""Updates the session state based on the event.""""""
    if not event.actions or not event.actions.state_delta:
      return
    for key, value in event.actions.state_delta.items():
      if key.startswith(State.TEMP_PREFIX):
        continue
      session.state.update({key: value})","def __update_session_state(self, session: Session, event: Event) -> None:
    """"""Updates the session state based on the event.""""""
    if not event.actions or not event.actions.state_delta:
      return
    for key, value in event.actions.state_delta.items():
      if key.startswith(State.TEMP_PREFIX):
        continue
      session.state.update({key: value})",Updates the session state based on the event.,Updates the session state based on the event.,"def __update_session_state(self, session: Session, event: Event) -> None:
    
    if not event.actions or not event.actions.state_delta:
      return
    for key, value in event.actions.state_delta.items():
      if key.startswith(State.TEMP_PREFIX):
        continue
      session.state.update({key: value})",Updates the session state based on the event.,"def __update_session_state ( self , session : Session , event : Event ) -> None : if not event . actions or not event . actions . state_delta : return for key , value in event . actions . state_delta . items ( ) : if key . startswith ( State . TEMP_PREFIX ) : continue session . state . update ( { key : value } )",Updates the session state based on the event.
/ag2/autogen/coding/docker_commandline_code_executor.py,restart,"def restart(self) -> None:
        """"""(Experimental) Restart the code executor.""""""
        self._container.restart()
        if self._container.status != ""running"":
            raise ValueError(f""Failed to restart container. Logs: {self._container.logs()}"")","def restart(self) -> None:
        """"""(Experimental) Restart the code executor.""""""
        self._container.restart()
        if self._container.status != ""running"":
            raise ValueError(f""Failed to restart container. Logs: {self._container.logs()}"")",(Experimental) Restart the code executor.,(Experimental) Restart the code executor.,"def restart(self) -> None:
        
        self._container.restart()
        if self._container.status != ""running"":
            raise ValueError(f""Failed to restart container. Logs: {self._container.logs()}"")",(Experimental) Restart the code executor.,"def restart ( self ) -> None : self . _container . restart ( ) if self . _container . status != ""running"" : raise ValueError ( f""Failed to restart container. Logs: {self._container.logs()}"" )",(Experimental) Restart the code executor.
/Sana/tools/metrics/pytorch-fid/src/pytorch_fid/fid_score.py,save_fid_stats,"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    """"""Calculates the FID of two paths""""""
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers)

    np.savez_compressed(paths[1], mu=m1, sigma=s1)","def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    """"""Calculates the FID of two paths""""""
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers)

    np.savez_compressed(paths[1], mu=m1, sigma=s1)",Calculates the FID of two paths,Calculates the FID of two paths,"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers)

    np.savez_compressed(paths[1], mu=m1, sigma=s1)",Calculates the FID of two paths,"def save_fid_stats ( paths , batch_size , device , dims , num_workers = 1 ) : if not os . path . exists ( paths [ 0 ] ) : raise RuntimeError ( ""Invalid path: %s"" % paths [ 0 ] ) if os . path . exists ( paths [ 1 ] ) : raise RuntimeError ( ""Existing output file: %s"" % paths [ 1 ] ) block_idx = InceptionV3 . BLOCK_INDEX_BY_DIM [ dims ] model = InceptionV3 ( [ block_idx ] ) . to ( device ) print ( f""Saving statistics for {paths[0]}"" ) m1 , s1 = compute_statistics_of_path ( paths [ 0 ] , model , batch_size , dims , device , num_workers ) np . savez_compressed ( paths [ 1 ] , mu = m1 , sigma = s1 )",Calculates the FID of two paths
/TinyZero/verl/protocol.py,reorder,"def reorder(self, indices):
        """"""
        Note that this operation is in-place
        """"""
        indices_np = indices.detach().numpy()
        self.batch = self.batch[indices]
        self.non_tensor_batch = {key: val[indices_np] for key, val in self.non_tensor_batch.items()}","def reorder(self, indices):
        """"""
        Note that this operation is in-place
        """"""
        indices_np = indices.detach().numpy()
        self.batch = self.batch[indices]
        self.non_tensor_batch = {key: val[indices_np] for key, val in self.non_tensor_batch.items()}",Note that this operation is in-place,Note that this operation is in-place,"def reorder(self, indices):
        
        indices_np = indices.detach().numpy()
        self.batch = self.batch[indices]
        self.non_tensor_batch = {key: val[indices_np] for key, val in self.non_tensor_batch.items()}",Note that this operation is in-place,"def reorder ( self , indices ) : indices_np = indices . detach ( ) . numpy ( ) self . batch = self . batch [ indices ] self . non_tensor_batch = { key : val [ indices_np ] for key , val in self . non_tensor_batch . items ( ) }",Note that this operation is in-place
/cursor-free-vip/reset_machine_manual.py,_update_macos_platform_uuid,"def _update_macos_platform_uuid(self, new_ids):
        """"""Update macOS Platform UUID""""""
        try:
            uuid_file = ""/var/root/Library/Preferences/SystemConfiguration/com.apple.platform.uuid.plist""
            if os.path.exists(uuid_file):
                # Use sudo to execute plutil command
                cmd = f'sudo plutil -replace ""UUID"" -string ""{new_ids[""telemetry.macMachineId""]}"" ""{uuid_file}""'
                result = os.system(cmd)
                if result == 0:
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('reset.macos_platform_uuid_updated')}{Style.RESET_ALL}"")
                else:
                    raise Exception(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.failed_to_execute_plutil_command')}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.update_macos_platform_uuid_failed', error=str(e))}{Style.RESET_ALL}"")
            raise","def _update_macos_platform_uuid(self, new_ids):
        """"""Update macOS Platform UUID""""""
        try:
            uuid_file = ""/var/root/Library/Preferences/SystemConfiguration/com.apple.platform.uuid.plist""
            if os.path.exists(uuid_file):
                # Use sudo to execute plutil command
                cmd = f'sudo plutil -replace ""UUID"" -string ""{new_ids[""telemetry.macMachineId""]}"" ""{uuid_file}""'
                result = os.system(cmd)
                if result == 0:
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('reset.macos_platform_uuid_updated')}{Style.RESET_ALL}"")
                else:
                    raise Exception(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.failed_to_execute_plutil_command')}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.update_macos_platform_uuid_failed', error=str(e))}{Style.RESET_ALL}"")
            raise",Update macOS Platform UUID,Update macOS Platform UUID,"def _update_macos_platform_uuid(self, new_ids):
        
        try:
            uuid_file = ""/var/root/Library/Preferences/SystemConfiguration/com.apple.platform.uuid.plist""
            if os.path.exists(uuid_file):
                # Use sudo to execute plutil command
                cmd = f'sudo plutil -replace ""UUID"" -string ""{new_ids[""telemetry.macMachineId""]}"" ""{uuid_file}""'
                result = os.system(cmd)
                if result == 0:
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('reset.macos_platform_uuid_updated')}{Style.RESET_ALL}"")
                else:
                    raise Exception(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.failed_to_execute_plutil_command')}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.update_macos_platform_uuid_failed', error=str(e))}{Style.RESET_ALL}"")
            raise",Update macOS Platform UUID,"def _update_macos_platform_uuid ( self , new_ids ) : try : uuid_file = ""/var/root/Library/Preferences/SystemConfiguration/com.apple.platform.uuid.plist"" if os . path . exists ( uuid_file ) : # Use sudo to execute plutil command cmd = f'sudo plutil -replace ""UUID"" -string ""{new_ids[""telemetry.macMachineId""]}"" ""{uuid_file}""' result = os . system ( cmd ) if result == 0 : print ( f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('reset.macos_platform_uuid_updated')}{Style.RESET_ALL}"" ) else : raise Exception ( f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.failed_to_execute_plutil_command')}{Style.RESET_ALL}"" ) except Exception as e : print ( f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.update_macos_platform_uuid_failed', error=str(e))}{Style.RESET_ALL}"" ) raise",Update macOS Platform UUID
/LightRAG/lightrag/api/routers/query_routes.py,to_query_params,"def to_query_params(self, is_stream: bool) -> ""QueryParam"":
        """"""Converts a QueryRequest instance into a QueryParam instance.""""""
        # Use Pydantic's `.model_dump(exclude_none=True)` to remove None values automatically
        request_data = self.model_dump(exclude_none=True, exclude={""query""})

        # Ensure `mode` and `stream` are set explicitly
        param = QueryParam(**request_data)
        param.stream = is_stream
        return param","def to_query_params(self, is_stream: bool) -> ""QueryParam"":
        """"""Converts a QueryRequest instance into a QueryParam instance.""""""
        # Use Pydantic's `.model_dump(exclude_none=True)` to remove None values automatically
        request_data = self.model_dump(exclude_none=True, exclude={""query""})

        # Ensure `mode` and `stream` are set explicitly
        param = QueryParam(**request_data)
        param.stream = is_stream
        return param",Converts a QueryRequest instance into a QueryParam instance.,Converts a QueryRequest instance into a QueryParam instance.,"def to_query_params(self, is_stream: bool) -> ""QueryParam"":
        
        # Use Pydantic's `.model_dump(exclude_none=True)` to remove None values automatically
        request_data = self.model_dump(exclude_none=True, exclude={""query""})

        # Ensure `mode` and `stream` are set explicitly
        param = QueryParam(**request_data)
        param.stream = is_stream
        return param",Converts a QueryRequest instance into a QueryParam instance.,"def to_query_params ( self , is_stream : bool ) -> ""QueryParam"" : # Use Pydantic's `.model_dump(exclude_none=True)` to remove None values automatically request_data = self . model_dump ( exclude_none = True , exclude = { ""query"" } ) # Ensure `mode` and `stream` are set explicitly param = QueryParam ( ** request_data ) param . stream = is_stream return param",Converts a QueryRequest instance into a QueryParam instance.
/F5-TTS/src/f5_tts/train/datasets/prepare_csv_wavs.py,process_audio_file,"def process_audio_file(audio_path, text, polyphone):
    """"""Process a single audio file by checking its existence and extracting duration.""""""
    if not Path(audio_path).exists():
        print(f""audio {audio_path} not found, skipping"")
        return None
    try:
        audio_duration = get_audio_duration(audio_path)
        if audio_duration <= 0:
            raise ValueError(f""Duration {audio_duration} is non-positive."")
        return (audio_path, text, audio_duration)
    except Exception as e:
        print(f""Warning: Failed to process {audio_path} due to error: {e}. Skipping corrupt file."")
        return None","def process_audio_file(audio_path, text, polyphone):
    """"""Process a single audio file by checking its existence and extracting duration.""""""
    if not Path(audio_path).exists():
        print(f""audio {audio_path} not found, skipping"")
        return None
    try:
        audio_duration = get_audio_duration(audio_path)
        if audio_duration <= 0:
            raise ValueError(f""Duration {audio_duration} is non-positive."")
        return (audio_path, text, audio_duration)
    except Exception as e:
        print(f""Warning: Failed to process {audio_path} due to error: {e}. Skipping corrupt file."")
        return None",Process a single audio file by checking its existence and extracting duration.,Process a single audio file by checking its existence and extracting duration.,"def process_audio_file(audio_path, text, polyphone):
    
    if not Path(audio_path).exists():
        print(f""audio {audio_path} not found, skipping"")
        return None
    try:
        audio_duration = get_audio_duration(audio_path)
        if audio_duration <= 0:
            raise ValueError(f""Duration {audio_duration} is non-positive."")
        return (audio_path, text, audio_duration)
    except Exception as e:
        print(f""Warning: Failed to process {audio_path} due to error: {e}. Skipping corrupt file."")
        return None",Process a single audio file by checking its existence and extracting duration.,"def process_audio_file ( audio_path , text , polyphone ) : if not Path ( audio_path ) . exists ( ) : print ( f""audio {audio_path} not found, skipping"" ) return None try : audio_duration = get_audio_duration ( audio_path ) if audio_duration <= 0 : raise ValueError ( f""Duration {audio_duration} is non-positive."" ) return ( audio_path , text , audio_duration ) except Exception as e : print ( f""Warning: Failed to process {audio_path} due to error: {e}. Skipping corrupt file."" ) return None",Process a single audio file by checking its existence and extracting duration.
/adk-samples/python/agents/brand-search-optimization/deployment/bq_populate_data.py,create_dataset_if_not_exists,"def create_dataset_if_not_exists():
    """"""Creates a BigQuery dataset if it does not already exist.""""""
    # Construct a BigQuery client object.
    dataset_id = f""{client.project}.{DATASET_ID}""
    dataset = bigquery.Dataset(dataset_id)
    dataset.location = ""US""
    client.delete_dataset(
        dataset_id, delete_contents=True, not_found_ok=True
    )  # Make an API request.
    dataset = client.create_dataset(dataset)  # Make an API request.
    print(""Created dataset {}.{}"".format(client.project, dataset.dataset_id))
    return dataset","def create_dataset_if_not_exists():
    """"""Creates a BigQuery dataset if it does not already exist.""""""
    # Construct a BigQuery client object.
    dataset_id = f""{client.project}.{DATASET_ID}""
    dataset = bigquery.Dataset(dataset_id)
    dataset.location = ""US""
    client.delete_dataset(
        dataset_id, delete_contents=True, not_found_ok=True
    )  # Make an API request.
    dataset = client.create_dataset(dataset)  # Make an API request.
    print(""Created dataset {}.{}"".format(client.project, dataset.dataset_id))
    return dataset",Creates a BigQuery dataset if it does not already exist.,Creates a BigQuery dataset if it does not already exist.,"def create_dataset_if_not_exists():
    
    # Construct a BigQuery client object.
    dataset_id = f""{client.project}.{DATASET_ID}""
    dataset = bigquery.Dataset(dataset_id)
    dataset.location = ""US""
    client.delete_dataset(
        dataset_id, delete_contents=True, not_found_ok=True
    )  # Make an API request.
    dataset = client.create_dataset(dataset)  # Make an API request.
    print(""Created dataset {}.{}"".format(client.project, dataset.dataset_id))
    return dataset",Creates a BigQuery dataset if it does not already exist.,"def create_dataset_if_not_exists ( ) : # Construct a BigQuery client object. dataset_id = f""{client.project}.{DATASET_ID}"" dataset = bigquery . Dataset ( dataset_id ) dataset . location = ""US"" client . delete_dataset ( dataset_id , delete_contents = True , not_found_ok = True ) # Make an API request. dataset = client . create_dataset ( dataset ) # Make an API request. print ( ""Created dataset {}.{}"" . format ( client . project , dataset . dataset_id ) ) return dataset",Creates a BigQuery dataset if it does not already exist.
/morphik-core/core/services/morphik_graph_service.py,_get_node_color,"def _get_node_color(self, node_type: str) -> str:
        """"""Get color for a node type to match the UI color scheme.""""""
        color_map = {
            ""person"": ""#4f46e5"",  # Indigo
            ""organization"": ""#06b6d4"",  # Cyan
            ""location"": ""#10b981"",  # Emerald
            ""date"": ""#f59e0b"",  # Amber
            ""concept"": ""#8b5cf6"",  # Violet
            ""event"": ""#ec4899"",  # Pink
            ""product"": ""#ef4444"",  # Red
            ""entity"": ""#4f46e5"",  # Indigo (for generic entities)
            ""attribute"": ""#f59e0b"",  # Amber
            ""relationship"": ""#ec4899"",  # Pink
            ""high_level_element"": ""#10b981"",  # Emerald
            ""semantic_unit"": ""#8b5cf6"",  # Violet
        }
        return color_map.get(node_type.lower(), ""#6b7280"")","def _get_node_color(self, node_type: str) -> str:
        """"""Get color for a node type to match the UI color scheme.""""""
        color_map = {
            ""person"": ""#4f46e5"",  # Indigo
            ""organization"": ""#06b6d4"",  # Cyan
            ""location"": ""#10b981"",  # Emerald
            ""date"": ""#f59e0b"",  # Amber
            ""concept"": ""#8b5cf6"",  # Violet
            ""event"": ""#ec4899"",  # Pink
            ""product"": ""#ef4444"",  # Red
            ""entity"": ""#4f46e5"",  # Indigo (for generic entities)
            ""attribute"": ""#f59e0b"",  # Amber
            ""relationship"": ""#ec4899"",  # Pink
            ""high_level_element"": ""#10b981"",  # Emerald
            ""semantic_unit"": ""#8b5cf6"",  # Violet
        }
        return color_map.get(node_type.lower(), ""#6b7280"")",Get color for a node type to match the UI color scheme.,Get color for a node type to match the UI color scheme.,"def _get_node_color(self, node_type: str) -> str:
        
        color_map = {
            ""person"": ""#4f46e5"",  # Indigo
            ""organization"": ""#06b6d4"",  # Cyan
            ""location"": ""#10b981"",  # Emerald
            ""date"": ""#f59e0b"",  # Amber
            ""concept"": ""#8b5cf6"",  # Violet
            ""event"": ""#ec4899"",  # Pink
            ""product"": ""#ef4444"",  # Red
            ""entity"": ""#4f46e5"",  # Indigo (for generic entities)
            ""attribute"": ""#f59e0b"",  # Amber
            ""relationship"": ""#ec4899"",  # Pink
            ""high_level_element"": ""#10b981"",  # Emerald
            ""semantic_unit"": ""#8b5cf6"",  # Violet
        }
        return color_map.get(node_type.lower(), ""#6b7280"")",Get color for a node type to match the UI color scheme.,"def _get_node_color ( self , node_type : str ) -> str : color_map = { ""person"" : ""#4f46e5"" , # Indigo ""organization"" : ""#06b6d4"" , # Cyan ""location"" : ""#10b981"" , # Emerald ""date"" : ""#f59e0b"" , # Amber ""concept"" : ""#8b5cf6"" , # Violet ""event"" : ""#ec4899"" , # Pink ""product"" : ""#ef4444"" , # Red ""entity"" : ""#4f46e5"" , # Indigo (for generic entities) ""attribute"" : ""#f59e0b"" , # Amber ""relationship"" : ""#ec4899"" , # Pink ""high_level_element"" : ""#10b981"" , # Emerald ""semantic_unit"" : ""#8b5cf6"" , # Violet } return color_map . get ( node_type . lower ( ) , ""#6b7280"" )",Get color for a node type to match the UI color scheme.
/morphik-core/core/tests/unit/test_tools.py,dummy_chunks,"def dummy_chunks():
    """"""Sample fake chunks for testing retrieve_chunks""""""
    return [
        FakeChunk(content=""text1"", metadata={}, score=0.5),
        FakeChunk(content=""data:image/png;base64,AAA"", metadata={""is_image"": True}, score=0.9),
        FakeChunk(content=""AAA"", metadata={""is_image"": True}, score=0.8),
    ]","def dummy_chunks():
    """"""Sample fake chunks for testing retrieve_chunks""""""
    return [
        FakeChunk(content=""text1"", metadata={}, score=0.5),
        FakeChunk(content=""data:image/png;base64,AAA"", metadata={""is_image"": True}, score=0.9),
        FakeChunk(content=""AAA"", metadata={""is_image"": True}, score=0.8),
    ]",Sample fake chunks for testing retrieve_chunks,Sample fake chunks for testing retrieve_chunks,"def dummy_chunks():
    
    return [
        FakeChunk(content=""text1"", metadata={}, score=0.5),
        FakeChunk(content=""data:image/png;base64,AAA"", metadata={""is_image"": True}, score=0.9),
        FakeChunk(content=""AAA"", metadata={""is_image"": True}, score=0.8),
    ]",Sample fake chunks for testing retrieve_chunks,"def dummy_chunks ( ) : return [ FakeChunk ( content = ""text1"" , metadata = { } , score = 0.5 ) , FakeChunk ( content = ""data:image/png;base64,AAA"" , metadata = { ""is_image"" : True } , score = 0.9 ) , FakeChunk ( content = ""AAA"" , metadata = { ""is_image"" : True } , score = 0.8 ) , ]",Sample fake chunks for testing retrieve_chunks
/NLWeb/code/retrieval/milvus_client.py,_search_by_url_sync,"def _search_by_url_sync(self, url: str, collection_name: str) -> Optional[List[str]]:
        """"""Synchronous implementation of search_by_url for thread execution""""""
        client = self._get_milvus_client()
        
        logger.debug(f""Querying collection: {collection_name} for URL: {url}"")
        res = client.query(
            collection_name=collection_name,
            filter=f""url == '{url}'"",
            limit=1,
            output_fields=[""url"", ""text"", ""name"", ""site""],
        )
        
        if len(res) == 0:
            logger.warning(f""No item found for URL: {url}"")
            return None
        
        item = res[0]
        txt = json.dumps(item[""text""])
        logger.info(f""Successfully retrieved item for URL: {url}"")
        return [item[""url""], txt, item[""name""], item[""site""]]","def _search_by_url_sync(self, url: str, collection_name: str) -> Optional[List[str]]:
        """"""Synchronous implementation of search_by_url for thread execution""""""
        client = self._get_milvus_client()
        
        logger.debug(f""Querying collection: {collection_name} for URL: {url}"")
        res = client.query(
            collection_name=collection_name,
            filter=f""url == '{url}'"",
            limit=1,
            output_fields=[""url"", ""text"", ""name"", ""site""],
        )
        
        if len(res) == 0:
            logger.warning(f""No item found for URL: {url}"")
            return None
        
        item = res[0]
        txt = json.dumps(item[""text""])
        logger.info(f""Successfully retrieved item for URL: {url}"")
        return [item[""url""], txt, item[""name""], item[""site""]]",Synchronous implementation of search_by_url for thread execution,Synchronous implementation of search_by_url for thread execution,"def _search_by_url_sync(self, url: str, collection_name: str) -> Optional[List[str]]:
        
        client = self._get_milvus_client()
        
        logger.debug(f""Querying collection: {collection_name} for URL: {url}"")
        res = client.query(
            collection_name=collection_name,
            filter=f""url == '{url}'"",
            limit=1,
            output_fields=[""url"", ""text"", ""name"", ""site""],
        )
        
        if len(res) == 0:
            logger.warning(f""No item found for URL: {url}"")
            return None
        
        item = res[0]
        txt = json.dumps(item[""text""])
        logger.info(f""Successfully retrieved item for URL: {url}"")
        return [item[""url""], txt, item[""name""], item[""site""]]",Synchronous implementation of search_by_url for thread execution,"def _search_by_url_sync ( self , url : str , collection_name : str ) -> Optional [ List [ str ] ] : client = self . _get_milvus_client ( ) logger . debug ( f""Querying collection: {collection_name} for URL: {url}"" ) res = client . query ( collection_name = collection_name , filter = f""url == '{url}'"" , limit = 1 , output_fields = [ ""url"" , ""text"" , ""name"" , ""site"" ] , ) if len ( res ) == 0 : logger . warning ( f""No item found for URL: {url}"" ) return None item = res [ 0 ] txt = json . dumps ( item [ ""text"" ] ) logger . info ( f""Successfully retrieved item for URL: {url}"" ) return [ item [ ""url"" ] , txt , item [ ""name"" ] , item [ ""site"" ] ]",Synchronous implementation of search_by_url for thread execution
/olmocr/scripts/chatgpt_tag_dolmadocs_v2.py,get_all_pages,"def get_all_pages(s3_client, document_files):
    """"""Get all pages from the document files for processing, preserving file and line order.""""""
    file_contents = {}

    # First, collect all file paths and their document info
    for file_path in tqdm(document_files, desc=""Loading document files""):
        lines = load_document_file(s3_client, file_path)
        if not lines:
            logger.warning(f""Empty or invalid file: {file_path}"")
            continue

        # Parse each line for document info
        documents = []
        for i, line in enumerate(lines):
            doc_info = get_document_info_from_line(line, file_path, i)
            # Always add an entry for each line, even if None, to preserve line alignment
            documents.append(doc_info)

        # Store all documents for this file
        file_contents[file_path] = documents
        logger.info(f""Loaded {len(documents)} documents from {file_path}"")

    logger.info(f""Loaded documents from {len(file_contents)} files"")
    return file_contents","def get_all_pages(s3_client, document_files):
    """"""Get all pages from the document files for processing, preserving file and line order.""""""
    file_contents = {}

    # First, collect all file paths and their document info
    for file_path in tqdm(document_files, desc=""Loading document files""):
        lines = load_document_file(s3_client, file_path)
        if not lines:
            logger.warning(f""Empty or invalid file: {file_path}"")
            continue

        # Parse each line for document info
        documents = []
        for i, line in enumerate(lines):
            doc_info = get_document_info_from_line(line, file_path, i)
            # Always add an entry for each line, even if None, to preserve line alignment
            documents.append(doc_info)

        # Store all documents for this file
        file_contents[file_path] = documents
        logger.info(f""Loaded {len(documents)} documents from {file_path}"")

    logger.info(f""Loaded documents from {len(file_contents)} files"")
    return file_contents","Get all pages from the document files for processing, preserving file and line order.","Get all pages from the document files for processing, preserving file and line order.","def get_all_pages(s3_client, document_files):
    
    file_contents = {}

    # First, collect all file paths and their document info
    for file_path in tqdm(document_files, desc=""Loading document files""):
        lines = load_document_file(s3_client, file_path)
        if not lines:
            logger.warning(f""Empty or invalid file: {file_path}"")
            continue

        # Parse each line for document info
        documents = []
        for i, line in enumerate(lines):
            doc_info = get_document_info_from_line(line, file_path, i)
            # Always add an entry for each line, even if None, to preserve line alignment
            documents.append(doc_info)

        # Store all documents for this file
        file_contents[file_path] = documents
        logger.info(f""Loaded {len(documents)} documents from {file_path}"")

    logger.info(f""Loaded documents from {len(file_contents)} files"")
    return file_contents","Get all pages from the document files for processing, preserving file and line order.","def get_all_pages ( s3_client , document_files ) : file_contents = { } # First, collect all file paths and their document info for file_path in tqdm ( document_files , desc = ""Loading document files"" ) : lines = load_document_file ( s3_client , file_path ) if not lines : logger . warning ( f""Empty or invalid file: {file_path}"" ) continue # Parse each line for document info documents = [ ] for i , line in enumerate ( lines ) : doc_info = get_document_info_from_line ( line , file_path , i ) # Always add an entry for each line, even if None, to preserve line alignment documents . append ( doc_info ) # Store all documents for this file file_contents [ file_path ] = documents logger . info ( f""Loaded {len(documents)} documents from {file_path}"" ) logger . info ( f""Loaded documents from {len(file_contents)} files"" ) return file_contents","Get all pages from the document files for processing, preserving file and line order."
/local-deep-research/examples/benchmarks/run_browsecomp.py,decrypt,"def decrypt(ciphertext_b64: str, password: str) -> str:
    """"""Decrypt base64-encoded ciphertext with XOR.""""""
    try:
        encrypted = base64.b64decode(ciphertext_b64)
        key = derive_key(password, len(encrypted))
        decrypted = bytes(a ^ b for a, b in zip(encrypted, key))
        return decrypted.decode()
    except Exception as e:
        logger.error(f""Error decrypting data: {str(e)}"")
        return f""Error: Could not decrypt data - {str(e)}""","def decrypt(ciphertext_b64: str, password: str) -> str:
    """"""Decrypt base64-encoded ciphertext with XOR.""""""
    try:
        encrypted = base64.b64decode(ciphertext_b64)
        key = derive_key(password, len(encrypted))
        decrypted = bytes(a ^ b for a, b in zip(encrypted, key))
        return decrypted.decode()
    except Exception as e:
        logger.error(f""Error decrypting data: {str(e)}"")
        return f""Error: Could not decrypt data - {str(e)}""",Decrypt base64-encoded ciphertext with XOR.,Decrypt base64-encoded ciphertext with XOR.,"def decrypt(ciphertext_b64: str, password: str) -> str:
    
    try:
        encrypted = base64.b64decode(ciphertext_b64)
        key = derive_key(password, len(encrypted))
        decrypted = bytes(a ^ b for a, b in zip(encrypted, key))
        return decrypted.decode()
    except Exception as e:
        logger.error(f""Error decrypting data: {str(e)}"")
        return f""Error: Could not decrypt data - {str(e)}""",Decrypt base64-encoded ciphertext with XOR.,"def decrypt ( ciphertext_b64 : str , password : str ) -> str : try : encrypted = base64 . b64decode ( ciphertext_b64 ) key = derive_key ( password , len ( encrypted ) ) decrypted = bytes ( a ^ b for a , b in zip ( encrypted , key ) ) return decrypted . decode ( ) except Exception as e : logger . error ( f""Error decrypting data: {str(e)}"" ) return f""Error: Could not decrypt data - {str(e)}""",Decrypt base64-encoded ciphertext with XOR.
/intentkit/app/admin/generator/llm_logger.py,_store_conversation_turn,"def _store_conversation_turn(
        self, prompt: str, response_content: Dict[str, Any], call_type: str
    ):
        """"""Store a conversation turn in memory.""""""
        if self.request_id not in _conversation_history:
            _conversation_history[self.request_id] = []

        # Add user message
        _conversation_history[self.request_id].append(
            {""role"": ""user"", ""content"": prompt}
        )

        # Add AI response based on call type
        ai_content = self._format_ai_response(response_content, call_type)
        if ai_content:
            _conversation_history[self.request_id].append(
                {""role"": ""assistant"", ""content"": ai_content}
            )

        # Update project metadata
        if self.request_id in _project_metadata:
            _project_metadata[self.request_id][""last_activity""] = time.time()","def _store_conversation_turn(
        self, prompt: str, response_content: Dict[str, Any], call_type: str
    ):
        """"""Store a conversation turn in memory.""""""
        if self.request_id not in _conversation_history:
            _conversation_history[self.request_id] = []

        # Add user message
        _conversation_history[self.request_id].append(
            {""role"": ""user"", ""content"": prompt}
        )

        # Add AI response based on call type
        ai_content = self._format_ai_response(response_content, call_type)
        if ai_content:
            _conversation_history[self.request_id].append(
                {""role"": ""assistant"", ""content"": ai_content}
            )

        # Update project metadata
        if self.request_id in _project_metadata:
            _project_metadata[self.request_id][""last_activity""] = time.time()",Store a conversation turn in memory.,Store a conversation turn in memory.,"def _store_conversation_turn(
        self, prompt: str, response_content: Dict[str, Any], call_type: str
    ):
        
        if self.request_id not in _conversation_history:
            _conversation_history[self.request_id] = []

        # Add user message
        _conversation_history[self.request_id].append(
            {""role"": ""user"", ""content"": prompt}
        )

        # Add AI response based on call type
        ai_content = self._format_ai_response(response_content, call_type)
        if ai_content:
            _conversation_history[self.request_id].append(
                {""role"": ""assistant"", ""content"": ai_content}
            )

        # Update project metadata
        if self.request_id in _project_metadata:
            _project_metadata[self.request_id][""last_activity""] = time.time()",Store a conversation turn in memory.,"def _store_conversation_turn ( self , prompt : str , response_content : Dict [ str , Any ] , call_type : str ) : if self . request_id not in _conversation_history : _conversation_history [ self . request_id ] = [ ] # Add user message _conversation_history [ self . request_id ] . append ( { ""role"" : ""user"" , ""content"" : prompt } ) # Add AI response based on call type ai_content = self . _format_ai_response ( response_content , call_type ) if ai_content : _conversation_history [ self . request_id ] . append ( { ""role"" : ""assistant"" , ""content"" : ai_content } ) # Update project metadata if self . request_id in _project_metadata : _project_metadata [ self . request_id ] [ ""last_activity"" ] = time . time ( )",Store a conversation turn in memory.
/Kokoro-FastAPI/api/src/services/text_processing/vocabulary.py,get_vocab,"def get_vocab():
    """"""Get the vocabulary dictionary mapping characters to token IDs""""""
    _pad = ""$""
    _punctuation = ';:,.!?¡¿—…""«»"""" '
    _letters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz""
    _letters_ipa = ""ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ""

    # Create vocabulary dictionary
    symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)
    return {symbol: i for i, symbol in enumerate(symbols)}","def get_vocab():
    """"""Get the vocabulary dictionary mapping characters to token IDs""""""
    _pad = ""$""
    _punctuation = ';:,.!?¡¿—…""«»"""" '
    _letters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz""
    _letters_ipa = ""ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ""

    # Create vocabulary dictionary
    symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)
    return {symbol: i for i, symbol in enumerate(symbols)}",Get the vocabulary dictionary mapping characters to token IDs,Get the vocabulary dictionary mapping characters to token IDs,"def get_vocab():
    
    _pad = ""$""
    _punctuation = ';:,.!?¡¿—…""«»"""" '
    _letters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz""
    _letters_ipa = ""ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ""

    # Create vocabulary dictionary
    symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)
    return {symbol: i for i, symbol in enumerate(symbols)}",Get the vocabulary dictionary mapping characters to token IDs,"def get_vocab ( ) : _pad = ""$"" _punctuation = ';:,.!?¡¿—…""«»"""" ' _letters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"" _letters_ipa = ""ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ"" # Create vocabulary dictionary symbols = [ _pad ] + list ( _punctuation ) + list ( _letters ) + list ( _letters_ipa ) return { symbol : i for i , symbol in enumerate ( symbols ) }",Get the vocabulary dictionary mapping characters to token IDs
/cua/libs/agent/agent/ui/gradio/app.py,save_settings,"def save_settings(settings: Dict[str, Any]):
    """"""Saves settings to the JSON file.""""""
    # Ensure sensitive keys are not saved
    settings.pop(""provider_api_key"", None)
    try:
        with open(SETTINGS_FILE, ""w"") as f:
            json.dump(settings, f, indent=4)
        print(f""DEBUG - Saved settings to {SETTINGS_FILE}"")
    except IOError as e:
        print(f""Warning: Could not save settings to {SETTINGS_FILE}: {e}"")","def save_settings(settings: Dict[str, Any]):
    """"""Saves settings to the JSON file.""""""
    # Ensure sensitive keys are not saved
    settings.pop(""provider_api_key"", None)
    try:
        with open(SETTINGS_FILE, ""w"") as f:
            json.dump(settings, f, indent=4)
        print(f""DEBUG - Saved settings to {SETTINGS_FILE}"")
    except IOError as e:
        print(f""Warning: Could not save settings to {SETTINGS_FILE}: {e}"")",Saves settings to the JSON file.,Saves settings to the JSON file.,"def save_settings(settings: Dict[str, Any]):
    
    # Ensure sensitive keys are not saved
    settings.pop(""provider_api_key"", None)
    try:
        with open(SETTINGS_FILE, ""w"") as f:
            json.dump(settings, f, indent=4)
        print(f""DEBUG - Saved settings to {SETTINGS_FILE}"")
    except IOError as e:
        print(f""Warning: Could not save settings to {SETTINGS_FILE}: {e}"")",Saves settings to the JSON file.,"def save_settings ( settings : Dict [ str , Any ] ) : # Ensure sensitive keys are not saved settings . pop ( ""provider_api_key"" , None ) try : with open ( SETTINGS_FILE , ""w"" ) as f : json . dump ( settings , f , indent = 4 ) print ( f""DEBUG - Saved settings to {SETTINGS_FILE}"" ) except IOError as e : print ( f""Warning: Could not save settings to {SETTINGS_FILE}: {e}"" )",Saves settings to the JSON file.
/adk-python/src/google/adk/agents/parallel_agent.py,_create_branch_ctx_for_sub_agent,"def _create_branch_ctx_for_sub_agent(
    agent: BaseAgent,
    sub_agent: BaseAgent,
    invocation_context: InvocationContext,
) -> InvocationContext:
  """"""Create isolated branch for every sub-agent.""""""
  invocation_context = invocation_context.model_copy()
  branch_suffix = f""{agent.name}.{sub_agent.name}""
  invocation_context.branch = (
      f""{invocation_context.branch}.{branch_suffix}""
      if invocation_context.branch
      else branch_suffix
  )
  return invocation_context","def _create_branch_ctx_for_sub_agent(
    agent: BaseAgent,
    sub_agent: BaseAgent,
    invocation_context: InvocationContext,
) -> InvocationContext:
  """"""Create isolated branch for every sub-agent.""""""
  invocation_context = invocation_context.model_copy()
  branch_suffix = f""{agent.name}.{sub_agent.name}""
  invocation_context.branch = (
      f""{invocation_context.branch}.{branch_suffix}""
      if invocation_context.branch
      else branch_suffix
  )
  return invocation_context",Create isolated branch for every sub-agent.,Create isolated branch for every sub-agent.,"def _create_branch_ctx_for_sub_agent(
    agent: BaseAgent,
    sub_agent: BaseAgent,
    invocation_context: InvocationContext,
) -> InvocationContext:
  
  invocation_context = invocation_context.model_copy()
  branch_suffix = f""{agent.name}.{sub_agent.name}""
  invocation_context.branch = (
      f""{invocation_context.branch}.{branch_suffix}""
      if invocation_context.branch
      else branch_suffix
  )
  return invocation_context",Create isolated branch for every sub-agent.,"def _create_branch_ctx_for_sub_agent ( agent : BaseAgent , sub_agent : BaseAgent , invocation_context : InvocationContext , ) -> InvocationContext : invocation_context = invocation_context . model_copy ( ) branch_suffix = f""{agent.name}.{sub_agent.name}"" invocation_context . branch = ( f""{invocation_context.branch}.{branch_suffix}"" if invocation_context . branch else branch_suffix ) return invocation_context",Create isolated branch for every sub-agent.
/mcp/src/amazon-keyspaces-mcp-server/awslabs/amazon_keyspaces_mcp_server/llm_context.py,build_list_tables_context,"def build_list_tables_context(keyspace_name: str, tables: List[TableInfo]) -> str:
    """"""Provide LLM context for tables.""""""
    context = {
        'cassandra_knowledge': build_cassandra_knowledge(),
        'amazon_keyspaces_knowledge': build_amazon_keyspaces_knowledge(),
    }

    # Add table-specific guidance
    tables_guidance = {
        'data_modeling': 'In Cassandra, tables are containers for related data, similar to tablesin relational databases. '
        'However, Cassandra tables  are optimized for specific access patterns based on their primary key '
        'design. The primary key determines how data is distributed physically in the database, and the '
        'attributes that can be specified for efficient query execution. Primary keys consist of a '
        'partition key (which determines data distribution) and optional cluster columns which determine '
        'how data is ordered within a partition.',
        'naming_conventions': 'Table names typically use snake_case and should be descriptive of the entity they represent.',
    }
    context['tables_guidance'] = tables_guidance

    return dict_to_markdown(context)","def build_list_tables_context(keyspace_name: str, tables: List[TableInfo]) -> str:
    """"""Provide LLM context for tables.""""""
    context = {
        'cassandra_knowledge': build_cassandra_knowledge(),
        'amazon_keyspaces_knowledge': build_amazon_keyspaces_knowledge(),
    }

    # Add table-specific guidance
    tables_guidance = {
        'data_modeling': 'In Cassandra, tables are containers for related data, similar to tablesin relational databases. '
        'However, Cassandra tables  are optimized for specific access patterns based on their primary key '
        'design. The primary key determines how data is distributed physically in the database, and the '
        'attributes that can be specified for efficient query execution. Primary keys consist of a '
        'partition key (which determines data distribution) and optional cluster columns which determine '
        'how data is ordered within a partition.',
        'naming_conventions': 'Table names typically use snake_case and should be descriptive of the entity they represent.',
    }
    context['tables_guidance'] = tables_guidance

    return dict_to_markdown(context)",Provide LLM context for tables.,Provide LLM context for tables.,"def build_list_tables_context(keyspace_name: str, tables: List[TableInfo]) -> str:
    
    context = {
        'cassandra_knowledge': build_cassandra_knowledge(),
        'amazon_keyspaces_knowledge': build_amazon_keyspaces_knowledge(),
    }

    # Add table-specific guidance
    tables_guidance = {
        'data_modeling': 'In Cassandra, tables are containers for related data, similar to tablesin relational databases. '
        'However, Cassandra tables  are optimized for specific access patterns based on their primary key '
        'design. The primary key determines how data is distributed physically in the database, and the '
        'attributes that can be specified for efficient query execution. Primary keys consist of a '
        'partition key (which determines data distribution) and optional cluster columns which determine '
        'how data is ordered within a partition.',
        'naming_conventions': 'Table names typically use snake_case and should be descriptive of the entity they represent.',
    }
    context['tables_guidance'] = tables_guidance

    return dict_to_markdown(context)",Provide LLM context for tables.,"def build_list_tables_context ( keyspace_name : str , tables : List [ TableInfo ] ) -> str : context = { 'cassandra_knowledge' : build_cassandra_knowledge ( ) , 'amazon_keyspaces_knowledge' : build_amazon_keyspaces_knowledge ( ) , } # Add table-specific guidance tables_guidance = { 'data_modeling' : 'In Cassandra, tables are containers for related data, similar to tablesin relational databases. ' 'However, Cassandra tables  are optimized for specific access patterns based on their primary key ' 'design. The primary key determines how data is distributed physically in the database, and the ' 'attributes that can be specified for efficient query execution. Primary keys consist of a ' 'partition key (which determines data distribution) and optional cluster columns which determine ' 'how data is ordered within a partition.' , 'naming_conventions' : 'Table names typically use snake_case and should be descriptive of the entity they represent.' , } context [ 'tables_guidance' ] = tables_guidance return dict_to_markdown ( context )",Provide LLM context for tables.
/owl/owl/utils/gaia.py,_generate_summary,"def _generate_summary(self) -> Dict[str, Any]:
        r""""""Generate and return a summary of the benchmark results.""""""
        correct = sum(result[""score""] for result in self._results)
        return {
            ""total"": len(self._results),
            ""correct"": correct,
            ""results"": self._results,
            ""accuracy"": correct / len(self._results) if len(self._results) > 0 else 0,
        }","def _generate_summary(self) -> Dict[str, Any]:
        r""""""Generate and return a summary of the benchmark results.""""""
        correct = sum(result[""score""] for result in self._results)
        return {
            ""total"": len(self._results),
            ""correct"": correct,
            ""results"": self._results,
            ""accuracy"": correct / len(self._results) if len(self._results) > 0 else 0,
        }",Generate and return a summary of the benchmark results.,Generate and return a summary of the benchmark results.,"def _generate_summary(self) -> Dict[str, Any]:
        
        correct = sum(result[""score""] for result in self._results)
        return {
            ""total"": len(self._results),
            ""correct"": correct,
            ""results"": self._results,
            ""accuracy"": correct / len(self._results) if len(self._results) > 0 else 0,
        }",Generate and return a summary of the benchmark results.,"def _generate_summary ( self ) -> Dict [ str , Any ] : correct = sum ( result [ ""score"" ] for result in self . _results ) return { ""total"" : len ( self . _results ) , ""correct"" : correct , ""results"" : self . _results , ""accuracy"" : correct / len ( self . _results ) if len ( self . _results ) > 0 else 0 , }",Generate and return a summary of the benchmark results.
/nesa/demo/nesa/env_setup.py,run_cmd,"def run_cmd(cmd: str, *, assert_success: bool = False, capture_output: bool = False) -> subprocess.CompletedProcess:
    """"""run a shell command and optionally assert success.""""""
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            text=True,
            capture_output=capture_output,
            check=assert_success,
        )
        return result
    except subprocess.CalledProcessError as e:
        print(f""Command failed: {cmd}\nExit code: {e.returncode}"")
        sys.exit(e.returncode)","def run_cmd(cmd: str, *, assert_success: bool = False, capture_output: bool = False) -> subprocess.CompletedProcess:
    """"""run a shell command and optionally assert success.""""""
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            text=True,
            capture_output=capture_output,
            check=assert_success,
        )
        return result
    except subprocess.CalledProcessError as e:
        print(f""Command failed: {cmd}\nExit code: {e.returncode}"")
        sys.exit(e.returncode)",run a shell command and optionally assert success.,run a shell command and optionally assert success.,"def run_cmd(cmd: str, *, assert_success: bool = False, capture_output: bool = False) -> subprocess.CompletedProcess:
    
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            text=True,
            capture_output=capture_output,
            check=assert_success,
        )
        return result
    except subprocess.CalledProcessError as e:
        print(f""Command failed: {cmd}\nExit code: {e.returncode}"")
        sys.exit(e.returncode)",run a shell command and optionally assert success.,"def run_cmd ( cmd : str , * , assert_success : bool = False , capture_output : bool = False ) -> subprocess . CompletedProcess : try : result = subprocess . run ( cmd , shell = True , text = True , capture_output = capture_output , check = assert_success , ) return result except subprocess . CalledProcessError as e : print ( f""Command failed: {cmd}\nExit code: {e.returncode}"" ) sys . exit ( e . returncode )",run a shell command and optionally assert success.
/adk-python/src/google/adk/tools/openapi_tool/openapi_spec_parser/openapi_spec_parser.py,resolve_ref,"def resolve_ref(ref_string, current_doc):
      """"""Resolves a single $ref string.""""""
      parts = ref_string.split(""/"")
      if parts[0] != ""#"":
        raise ValueError(f""External references not supported: {ref_string}"")

      current = current_doc
      for part in parts[1:]:
        if part in current:
          current = current[part]
        else:
          return None  # Reference not found
      return current","def resolve_ref(ref_string, current_doc):
      """"""Resolves a single $ref string.""""""
      parts = ref_string.split(""/"")
      if parts[0] != ""#"":
        raise ValueError(f""External references not supported: {ref_string}"")

      current = current_doc
      for part in parts[1:]:
        if part in current:
          current = current[part]
        else:
          return None  # Reference not found
      return current",Resolves a single $ref string.,Resolves a single $ref string.,"def resolve_ref(ref_string, current_doc):
      
      parts = ref_string.split(""/"")
      if parts[0] != ""#"":
        raise ValueError(f""External references not supported: {ref_string}"")

      current = current_doc
      for part in parts[1:]:
        if part in current:
          current = current[part]
        else:
          return None  # Reference not found
      return current",Resolves a single $ref string.,"def resolve_ref ( ref_string , current_doc ) : parts = ref_string . split ( ""/"" ) if parts [ 0 ] != ""#"" : raise ValueError ( f""External references not supported: {ref_string}"" ) current = current_doc for part in parts [ 1 : ] : if part in current : current = current [ part ] else : return None # Reference not found return current",Resolves a single $ref string.
/fastmcp/examples/smart_home/src/smart_home/lights/server.py,toggle_light,"def toggle_light(light_name: str, state: bool) -> dict[str, Any]:
    """"""Turns a specific light on (true) or off (false) using phue2.""""""
    if not (bridge := _get_bridge()):
        return {""error"": ""Bridge not connected"", ""success"": False}
    try:
        result = bridge.set_light(light_name, ""on"", state)
        return {
            ""light"": light_name,
            ""set_on_state"": state,
            ""success"": True,
            ""phue2_result"": result,
        }
    except (KeyError, PhueException, Exception) as e:
        return handle_phue_error(light_name, ""toggle_light"", e)","def toggle_light(light_name: str, state: bool) -> dict[str, Any]:
    """"""Turns a specific light on (true) or off (false) using phue2.""""""
    if not (bridge := _get_bridge()):
        return {""error"": ""Bridge not connected"", ""success"": False}
    try:
        result = bridge.set_light(light_name, ""on"", state)
        return {
            ""light"": light_name,
            ""set_on_state"": state,
            ""success"": True,
            ""phue2_result"": result,
        }
    except (KeyError, PhueException, Exception) as e:
        return handle_phue_error(light_name, ""toggle_light"", e)",Turns a specific light on (true) or off (false) using phue2.,Turns a specific light on (true) or off (false) using phue2.,"def toggle_light(light_name: str, state: bool) -> dict[str, Any]:
    
    if not (bridge := _get_bridge()):
        return {""error"": ""Bridge not connected"", ""success"": False}
    try:
        result = bridge.set_light(light_name, ""on"", state)
        return {
            ""light"": light_name,
            ""set_on_state"": state,
            ""success"": True,
            ""phue2_result"": result,
        }
    except (KeyError, PhueException, Exception) as e:
        return handle_phue_error(light_name, ""toggle_light"", e)",Turns a specific light on (true) or off (false) using phue2.,"def toggle_light ( light_name : str , state : bool ) -> dict [ str , Any ] : if not ( bridge := _get_bridge ( ) ) : return { ""error"" : ""Bridge not connected"" , ""success"" : False } try : result = bridge . set_light ( light_name , ""on"" , state ) return { ""light"" : light_name , ""set_on_state"" : state , ""success"" : True , ""phue2_result"" : result , } except ( KeyError , PhueException , Exception ) as e : return handle_phue_error ( light_name , ""toggle_light"" , e )",Turns a specific light on (true) or off (false) using phue2.
/ag2/notebook/mcp/mcp_filesystem.py,list_files,"def list_files(relative_path: str = """") -> List[str]:
    """"""
    List files and directories under CONTEXT_PATH/relative_path. Pass empty string to list root.
    """"""
    path = (CONTEXT_PATH / relative_path).resolve()
    if not str(path).startswith(str(CONTEXT_PATH)):
        return [f""Access denied: {relative_path}""]
    if not path.exists() or not path.is_dir():
        return [f""Not a directory: {relative_path}""]
    return os.listdir(path)","def list_files(relative_path: str = """") -> List[str]:
    """"""
    List files and directories under CONTEXT_PATH/relative_path. Pass empty string to list root.
    """"""
    path = (CONTEXT_PATH / relative_path).resolve()
    if not str(path).startswith(str(CONTEXT_PATH)):
        return [f""Access denied: {relative_path}""]
    if not path.exists() or not path.is_dir():
        return [f""Not a directory: {relative_path}""]
    return os.listdir(path)",List files and directories under CONTEXT_PATH/relative_path. Pass empty string to list root.,List files and directories under CONTEXT_PATH/relative_path.,"def list_files(relative_path: str = """") -> List[str]:
    
    path = (CONTEXT_PATH / relative_path).resolve()
    if not str(path).startswith(str(CONTEXT_PATH)):
        return [f""Access denied: {relative_path}""]
    if not path.exists() or not path.is_dir():
        return [f""Not a directory: {relative_path}""]
    return os.listdir(path)",List files and directories under CONTEXT_PATH/relative_path.,"def list_files ( relative_path : str = """" ) -> List [ str ] : path = ( CONTEXT_PATH / relative_path ) . resolve ( ) if not str ( path ) . startswith ( str ( CONTEXT_PATH ) ) : return [ f""Access denied: {relative_path}"" ] if not path . exists ( ) or not path . is_dir ( ) : return [ f""Not a directory: {relative_path}"" ] return os . listdir ( path )",List files and directories under CONTEXT_PATH/relative_path.
/KAG/kag/tools/algorithm_tool/chunk_retriever/ppr_chunk_retriever.py,process_query,"def process_query(ner_query):
            """"""Process a single query in parallel.""""""
            candidate_entities = self.ner.invoke(ner_query, **kwargs)
            for candidate_entity in candidate_entities:
                query_type = candidate_entity.get_entity_first_type_or_un_std()

                ner_id = f""{candidate_entity.entity_name}_{query_type}""
                if ner_id not in ner_maps:
                    ner_maps[ner_id] = {
                        ""candidate"": candidate_entity,
                        ""query"": ner_query,
                        ""query_type"": query_type,
                    }","def process_query(ner_query):
            """"""Process a single query in parallel.""""""
            candidate_entities = self.ner.invoke(ner_query, **kwargs)
            for candidate_entity in candidate_entities:
                query_type = candidate_entity.get_entity_first_type_or_un_std()

                ner_id = f""{candidate_entity.entity_name}_{query_type}""
                if ner_id not in ner_maps:
                    ner_maps[ner_id] = {
                        ""candidate"": candidate_entity,
                        ""query"": ner_query,
                        ""query_type"": query_type,
                    }",Process a single query in parallel.,Process a single query in parallel.,"def process_query(ner_query):
            
            candidate_entities = self.ner.invoke(ner_query, **kwargs)
            for candidate_entity in candidate_entities:
                query_type = candidate_entity.get_entity_first_type_or_un_std()

                ner_id = f""{candidate_entity.entity_name}_{query_type}""
                if ner_id not in ner_maps:
                    ner_maps[ner_id] = {
                        ""candidate"": candidate_entity,
                        ""query"": ner_query,
                        ""query_type"": query_type,
                    }",Process a single query in parallel.,"def process_query ( ner_query ) : candidate_entities = self . ner . invoke ( ner_query , ** kwargs ) for candidate_entity in candidate_entities : query_type = candidate_entity . get_entity_first_type_or_un_std ( ) ner_id = f""{candidate_entity.entity_name}_{query_type}"" if ner_id not in ner_maps : ner_maps [ ner_id ] = { ""candidate"" : candidate_entity , ""query"" : ner_query , ""query_type"" : query_type , }",Process a single query in parallel.
/airweave/backend/airweave/platform/sources/jira.py,_create_project_entity,"def _create_project_entity(self, project_data):
        """"""Transform raw project data into a JiraProjectEntity.""""""
        logger.debug(
            f""Creating project entity for: {project_data.get('key')} - {project_data.get('name')}""
        )
        # Use a composite ID format that includes the entity type for uniqueness
        entity_id = f""project-{project_data['id']}""

        return JiraProjectEntity(
            entity_id=entity_id,  # Modified to use unique ID
            breadcrumbs=[],  # top-level object, no parent
            project_key=project_data[""key""],
            name=project_data.get(""name""),
            description=project_data.get(""description""),
        )","def _create_project_entity(self, project_data):
        """"""Transform raw project data into a JiraProjectEntity.""""""
        logger.debug(
            f""Creating project entity for: {project_data.get('key')} - {project_data.get('name')}""
        )
        # Use a composite ID format that includes the entity type for uniqueness
        entity_id = f""project-{project_data['id']}""

        return JiraProjectEntity(
            entity_id=entity_id,  # Modified to use unique ID
            breadcrumbs=[],  # top-level object, no parent
            project_key=project_data[""key""],
            name=project_data.get(""name""),
            description=project_data.get(""description""),
        )",Transform raw project data into a JiraProjectEntity.,Transform raw project data into a JiraProjectEntity.,"def _create_project_entity(self, project_data):
        
        logger.debug(
            f""Creating project entity for: {project_data.get('key')} - {project_data.get('name')}""
        )
        # Use a composite ID format that includes the entity type for uniqueness
        entity_id = f""project-{project_data['id']}""

        return JiraProjectEntity(
            entity_id=entity_id,  # Modified to use unique ID
            breadcrumbs=[],  # top-level object, no parent
            project_key=project_data[""key""],
            name=project_data.get(""name""),
            description=project_data.get(""description""),
        )",Transform raw project data into a JiraProjectEntity.,"def _create_project_entity ( self , project_data ) : logger . debug ( f""Creating project entity for: {project_data.get('key')} - {project_data.get('name')}"" ) # Use a composite ID format that includes the entity type for uniqueness entity_id = f""project-{project_data['id']}"" return JiraProjectEntity ( entity_id = entity_id , # Modified to use unique ID breadcrumbs = [ ] , # top-level object, no parent project_key = project_data [ ""key"" ] , name = project_data . get ( ""name"" ) , description = project_data . get ( ""description"" ) , )",Transform raw project data into a JiraProjectEntity.
/mcp-agent/src/mcp_agent/workflows/intent_classifier/intent_classifier_llm.py,_generate_context,"def _generate_context(self) -> str:
        """"""Generate a formatted context string describing all intents""""""
        context_parts = []

        for idx, intent in enumerate(self.intents.values(), 1):
            description = (
                f""{idx}. Intent: {intent.name}\nDescription: {intent.description}""
            )

            if intent.examples:
                examples = ""\n"".join(f""- {example}"" for example in intent.examples)
                description += f""\nExamples:\n{examples}""

            if intent.metadata:
                metadata = ""\n"".join(
                    f""- {key}: {value}"" for key, value in intent.metadata.items()
                )
                description += f""\nAdditional Information:\n{metadata}""

            context_parts.append(description)

        return ""\n\n"".join(context_parts)","def _generate_context(self) -> str:
        """"""Generate a formatted context string describing all intents""""""
        context_parts = []

        for idx, intent in enumerate(self.intents.values(), 1):
            description = (
                f""{idx}. Intent: {intent.name}\nDescription: {intent.description}""
            )

            if intent.examples:
                examples = ""\n"".join(f""- {example}"" for example in intent.examples)
                description += f""\nExamples:\n{examples}""

            if intent.metadata:
                metadata = ""\n"".join(
                    f""- {key}: {value}"" for key, value in intent.metadata.items()
                )
                description += f""\nAdditional Information:\n{metadata}""

            context_parts.append(description)

        return ""\n\n"".join(context_parts)",Generate a formatted context string describing all intents,Generate a formatted context string describing all intents,"def _generate_context(self) -> str:
        
        context_parts = []

        for idx, intent in enumerate(self.intents.values(), 1):
            description = (
                f""{idx}. Intent: {intent.name}\nDescription: {intent.description}""
            )

            if intent.examples:
                examples = ""\n"".join(f""- {example}"" for example in intent.examples)
                description += f""\nExamples:\n{examples}""

            if intent.metadata:
                metadata = ""\n"".join(
                    f""- {key}: {value}"" for key, value in intent.metadata.items()
                )
                description += f""\nAdditional Information:\n{metadata}""

            context_parts.append(description)

        return ""\n\n"".join(context_parts)",Generate a formatted context string describing all intents,"def _generate_context ( self ) -> str : context_parts = [ ] for idx , intent in enumerate ( self . intents . values ( ) , 1 ) : description = ( f""{idx}. Intent: {intent.name}\nDescription: {intent.description}"" ) if intent . examples : examples = ""\n"" . join ( f""- {example}"" for example in intent . examples ) description += f""\nExamples:\n{examples}"" if intent . metadata : metadata = ""\n"" . join ( f""- {key}: {value}"" for key , value in intent . metadata . items ( ) ) description += f""\nAdditional Information:\n{metadata}"" context_parts . append ( description ) return ""\n\n"" . join ( context_parts )",Generate a formatted context string describing all intents
/airweave/backend/airweave/schemas/source_connection.py,from_orm_with_collection_mapping,"def from_orm_with_collection_mapping(cls, obj):
        """"""Create a SourceConnection from a source_connection ORM model.""""""
        # Convert to dict and filter out SQLAlchemy internal attributes
        obj_dict = {k: v for k, v in obj.__dict__.items() if not k.startswith(""_"")}

        # Map the readable_collection_id to collection if needed
        if hasattr(obj, ""readable_collection_id""):
            obj_dict[""collection""] = obj.readable_collection_id

        return cls.model_validate(obj_dict)","def from_orm_with_collection_mapping(cls, obj):
        """"""Create a SourceConnection from a source_connection ORM model.""""""
        # Convert to dict and filter out SQLAlchemy internal attributes
        obj_dict = {k: v for k, v in obj.__dict__.items() if not k.startswith(""_"")}

        # Map the readable_collection_id to collection if needed
        if hasattr(obj, ""readable_collection_id""):
            obj_dict[""collection""] = obj.readable_collection_id

        return cls.model_validate(obj_dict)",Create a SourceConnection from a source_connection ORM model.,Create a SourceConnection from a source_connection ORM model.,"def from_orm_with_collection_mapping(cls, obj):
        
        # Convert to dict and filter out SQLAlchemy internal attributes
        obj_dict = {k: v for k, v in obj.__dict__.items() if not k.startswith(""_"")}

        # Map the readable_collection_id to collection if needed
        if hasattr(obj, ""readable_collection_id""):
            obj_dict[""collection""] = obj.readable_collection_id

        return cls.model_validate(obj_dict)",Create a SourceConnection from a source_connection ORM model.,"def from_orm_with_collection_mapping ( cls , obj ) : # Convert to dict and filter out SQLAlchemy internal attributes obj_dict = { k : v for k , v in obj . __dict__ . items ( ) if not k . startswith ( ""_"" ) } # Map the readable_collection_id to collection if needed if hasattr ( obj , ""readable_collection_id"" ) : obj_dict [ ""collection"" ] = obj . readable_collection_id return cls . model_validate ( obj_dict )",Create a SourceConnection from a source_connection ORM model.
/ag2/autogen/agentchat/group/targets/group_manager_target.py,_replace_agentlist_placeholder,"def _replace_agentlist_placeholder(cls: Type[""GroupManagerSelectionMessageContextStr""], v: Any) -> Union[str, Any]:  # noqa: N805
        """"""Replace {agentlist} placeholder before validation/assignment.""""""
        if isinstance(v, str):
            if ""{agentlist}"" in v:
                return v.replace(""{agentlist}"", ""<<agent_list>>"")  # Perform the replacement
            else:
                return v  # If no replacement is needed, return the original value
        return """"","def _replace_agentlist_placeholder(cls: Type[""GroupManagerSelectionMessageContextStr""], v: Any) -> Union[str, Any]:  # noqa: N805
        """"""Replace {agentlist} placeholder before validation/assignment.""""""
        if isinstance(v, str):
            if ""{agentlist}"" in v:
                return v.replace(""{agentlist}"", ""<<agent_list>>"")  # Perform the replacement
            else:
                return v  # If no replacement is needed, return the original value
        return """"",Replace {agentlist} placeholder before validation/assignment.,Replace {agentlist} placeholder before validation/assignment.,"def _replace_agentlist_placeholder(cls: Type[""GroupManagerSelectionMessageContextStr""], v: Any) -> Union[str, Any]:  # noqa: N805
        
        if isinstance(v, str):
            if ""{agentlist}"" in v:
                return v.replace(""{agentlist}"", ""<<agent_list>>"")  # Perform the replacement
            else:
                return v  # If no replacement is needed, return the original value
        return """"",Replace {agentlist} placeholder before validation/assignment.,"def _replace_agentlist_placeholder ( cls : Type [ ""GroupManagerSelectionMessageContextStr"" ] , v : Any ) -> Union [ str , Any ] : # noqa: N805 if isinstance ( v , str ) : if ""{agentlist}"" in v : return v . replace ( ""{agentlist}"" , ""<<agent_list>>"" ) # Perform the replacement else : return v # If no replacement is needed, return the original value return """"",Replace {agentlist} placeholder before validation/assignment.
/nexa-sdk/nexa/gguf/outetts/wav_tokenizer/decoder/pretrained_model.py,from_pretrained0802,"def from_pretrained0802(self, config_path, model_path):
        """"""
        Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.
        """"""
        model = self.from_hparams0802(config_path)
        state_dict_raw = torch.load(model_path, map_location=""cpu"")['state_dict']
        state_dict = dict()
        for k, v in state_dict_raw.items():
            if k.startswith('backbone.') or k.startswith('head.') or k.startswith('feature_extractor.'):
                state_dict[k] = v
        # if isinstance(model.feature_extractor, EncodecFeatures):
        #     encodec_parameters = {
        #         ""feature_extractor.encodec."" + key: value
        #         for key, value in model.feature_extractor.encodec.state_dict().items()
        #     }
        #     state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model","def from_pretrained0802(self, config_path, model_path):
        """"""
        Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.
        """"""
        model = self.from_hparams0802(config_path)
        state_dict_raw = torch.load(model_path, map_location=""cpu"")['state_dict']
        state_dict = dict()
        for k, v in state_dict_raw.items():
            if k.startswith('backbone.') or k.startswith('head.') or k.startswith('feature_extractor.'):
                state_dict[k] = v
        # if isinstance(model.feature_extractor, EncodecFeatures):
        #     encodec_parameters = {
        #         ""feature_extractor.encodec."" + key: value
        #         for key, value in model.feature_extractor.encodec.state_dict().items()
        #     }
        #     state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model",Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.,Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.,"def from_pretrained0802(self, config_path, model_path):
        
        model = self.from_hparams0802(config_path)
        state_dict_raw = torch.load(model_path, map_location=""cpu"")['state_dict']
        state_dict = dict()
        for k, v in state_dict_raw.items():
            if k.startswith('backbone.') or k.startswith('head.') or k.startswith('feature_extractor.'):
                state_dict[k] = v
        # if isinstance(model.feature_extractor, EncodecFeatures):
        #     encodec_parameters = {
        #         ""feature_extractor.encodec."" + key: value
        #         for key, value in model.feature_extractor.encodec.state_dict().items()
        #     }
        #     state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model",Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.,"def from_pretrained0802 ( self , config_path , model_path ) : model = self . from_hparams0802 ( config_path ) state_dict_raw = torch . load ( model_path , map_location = ""cpu"" ) [ 'state_dict' ] state_dict = dict ( ) for k , v in state_dict_raw . items ( ) : if k . startswith ( 'backbone.' ) or k . startswith ( 'head.' ) or k . startswith ( 'feature_extractor.' ) : state_dict [ k ] = v # if isinstance(model.feature_extractor, EncodecFeatures): #     encodec_parameters = { #         ""feature_extractor.encodec."" + key: value #         for key, value in model.feature_extractor.encodec.state_dict().items() #     } #     state_dict.update(encodec_parameters) model . load_state_dict ( state_dict ) model . eval ( ) return model",Class method to create a new Vocos model instance from a pre-trained model stored in the Hugging Face model hub.
/mcp-agent/src/mcp_agent/workflows/llm/augmented_llm_google.py,from_mcp_tool_result,"def from_mcp_tool_result(
        cls, result: CallToolResult, tool_use_id: str
    ) -> types.Content:
        """"""Convert an MCP tool result to an LLM input type""""""
        if result.isError:
            function_response = {""error"": str(result.content)}
        else:
            function_response_parts = mcp_content_to_google_parts(result.content)
            function_response = {""result"": function_response_parts}

        function_response_part = types.Part.from_function_response(
            name=tool_use_id,
            response=function_response,
        )

        function_response_content = types.Content(
            role=""tool"", parts=[function_response_part]
        )

        return function_response_content","def from_mcp_tool_result(
        cls, result: CallToolResult, tool_use_id: str
    ) -> types.Content:
        """"""Convert an MCP tool result to an LLM input type""""""
        if result.isError:
            function_response = {""error"": str(result.content)}
        else:
            function_response_parts = mcp_content_to_google_parts(result.content)
            function_response = {""result"": function_response_parts}

        function_response_part = types.Part.from_function_response(
            name=tool_use_id,
            response=function_response,
        )

        function_response_content = types.Content(
            role=""tool"", parts=[function_response_part]
        )

        return function_response_content",Convert an MCP tool result to an LLM input type,Convert an MCP tool result to an LLM input type,"def from_mcp_tool_result(
        cls, result: CallToolResult, tool_use_id: str
    ) -> types.Content:
        
        if result.isError:
            function_response = {""error"": str(result.content)}
        else:
            function_response_parts = mcp_content_to_google_parts(result.content)
            function_response = {""result"": function_response_parts}

        function_response_part = types.Part.from_function_response(
            name=tool_use_id,
            response=function_response,
        )

        function_response_content = types.Content(
            role=""tool"", parts=[function_response_part]
        )

        return function_response_content",Convert an MCP tool result to an LLM input type,"def from_mcp_tool_result ( cls , result : CallToolResult , tool_use_id : str ) -> types . Content : if result . isError : function_response = { ""error"" : str ( result . content ) } else : function_response_parts = mcp_content_to_google_parts ( result . content ) function_response = { ""result"" : function_response_parts } function_response_part = types . Part . from_function_response ( name = tool_use_id , response = function_response , ) function_response_content = types . Content ( role = ""tool"" , parts = [ function_response_part ] ) return function_response_content",Convert an MCP tool result to an LLM input type
/nesa/demo/modules/shared.py,load_user_config,"def load_user_config():
    '''
    Loads custom model-specific settings
    '''
    if Path(f'{args.model_dir}/config-user.yaml').exists():
        file_content = open(f'{args.model_dir}/config-user.yaml', 'r').read().strip()

        if file_content:
            user_config = yaml.safe_load(file_content)
        else:
            user_config = {}
    else:
        user_config = {}

    for model_name in user_config:
        user_config[model_name] = transform_legacy_kv_cache_options(user_config[model_name])

    return user_config","def load_user_config():
    '''
    Loads custom model-specific settings
    '''
    if Path(f'{args.model_dir}/config-user.yaml').exists():
        file_content = open(f'{args.model_dir}/config-user.yaml', 'r').read().strip()

        if file_content:
            user_config = yaml.safe_load(file_content)
        else:
            user_config = {}
    else:
        user_config = {}

    for model_name in user_config:
        user_config[model_name] = transform_legacy_kv_cache_options(user_config[model_name])

    return user_config",Loads custom model-specific settings,Loads custom model-specific settings,"def load_user_config():
    
    if Path(f'{args.model_dir}/config-user.yaml').exists():
        file_content = open(f'{args.model_dir}/config-user.yaml', 'r').read().strip()

        if file_content:
            user_config = yaml.safe_load(file_content)
        else:
            user_config = {}
    else:
        user_config = {}

    for model_name in user_config:
        user_config[model_name] = transform_legacy_kv_cache_options(user_config[model_name])

    return user_config",Loads custom model-specific settings,"def load_user_config ( ) : if Path ( f'{args.model_dir}/config-user.yaml' ) . exists ( ) : file_content = open ( f'{args.model_dir}/config-user.yaml' , 'r' ) . read ( ) . strip ( ) if file_content : user_config = yaml . safe_load ( file_content ) else : user_config = { } else : user_config = { } for model_name in user_config : user_config [ model_name ] = transform_legacy_kv_cache_options ( user_config [ model_name ] ) return user_config",Loads custom model-specific settings
/OpenManus-RL/openmanus_rl/agentgym/agentenv-webarena/webarena/evaluation_harness/helper_functions.py,shopping_get_sku_latest_review_author,"def shopping_get_sku_latest_review_author(sku: str) -> str:
    """"""Get the latest review for shopping admin.""""""
    header = {
        ""Authorization"": f""Bearer {shopping_get_auth_token()}"",
        ""Content-Type"": ""application/json"",
    }
    response = requests.get(
        f""{SHOPPING}/rest/V1/products/{sku}/reviews"", headers=header
    )
    assert response.status_code == 200
    response_obj = response.json()
    if len(response_obj) == 0:
        return """"
    author: str = response_obj[-1][""nickname""]
    return author","def shopping_get_sku_latest_review_author(sku: str) -> str:
    """"""Get the latest review for shopping admin.""""""
    header = {
        ""Authorization"": f""Bearer {shopping_get_auth_token()}"",
        ""Content-Type"": ""application/json"",
    }
    response = requests.get(
        f""{SHOPPING}/rest/V1/products/{sku}/reviews"", headers=header
    )
    assert response.status_code == 200
    response_obj = response.json()
    if len(response_obj) == 0:
        return """"
    author: str = response_obj[-1][""nickname""]
    return author",Get the latest review for shopping admin.,Get the latest review for shopping admin.,"def shopping_get_sku_latest_review_author(sku: str) -> str:
    
    header = {
        ""Authorization"": f""Bearer {shopping_get_auth_token()}"",
        ""Content-Type"": ""application/json"",
    }
    response = requests.get(
        f""{SHOPPING}/rest/V1/products/{sku}/reviews"", headers=header
    )
    assert response.status_code == 200
    response_obj = response.json()
    if len(response_obj) == 0:
        return """"
    author: str = response_obj[-1][""nickname""]
    return author",Get the latest review for shopping admin.,"def shopping_get_sku_latest_review_author ( sku : str ) -> str : header = { ""Authorization"" : f""Bearer {shopping_get_auth_token()}"" , ""Content-Type"" : ""application/json"" , } response = requests . get ( f""{SHOPPING}/rest/V1/products/{sku}/reviews"" , headers = header ) assert response . status_code == 200 response_obj = response . json ( ) if len ( response_obj ) == 0 : return """" author : str = response_obj [ - 1 ] [ ""nickname"" ] return author",Get the latest review for shopping admin.
/cua/libs/som/som/ocr.py,__init__,"def __init__(self):
        """"""Initialize the OCR processor.""""""
        self.reader = None
        # Determine best available device
        self.device = ""cpu""
        if torch.cuda.is_available():
            self.device = ""cuda""
        elif (
            hasattr(torch, ""backends"")
            and hasattr(torch.backends, ""mps"")
            and torch.backends.mps.is_available()
        ):
            self.device = ""mps""
        logger.info(f""OCR processor initialized with device: {self.device}"")","def __init__(self):
        """"""Initialize the OCR processor.""""""
        self.reader = None
        # Determine best available device
        self.device = ""cpu""
        if torch.cuda.is_available():
            self.device = ""cuda""
        elif (
            hasattr(torch, ""backends"")
            and hasattr(torch.backends, ""mps"")
            and torch.backends.mps.is_available()
        ):
            self.device = ""mps""
        logger.info(f""OCR processor initialized with device: {self.device}"")",Initialize the OCR processor.,Initialize the OCR processor.,"def __init__(self):
        
        self.reader = None
        # Determine best available device
        self.device = ""cpu""
        if torch.cuda.is_available():
            self.device = ""cuda""
        elif (
            hasattr(torch, ""backends"")
            and hasattr(torch.backends, ""mps"")
            and torch.backends.mps.is_available()
        ):
            self.device = ""mps""
        logger.info(f""OCR processor initialized with device: {self.device}"")",Initialize the OCR processor.,"def __init__ ( self ) : self . reader = None # Determine best available device self . device = ""cpu"" if torch . cuda . is_available ( ) : self . device = ""cuda"" elif ( hasattr ( torch , ""backends"" ) and hasattr ( torch . backends , ""mps"" ) and torch . backends . mps . is_available ( ) ) : self . device = ""mps"" logger . info ( f""OCR processor initialized with device: {self.device}"" )",Initialize the OCR processor.
/python-sdk/tests/server/auth/test_error_handling.py,pkce_challenge,"def pkce_challenge():
    """"""Create a PKCE challenge with code_verifier and code_challenge.""""""
    import base64
    import hashlib
    import secrets

    # Generate a code verifier
    code_verifier = secrets.token_urlsafe(64)[:128]

    # Create code challenge using S256 method
    code_verifier_bytes = code_verifier.encode(""ascii"")
    sha256 = hashlib.sha256(code_verifier_bytes).digest()
    code_challenge = base64.urlsafe_b64encode(sha256).decode().rstrip(""="")

    return {""code_verifier"": code_verifier, ""code_challenge"": code_challenge}","def pkce_challenge():
    """"""Create a PKCE challenge with code_verifier and code_challenge.""""""
    import base64
    import hashlib
    import secrets

    # Generate a code verifier
    code_verifier = secrets.token_urlsafe(64)[:128]

    # Create code challenge using S256 method
    code_verifier_bytes = code_verifier.encode(""ascii"")
    sha256 = hashlib.sha256(code_verifier_bytes).digest()
    code_challenge = base64.urlsafe_b64encode(sha256).decode().rstrip(""="")

    return {""code_verifier"": code_verifier, ""code_challenge"": code_challenge}",Create a PKCE challenge with code_verifier and code_challenge.,Create a PKCE challenge with code_verifier and code_challenge.,"def pkce_challenge():
    
    import base64
    import hashlib
    import secrets

    # Generate a code verifier
    code_verifier = secrets.token_urlsafe(64)[:128]

    # Create code challenge using S256 method
    code_verifier_bytes = code_verifier.encode(""ascii"")
    sha256 = hashlib.sha256(code_verifier_bytes).digest()
    code_challenge = base64.urlsafe_b64encode(sha256).decode().rstrip(""="")

    return {""code_verifier"": code_verifier, ""code_challenge"": code_challenge}",Create a PKCE challenge with code_verifier and code_challenge.,"def pkce_challenge ( ) : import base64 import hashlib import secrets # Generate a code verifier code_verifier = secrets . token_urlsafe ( 64 ) [ : 128 ] # Create code challenge using S256 method code_verifier_bytes = code_verifier . encode ( ""ascii"" ) sha256 = hashlib . sha256 ( code_verifier_bytes ) . digest ( ) code_challenge = base64 . urlsafe_b64encode ( sha256 ) . decode ( ) . rstrip ( ""="" ) return { ""code_verifier"" : code_verifier , ""code_challenge"" : code_challenge }",Create a PKCE challenge with code_verifier and code_challenge.
/python-sdk/src/mcp/server/fastmcp/tools/tool_manager.py,add_tool,"def add_tool(
        self,
        fn: Callable[..., Any],
        name: str | None = None,
        description: str | None = None,
        annotations: ToolAnnotations | None = None,
    ) -> Tool:
        """"""Add a tool to the server.""""""
        tool = Tool.from_function(
            fn, name=name, description=description, annotations=annotations
        )
        existing = self._tools.get(tool.name)
        if existing:
            if self.warn_on_duplicate_tools:
                logger.warning(f""Tool already exists: {tool.name}"")
            return existing
        self._tools[tool.name] = tool
        return tool","def add_tool(
        self,
        fn: Callable[..., Any],
        name: str | None = None,
        description: str | None = None,
        annotations: ToolAnnotations | None = None,
    ) -> Tool:
        """"""Add a tool to the server.""""""
        tool = Tool.from_function(
            fn, name=name, description=description, annotations=annotations
        )
        existing = self._tools.get(tool.name)
        if existing:
            if self.warn_on_duplicate_tools:
                logger.warning(f""Tool already exists: {tool.name}"")
            return existing
        self._tools[tool.name] = tool
        return tool",Add a tool to the server.,Add a tool to the server.,"def add_tool(
        self,
        fn: Callable[..., Any],
        name: str | None = None,
        description: str | None = None,
        annotations: ToolAnnotations | None = None,
    ) -> Tool:
        
        tool = Tool.from_function(
            fn, name=name, description=description, annotations=annotations
        )
        existing = self._tools.get(tool.name)
        if existing:
            if self.warn_on_duplicate_tools:
                logger.warning(f""Tool already exists: {tool.name}"")
            return existing
        self._tools[tool.name] = tool
        return tool",Add a tool to the server.,"def add_tool ( self , fn : Callable [ ... , Any ] , name : str | None = None , description : str | None = None , annotations : ToolAnnotations | None = None , ) -> Tool : tool = Tool . from_function ( fn , name = name , description = description , annotations = annotations ) existing = self . _tools . get ( tool . name ) if existing : if self . warn_on_duplicate_tools : logger . warning ( f""Tool already exists: {tool.name}"" ) return existing self . _tools [ tool . name ] = tool return tool",Add a tool to the server.
/BabelDOC/babeldoc/translation_config.py,cleanup_temp_files,"def cleanup_temp_files(self):
        """"""Clean up all temporary files including part working directories""""""
        try:
            for part_index in list(self._part_working_dirs.keys()):
                self.cleanup_part_working_dir(part_index)
            if self._is_temp_dir:
                logger.info(f""cleanup temp files: {self.working_dir}"")
                shutil.rmtree(self.working_dir)
        except Exception:
            logger.exception(""Error cleaning up temporary files"")","def cleanup_temp_files(self):
        """"""Clean up all temporary files including part working directories""""""
        try:
            for part_index in list(self._part_working_dirs.keys()):
                self.cleanup_part_working_dir(part_index)
            if self._is_temp_dir:
                logger.info(f""cleanup temp files: {self.working_dir}"")
                shutil.rmtree(self.working_dir)
        except Exception:
            logger.exception(""Error cleaning up temporary files"")",Clean up all temporary files including part working directories,Clean up all temporary files including part working directories,"def cleanup_temp_files(self):
        
        try:
            for part_index in list(self._part_working_dirs.keys()):
                self.cleanup_part_working_dir(part_index)
            if self._is_temp_dir:
                logger.info(f""cleanup temp files: {self.working_dir}"")
                shutil.rmtree(self.working_dir)
        except Exception:
            logger.exception(""Error cleaning up temporary files"")",Clean up all temporary files including part working directories,"def cleanup_temp_files ( self ) : try : for part_index in list ( self . _part_working_dirs . keys ( ) ) : self . cleanup_part_working_dir ( part_index ) if self . _is_temp_dir : logger . info ( f""cleanup temp files: {self.working_dir}"" ) shutil . rmtree ( self . working_dir ) except Exception : logger . exception ( ""Error cleaning up temporary files"" )",Clean up all temporary files including part working directories
/Kokoro-FastAPI/examples/assorted_checks/validate_wavs.py,print_validation_result,"def print_validation_result(result: dict, rel_path: Path):
    """"""Print full validation details for a single file.""""""
    print(f""\nValidating: {rel_path}"")
    if ""error"" in result:
        print(f""Error: {result['error']}"")
    else:
        print(f""Duration: {result['duration']}"")
        print(f""Sample Rate: {result['sample_rate']} Hz"")
        print(f""Peak Amplitude: {result['peak_amplitude']}"")
        print(f""RMS Level: {result['rms_level']}"")
        print(f""DC Offset: {result['dc_offset']}"")

        if result[""issues""]:
            print(""\nIssues Found:"")
            for issue in result[""issues""]:
                print(f""- {issue}"")
        else:
            print(""\nNo issues found"")","def print_validation_result(result: dict, rel_path: Path):
    """"""Print full validation details for a single file.""""""
    print(f""\nValidating: {rel_path}"")
    if ""error"" in result:
        print(f""Error: {result['error']}"")
    else:
        print(f""Duration: {result['duration']}"")
        print(f""Sample Rate: {result['sample_rate']} Hz"")
        print(f""Peak Amplitude: {result['peak_amplitude']}"")
        print(f""RMS Level: {result['rms_level']}"")
        print(f""DC Offset: {result['dc_offset']}"")

        if result[""issues""]:
            print(""\nIssues Found:"")
            for issue in result[""issues""]:
                print(f""- {issue}"")
        else:
            print(""\nNo issues found"")",Print full validation details for a single file.,Print full validation details for a single file.,"def print_validation_result(result: dict, rel_path: Path):
    
    print(f""\nValidating: {rel_path}"")
    if ""error"" in result:
        print(f""Error: {result['error']}"")
    else:
        print(f""Duration: {result['duration']}"")
        print(f""Sample Rate: {result['sample_rate']} Hz"")
        print(f""Peak Amplitude: {result['peak_amplitude']}"")
        print(f""RMS Level: {result['rms_level']}"")
        print(f""DC Offset: {result['dc_offset']}"")

        if result[""issues""]:
            print(""\nIssues Found:"")
            for issue in result[""issues""]:
                print(f""- {issue}"")
        else:
            print(""\nNo issues found"")",Print full validation details for a single file.,"def print_validation_result ( result : dict , rel_path : Path ) : print ( f""\nValidating: {rel_path}"" ) if ""error"" in result : print ( f""Error: {result['error']}"" ) else : print ( f""Duration: {result['duration']}"" ) print ( f""Sample Rate: {result['sample_rate']} Hz"" ) print ( f""Peak Amplitude: {result['peak_amplitude']}"" ) print ( f""RMS Level: {result['rms_level']}"" ) print ( f""DC Offset: {result['dc_offset']}"" ) if result [ ""issues"" ] : print ( ""\nIssues Found:"" ) for issue in result [ ""issues"" ] : print ( f""- {issue}"" ) else : print ( ""\nNo issues found"" )",Print full validation details for a single file.
/intentkit/skills/venice_image/base.py,get_api_key,"def get_api_key(self, context: SkillContext) -> Optional[str]:
        """"""Get the API key, prioritizing agent config then system config.""""""
        # Check agent config first
        agent_api_key = context.config.get(""api_key"")
        if agent_api_key:
            logger.debug(f""Using agent-specific Venice API key for skill {self.name}"")
            return agent_api_key

        # Fallback to system config
        system_api_key = self.skill_store.get_system_config(""venice_api_key"")
        if system_api_key:
            logger.debug(f""Using system Venice API key for skill {self.name}"")
            return system_api_key

        logger.warning(
            f""No Venice API key found in agent or system config for skill {self.name}""
        )
        return None","def get_api_key(self, context: SkillContext) -> Optional[str]:
        """"""Get the API key, prioritizing agent config then system config.""""""
        # Check agent config first
        agent_api_key = context.config.get(""api_key"")
        if agent_api_key:
            logger.debug(f""Using agent-specific Venice API key for skill {self.name}"")
            return agent_api_key

        # Fallback to system config
        system_api_key = self.skill_store.get_system_config(""venice_api_key"")
        if system_api_key:
            logger.debug(f""Using system Venice API key for skill {self.name}"")
            return system_api_key

        logger.warning(
            f""No Venice API key found in agent or system config for skill {self.name}""
        )
        return None","Get the API key, prioritizing agent config then system config.","Get the API key, prioritizing agent config then system config.","def get_api_key(self, context: SkillContext) -> Optional[str]:
        
        # Check agent config first
        agent_api_key = context.config.get(""api_key"")
        if agent_api_key:
            logger.debug(f""Using agent-specific Venice API key for skill {self.name}"")
            return agent_api_key

        # Fallback to system config
        system_api_key = self.skill_store.get_system_config(""venice_api_key"")
        if system_api_key:
            logger.debug(f""Using system Venice API key for skill {self.name}"")
            return system_api_key

        logger.warning(
            f""No Venice API key found in agent or system config for skill {self.name}""
        )
        return None","Get the API key, prioritizing agent config then system config.","def get_api_key ( self , context : SkillContext ) -> Optional [ str ] : # Check agent config first agent_api_key = context . config . get ( ""api_key"" ) if agent_api_key : logger . debug ( f""Using agent-specific Venice API key for skill {self.name}"" ) return agent_api_key # Fallback to system config system_api_key = self . skill_store . get_system_config ( ""venice_api_key"" ) if system_api_key : logger . debug ( f""Using system Venice API key for skill {self.name}"" ) return system_api_key logger . warning ( f""No Venice API key found in agent or system config for skill {self.name}"" ) return None","Get the API key, prioritizing agent config then system config."
/ag2/autogen/agentchat/contrib/text_analyzer_agent.py,analyze_text,"def analyze_text(self, text_to_analyze, analysis_instructions):
        """"""Analyzes the given text as instructed, and returns the analysis.""""""
        # Assemble the message.
        text_to_analyze = ""# TEXT\n"" + text_to_analyze + ""\n""
        analysis_instructions = ""# INSTRUCTIONS\n"" + analysis_instructions + ""\n""
        msg_text = ""\n"".join([
            analysis_instructions,
            text_to_analyze,
            analysis_instructions,
        ])  # Repeat the instructions.
        # Generate and return the analysis string.
        return self.generate_oai_reply([{""role"": ""user"", ""content"": msg_text}], None, None)[1]","def analyze_text(self, text_to_analyze, analysis_instructions):
        """"""Analyzes the given text as instructed, and returns the analysis.""""""
        # Assemble the message.
        text_to_analyze = ""# TEXT\n"" + text_to_analyze + ""\n""
        analysis_instructions = ""# INSTRUCTIONS\n"" + analysis_instructions + ""\n""
        msg_text = ""\n"".join([
            analysis_instructions,
            text_to_analyze,
            analysis_instructions,
        ])  # Repeat the instructions.
        # Generate and return the analysis string.
        return self.generate_oai_reply([{""role"": ""user"", ""content"": msg_text}], None, None)[1]","Analyzes the given text as instructed, and returns the analysis.","Analyzes the given text as instructed, and returns the analysis.","def analyze_text(self, text_to_analyze, analysis_instructions):
        
        # Assemble the message.
        text_to_analyze = ""# TEXT\n"" + text_to_analyze + ""\n""
        analysis_instructions = ""# INSTRUCTIONS\n"" + analysis_instructions + ""\n""
        msg_text = ""\n"".join([
            analysis_instructions,
            text_to_analyze,
            analysis_instructions,
        ])  # Repeat the instructions.
        # Generate and return the analysis string.
        return self.generate_oai_reply([{""role"": ""user"", ""content"": msg_text}], None, None)[1]","Analyzes the given text as instructed, and returns the analysis.","def analyze_text ( self , text_to_analyze , analysis_instructions ) : # Assemble the message. text_to_analyze = ""# TEXT\n"" + text_to_analyze + ""\n"" analysis_instructions = ""# INSTRUCTIONS\n"" + analysis_instructions + ""\n"" msg_text = ""\n"" . join ( [ analysis_instructions , text_to_analyze , analysis_instructions , ] ) # Repeat the instructions. # Generate and return the analysis string. return self . generate_oai_reply ( [ { ""role"" : ""user"" , ""content"" : msg_text } ] , None , None ) [ 1 ]","Analyzes the given text as instructed, and returns the analysis."
/ragaai-catalyst/ragaai_catalyst/redteaming/red_teaming.py,_get_save_path,"def _get_save_path(self, description: str) -> str:
        """"""Generate a path for saving the final DataFrame.""""""
        timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
        output_dir = os.path.join(os.path.dirname(__file__), ""results"")
        os.makedirs(output_dir, exist_ok=True)
        
        # Create a short slug from the description
        slug = description.lower()[:30].replace("" "", ""_"")
        return os.path.join(output_dir, f""red_teaming_{slug}_{timestamp}.csv"")","def _get_save_path(self, description: str) -> str:
        """"""Generate a path for saving the final DataFrame.""""""
        timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
        output_dir = os.path.join(os.path.dirname(__file__), ""results"")
        os.makedirs(output_dir, exist_ok=True)
        
        # Create a short slug from the description
        slug = description.lower()[:30].replace("" "", ""_"")
        return os.path.join(output_dir, f""red_teaming_{slug}_{timestamp}.csv"")",Generate a path for saving the final DataFrame.,Generate a path for saving the final DataFrame.,"def _get_save_path(self, description: str) -> str:
        
        timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
        output_dir = os.path.join(os.path.dirname(__file__), ""results"")
        os.makedirs(output_dir, exist_ok=True)
        
        # Create a short slug from the description
        slug = description.lower()[:30].replace("" "", ""_"")
        return os.path.join(output_dir, f""red_teaming_{slug}_{timestamp}.csv"")",Generate a path for saving the final DataFrame.,"def _get_save_path ( self , description : str ) -> str : timestamp = datetime . now ( ) . strftime ( ""%Y%m%d_%H%M%S"" ) output_dir = os . path . join ( os . path . dirname ( __file__ ) , ""results"" ) os . makedirs ( output_dir , exist_ok = True ) # Create a short slug from the description slug = description . lower ( ) [ : 30 ] . replace ( "" "" , ""_"" ) return os . path . join ( output_dir , f""red_teaming_{slug}_{timestamp}.csv"" )",Generate a path for saving the final DataFrame.
/potpie/app/alembic/env.py,process_revision_directives,"def process_revision_directives(context, revision, directives):
    """"""Automatically prepend timestamp to migration filenames.""""""
    for directive in directives:
        if isinstance(directive, ops.MigrationScript):
            # Get the current timestamp
            timestamp = time.strftime(""%Y%m%d%H%M%S"")
            # Modify the revision ID to include the timestamp
            directive.rev_id = f""{timestamp}_{directive.rev_id}""","def process_revision_directives(context, revision, directives):
    """"""Automatically prepend timestamp to migration filenames.""""""
    for directive in directives:
        if isinstance(directive, ops.MigrationScript):
            # Get the current timestamp
            timestamp = time.strftime(""%Y%m%d%H%M%S"")
            # Modify the revision ID to include the timestamp
            directive.rev_id = f""{timestamp}_{directive.rev_id}""",Automatically prepend timestamp to migration filenames.,Automatically prepend timestamp to migration filenames.,"def process_revision_directives(context, revision, directives):
    
    for directive in directives:
        if isinstance(directive, ops.MigrationScript):
            # Get the current timestamp
            timestamp = time.strftime(""%Y%m%d%H%M%S"")
            # Modify the revision ID to include the timestamp
            directive.rev_id = f""{timestamp}_{directive.rev_id}""",Automatically prepend timestamp to migration filenames.,"def process_revision_directives ( context , revision , directives ) : for directive in directives : if isinstance ( directive , ops . MigrationScript ) : # Get the current timestamp timestamp = time . strftime ( ""%Y%m%d%H%M%S"" ) # Modify the revision ID to include the timestamp directive . rev_id = f""{timestamp}_{directive.rev_id}""",Automatically prepend timestamp to migration filenames.
/alphafold3/src/alphafold3/model/data3.py,get_profile_features,"def get_profile_features(
    msa: np.ndarray, deletion_matrix: np.ndarray
) -> FeatureDict:
  """"""Returns the MSA profile and deletion_mean features.""""""
  num_restypes = residue_names.POLYMER_TYPES_NUM_WITH_UNKNOWN_AND_GAP
  profile = msa_profile.compute_msa_profile(
      msa=msa, num_residue_types=num_restypes
  )

  return {
      'profile': profile.astype(np.float32),
      'deletion_mean': np.mean(deletion_matrix, axis=0),
  }","def get_profile_features(
    msa: np.ndarray, deletion_matrix: np.ndarray
) -> FeatureDict:
  """"""Returns the MSA profile and deletion_mean features.""""""
  num_restypes = residue_names.POLYMER_TYPES_NUM_WITH_UNKNOWN_AND_GAP
  profile = msa_profile.compute_msa_profile(
      msa=msa, num_residue_types=num_restypes
  )

  return {
      'profile': profile.astype(np.float32),
      'deletion_mean': np.mean(deletion_matrix, axis=0),
  }",Returns the MSA profile and deletion_mean features.,Returns the MSA profile and deletion_mean features.,"def get_profile_features(
    msa: np.ndarray, deletion_matrix: np.ndarray
) -> FeatureDict:
  
  num_restypes = residue_names.POLYMER_TYPES_NUM_WITH_UNKNOWN_AND_GAP
  profile = msa_profile.compute_msa_profile(
      msa=msa, num_residue_types=num_restypes
  )

  return {
      'profile': profile.astype(np.float32),
      'deletion_mean': np.mean(deletion_matrix, axis=0),
  }",Returns the MSA profile and deletion_mean features.,"def get_profile_features ( msa : np . ndarray , deletion_matrix : np . ndarray ) -> FeatureDict : num_restypes = residue_names . POLYMER_TYPES_NUM_WITH_UNKNOWN_AND_GAP profile = msa_profile . compute_msa_profile ( msa = msa , num_residue_types = num_restypes ) return { 'profile' : profile . astype ( np . float32 ) , 'deletion_mean' : np . mean ( deletion_matrix , axis = 0 ) , }",Returns the MSA profile and deletion_mean features.
/aci/backend/aci/common/db/crud/subscriptions.py,delete_subscription_by_stripe_id,"def delete_subscription_by_stripe_id(db_session: Session, stripe_subscription_id: str) -> None:
    """"""
    Mark a subscription as deleted by Stripe subscription ID. Returns True if marked, False otherwise.
    """"""
    subscription = get_subscription_by_stripe_id(db_session, stripe_subscription_id)
    if not subscription:
        logger.warning(
            f""Subscription not found for stripe_subscription_id={stripe_subscription_id} during delete attempt.""
        )
        return

    db_session.delete(subscription)
    db_session.flush()
    logger.info(
        ""Deleted subscription"",
        extra={""stripe_subscription_id"": stripe_subscription_id},
    )","def delete_subscription_by_stripe_id(db_session: Session, stripe_subscription_id: str) -> None:
    """"""
    Mark a subscription as deleted by Stripe subscription ID. Returns True if marked, False otherwise.
    """"""
    subscription = get_subscription_by_stripe_id(db_session, stripe_subscription_id)
    if not subscription:
        logger.warning(
            f""Subscription not found for stripe_subscription_id={stripe_subscription_id} during delete attempt.""
        )
        return

    db_session.delete(subscription)
    db_session.flush()
    logger.info(
        ""Deleted subscription"",
        extra={""stripe_subscription_id"": stripe_subscription_id},
    )","Mark a subscription as deleted by Stripe subscription ID. Returns True if marked, False otherwise.",Mark a subscription as deleted by Stripe subscription ID.,"def delete_subscription_by_stripe_id(db_session: Session, stripe_subscription_id: str) -> None:
    
    subscription = get_subscription_by_stripe_id(db_session, stripe_subscription_id)
    if not subscription:
        logger.warning(
            f""Subscription not found for stripe_subscription_id={stripe_subscription_id} during delete attempt.""
        )
        return

    db_session.delete(subscription)
    db_session.flush()
    logger.info(
        ""Deleted subscription"",
        extra={""stripe_subscription_id"": stripe_subscription_id},
    )",Mark a subscription as deleted by Stripe subscription ID.,"def delete_subscription_by_stripe_id ( db_session : Session , stripe_subscription_id : str ) -> None : subscription = get_subscription_by_stripe_id ( db_session , stripe_subscription_id ) if not subscription : logger . warning ( f""Subscription not found for stripe_subscription_id={stripe_subscription_id} during delete attempt."" ) return db_session . delete ( subscription ) db_session . flush ( ) logger . info ( ""Deleted subscription"" , extra = { ""stripe_subscription_id"" : stripe_subscription_id } , )",Mark a subscription as deleted by Stripe subscription ID.
/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/train/trainer_vit_llrd.py,_safe_save,"def _safe_save(self, output_dir: str):
        """"""Collects the state dict and dump to disk.""""""
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)","def _safe_save(self, output_dir: str):
        """"""Collects the state dict and dump to disk.""""""
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,Collects the state dict and dump to disk.,"def _safe_save(self, output_dir: str):
        
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,"def _safe_save ( self , output_dir : str ) : state_dict = self . model . state_dict ( ) if self . args . should_save : cpu_state_dict = { key : value . cpu ( ) for key , value in state_dict . items ( ) } del state_dict self . _save ( output_dir , state_dict = cpu_state_dict )",Collects the state dict and dump to disk.
/KAG/knext/common/rest/exceptions.py,__str__,"def __str__(self):
        """"""Custom error messages for exception""""""
        error_message = ""({0})\n"" ""Reason: {1}\n"".format(self.status, self.reason)
        if self.headers:
            error_message += ""HTTP response headers: {0}\n"".format(self.headers)

        if self.body:
            error_message += ""HTTP response body: {0}\n"".format(self.body)

        return error_message","def __str__(self):
        """"""Custom error messages for exception""""""
        error_message = ""({0})\n"" ""Reason: {1}\n"".format(self.status, self.reason)
        if self.headers:
            error_message += ""HTTP response headers: {0}\n"".format(self.headers)

        if self.body:
            error_message += ""HTTP response body: {0}\n"".format(self.body)

        return error_message",Custom error messages for exception,Custom error messages for exception,"def __str__(self):
        
        error_message = ""({0})\n"" ""Reason: {1}\n"".format(self.status, self.reason)
        if self.headers:
            error_message += ""HTTP response headers: {0}\n"".format(self.headers)

        if self.body:
            error_message += ""HTTP response body: {0}\n"".format(self.body)

        return error_message",Custom error messages for exception,"def __str__ ( self ) : error_message = ""({0})\n"" ""Reason: {1}\n"" . format ( self . status , self . reason ) if self . headers : error_message += ""HTTP response headers: {0}\n"" . format ( self . headers ) if self . body : error_message += ""HTTP response body: {0}\n"" . format ( self . body ) return error_message",Custom error messages for exception
/ag2/autogen/coding/jupyter/embedded_ipython_code_executor.py,_save_image,"def _save_image(self, image_data_base64: str) -> str:
        """"""Save image data to a file.""""""
        image_data = base64.b64decode(image_data_base64)
        # Randomly generate a filename.
        filename = f""{uuid.uuid4().hex}.png""
        path = os.path.join(self.output_dir, filename)
        with open(path, ""wb"") as f:
            f.write(image_data)
        return os.path.abspath(path)","def _save_image(self, image_data_base64: str) -> str:
        """"""Save image data to a file.""""""
        image_data = base64.b64decode(image_data_base64)
        # Randomly generate a filename.
        filename = f""{uuid.uuid4().hex}.png""
        path = os.path.join(self.output_dir, filename)
        with open(path, ""wb"") as f:
            f.write(image_data)
        return os.path.abspath(path)",Save image data to a file.,Save image data to a file.,"def _save_image(self, image_data_base64: str) -> str:
        
        image_data = base64.b64decode(image_data_base64)
        # Randomly generate a filename.
        filename = f""{uuid.uuid4().hex}.png""
        path = os.path.join(self.output_dir, filename)
        with open(path, ""wb"") as f:
            f.write(image_data)
        return os.path.abspath(path)",Save image data to a file.,"def _save_image ( self , image_data_base64 : str ) -> str : image_data = base64 . b64decode ( image_data_base64 ) # Randomly generate a filename. filename = f""{uuid.uuid4().hex}.png"" path = os . path . join ( self . output_dir , filename ) with open ( path , ""wb"" ) as f : f . write ( image_data ) return os . path . abspath ( path )",Save image data to a file.
/deep-searcher/tests/vector_db/test_qdrant.py,setUp,"def setUp(self):
        """"""Set up test fixtures.""""""
        # Create mock modules
        self.mock_qdrant = MagicMock()
        self.mock_models = MagicMock()
        self.mock_qdrant.models = self.mock_models
        
        # Create the module patcher
        self.module_patcher = patch.dict('sys.modules', {
            'qdrant_client': self.mock_qdrant,
            'qdrant_client.models': self.mock_models
        })
        self.module_patcher.start()
        
        # Import after mocking
        from deepsearcher.vector_db import Qdrant
        from deepsearcher.loader.splitter import Chunk
        from deepsearcher.vector_db.base import RetrievalResult
        
        self.Qdrant = Qdrant
        self.Chunk = Chunk
        self.RetrievalResult = RetrievalResult","def setUp(self):
        """"""Set up test fixtures.""""""
        # Create mock modules
        self.mock_qdrant = MagicMock()
        self.mock_models = MagicMock()
        self.mock_qdrant.models = self.mock_models
        
        # Create the module patcher
        self.module_patcher = patch.dict('sys.modules', {
            'qdrant_client': self.mock_qdrant,
            'qdrant_client.models': self.mock_models
        })
        self.module_patcher.start()
        
        # Import after mocking
        from deepsearcher.vector_db import Qdrant
        from deepsearcher.loader.splitter import Chunk
        from deepsearcher.vector_db.base import RetrievalResult
        
        self.Qdrant = Qdrant
        self.Chunk = Chunk
        self.RetrievalResult = RetrievalResult",Set up test fixtures.,Set up test fixtures.,"def setUp(self):
        
        # Create mock modules
        self.mock_qdrant = MagicMock()
        self.mock_models = MagicMock()
        self.mock_qdrant.models = self.mock_models
        
        # Create the module patcher
        self.module_patcher = patch.dict('sys.modules', {
            'qdrant_client': self.mock_qdrant,
            'qdrant_client.models': self.mock_models
        })
        self.module_patcher.start()
        
        # Import after mocking
        from deepsearcher.vector_db import Qdrant
        from deepsearcher.loader.splitter import Chunk
        from deepsearcher.vector_db.base import RetrievalResult
        
        self.Qdrant = Qdrant
        self.Chunk = Chunk
        self.RetrievalResult = RetrievalResult",Set up test fixtures.,"def setUp ( self ) : # Create mock modules self . mock_qdrant = MagicMock ( ) self . mock_models = MagicMock ( ) self . mock_qdrant . models = self . mock_models # Create the module patcher self . module_patcher = patch . dict ( 'sys.modules' , { 'qdrant_client' : self . mock_qdrant , 'qdrant_client.models' : self . mock_models } ) self . module_patcher . start ( ) # Import after mocking from deepsearcher . vector_db import Qdrant from deepsearcher . loader . splitter import Chunk from deepsearcher . vector_db . base import RetrievalResult self . Qdrant = Qdrant self . Chunk = Chunk self . RetrievalResult = RetrievalResult",Set up test fixtures.
/verl/verl/workers/megatron_workers.py,execute_method,"def execute_method(self, method: Union[str, bytes], *args, **kwargs):
        """"""Called by ExternalRayDistributedExecutor collective_rpc.""""""
        if self.vllm_tp_rank == 0 and method != ""execute_model"":
            print(f""[DP={self.vllm_dp_rank},TP={self.vllm_tp_rank}] execute_method: {method if isinstance(method, str) else 'Callable'}"")
        return self.rollout.execute_method(method, *args, **kwargs)","def execute_method(self, method: Union[str, bytes], *args, **kwargs):
        """"""Called by ExternalRayDistributedExecutor collective_rpc.""""""
        if self.vllm_tp_rank == 0 and method != ""execute_model"":
            print(f""[DP={self.vllm_dp_rank},TP={self.vllm_tp_rank}] execute_method: {method if isinstance(method, str) else 'Callable'}"")
        return self.rollout.execute_method(method, *args, **kwargs)",Called by ExternalRayDistributedExecutor collective_rpc.,Called by ExternalRayDistributedExecutor collective_rpc.,"def execute_method(self, method: Union[str, bytes], *args, **kwargs):
        
        if self.vllm_tp_rank == 0 and method != ""execute_model"":
            print(f""[DP={self.vllm_dp_rank},TP={self.vllm_tp_rank}] execute_method: {method if isinstance(method, str) else 'Callable'}"")
        return self.rollout.execute_method(method, *args, **kwargs)",Called by ExternalRayDistributedExecutor collective_rpc.,"def execute_method ( self , method : Union [ str , bytes ] , * args , ** kwargs ) : if self . vllm_tp_rank == 0 and method != ""execute_model"" : print ( f""[DP={self.vllm_dp_rank},TP={self.vllm_tp_rank}] execute_method: {method if isinstance(method, str) else 'Callable'}"" ) return self . rollout . execute_method ( method , * args , ** kwargs )",Called by ExternalRayDistributedExecutor collective_rpc.
/cua/libs/agent/agent/core/visualization.py,visualize_scroll,"def visualize_scroll(self, direction: str, clicks: int, img_base64: str) -> None:
        """"""Visualize a scroll action by drawing arrows on the screenshot.""""""
        if (
            not self.agent.save_trajectory
            or not hasattr(self.agent, ""experiment_manager"")
            or not self.agent.experiment_manager
        ):
            return

        try:
            # Use the visualization utility
            img = visualize_scroll(direction, clicks, img_base64)

            # Save the visualization
            self.agent.experiment_manager.save_action_visualization(
                img, ""scroll"", f""{direction}_{clicks}""
            )
        except Exception as e:
            logger.error(f""Error visualizing scroll: {str(e)}"")","def visualize_scroll(self, direction: str, clicks: int, img_base64: str) -> None:
        """"""Visualize a scroll action by drawing arrows on the screenshot.""""""
        if (
            not self.agent.save_trajectory
            or not hasattr(self.agent, ""experiment_manager"")
            or not self.agent.experiment_manager
        ):
            return

        try:
            # Use the visualization utility
            img = visualize_scroll(direction, clicks, img_base64)

            # Save the visualization
            self.agent.experiment_manager.save_action_visualization(
                img, ""scroll"", f""{direction}_{clicks}""
            )
        except Exception as e:
            logger.error(f""Error visualizing scroll: {str(e)}"")",Visualize a scroll action by drawing arrows on the screenshot.,Visualize a scroll action by drawing arrows on the screenshot.,"def visualize_scroll(self, direction: str, clicks: int, img_base64: str) -> None:
        
        if (
            not self.agent.save_trajectory
            or not hasattr(self.agent, ""experiment_manager"")
            or not self.agent.experiment_manager
        ):
            return

        try:
            # Use the visualization utility
            img = visualize_scroll(direction, clicks, img_base64)

            # Save the visualization
            self.agent.experiment_manager.save_action_visualization(
                img, ""scroll"", f""{direction}_{clicks}""
            )
        except Exception as e:
            logger.error(f""Error visualizing scroll: {str(e)}"")",Visualize a scroll action by drawing arrows on the screenshot.,"def visualize_scroll ( self , direction : str , clicks : int , img_base64 : str ) -> None : if ( not self . agent . save_trajectory or not hasattr ( self . agent , ""experiment_manager"" ) or not self . agent . experiment_manager ) : return try : # Use the visualization utility img = visualize_scroll ( direction , clicks , img_base64 ) # Save the visualization self . agent . experiment_manager . save_action_visualization ( img , ""scroll"" , f""{direction}_{clicks}"" ) except Exception as e : logger . error ( f""Error visualizing scroll: {str(e)}"" )",Visualize a scroll action by drawing arrows on the screenshot.
/verl/verl/tools/utils/search_r1_like_utils.py,_passages2string,"def _passages2string(retrieval_result):
    """"""Convert retrieval results to formatted string.""""""
    format_reference = """"
    for idx, doc_item in enumerate(retrieval_result):
        content = doc_item[""document""][""contents""]
        title = content.split(""\n"")[0]
        text = ""\n"".join(content.split(""\n"")[1:])
        format_reference += f""Doc {idx + 1} (Title: {title})\n{text}\n\n""
    return format_reference.strip()","def _passages2string(retrieval_result):
    """"""Convert retrieval results to formatted string.""""""
    format_reference = """"
    for idx, doc_item in enumerate(retrieval_result):
        content = doc_item[""document""][""contents""]
        title = content.split(""\n"")[0]
        text = ""\n"".join(content.split(""\n"")[1:])
        format_reference += f""Doc {idx + 1} (Title: {title})\n{text}\n\n""
    return format_reference.strip()",Convert retrieval results to formatted string.,Convert retrieval results to formatted string.,"def _passages2string(retrieval_result):
    
    format_reference = """"
    for idx, doc_item in enumerate(retrieval_result):
        content = doc_item[""document""][""contents""]
        title = content.split(""\n"")[0]
        text = ""\n"".join(content.split(""\n"")[1:])
        format_reference += f""Doc {idx + 1} (Title: {title})\n{text}\n\n""
    return format_reference.strip()",Convert retrieval results to formatted string.,"def _passages2string ( retrieval_result ) : format_reference = """" for idx , doc_item in enumerate ( retrieval_result ) : content = doc_item [ ""document"" ] [ ""contents"" ] title = content . split ( ""\n"" ) [ 0 ] text = ""\n"" . join ( content . split ( ""\n"" ) [ 1 : ] ) format_reference += f""Doc {idx + 1} (Title: {title})\n{text}\n\n"" return format_reference . strip ( )",Convert retrieval results to formatted string.
/mcp-use/mcp_use/managers/tools/search_tools.py,format_search_results,"def format_search_results(self, results: list[tuple[BaseTool, str, float]]) -> str:
        """"""Format search results in a consistent format.""""""

        # Only show top_k results
        results = results

        formatted_output = ""Search results\n\n""

        for i, (tool, server_name, score) in enumerate(results):
            # Format score as percentage
            if i < 5:
                score_pct = f""{score * 100:.1f}%""
                logger.info(f""{i}: {tool.name} ({score_pct} match)"")
            formatted_output += f""[{i + 1}] Tool: {tool.name} ({score_pct} match)\n""
            formatted_output += f""    Server: {server_name}\n""
            formatted_output += f""    Description: {tool.description}\n\n""

        # Add footer with information about how to use the results
        formatted_output += (
            ""\nTo use a tool, connect to the appropriate server first, then invoke the tool.""
        )

        return formatted_output","def format_search_results(self, results: list[tuple[BaseTool, str, float]]) -> str:
        """"""Format search results in a consistent format.""""""

        # Only show top_k results
        results = results

        formatted_output = ""Search results\n\n""

        for i, (tool, server_name, score) in enumerate(results):
            # Format score as percentage
            if i < 5:
                score_pct = f""{score * 100:.1f}%""
                logger.info(f""{i}: {tool.name} ({score_pct} match)"")
            formatted_output += f""[{i + 1}] Tool: {tool.name} ({score_pct} match)\n""
            formatted_output += f""    Server: {server_name}\n""
            formatted_output += f""    Description: {tool.description}\n\n""

        # Add footer with information about how to use the results
        formatted_output += (
            ""\nTo use a tool, connect to the appropriate server first, then invoke the tool.""
        )

        return formatted_output",Format search results in a consistent format.,Format search results in a consistent format.,"def format_search_results(self, results: list[tuple[BaseTool, str, float]]) -> str:
        

        # Only show top_k results
        results = results

        formatted_output = ""Search results\n\n""

        for i, (tool, server_name, score) in enumerate(results):
            # Format score as percentage
            if i < 5:
                score_pct = f""{score * 100:.1f}%""
                logger.info(f""{i}: {tool.name} ({score_pct} match)"")
            formatted_output += f""[{i + 1}] Tool: {tool.name} ({score_pct} match)\n""
            formatted_output += f""    Server: {server_name}\n""
            formatted_output += f""    Description: {tool.description}\n\n""

        # Add footer with information about how to use the results
        formatted_output += (
            ""\nTo use a tool, connect to the appropriate server first, then invoke the tool.""
        )

        return formatted_output",Format search results in a consistent format.,"def format_search_results ( self , results : list [ tuple [ BaseTool , str , float ] ] ) -> str : # Only show top_k results results = results formatted_output = ""Search results\n\n"" for i , ( tool , server_name , score ) in enumerate ( results ) : # Format score as percentage if i < 5 : score_pct = f""{score * 100:.1f}%"" logger . info ( f""{i}: {tool.name} ({score_pct} match)"" ) formatted_output += f""[{i + 1}] Tool: {tool.name} ({score_pct} match)\n"" formatted_output += f""    Server: {server_name}\n"" formatted_output += f""    Description: {tool.description}\n\n"" # Add footer with information about how to use the results formatted_output += ( ""\nTo use a tool, connect to the appropriate server first, then invoke the tool."" ) return formatted_output",Format search results in a consistent format.
/ag2/autogen/agentchat/realtime/experimental/clients/oai/rtc_client.py,session_init_data,"def session_init_data(self) -> list[dict[str, Any]]:
        """"""Control initial session with OpenAI.""""""
        session_update = {
            ""turn_detection"": {""type"": ""server_vad""},
            ""voice"": self._voice,
            ""modalities"": [""audio"", ""text""],
            ""temperature"": self._temperature,
        }
        return [{""type"": ""session.update"", ""session"": session_update}]","def session_init_data(self) -> list[dict[str, Any]]:
        """"""Control initial session with OpenAI.""""""
        session_update = {
            ""turn_detection"": {""type"": ""server_vad""},
            ""voice"": self._voice,
            ""modalities"": [""audio"", ""text""],
            ""temperature"": self._temperature,
        }
        return [{""type"": ""session.update"", ""session"": session_update}]",Control initial session with OpenAI.,Control initial session with OpenAI.,"def session_init_data(self) -> list[dict[str, Any]]:
        
        session_update = {
            ""turn_detection"": {""type"": ""server_vad""},
            ""voice"": self._voice,
            ""modalities"": [""audio"", ""text""],
            ""temperature"": self._temperature,
        }
        return [{""type"": ""session.update"", ""session"": session_update}]",Control initial session with OpenAI.,"def session_init_data ( self ) -> list [ dict [ str , Any ] ] : session_update = { ""turn_detection"" : { ""type"" : ""server_vad"" } , ""voice"" : self . _voice , ""modalities"" : [ ""audio"" , ""text"" ] , ""temperature"" : self . _temperature , } return [ { ""type"" : ""session.update"" , ""session"" : session_update } ]",Control initial session with OpenAI.
/airweave/backend/airweave/platform/file_handling/async_markitdown.py,_is_supported,"def _is_supported(self, file_path: str) -> bool:
        """"""Check if the file extension is supported.""""""
        ext = Path(file_path).suffix.lower()
        if ext not in self.SUPPORTED_EXTENSIONS:
            logger.warning(f""Unsupported file extension: {ext} for file: {file_path}"")
            return False
        return True","def _is_supported(self, file_path: str) -> bool:
        """"""Check if the file extension is supported.""""""
        ext = Path(file_path).suffix.lower()
        if ext not in self.SUPPORTED_EXTENSIONS:
            logger.warning(f""Unsupported file extension: {ext} for file: {file_path}"")
            return False
        return True",Check if the file extension is supported.,Check if the file extension is supported.,"def _is_supported(self, file_path: str) -> bool:
        
        ext = Path(file_path).suffix.lower()
        if ext not in self.SUPPORTED_EXTENSIONS:
            logger.warning(f""Unsupported file extension: {ext} for file: {file_path}"")
            return False
        return True",Check if the file extension is supported.,"def _is_supported ( self , file_path : str ) -> bool : ext = Path ( file_path ) . suffix . lower ( ) if ext not in self . SUPPORTED_EXTENSIONS : logger . warning ( f""Unsupported file extension: {ext} for file: {file_path}"" ) return False return True",Check if the file extension is supported.
/olmocr/olmocr/eval/evalhtml.py,generate_diff_html,"def generate_diff_html(a, b):
    """"""
    Generates HTML with differences between strings a and b.
    Additions in 'b' are highlighted in green, deletions from 'a' are highlighted in red.
    """"""
    seq_matcher = SequenceMatcher(None, a, b)
    output_html = """"
    for opcode, a0, a1, b0, b1 in seq_matcher.get_opcodes():
        if opcode == ""equal"":
            output_html += a[a0:a1]
        elif opcode == ""insert"":
            output_html += f""<span class='added'>{b[b0:b1]}</span>""
        elif opcode == ""delete"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span>""
        elif opcode == ""replace"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span><span class='added'>{b[b0:b1]}</span>""
    return output_html","def generate_diff_html(a, b):
    """"""
    Generates HTML with differences between strings a and b.
    Additions in 'b' are highlighted in green, deletions from 'a' are highlighted in red.
    """"""
    seq_matcher = SequenceMatcher(None, a, b)
    output_html = """"
    for opcode, a0, a1, b0, b1 in seq_matcher.get_opcodes():
        if opcode == ""equal"":
            output_html += a[a0:a1]
        elif opcode == ""insert"":
            output_html += f""<span class='added'>{b[b0:b1]}</span>""
        elif opcode == ""delete"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span>""
        elif opcode == ""replace"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span><span class='added'>{b[b0:b1]}</span>""
    return output_html","Generates HTML with differences between strings a and b.
Additions in 'b' are highlighted in green, deletions from 'a' are highlighted in red.",Generates HTML with differences between strings a and b.,"def generate_diff_html(a, b):
    
    seq_matcher = SequenceMatcher(None, a, b)
    output_html = """"
    for opcode, a0, a1, b0, b1 in seq_matcher.get_opcodes():
        if opcode == ""equal"":
            output_html += a[a0:a1]
        elif opcode == ""insert"":
            output_html += f""<span class='added'>{b[b0:b1]}</span>""
        elif opcode == ""delete"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span>""
        elif opcode == ""replace"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span><span class='added'>{b[b0:b1]}</span>""
    return output_html",Generates HTML with differences between strings a and b.,"def generate_diff_html ( a , b ) : seq_matcher = SequenceMatcher ( None , a , b ) output_html = """" for opcode , a0 , a1 , b0 , b1 in seq_matcher . get_opcodes ( ) : if opcode == ""equal"" : output_html += a [ a0 : a1 ] elif opcode == ""insert"" : output_html += f""<span class='added'>{b[b0:b1]}</span>"" elif opcode == ""delete"" : output_html += f""<span class='removed'>{a[a0:a1]}</span>"" elif opcode == ""replace"" : output_html += f""<span class='removed'>{a[a0:a1]}</span><span class='added'>{b[b0:b1]}</span>"" return output_html",Generates HTML with differences between strings a and b.
/local-deep-research/src/local_deep_research/benchmarks/datasets/simpleqa.py,get_dataset_info,"def get_dataset_info(cls) -> Dict[str, str]:
        """"""Get basic information about the dataset.""""""
        return {
            ""id"": ""simpleqa"",
            ""name"": ""SimpleQA"",
            ""description"": ""Simple question-answering evaluation dataset"",
            ""url"": cls.get_default_dataset_path(),
        }","def get_dataset_info(cls) -> Dict[str, str]:
        """"""Get basic information about the dataset.""""""
        return {
            ""id"": ""simpleqa"",
            ""name"": ""SimpleQA"",
            ""description"": ""Simple question-answering evaluation dataset"",
            ""url"": cls.get_default_dataset_path(),
        }",Get basic information about the dataset.,Get basic information about the dataset.,"def get_dataset_info(cls) -> Dict[str, str]:
        
        return {
            ""id"": ""simpleqa"",
            ""name"": ""SimpleQA"",
            ""description"": ""Simple question-answering evaluation dataset"",
            ""url"": cls.get_default_dataset_path(),
        }",Get basic information about the dataset.,"def get_dataset_info ( cls ) -> Dict [ str , str ] : return { ""id"" : ""simpleqa"" , ""name"" : ""SimpleQA"" , ""description"" : ""Simple question-answering evaluation dataset"" , ""url"" : cls . get_default_dataset_path ( ) , }",Get basic information about the dataset.
/adk-python/src/google/adk/code_executors/container_code_executor.py,_build_docker_image,"def _build_docker_image(self):
    """"""Builds the Docker image.""""""
    if not self.docker_path:
      raise ValueError('Docker path is not set.')
    if not os.path.exists(self.docker_path):
      raise FileNotFoundError(f'Invalid Docker path: {self.docker_path}')

    logger.info('Building Docker image...')
    self._client.images.build(
        path=self.docker_path,
        tag=self.image,
        rm=True,
    )
    logger.info('Docker image: %s built.', self.image)","def _build_docker_image(self):
    """"""Builds the Docker image.""""""
    if not self.docker_path:
      raise ValueError('Docker path is not set.')
    if not os.path.exists(self.docker_path):
      raise FileNotFoundError(f'Invalid Docker path: {self.docker_path}')

    logger.info('Building Docker image...')
    self._client.images.build(
        path=self.docker_path,
        tag=self.image,
        rm=True,
    )
    logger.info('Docker image: %s built.', self.image)",Builds the Docker image.,Builds the Docker image.,"def _build_docker_image(self):
    
    if not self.docker_path:
      raise ValueError('Docker path is not set.')
    if not os.path.exists(self.docker_path):
      raise FileNotFoundError(f'Invalid Docker path: {self.docker_path}')

    logger.info('Building Docker image...')
    self._client.images.build(
        path=self.docker_path,
        tag=self.image,
        rm=True,
    )
    logger.info('Docker image: %s built.', self.image)",Builds the Docker image.,"def _build_docker_image ( self ) : if not self . docker_path : raise ValueError ( 'Docker path is not set.' ) if not os . path . exists ( self . docker_path ) : raise FileNotFoundError ( f'Invalid Docker path: {self.docker_path}' ) logger . info ( 'Building Docker image...' ) self . _client . images . build ( path = self . docker_path , tag = self . image , rm = True , ) logger . info ( 'Docker image: %s built.' , self . image )",Builds the Docker image.
/potpie/app/modules/intelligence/tools/code_query_tools/intelligent_code_graph_tool.py,_create_relevant_node,"def _create_relevant_node(
        self, node: Dict[str, Any], relevance_score: float, reason: str
    ) -> Dict[str, Any]:
        """"""Create a node with relevance information""""""
        relevant_node = {
            ""id"": node[""id""],
            ""name"": node[""name""],
            ""type"": node[""type""],
            ""relevance_score"": relevance_score,
            ""reason"": reason,
            ""children"": [],
        }

        for field in [""file_path"", ""start_line"", ""end_line"", ""relationship""]:
            if field in node:
                relevant_node[field] = node[field]

        return relevant_node","def _create_relevant_node(
        self, node: Dict[str, Any], relevance_score: float, reason: str
    ) -> Dict[str, Any]:
        """"""Create a node with relevance information""""""
        relevant_node = {
            ""id"": node[""id""],
            ""name"": node[""name""],
            ""type"": node[""type""],
            ""relevance_score"": relevance_score,
            ""reason"": reason,
            ""children"": [],
        }

        for field in [""file_path"", ""start_line"", ""end_line"", ""relationship""]:
            if field in node:
                relevant_node[field] = node[field]

        return relevant_node",Create a node with relevance information,Create a node with relevance information,"def _create_relevant_node(
        self, node: Dict[str, Any], relevance_score: float, reason: str
    ) -> Dict[str, Any]:
        
        relevant_node = {
            ""id"": node[""id""],
            ""name"": node[""name""],
            ""type"": node[""type""],
            ""relevance_score"": relevance_score,
            ""reason"": reason,
            ""children"": [],
        }

        for field in [""file_path"", ""start_line"", ""end_line"", ""relationship""]:
            if field in node:
                relevant_node[field] = node[field]

        return relevant_node",Create a node with relevance information,"def _create_relevant_node ( self , node : Dict [ str , Any ] , relevance_score : float , reason : str ) -> Dict [ str , Any ] : relevant_node = { ""id"" : node [ ""id"" ] , ""name"" : node [ ""name"" ] , ""type"" : node [ ""type"" ] , ""relevance_score"" : relevance_score , ""reason"" : reason , ""children"" : [ ] , } for field in [ ""file_path"" , ""start_line"" , ""end_line"" , ""relationship"" ] : if field in node : relevant_node [ field ] = node [ field ] return relevant_node",Create a node with relevance information
/ag2/autogen/agentchat/group/speaker_selection_result.py,get_speaker_selection_result,"def get_speaker_selection_result(self, groupchat: ""GroupChat"") -> Optional[Union[Agent, str]]:
        """"""Get the speaker selection result. If None, the conversation will end.""""""
        if self.agent_name is not None:
            # Find the agent by name in the groupchat
            for agent in groupchat.agents:
                if agent.name == self.agent_name:
                    return agent
            raise ValueError(f""Agent '{self.agent_name}' not found in groupchat."")
        elif self.speaker_selection_method is not None:
            return self.speaker_selection_method
        elif self.terminate is not None and self.terminate:
            return None
        else:
            raise ValueError(
                ""Unable to establish speaker selection result. No terminate, agent, or speaker selection method provided.""
            )","def get_speaker_selection_result(self, groupchat: ""GroupChat"") -> Optional[Union[Agent, str]]:
        """"""Get the speaker selection result. If None, the conversation will end.""""""
        if self.agent_name is not None:
            # Find the agent by name in the groupchat
            for agent in groupchat.agents:
                if agent.name == self.agent_name:
                    return agent
            raise ValueError(f""Agent '{self.agent_name}' not found in groupchat."")
        elif self.speaker_selection_method is not None:
            return self.speaker_selection_method
        elif self.terminate is not None and self.terminate:
            return None
        else:
            raise ValueError(
                ""Unable to establish speaker selection result. No terminate, agent, or speaker selection method provided.""
            )","Get the speaker selection result. If None, the conversation will end.",Get the speaker selection result.,"def get_speaker_selection_result(self, groupchat: ""GroupChat"") -> Optional[Union[Agent, str]]:
        
        if self.agent_name is not None:
            # Find the agent by name in the groupchat
            for agent in groupchat.agents:
                if agent.name == self.agent_name:
                    return agent
            raise ValueError(f""Agent '{self.agent_name}' not found in groupchat."")
        elif self.speaker_selection_method is not None:
            return self.speaker_selection_method
        elif self.terminate is not None and self.terminate:
            return None
        else:
            raise ValueError(
                ""Unable to establish speaker selection result. No terminate, agent, or speaker selection method provided.""
            )",Get the speaker selection result.,"def get_speaker_selection_result ( self , groupchat : ""GroupChat"" ) -> Optional [ Union [ Agent , str ] ] : if self . agent_name is not None : # Find the agent by name in the groupchat for agent in groupchat . agents : if agent . name == self . agent_name : return agent raise ValueError ( f""Agent '{self.agent_name}' not found in groupchat."" ) elif self . speaker_selection_method is not None : return self . speaker_selection_method elif self . terminate is not None and self . terminate : return None else : raise ValueError ( ""Unable to establish speaker selection result. No terminate, agent, or speaker selection method provided."" )",Get the speaker selection result.
/markitdown/packages/markitdown/src/markitdown/_uri_utils.py,file_uri_to_path,"def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    """"""Convert a file URI to a local file path""""""
    parsed = urlparse(file_uri)
    if parsed.scheme != ""file"":
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path","def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    """"""Convert a file URI to a local file path""""""
    parsed = urlparse(file_uri)
    if parsed.scheme != ""file"":
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path",Convert a file URI to a local file path,Convert a file URI to a local file path,"def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    
    parsed = urlparse(file_uri)
    if parsed.scheme != ""file"":
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path",Convert a file URI to a local file path,"def file_uri_to_path ( file_uri : str ) -> Tuple [ str | None , str ] : parsed = urlparse ( file_uri ) if parsed . scheme != ""file"" : raise ValueError ( f""Not a file URL: {file_uri}"" ) netloc = parsed . netloc if parsed . netloc else None path = os . path . abspath ( url2pathname ( parsed . path ) ) return netloc , path",Convert a file URI to a local file path
/morphik-core/core/parser/morphik_parser.py,_is_video_file,"def _is_video_file(self, file: bytes, filename: str) -> bool:
        """"""Check if the file is a video file.""""""
        try:
            kind = filetype.guess(file)
            return kind is not None and kind.mime.startswith(""video/"")
        except Exception as e:
            logging.error(f""Error detecting file type: {str(e)}"")
            return False","def _is_video_file(self, file: bytes, filename: str) -> bool:
        """"""Check if the file is a video file.""""""
        try:
            kind = filetype.guess(file)
            return kind is not None and kind.mime.startswith(""video/"")
        except Exception as e:
            logging.error(f""Error detecting file type: {str(e)}"")
            return False",Check if the file is a video file.,Check if the file is a video file.,"def _is_video_file(self, file: bytes, filename: str) -> bool:
        
        try:
            kind = filetype.guess(file)
            return kind is not None and kind.mime.startswith(""video/"")
        except Exception as e:
            logging.error(f""Error detecting file type: {str(e)}"")
            return False",Check if the file is a video file.,"def _is_video_file ( self , file : bytes , filename : str ) -> bool : try : kind = filetype . guess ( file ) return kind is not None and kind . mime . startswith ( ""video/"" ) except Exception as e : logging . error ( f""Error detecting file type: {str(e)}"" ) return False",Check if the file is a video file.
/mcp-agent/src/mcp_agent/core/context_dependent.py,context,"def context(self) -> ""Context"":
        """"""
        Get context, with graceful fallback to global context if needed.
        Raises clear error if no context is available.
        """"""
        # First try instance context
        if self._context is not None:
            return self._context

        try:
            # Fall back to global context if available
            from mcp_agent.core.context import get_current_context

            return get_current_context()
        except Exception as e:
            raise RuntimeError(
                f""No context available for {self.__class__.__name__}. ""
                ""Either initialize MCPApp first or pass context explicitly.""
            ) from e","def context(self) -> ""Context"":
        """"""
        Get context, with graceful fallback to global context if needed.
        Raises clear error if no context is available.
        """"""
        # First try instance context
        if self._context is not None:
            return self._context

        try:
            # Fall back to global context if available
            from mcp_agent.core.context import get_current_context

            return get_current_context()
        except Exception as e:
            raise RuntimeError(
                f""No context available for {self.__class__.__name__}. ""
                ""Either initialize MCPApp first or pass context explicitly.""
            ) from e","Get context, with graceful fallback to global context if needed.
Raises clear error if no context is available.","Get context, with graceful fallback to global context if needed.","def context(self) -> ""Context"":
        
        # First try instance context
        if self._context is not None:
            return self._context

        try:
            # Fall back to global context if available
            from mcp_agent.core.context import get_current_context

            return get_current_context()
        except Exception as e:
            raise RuntimeError(
                f""No context available for {self.__class__.__name__}. ""
                ""Either initialize MCPApp first or pass context explicitly.""
            ) from e","Get context, with graceful fallback to global context if needed.","def context ( self ) -> ""Context"" : # First try instance context if self . _context is not None : return self . _context try : # Fall back to global context if available from mcp_agent . core . context import get_current_context return get_current_context ( ) except Exception as e : raise RuntimeError ( f""No context available for {self.__class__.__name__}. "" ""Either initialize MCPApp first or pass context explicitly."" ) from e","Get context, with graceful fallback to global context if needed."
/morphik-core/core/completion/litellm_completion.py,get_system_message,"def get_system_message() -> Dict[str, str]:
    """"""Return the standard system message for Morphik's query agent.""""""
    return {
        ""role"": ""system"",
        ""content"": """"""You are Morphik's powerful query agent. Your role is to:

1. Analyze the provided context chunks from documents carefully
2. Use the context to answer questions accurately and comprehensively
3. Be clear and concise in your answers
4. When relevant, cite specific parts of the context to support your answers
5. For image-based queries, analyze the visual content in conjunction with any text context provided
6. Format your responses using Markdown.

Remember: Your primary goal is to provide accurate, context-aware responses that help users understand
and utilize the information in their documents effectively."""""",
    }","def get_system_message() -> Dict[str, str]:
    """"""Return the standard system message for Morphik's query agent.""""""
    return {
        ""role"": ""system"",
        ""content"": """"""You are Morphik's powerful query agent. Your role is to:

1. Analyze the provided context chunks from documents carefully
2. Use the context to answer questions accurately and comprehensively
3. Be clear and concise in your answers
4. When relevant, cite specific parts of the context to support your answers
5. For image-based queries, analyze the visual content in conjunction with any text context provided
6. Format your responses using Markdown.

Remember: Your primary goal is to provide accurate, context-aware responses that help users understand
and utilize the information in their documents effectively."""""",
    }",Return the standard system message for Morphik's query agent.,Return the standard system message for Morphik's query agent.,"def get_system_message() -> Dict[str, str]:
    
    return {
        ""role"": ""system"",
        ""content"": ,
    }",Return the standard system message for Morphik's query agent.,"def get_system_message ( ) -> Dict [ str , str ] : return { ""role"" : ""system"" , ""content"" : , }",Return the standard system message for Morphik's query agent.
/mcp-agent/src/mcp_agent/workflows/orchestrator/orchestrator_models.py,format_step_result,"def format_step_result(step_result: StepResult) -> str:
    """"""Format a step result for display to planners""""""
    tasks_str = ""\n"".join(
        f""  - {format_task_result(task)}"" for task in step_result.task_results
    )
    return STEP_RESULT_TEMPLATE.format(
        step_description=step_result.step.description,
        step_result=step_result.result,
        tasks_str=tasks_str,
    )","def format_step_result(step_result: StepResult) -> str:
    """"""Format a step result for display to planners""""""
    tasks_str = ""\n"".join(
        f""  - {format_task_result(task)}"" for task in step_result.task_results
    )
    return STEP_RESULT_TEMPLATE.format(
        step_description=step_result.step.description,
        step_result=step_result.result,
        tasks_str=tasks_str,
    )",Format a step result for display to planners,Format a step result for display to planners,"def format_step_result(step_result: StepResult) -> str:
    
    tasks_str = ""\n"".join(
        f""  - {format_task_result(task)}"" for task in step_result.task_results
    )
    return STEP_RESULT_TEMPLATE.format(
        step_description=step_result.step.description,
        step_result=step_result.result,
        tasks_str=tasks_str,
    )",Format a step result for display to planners,"def format_step_result ( step_result : StepResult ) -> str : tasks_str = ""\n"" . join ( f""  - {format_task_result(task)}"" for task in step_result . task_results ) return STEP_RESULT_TEMPLATE . format ( step_description = step_result . step . description , step_result = step_result . result , tasks_str = tasks_str , )",Format a step result for display to planners
/ragaai-catalyst/ragaai_catalyst/tracers/agentic_tracing/upload/upload_agentic_traces.py,update_presigned_url,"def update_presigned_url(self, presigned_url, base_url):
        """"""Replaces the domain (and port, if applicable) of the presigned URL 
        with that of the base URL only if the base URL contains 'localhost' or an IP address.""""""
        #To Do: If Proxy URL has domain name how do we handle such cases
        
        presigned_parts = urlparse(presigned_url)
        base_parts = urlparse(base_url)
        # Check if base_url contains localhost or an IP address
        if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
            new_netloc = base_parts.hostname  # Extract domain from base_url
            if base_parts.port:  # Add port if present in base_url
                new_netloc += f"":{base_parts.port}""
            updated_parts = presigned_parts._replace(netloc=new_netloc)
            return urlunparse(updated_parts)
        return presigned_url","def update_presigned_url(self, presigned_url, base_url):
        """"""Replaces the domain (and port, if applicable) of the presigned URL 
        with that of the base URL only if the base URL contains 'localhost' or an IP address.""""""
        #To Do: If Proxy URL has domain name how do we handle such cases
        
        presigned_parts = urlparse(presigned_url)
        base_parts = urlparse(base_url)
        # Check if base_url contains localhost or an IP address
        if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
            new_netloc = base_parts.hostname  # Extract domain from base_url
            if base_parts.port:  # Add port if present in base_url
                new_netloc += f"":{base_parts.port}""
            updated_parts = presigned_parts._replace(netloc=new_netloc)
            return urlunparse(updated_parts)
        return presigned_url","Replaces the domain (and port, if applicable) of the presigned URL 
with that of the base URL only if the base URL contains 'localhost' or an IP address.","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL only if the base URL contains 'localhost' or an IP address.","def update_presigned_url(self, presigned_url, base_url):
        
        #To Do: If Proxy URL has domain name how do we handle such cases
        
        presigned_parts = urlparse(presigned_url)
        base_parts = urlparse(base_url)
        # Check if base_url contains localhost or an IP address
        if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
            new_netloc = base_parts.hostname  # Extract domain from base_url
            if base_parts.port:  # Add port if present in base_url
                new_netloc += f"":{base_parts.port}""
            updated_parts = presigned_parts._replace(netloc=new_netloc)
            return urlunparse(updated_parts)
        return presigned_url","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL only if the base URL contains 'localhost' or an IP address.","def update_presigned_url ( self , presigned_url , base_url ) : #To Do: If Proxy URL has domain name how do we handle such cases presigned_parts = urlparse ( presigned_url ) base_parts = urlparse ( base_url ) # Check if base_url contains localhost or an IP address if re . match ( r'^(localhost|\d{1,3}(\.\d{1,3}){3})$' , base_parts . hostname ) : new_netloc = base_parts . hostname # Extract domain from base_url if base_parts . port : # Add port if present in base_url new_netloc += f"":{base_parts.port}"" updated_parts = presigned_parts . _replace ( netloc = new_netloc ) return urlunparse ( updated_parts ) return presigned_url","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL only if the base URL contains 'localhost' or an IP address."
/VideoLingo/core/_11_merge_audio.py,get_audio_files,"def get_audio_files(df):
    """"""Generate a list of audio file paths""""""
    audios = []
    for index, row in df.iterrows():
        number = row['number']
        line_count = len(eval(row['lines']) if isinstance(row['lines'], str) else row['lines'])
        for line_index in range(line_count):
            temp_file = OUTPUT_FILE_TEMPLATE.format(f""{number}_{line_index}"")
            audios.append(temp_file)
    return audios","def get_audio_files(df):
    """"""Generate a list of audio file paths""""""
    audios = []
    for index, row in df.iterrows():
        number = row['number']
        line_count = len(eval(row['lines']) if isinstance(row['lines'], str) else row['lines'])
        for line_index in range(line_count):
            temp_file = OUTPUT_FILE_TEMPLATE.format(f""{number}_{line_index}"")
            audios.append(temp_file)
    return audios",Generate a list of audio file paths,Generate a list of audio file paths,"def get_audio_files(df):
    
    audios = []
    for index, row in df.iterrows():
        number = row['number']
        line_count = len(eval(row['lines']) if isinstance(row['lines'], str) else row['lines'])
        for line_index in range(line_count):
            temp_file = OUTPUT_FILE_TEMPLATE.format(f""{number}_{line_index}"")
            audios.append(temp_file)
    return audios",Generate a list of audio file paths,"def get_audio_files ( df ) : audios = [ ] for index , row in df . iterrows ( ) : number = row [ 'number' ] line_count = len ( eval ( row [ 'lines' ] ) if isinstance ( row [ 'lines' ] , str ) else row [ 'lines' ] ) for line_index in range ( line_count ) : temp_file = OUTPUT_FILE_TEMPLATE . format ( f""{number}_{line_index}"" ) audios . append ( temp_file ) return audios",Generate a list of audio file paths
/openai-agents-python/src/agents/model_settings.py,resolve,"def resolve(self, override: ModelSettings | None) -> ModelSettings:
        """"""Produce a new ModelSettings by overlaying any non-None values from the
        override on top of this instance.""""""
        if override is None:
            return self

        changes = {
            field.name: getattr(override, field.name)
            for field in fields(self)
            if getattr(override, field.name) is not None
        }
        return replace(self, **changes)","def resolve(self, override: ModelSettings | None) -> ModelSettings:
        """"""Produce a new ModelSettings by overlaying any non-None values from the
        override on top of this instance.""""""
        if override is None:
            return self

        changes = {
            field.name: getattr(override, field.name)
            for field in fields(self)
            if getattr(override, field.name) is not None
        }
        return replace(self, **changes)","Produce a new ModelSettings by overlaying any non-None values from the
override on top of this instance.",Produce a new ModelSettings by overlaying any non-None values from the override on top of this instance.,"def resolve(self, override: ModelSettings | None) -> ModelSettings:
        
        if override is None:
            return self

        changes = {
            field.name: getattr(override, field.name)
            for field in fields(self)
            if getattr(override, field.name) is not None
        }
        return replace(self, **changes)",Produce a new ModelSettings by overlaying any non-None values from the override on top of this instance.,"def resolve ( self , override : ModelSettings | None ) -> ModelSettings : if override is None : return self changes = { field . name : getattr ( override , field . name ) for field in fields ( self ) if getattr ( override , field . name ) is not None } return replace ( self , ** changes )",Produce a new ModelSettings by overlaying any non-None values from the override on top of this instance.
/cursor-free-vip/totally_reset_cursor.py,generate_new_ids,"def generate_new_ids(self):
        """"""Generate new machine ID""""""
        # Generate new UUID
        dev_device_id = str(uuid.uuid4())

        # Generate new machineId (64 characters of hexadecimal)
        machine_id = hashlib.sha256(os.urandom(32)).hexdigest()

        # Generate new macMachineId (128 characters of hexadecimal)
        mac_machine_id = hashlib.sha512(os.urandom(64)).hexdigest()

        # Generate new sqmId
        sqm_id = ""{"" + str(uuid.uuid4()).upper() + ""}""

        self.update_machine_id_file(dev_device_id)

        return {
            ""telemetry.devDeviceId"": dev_device_id,
            ""telemetry.macMachineId"": mac_machine_id,
            ""telemetry.machineId"": machine_id,
            ""telemetry.sqmId"": sqm_id,
            ""storage.serviceMachineId"": dev_device_id,  # Add storage.serviceMachineId
        }","def generate_new_ids(self):
        """"""Generate new machine ID""""""
        # Generate new UUID
        dev_device_id = str(uuid.uuid4())

        # Generate new machineId (64 characters of hexadecimal)
        machine_id = hashlib.sha256(os.urandom(32)).hexdigest()

        # Generate new macMachineId (128 characters of hexadecimal)
        mac_machine_id = hashlib.sha512(os.urandom(64)).hexdigest()

        # Generate new sqmId
        sqm_id = ""{"" + str(uuid.uuid4()).upper() + ""}""

        self.update_machine_id_file(dev_device_id)

        return {
            ""telemetry.devDeviceId"": dev_device_id,
            ""telemetry.macMachineId"": mac_machine_id,
            ""telemetry.machineId"": machine_id,
            ""telemetry.sqmId"": sqm_id,
            ""storage.serviceMachineId"": dev_device_id,  # Add storage.serviceMachineId
        }",Generate new machine ID,Generate new machine ID,"def generate_new_ids(self):
        
        # Generate new UUID
        dev_device_id = str(uuid.uuid4())

        # Generate new machineId (64 characters of hexadecimal)
        machine_id = hashlib.sha256(os.urandom(32)).hexdigest()

        # Generate new macMachineId (128 characters of hexadecimal)
        mac_machine_id = hashlib.sha512(os.urandom(64)).hexdigest()

        # Generate new sqmId
        sqm_id = ""{"" + str(uuid.uuid4()).upper() + ""}""

        self.update_machine_id_file(dev_device_id)

        return {
            ""telemetry.devDeviceId"": dev_device_id,
            ""telemetry.macMachineId"": mac_machine_id,
            ""telemetry.machineId"": machine_id,
            ""telemetry.sqmId"": sqm_id,
            ""storage.serviceMachineId"": dev_device_id,  # Add storage.serviceMachineId
        }",Generate new machine ID,"def generate_new_ids ( self ) : # Generate new UUID dev_device_id = str ( uuid . uuid4 ( ) ) # Generate new machineId (64 characters of hexadecimal) machine_id = hashlib . sha256 ( os . urandom ( 32 ) ) . hexdigest ( ) # Generate new macMachineId (128 characters of hexadecimal) mac_machine_id = hashlib . sha512 ( os . urandom ( 64 ) ) . hexdigest ( ) # Generate new sqmId sqm_id = ""{"" + str ( uuid . uuid4 ( ) ) . upper ( ) + ""}"" self . update_machine_id_file ( dev_device_id ) return { ""telemetry.devDeviceId"" : dev_device_id , ""telemetry.macMachineId"" : mac_machine_id , ""telemetry.machineId"" : machine_id , ""telemetry.sqmId"" : sqm_id , ""storage.serviceMachineId"" : dev_device_id , # Add storage.serviceMachineId }",Generate new machine ID
/airweave/backend/airweave/platform/sources/notion.py,_format_child_blocks,"def _format_child_blocks(
        self, block_content: dict, block: dict, block_type: str, page_breadcrumbs: List[Breadcrumb]
    ) -> str:
        """"""Format child page and database blocks.""""""
        if block_type == ""child_page"":
            title = block_content.get(""title"", ""Untitled Page"")
            return f""📄 **[{title}]** (Child Page)""
        else:  # child_database
            return self._format_child_database_block(block_content, block, page_breadcrumbs)","def _format_child_blocks(
        self, block_content: dict, block: dict, block_type: str, page_breadcrumbs: List[Breadcrumb]
    ) -> str:
        """"""Format child page and database blocks.""""""
        if block_type == ""child_page"":
            title = block_content.get(""title"", ""Untitled Page"")
            return f""📄 **[{title}]** (Child Page)""
        else:  # child_database
            return self._format_child_database_block(block_content, block, page_breadcrumbs)",Format child page and database blocks.,Format child page and database blocks.,"def _format_child_blocks(
        self, block_content: dict, block: dict, block_type: str, page_breadcrumbs: List[Breadcrumb]
    ) -> str:
        
        if block_type == ""child_page"":
            title = block_content.get(""title"", ""Untitled Page"")
            return f""📄 **[{title}]** (Child Page)""
        else:  # child_database
            return self._format_child_database_block(block_content, block, page_breadcrumbs)",Format child page and database blocks.,"def _format_child_blocks ( self , block_content : dict , block : dict , block_type : str , page_breadcrumbs : List [ Breadcrumb ] ) -> str : if block_type == ""child_page"" : title = block_content . get ( ""title"" , ""Untitled Page"" ) return f""📄 **[{title}]** (Child Page)"" else : # child_database return self . _format_child_database_block ( block_content , block , page_breadcrumbs )",Format child page and database blocks.
/open-r1/src/open_r1/utils/competitive_programming/ioi_utils.py,load_ioi_tests_for_year,"def load_ioi_tests_for_year(year: int) -> dict[str, dict[str, tuple[str, str]]]:
    """"""
    Load IOI tests for a given year.
    """"""
    tests_dataset = load_dataset(""open-r1/ioi-test-cases"", name=f""{year}"", split=""train"")
    test_cases = defaultdict(dict)
    for test_case in tests_dataset:
        test_cases[test_case[""problem_id""]][test_case[""test_name""]] = test_case[""test_input""], test_case[""test_output""]
    return test_cases","def load_ioi_tests_for_year(year: int) -> dict[str, dict[str, tuple[str, str]]]:
    """"""
    Load IOI tests for a given year.
    """"""
    tests_dataset = load_dataset(""open-r1/ioi-test-cases"", name=f""{year}"", split=""train"")
    test_cases = defaultdict(dict)
    for test_case in tests_dataset:
        test_cases[test_case[""problem_id""]][test_case[""test_name""]] = test_case[""test_input""], test_case[""test_output""]
    return test_cases",Load IOI tests for a given year.,Load IOI tests for a given year.,"def load_ioi_tests_for_year(year: int) -> dict[str, dict[str, tuple[str, str]]]:
    
    tests_dataset = load_dataset(""open-r1/ioi-test-cases"", name=f""{year}"", split=""train"")
    test_cases = defaultdict(dict)
    for test_case in tests_dataset:
        test_cases[test_case[""problem_id""]][test_case[""test_name""]] = test_case[""test_input""], test_case[""test_output""]
    return test_cases",Load IOI tests for a given year.,"def load_ioi_tests_for_year ( year : int ) -> dict [ str , dict [ str , tuple [ str , str ] ] ] : tests_dataset = load_dataset ( ""open-r1/ioi-test-cases"" , name = f""{year}"" , split = ""train"" ) test_cases = defaultdict ( dict ) for test_case in tests_dataset : test_cases [ test_case [ ""problem_id"" ] ] [ test_case [ ""test_name"" ] ] = test_case [ ""test_input"" ] , test_case [ ""test_output"" ] return test_cases",Load IOI tests for a given year.
/NLWeb/code/llm/azure_deepseek.py,get_api_version,"def get_api_version(cls) -> str:
        """"""Get DeepSeek Azure API version from config""""""
        logger.debug(""Retrieving DeepSeek Azure API version from config"")
        provider_config = CONFIG.llm_endpoints.get(""deepseek_azure"")
        if provider_config and provider_config.api_version:
            logger.debug(f""DeepSeek Azure API version: {provider_config.api_version}"")
            return provider_config.api_version
        logger.warning(""DeepSeek Azure API version not found in config"")
        return None","def get_api_version(cls) -> str:
        """"""Get DeepSeek Azure API version from config""""""
        logger.debug(""Retrieving DeepSeek Azure API version from config"")
        provider_config = CONFIG.llm_endpoints.get(""deepseek_azure"")
        if provider_config and provider_config.api_version:
            logger.debug(f""DeepSeek Azure API version: {provider_config.api_version}"")
            return provider_config.api_version
        logger.warning(""DeepSeek Azure API version not found in config"")
        return None",Get DeepSeek Azure API version from config,Get DeepSeek Azure API version from config,"def get_api_version(cls) -> str:
        
        logger.debug(""Retrieving DeepSeek Azure API version from config"")
        provider_config = CONFIG.llm_endpoints.get(""deepseek_azure"")
        if provider_config and provider_config.api_version:
            logger.debug(f""DeepSeek Azure API version: {provider_config.api_version}"")
            return provider_config.api_version
        logger.warning(""DeepSeek Azure API version not found in config"")
        return None",Get DeepSeek Azure API version from config,"def get_api_version ( cls ) -> str : logger . debug ( ""Retrieving DeepSeek Azure API version from config"" ) provider_config = CONFIG . llm_endpoints . get ( ""deepseek_azure"" ) if provider_config and provider_config . api_version : logger . debug ( f""DeepSeek Azure API version: {provider_config.api_version}"" ) return provider_config . api_version logger . warning ( ""DeepSeek Azure API version not found in config"" ) return None",Get DeepSeek Azure API version from config
/Second-Me/lpm_kernel/file_data/document_service.py,_update_analyze_status_failed,"def _update_analyze_status_failed(self, doc_id: int) -> None:
        """"""update status as failed""""""
        try:
            with self._repository._db.session() as session:
                document = session.get(self._repository.model, doc_id)
                if document:
                    document.analyze_status = ProcessStatus.FAILED
                    session.commit()
                    logger.debug(f""Updated analyze status for document {doc_id} to FAILED"")
                else:
                    logger.warning(f""Document not found with id: {doc_id}"")
        except Exception as e:
            logger.error(f""Error updating document analyze status: {str(e)}"")","def _update_analyze_status_failed(self, doc_id: int) -> None:
        """"""update status as failed""""""
        try:
            with self._repository._db.session() as session:
                document = session.get(self._repository.model, doc_id)
                if document:
                    document.analyze_status = ProcessStatus.FAILED
                    session.commit()
                    logger.debug(f""Updated analyze status for document {doc_id} to FAILED"")
                else:
                    logger.warning(f""Document not found with id: {doc_id}"")
        except Exception as e:
            logger.error(f""Error updating document analyze status: {str(e)}"")",update status as failed,update status as failed,"def _update_analyze_status_failed(self, doc_id: int) -> None:
        
        try:
            with self._repository._db.session() as session:
                document = session.get(self._repository.model, doc_id)
                if document:
                    document.analyze_status = ProcessStatus.FAILED
                    session.commit()
                    logger.debug(f""Updated analyze status for document {doc_id} to FAILED"")
                else:
                    logger.warning(f""Document not found with id: {doc_id}"")
        except Exception as e:
            logger.error(f""Error updating document analyze status: {str(e)}"")",update status as failed,"def _update_analyze_status_failed ( self , doc_id : int ) -> None : try : with self . _repository . _db . session ( ) as session : document = session . get ( self . _repository . model , doc_id ) if document : document . analyze_status = ProcessStatus . FAILED session . commit ( ) logger . debug ( f""Updated analyze status for document {doc_id} to FAILED"" ) else : logger . warning ( f""Document not found with id: {doc_id}"" ) except Exception as e : logger . error ( f""Error updating document analyze status: {str(e)}"" )",update status as failed
/python-sdk/src/mcp/server/fastmcp/resources/templates.py,matches,"def matches(self, uri: str) -> dict[str, Any] | None:
        """"""Check if URI matches template and extract parameters.""""""
        # Convert template to regex pattern
        pattern = self.uri_template.replace(""{"", ""(?P<"").replace(""}"", "">[^/]+)"")
        match = re.match(f""^{pattern}$"", uri)
        if match:
            return match.groupdict()
        return None","def matches(self, uri: str) -> dict[str, Any] | None:
        """"""Check if URI matches template and extract parameters.""""""
        # Convert template to regex pattern
        pattern = self.uri_template.replace(""{"", ""(?P<"").replace(""}"", "">[^/]+)"")
        match = re.match(f""^{pattern}$"", uri)
        if match:
            return match.groupdict()
        return None",Check if URI matches template and extract parameters.,Check if URI matches template and extract parameters.,"def matches(self, uri: str) -> dict[str, Any] | None:
        
        # Convert template to regex pattern
        pattern = self.uri_template.replace(""{"", ""(?P<"").replace(""}"", "">[^/]+)"")
        match = re.match(f""^{pattern}$"", uri)
        if match:
            return match.groupdict()
        return None",Check if URI matches template and extract parameters.,"def matches ( self , uri : str ) -> dict [ str , Any ] | None : # Convert template to regex pattern pattern = self . uri_template . replace ( ""{"" , ""(?P<"" ) . replace ( ""}"" , "">[^/]+)"" ) match = re . match ( f""^{pattern}$"" , uri ) if match : return match . groupdict ( ) return None",Check if URI matches template and extract parameters.
/BabelDOC/babeldoc/assets/assets.py,_retry_if_not_cancelled_and_failed,"def _retry_if_not_cancelled_and_failed(retry_state):
    """"""Only retry if the exception is not CancelledError and the attempt failed.""""""
    if retry_state.outcome.failed:
        exception = retry_state.outcome.exception()
        # Don't retry on CancelledError
        if isinstance(exception, asyncio.CancelledError):
            logger.debug(""Operation was cancelled, not retrying"")
            return False
        # Retry on network related errors
        if isinstance(
            exception, httpx.HTTPError | ConnectionError | ValueError | TimeoutError
        ):
            logger.warning(f""Network error occurred: {exception}, will retry"")
            return True
    # Don't retry on success
    return False","def _retry_if_not_cancelled_and_failed(retry_state):
    """"""Only retry if the exception is not CancelledError and the attempt failed.""""""
    if retry_state.outcome.failed:
        exception = retry_state.outcome.exception()
        # Don't retry on CancelledError
        if isinstance(exception, asyncio.CancelledError):
            logger.debug(""Operation was cancelled, not retrying"")
            return False
        # Retry on network related errors
        if isinstance(
            exception, httpx.HTTPError | ConnectionError | ValueError | TimeoutError
        ):
            logger.warning(f""Network error occurred: {exception}, will retry"")
            return True
    # Don't retry on success
    return False",Only retry if the exception is not CancelledError and the attempt failed.,Only retry if the exception is not CancelledError and the attempt failed.,"def _retry_if_not_cancelled_and_failed(retry_state):
    
    if retry_state.outcome.failed:
        exception = retry_state.outcome.exception()
        # Don't retry on CancelledError
        if isinstance(exception, asyncio.CancelledError):
            logger.debug(""Operation was cancelled, not retrying"")
            return False
        # Retry on network related errors
        if isinstance(
            exception, httpx.HTTPError | ConnectionError | ValueError | TimeoutError
        ):
            logger.warning(f""Network error occurred: {exception}, will retry"")
            return True
    # Don't retry on success
    return False",Only retry if the exception is not CancelledError and the attempt failed.,"def _retry_if_not_cancelled_and_failed ( retry_state ) : if retry_state . outcome . failed : exception = retry_state . outcome . exception ( ) # Don't retry on CancelledError if isinstance ( exception , asyncio . CancelledError ) : logger . debug ( ""Operation was cancelled, not retrying"" ) return False # Retry on network related errors if isinstance ( exception , httpx . HTTPError | ConnectionError | ValueError | TimeoutError ) : logger . warning ( f""Network error occurred: {exception}, will retry"" ) return True # Don't retry on success return False",Only retry if the exception is not CancelledError and the attempt failed.
/airweave/backend/airweave/platform/configs/_base.py,validate_config_values,"def validate_config_values(self):
        """"""Validate that no values are dictionaries (depth 0).""""""
        for key, value in self.__dict__.items():
            if isinstance(value, dict):
                raise ValueError(f""Value for '{key}' must not be a dictionary (depth 0 only)"")
        return self","def validate_config_values(self):
        """"""Validate that no values are dictionaries (depth 0).""""""
        for key, value in self.__dict__.items():
            if isinstance(value, dict):
                raise ValueError(f""Value for '{key}' must not be a dictionary (depth 0 only)"")
        return self",Validate that no values are dictionaries (depth 0).,Validate that no values are dictionaries (depth 0).,"def validate_config_values(self):
        
        for key, value in self.__dict__.items():
            if isinstance(value, dict):
                raise ValueError(f""Value for '{key}' must not be a dictionary (depth 0 only)"")
        return self",Validate that no values are dictionaries (depth 0).,"def validate_config_values ( self ) : for key , value in self . __dict__ . items ( ) : if isinstance ( value , dict ) : raise ValueError ( f""Value for '{key}' must not be a dictionary (depth 0 only)"" ) return self",Validate that no values are dictionaries (depth 0).
/R1-V/src/r1-v/src/open_r1/evaluate.py,prompt_fn,"def prompt_fn(line, task_name: str = None):
    """"""Assumes the model is either prompted to emit \\boxed{answer} or does so automatically""""""
    return Doc(
        task_name=task_name,
        query=line[""problem""],
        choices=[line[""solution""]],
        gold_index=0,
    )","def prompt_fn(line, task_name: str = None):
    """"""Assumes the model is either prompted to emit \\boxed{answer} or does so automatically""""""
    return Doc(
        task_name=task_name,
        query=line[""problem""],
        choices=[line[""solution""]],
        gold_index=0,
    )",Assumes the model is either prompted to emit \boxed{answer} or does so automatically,Assumes the model is either prompted to emit \boxed{answer} or does so automatically,"def prompt_fn(line, task_name: str = None):
    
    return Doc(
        task_name=task_name,
        query=line[""problem""],
        choices=[line[""solution""]],
        gold_index=0,
    )",Assumes the model is either prompted to emit \boxed{answer} or does so automatically,"def prompt_fn ( line , task_name : str = None ) : return Doc ( task_name = task_name , query = line [ ""problem"" ] , choices = [ line [ ""solution"" ] ] , gold_index = 0 , )",Assumes the model is either prompted to emit \boxed{answer} or does so automatically
/smolagents/src/smolagents/tools.py,_get_gradio_app_code,"def _get_gradio_app_code(self, tool_module_name: str = ""tool"") -> str:
        """"""Get the Gradio app code.""""""
        class_name = self.__class__.__name__
        return textwrap.dedent(
            f""""""\
            from smolagents import launch_gradio_demo
            from {tool_module_name} import {class_name}

            tool = {class_name}()
            launch_gradio_demo(tool)
            """"""
        )","def _get_gradio_app_code(self, tool_module_name: str = ""tool"") -> str:
        """"""Get the Gradio app code.""""""
        class_name = self.__class__.__name__
        return textwrap.dedent(
            f""""""\
            from smolagents import launch_gradio_demo
            from {tool_module_name} import {class_name}

            tool = {class_name}()
            launch_gradio_demo(tool)
            """"""
        )",Get the Gradio app code.,Get the Gradio app code.,"def _get_gradio_app_code(self, tool_module_name: str = ""tool"") -> str:
        
        class_name = self.__class__.__name__
        return textwrap.dedent(
            f
        )",Get the Gradio app code.,"def _get_gradio_app_code ( self , tool_module_name : str = ""tool"" ) -> str : class_name = self . __class__ . __name__ return textwrap . dedent ( f )",Get the Gradio app code.
/Second-Me/lpm_kernel/api/domains/space/space_dto.py,to_dict,"def to_dict(self) -> dict:
        """"""Convert DTO to dictionary""""""
        return {
            ""id"": self.id,
            ""space_id"": self.space_id,
            ""sender_endpoint"": self.sender_endpoint,
            ""content"": self.content,
            ""message_type"": self.message_type,
            ""round"": self.round,
            ""create_time"": self.create_time.isoformat(),
            ""role"": self.role
        }","def to_dict(self) -> dict:
        """"""Convert DTO to dictionary""""""
        return {
            ""id"": self.id,
            ""space_id"": self.space_id,
            ""sender_endpoint"": self.sender_endpoint,
            ""content"": self.content,
            ""message_type"": self.message_type,
            ""round"": self.round,
            ""create_time"": self.create_time.isoformat(),
            ""role"": self.role
        }",Convert DTO to dictionary,Convert DTO to dictionary,"def to_dict(self) -> dict:
        
        return {
            ""id"": self.id,
            ""space_id"": self.space_id,
            ""sender_endpoint"": self.sender_endpoint,
            ""content"": self.content,
            ""message_type"": self.message_type,
            ""round"": self.round,
            ""create_time"": self.create_time.isoformat(),
            ""role"": self.role
        }",Convert DTO to dictionary,"def to_dict ( self ) -> dict : return { ""id"" : self . id , ""space_id"" : self . space_id , ""sender_endpoint"" : self . sender_endpoint , ""content"" : self . content , ""message_type"" : self . message_type , ""round"" : self . round , ""create_time"" : self . create_time . isoformat ( ) , ""role"" : self . role }",Convert DTO to dictionary
/open_deep_research/src/open_deep_research/configuration.py,from_runnable_config,"def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        """"""Create a Configuration instance from a RunnableConfig.""""""
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        values: dict[str, Any] = {
            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))
            for f in fields(cls)
            if f.init
        }
        return cls(**{k: v for k, v in values.items() if v})","def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        """"""Create a Configuration instance from a RunnableConfig.""""""
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        values: dict[str, Any] = {
            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))
            for f in fields(cls)
            if f.init
        }
        return cls(**{k: v for k, v in values.items() if v})",Create a Configuration instance from a RunnableConfig.,Create a Configuration instance from a RunnableConfig.,"def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        values: dict[str, Any] = {
            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))
            for f in fields(cls)
            if f.init
        }
        return cls(**{k: v for k, v in values.items() if v})",Create a Configuration instance from a RunnableConfig.,"def from_runnable_config ( cls , config : Optional [ RunnableConfig ] = None ) -> ""Configuration"" : configurable = ( config [ ""configurable"" ] if config and ""configurable"" in config else { } ) values : dict [ str , Any ] = { f . name : os . environ . get ( f . name . upper ( ) , configurable . get ( f . name ) ) for f in fields ( cls ) if f . init } return cls ( ** { k : v for k , v in values . items ( ) if v } )",Create a Configuration instance from a RunnableConfig.
/olmocr/olmocr/bench/katex/render.py,get_equation_hash,"def get_equation_hash(equation, bg_color=""white"", text_color=""black"", font_size=24):
    """"""
    Calculate SHA1 hash of the equation string and rendering parameters.
    """"""
    params_str = f""{equation}|{bg_color}|{text_color}|{font_size}""
    return hashlib.sha1(params_str.encode(""utf-8"")).hexdigest()","def get_equation_hash(equation, bg_color=""white"", text_color=""black"", font_size=24):
    """"""
    Calculate SHA1 hash of the equation string and rendering parameters.
    """"""
    params_str = f""{equation}|{bg_color}|{text_color}|{font_size}""
    return hashlib.sha1(params_str.encode(""utf-8"")).hexdigest()",Calculate SHA1 hash of the equation string and rendering parameters.,Calculate SHA1 hash of the equation string and rendering parameters.,"def get_equation_hash(equation, bg_color=""white"", text_color=""black"", font_size=24):
    
    params_str = f""{equation}|{bg_color}|{text_color}|{font_size}""
    return hashlib.sha1(params_str.encode(""utf-8"")).hexdigest()",Calculate SHA1 hash of the equation string and rendering parameters.,"def get_equation_hash ( equation , bg_color = ""white"" , text_color = ""black"" , font_size = 24 ) : params_str = f""{equation}|{bg_color}|{text_color}|{font_size}"" return hashlib . sha1 ( params_str . encode ( ""utf-8"" ) ) . hexdigest ( )",Calculate SHA1 hash of the equation string and rendering parameters.
/Sana/diffusion/model/gaussian_diffusion.py,q_posterior_mean_variance,"def q_posterior_mean_variance(self, x_start, x_t, t):
        """"""
        Compute the mean and variance of the diffusion posterior:
            q(x_{t-1} | x_t, x_0)
        """"""
        assert x_start.shape == x_t.shape
        posterior_mean = (
            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start
            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t
        )
        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)
        posterior_log_variance_clipped = _extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)
        assert (
            posterior_mean.shape[0]
            == posterior_variance.shape[0]
            == posterior_log_variance_clipped.shape[0]
            == x_start.shape[0]
        )
        return posterior_mean, posterior_variance, posterior_log_variance_clipped","def q_posterior_mean_variance(self, x_start, x_t, t):
        """"""
        Compute the mean and variance of the diffusion posterior:
            q(x_{t-1} | x_t, x_0)
        """"""
        assert x_start.shape == x_t.shape
        posterior_mean = (
            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start
            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t
        )
        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)
        posterior_log_variance_clipped = _extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)
        assert (
            posterior_mean.shape[0]
            == posterior_variance.shape[0]
            == posterior_log_variance_clipped.shape[0]
            == x_start.shape[0]
        )
        return posterior_mean, posterior_variance, posterior_log_variance_clipped","Compute the mean and variance of the diffusion posterior:
    q(x_{t-1} | x_t, x_0)","Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)","def q_posterior_mean_variance(self, x_start, x_t, t):
        
        assert x_start.shape == x_t.shape
        posterior_mean = (
            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start
            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t
        )
        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)
        posterior_log_variance_clipped = _extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)
        assert (
            posterior_mean.shape[0]
            == posterior_variance.shape[0]
            == posterior_log_variance_clipped.shape[0]
            == x_start.shape[0]
        )
        return posterior_mean, posterior_variance, posterior_log_variance_clipped","Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)","def q_posterior_mean_variance ( self , x_start , x_t , t ) : assert x_start . shape == x_t . shape posterior_mean = ( _extract_into_tensor ( self . posterior_mean_coef1 , t , x_t . shape ) * x_start + _extract_into_tensor ( self . posterior_mean_coef2 , t , x_t . shape ) * x_t ) posterior_variance = _extract_into_tensor ( self . posterior_variance , t , x_t . shape ) posterior_log_variance_clipped = _extract_into_tensor ( self . posterior_log_variance_clipped , t , x_t . shape ) assert ( posterior_mean . shape [ 0 ] == posterior_variance . shape [ 0 ] == posterior_log_variance_clipped . shape [ 0 ] == x_start . shape [ 0 ] ) return posterior_mean , posterior_variance , posterior_log_variance_clipped","Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)"
/VideoLingo/core/_6_gen_sub.py,convert_to_srt_format,"def convert_to_srt_format(start_time, end_time):
    """"""Convert time (in seconds) to the format: hours:minutes:seconds,milliseconds""""""
    def seconds_to_hmsm(seconds):
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        milliseconds = int(seconds * 1000) % 1000
        return f""{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}""

    start_srt = seconds_to_hmsm(start_time)
    end_srt = seconds_to_hmsm(end_time)
    return f""{start_srt} --> {end_srt}""","def convert_to_srt_format(start_time, end_time):
    """"""Convert time (in seconds) to the format: hours:minutes:seconds,milliseconds""""""
    def seconds_to_hmsm(seconds):
        seconds = seconds % 60
        milliseconds = int(seconds * 1000) % 1000
        return f""{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}""

    start_srt = seconds_to_hmsm(start_time)
    end_srt = seconds_to_hmsm(end_time)
    return f""{start_srt} --> {end_srt}""","Convert time (in seconds) to the format: hours:minutes:seconds,milliseconds","Convert time (in seconds) to the format: hours:minutes:seconds,milliseconds","def convert_to_srt_format(start_time, end_time):
    
    def seconds_to_hmsm(seconds):
        seconds = seconds % 60
        milliseconds = int(seconds * 1000) % 1000
        return f""{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}""

    start_srt = seconds_to_hmsm(start_time)
    end_srt = seconds_to_hmsm(end_time)
    return f""{start_srt} --> {end_srt}""","Convert time (in seconds) to the format: hours:minutes:seconds,milliseconds","def convert_to_srt_format ( start_time , end_time ) : def seconds_to_hmsm ( seconds ) : seconds = seconds % 60 milliseconds = int ( seconds * 1000 ) % 1000 return f""{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}"" start_srt = seconds_to_hmsm ( start_time ) end_srt = seconds_to_hmsm ( end_time ) return f""{start_srt} --> {end_srt}""","Convert time (in seconds) to the format: hours:minutes:seconds,milliseconds"
/ragaai-catalyst/tests/run_pytest_and_print_and_save_results.py,save_report,"def save_report(report, filename=None):
    """"""Save the report to a file.""""""
    if filename is None:
        filename = f""test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt""
    with open(filename, 'w') as file:
        file.write(report)
    print(f""Report saved to {os.path.abspath(filename)}"")","def save_report(report, filename=None):
    """"""Save the report to a file.""""""
    if filename is None:
        filename = f""test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt""
    with open(filename, 'w') as file:
        file.write(report)
    print(f""Report saved to {os.path.abspath(filename)}"")",Save the report to a file.,Save the report to a file.,"def save_report(report, filename=None):
    
    if filename is None:
        filename = f""test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt""
    with open(filename, 'w') as file:
        file.write(report)
    print(f""Report saved to {os.path.abspath(filename)}"")",Save the report to a file.,"def save_report ( report , filename = None ) : if filename is None : filename = f""test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"" with open ( filename , 'w' ) as file : file . write ( report ) print ( f""Report saved to {os.path.abspath(filename)}"" )",Save the report to a file.
/TinyZero/verl/third_party/vllm/vllm_v_0_5_4/spmd_gpu_executor.py,initialize_cache,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        """"""Initialize the KV cache in all workers.
        """"""

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f'before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f'after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )","def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        """"""Initialize the KV cache in all workers.
        """"""

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f'before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f'after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )",Initialize the KV cache in all workers.,Initialize the KV cache in all workers.,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f'before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f'after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )",Initialize the KV cache in all workers.,"def initialize_cache ( self , num_gpu_blocks : int , num_cpu_blocks : int ) -> None : # NOTE: We log here to avoid multiple logs when number of workers is # greater than one. We could log in the engine, but not all executors # have GPUs. logger . info ( ""# GPU blocks: %d, # CPU blocks: %d"" , num_gpu_blocks , num_cpu_blocks ) self . cache_config . num_gpu_blocks = num_gpu_blocks self . cache_config . num_cpu_blocks = num_cpu_blocks if torch . distributed . get_rank ( ) == 0 : print ( f'before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB' ) self . worker . initialize_cache ( num_gpu_blocks = num_gpu_blocks , num_cpu_blocks = num_cpu_blocks ) if torch . distributed . get_rank ( ) == 0 : print ( f'after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB' )",Initialize the KV cache in all workers.
/magentic-ui/src/magentic_ui/backend/web/deps.py,get_db_context,"def get_db_context():
    """"""Provide a transactional scope around a series of operations.""""""
    if not _db_manager:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database manager not initialized"",
        )
    try:
        yield _db_manager
    except Exception as e:
        logger.error(f""Database operation failed: {str(e)}"")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database operation failed"",
        ) from e","def get_db_context():
    """"""Provide a transactional scope around a series of operations.""""""
    if not _db_manager:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database manager not initialized"",
        )
    try:
        yield _db_manager
    except Exception as e:
        logger.error(f""Database operation failed: {str(e)}"")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database operation failed"",
        ) from e",Provide a transactional scope around a series of operations.,Provide a transactional scope around a series of operations.,"def get_db_context():
    
    if not _db_manager:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database manager not initialized"",
        )
    try:
        yield _db_manager
    except Exception as e:
        logger.error(f""Database operation failed: {str(e)}"")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database operation failed"",
        ) from e",Provide a transactional scope around a series of operations.,"def get_db_context ( ) : if not _db_manager : raise HTTPException ( status_code = status . HTTP_500_INTERNAL_SERVER_ERROR , detail = ""Database manager not initialized"" , ) try : yield _db_manager except Exception as e : logger . error ( f""Database operation failed: {str(e)}"" ) raise HTTPException ( status_code = status . HTTP_500_INTERNAL_SERVER_ERROR , detail = ""Database operation failed"" , ) from e",Provide a transactional scope around a series of operations.
/Second-Me/lpm_kernel/file_data/process_factory.py,get_processor,"def get_processor(cls, file_type: FileType) -> Type[BaseFileProcessor]:
        """"""Get processor before ensuring initialization""""""
        if not cls._initialized:
            cls.init()
        print(f""Current registered processors: {cls._processors}"")
        if file_type not in cls._processors:
            raise ValueError(f""No processor found for {file_type}"")
        return cls._processors[file_type]","def get_processor(cls, file_type: FileType) -> Type[BaseFileProcessor]:
        """"""Get processor before ensuring initialization""""""
        if not cls._initialized:
            cls.init()
        print(f""Current registered processors: {cls._processors}"")
        if file_type not in cls._processors:
            raise ValueError(f""No processor found for {file_type}"")
        return cls._processors[file_type]",Get processor before ensuring initialization,Get processor before ensuring initialization,"def get_processor(cls, file_type: FileType) -> Type[BaseFileProcessor]:
        
        if not cls._initialized:
            cls.init()
        print(f""Current registered processors: {cls._processors}"")
        if file_type not in cls._processors:
            raise ValueError(f""No processor found for {file_type}"")
        return cls._processors[file_type]",Get processor before ensuring initialization,"def get_processor ( cls , file_type : FileType ) -> Type [ BaseFileProcessor ] : if not cls . _initialized : cls . init ( ) print ( f""Current registered processors: {cls._processors}"" ) if file_type not in cls . _processors : raise ValueError ( f""No processor found for {file_type}"" ) return cls . _processors [ file_type ]",Get processor before ensuring initialization
/OpenManus-RL/openmanus_rl/agentgym/agentenv-webarena/webarena/tests/test_evaluation_harness/test_evaluators.py,tf_roll_out,"def tf_roll_out(
    agent: Agent, env: ScriptBrowserEnv, config_file: str
) -> list[Any]:
    """"""Roll out the agent using teacher forcing actions""""""
    obs, state_info = env.reset(options={""config_file"": config_file})

    trajectory: list[Any] = [{""observation"": obs, ""info"": state_info}]
    while True:
        action = agent.next_action(
            trajectory=trajectory, intent="""", meta_data={}
        )
        trajectory.append(action)
        if action[""action_type""] == ActionTypes.STOP:
            break

        # preceed to next action
        obs, reward, terminated, truncated, info = env.step(action)
        state_info = {""observation"": obs, ""info"": info}
        trajectory.append(state_info)

    return trajectory","def tf_roll_out(
    agent: Agent, env: ScriptBrowserEnv, config_file: str
) -> list[Any]:
    """"""Roll out the agent using teacher forcing actions""""""
    obs, state_info = env.reset(options={""config_file"": config_file})

    trajectory: list[Any] = [{""observation"": obs, ""info"": state_info}]
    while True:
        action = agent.next_action(
            trajectory=trajectory, intent="""", meta_data={}
        )
        trajectory.append(action)
        if action[""action_type""] == ActionTypes.STOP:
            break

        # preceed to next action
        obs, reward, terminated, truncated, info = env.step(action)
        state_info = {""observation"": obs, ""info"": info}
        trajectory.append(state_info)

    return trajectory",Roll out the agent using teacher forcing actions,Roll out the agent using teacher forcing actions,"def tf_roll_out(
    agent: Agent, env: ScriptBrowserEnv, config_file: str
) -> list[Any]:
    
    obs, state_info = env.reset(options={""config_file"": config_file})

    trajectory: list[Any] = [{""observation"": obs, ""info"": state_info}]
    while True:
        action = agent.next_action(
            trajectory=trajectory, intent="""", meta_data={}
        )
        trajectory.append(action)
        if action[""action_type""] == ActionTypes.STOP:
            break

        # preceed to next action
        obs, reward, terminated, truncated, info = env.step(action)
        state_info = {""observation"": obs, ""info"": info}
        trajectory.append(state_info)

    return trajectory",Roll out the agent using teacher forcing actions,"def tf_roll_out ( agent : Agent , env : ScriptBrowserEnv , config_file : str ) -> list [ Any ] : obs , state_info = env . reset ( options = { ""config_file"" : config_file } ) trajectory : list [ Any ] = [ { ""observation"" : obs , ""info"" : state_info } ] while True : action = agent . next_action ( trajectory = trajectory , intent = """" , meta_data = { } ) trajectory . append ( action ) if action [ ""action_type"" ] == ActionTypes . STOP : break # preceed to next action obs , reward , terminated , truncated , info = env . step ( action ) state_info = { ""observation"" : obs , ""info"" : info } trajectory . append ( state_info ) return trajectory",Roll out the agent using teacher forcing actions
/Agent-S/gui_agents/s2/cli_app.py,show_permission_dialog,"def show_permission_dialog(code: str, action_description: str):
    """"""Show a platform-specific permission dialog and return True if approved.""""""
    if platform.system() == ""Darwin"":
        result = os.system(
            f'osascript -e \'display dialog ""Do you want to execute this action?\n\n{code} which will try to {action_description}"" with title ""Action Permission"" buttons {{""Cancel"", ""OK""}} default button ""OK"" cancel button ""Cancel""\''
        )
        return result == 0
    elif platform.system() == ""Linux"":
        result = os.system(
            f'zenity --question --title=""Action Permission"" --text=""Do you want to execute this action?\n\n{code}"" --width=400 --height=200'
        )
        return result == 0
    return False","def show_permission_dialog(code: str, action_description: str):
    """"""Show a platform-specific permission dialog and return True if approved.""""""
    if platform.system() == ""Darwin"":
        result = os.system(
            f'osascript -e \'display dialog ""Do you want to execute this action?\n\n{code} which will try to {action_description}"" with title ""Action Permission"" buttons {{""Cancel"", ""OK""}} default button ""OK"" cancel button ""Cancel""\''
        )
        return result == 0
    elif platform.system() == ""Linux"":
        result = os.system(
            f'zenity --question --title=""Action Permission"" --text=""Do you want to execute this action?\n\n{code}"" --width=400 --height=200'
        )
        return result == 0
    return False",Show a platform-specific permission dialog and return True if approved.,Show a platform-specific permission dialog and return True if approved.,"def show_permission_dialog(code: str, action_description: str):
    
    if platform.system() == ""Darwin"":
        result = os.system(
            f'osascript -e \'display dialog ""Do you want to execute this action?\n\n{code} which will try to {action_description}"" with title ""Action Permission"" buttons {{""Cancel"", ""OK""}} default button ""OK"" cancel button ""Cancel""\''
        )
        return result == 0
    elif platform.system() == ""Linux"":
        result = os.system(
            f'zenity --question --title=""Action Permission"" --text=""Do you want to execute this action?\n\n{code}"" --width=400 --height=200'
        )
        return result == 0
    return False",Show a platform-specific permission dialog and return True if approved.,"def show_permission_dialog ( code : str , action_description : str ) : if platform . system ( ) == ""Darwin"" : result = os . system ( f'osascript -e \'display dialog ""Do you want to execute this action?\n\n{code} which will try to {action_description}"" with title ""Action Permission"" buttons {{""Cancel"", ""OK""}} default button ""OK"" cancel button ""Cancel""\'' ) return result == 0 elif platform . system ( ) == ""Linux"" : result = os . system ( f'zenity --question --title=""Action Permission"" --text=""Do you want to execute this action?\n\n{code}"" --width=400 --height=200' ) return result == 0 return False",Show a platform-specific permission dialog and return True if approved.
/Scrapling/scrapling/engines/pw.py,__launch_kwargs,"def __launch_kwargs(self):
        """"""Creates the arguments we will use while launching playwright's browser""""""
        launch_kwargs = {'headless': self.headless, 'ignore_default_args': self.harmful_default_args, 'channel': 'chrome' if self.real_chrome else 'chromium'}
        if self.stealth:
            launch_kwargs.update({'args': self.__set_flags(), 'chromium_sandbox': True})

        return launch_kwargs","def __launch_kwargs(self):
        """"""Creates the arguments we will use while launching playwright's browser""""""
        launch_kwargs = {'headless': self.headless, 'ignore_default_args': self.harmful_default_args, 'channel': 'chrome' if self.real_chrome else 'chromium'}
        if self.stealth:
            launch_kwargs.update({'args': self.__set_flags(), 'chromium_sandbox': True})

        return launch_kwargs",Creates the arguments we will use while launching playwright's browser,Creates the arguments we will use while launching playwright's browser,"def __launch_kwargs(self):
        
        launch_kwargs = {'headless': self.headless, 'ignore_default_args': self.harmful_default_args, 'channel': 'chrome' if self.real_chrome else 'chromium'}
        if self.stealth:
            launch_kwargs.update({'args': self.__set_flags(), 'chromium_sandbox': True})

        return launch_kwargs",Creates the arguments we will use while launching playwright's browser,"def __launch_kwargs ( self ) : launch_kwargs = { 'headless' : self . headless , 'ignore_default_args' : self . harmful_default_args , 'channel' : 'chrome' if self . real_chrome else 'chromium' } if self . stealth : launch_kwargs . update ( { 'args' : self . __set_flags ( ) , 'chromium_sandbox' : True } ) return launch_kwargs",Creates the arguments we will use while launching playwright's browser
/NLWeb/code/llm/anthropic.py,clean_response,"def clean_response(cls, content: str) -> Dict[str, Any]:
        """"""
        Strip markdown fences and extract the first JSON object.
        """"""
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            raise ValueError(""No JSON object found in response"")
        return json.loads(match.group(1))","def clean_response(cls, content: str) -> Dict[str, Any]:
        """"""
        Strip markdown fences and extract the first JSON object.
        """"""
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            raise ValueError(""No JSON object found in response"")
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,Strip markdown fences and extract the first JSON object.,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            raise ValueError(""No JSON object found in response"")
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,"def clean_response ( cls , content : str ) -> Dict [ str , Any ] : cleaned = re . sub ( r""```(?:json)?\s*"" , """" , content ) . strip ( ) match = re . search ( r""(\{.*\})"" , cleaned , re . S ) if not match : logger . error ( ""Failed to parse JSON from content: %r"" , content ) raise ValueError ( ""No JSON object found in response"" ) return json . loads ( match . group ( 1 ) )",Strip markdown fences and extract the first JSON object.
/ragaai-catalyst/examples/pii_masking_example/llamaindex_agentic_fastapi/request.py,make_request,"def make_request(prompt):
    """"""Make request and print raw response""""""
    payload = {""input"": prompt}
    
    try:
        response = requests.post(
            API_URL,
            json=payload,
            stream=True
        )
        
        print(f""\nMaking request with prompt: '{prompt}'\n"")
        print(""Raw response:"")
        for line in response.iter_lines():
            if line:
                print(line.decode('utf-8'))
                
    except Exception as e:
        print(f""Error making request: {e}"")","def make_request(prompt):
    """"""Make request and print raw response""""""
    payload = {""input"": prompt}
    
    try:
        response = requests.post(
            API_URL,
            json=payload,
            stream=True
        )
        
        print(f""\nMaking request with prompt: '{prompt}'\n"")
        print(""Raw response:"")
        for line in response.iter_lines():
            if line:
                print(line.decode('utf-8'))
                
    except Exception as e:
        print(f""Error making request: {e}"")",Make request and print raw response,Make request and print raw response,"def make_request(prompt):
    
    payload = {""input"": prompt}
    
    try:
        response = requests.post(
            API_URL,
            json=payload,
            stream=True
        )
        
        print(f""\nMaking request with prompt: '{prompt}'\n"")
        print(""Raw response:"")
        for line in response.iter_lines():
            if line:
                print(line.decode('utf-8'))
                
    except Exception as e:
        print(f""Error making request: {e}"")",Make request and print raw response,"def make_request ( prompt ) : payload = { ""input"" : prompt } try : response = requests . post ( API_URL , json = payload , stream = True ) print ( f""\nMaking request with prompt: '{prompt}'\n"" ) print ( ""Raw response:"" ) for line in response . iter_lines ( ) : if line : print ( line . decode ( 'utf-8' ) ) except Exception as e : print ( f""Error making request: {e}"" )",Make request and print raw response
/VideoLingo/core/tts_backend/_302_f5tts.py,_merge_audio,"def _merge_audio(files, output: str) -> bool:
    """"""Merge audio files, add a brief silence""""""
    try:
        # Create an empty audio segment
        combined = AudioSegment.empty()
        silence = AudioSegment.silent(duration=100)  # 100ms silence
        
        # Add audio files one by one
        for file in files:
            audio = AudioSegment.from_wav(file)
            combined += audio + silence
        combined += silence
        combined.export(output, format=""wav"", parameters=[""-acodec"", ""pcm_s16le"", ""-ar"", ""16000"", ""-ac"", ""1""])
        
        if os.path.getsize(output) == 0:
            rprint(f""[red]Output file size is 0"")
            return False
            
        rprint(f""[green]Successfully merged audio files"")
        return True
        
    except Exception as e:
        rprint(f""[red]Failed to merge audio: {str(e)}"")
        return False","def _merge_audio(files, output: str) -> bool:
    """"""Merge audio files, add a brief silence""""""
    try:
        # Create an empty audio segment
        combined = AudioSegment.empty()
        silence = AudioSegment.silent(duration=100)  # 100ms silence
        
        # Add audio files one by one
        for file in files:
            audio = AudioSegment.from_wav(file)
            combined += audio + silence
        combined += silence
        combined.export(output, format=""wav"", parameters=[""-acodec"", ""pcm_s16le"", ""-ar"", ""16000"", ""-ac"", ""1""])
        
        if os.path.getsize(output) == 0:
            rprint(f""[red]Output file size is 0"")
            return False
            
        rprint(f""[green]Successfully merged audio files"")
        return True
        
    except Exception as e:
        rprint(f""[red]Failed to merge audio: {str(e)}"")
        return False","Merge audio files, add a brief silence","Merge audio files, add a brief silence","def _merge_audio(files, output: str) -> bool:
    
    try:
        # Create an empty audio segment
        combined = AudioSegment.empty()
        silence = AudioSegment.silent(duration=100)  # 100ms silence
        
        # Add audio files one by one
        for file in files:
            audio = AudioSegment.from_wav(file)
            combined += audio + silence
        combined += silence
        combined.export(output, format=""wav"", parameters=[""-acodec"", ""pcm_s16le"", ""-ar"", ""16000"", ""-ac"", ""1""])
        
        if os.path.getsize(output) == 0:
            rprint(f""[red]Output file size is 0"")
            return False
            
        rprint(f""[green]Successfully merged audio files"")
        return True
        
    except Exception as e:
        rprint(f""[red]Failed to merge audio: {str(e)}"")
        return False","Merge audio files, add a brief silence","def _merge_audio ( files , output : str ) -> bool : try : # Create an empty audio segment combined = AudioSegment . empty ( ) silence = AudioSegment . silent ( duration = 100 ) # 100ms silence # Add audio files one by one for file in files : audio = AudioSegment . from_wav ( file ) combined += audio + silence combined += silence combined . export ( output , format = ""wav"" , parameters = [ ""-acodec"" , ""pcm_s16le"" , ""-ar"" , ""16000"" , ""-ac"" , ""1"" ] ) if os . path . getsize ( output ) == 0 : rprint ( f""[red]Output file size is 0"" ) return False rprint ( f""[green]Successfully merged audio files"" ) return True except Exception as e : rprint ( f""[red]Failed to merge audio: {str(e)}"" ) return False","Merge audio files, add a brief silence"
/preswald/preswald/interfaces/dependency_tracker.py,pop_context,"def pop_context():
    """"""
    Pop the topmost context off the stack, ending dependency tracking for the current atom.
    """"""
    if _context_stack:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f""Popping context for atom {ctx.atom_name}"")
        _context_stack.pop()
    else:
        logger.warning(""[DAG] Attempted to pop context, but stack was empty"")","def pop_context():
    """"""
    Pop the topmost context off the stack, ending dependency tracking for the current atom.
    """"""
    if _context_stack:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f""Popping context for atom {ctx.atom_name}"")
        _context_stack.pop()
    else:
        logger.warning(""[DAG] Attempted to pop context, but stack was empty"")","Pop the topmost context off the stack, ending dependency tracking for the current atom.","Pop the topmost context off the stack, ending dependency tracking for the current atom.","def pop_context():
    
    if _context_stack:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f""Popping context for atom {ctx.atom_name}"")
        _context_stack.pop()
    else:
        logger.warning(""[DAG] Attempted to pop context, but stack was empty"")","Pop the topmost context off the stack, ending dependency tracking for the current atom.","def pop_context ( ) : if _context_stack : if logger . isEnabledFor ( logging . DEBUG ) : logger . debug ( f""Popping context for atom {ctx.atom_name}"" ) _context_stack . pop ( ) else : logger . warning ( ""[DAG] Attempted to pop context, but stack was empty"" )","Pop the topmost context off the stack, ending dependency tracking for the current atom."
/EasyR1/verl/utils/checkpoint/checkpoint_manager.py,remove_obsolete_ckpt,"def remove_obsolete_ckpt(path: str, global_step: int, save_limit: int = -1, directory_format: str = ""global_step_{}""):
    """"""
    Remove the obsolete checkpoints that exceed the save_limit.
    """"""
    if save_limit <= 0:
        return

    if not os.path.exists(path):
        return

    pattern = re.escape(directory_format).replace(r""\{\}"", r""(\d+)"")
    ckpt_folders = []
    for folder in os.listdir(path):
        if match := re.match(pattern, folder):
            step = int(match.group(1))
            if step < global_step:
                ckpt_folders.append((step, folder))

    ckpt_folders.sort(reverse=True)
    for _, folder in ckpt_folders[save_limit - 1 :]:
        folder_path = os.path.join(path, folder)
        shutil.rmtree(folder_path, ignore_errors=True)
        print(f""Removed obsolete checkpoint: {folder_path}"")","def remove_obsolete_ckpt(path: str, global_step: int, save_limit: int = -1, directory_format: str = ""global_step_{}""):
    """"""
    Remove the obsolete checkpoints that exceed the save_limit.
    """"""
    if save_limit <= 0:
        return

    if not os.path.exists(path):
        return

    pattern = re.escape(directory_format).replace(r""\{\}"", r""(\d+)"")
    ckpt_folders = []
    for folder in os.listdir(path):
        if match := re.match(pattern, folder):
            step = int(match.group(1))
            if step < global_step:
                ckpt_folders.append((step, folder))

    ckpt_folders.sort(reverse=True)
    for _, folder in ckpt_folders[save_limit - 1 :]:
        folder_path = os.path.join(path, folder)
        shutil.rmtree(folder_path, ignore_errors=True)
        print(f""Removed obsolete checkpoint: {folder_path}"")",Remove the obsolete checkpoints that exceed the save_limit.,Remove the obsolete checkpoints that exceed the save_limit.,"def remove_obsolete_ckpt(path: str, global_step: int, save_limit: int = -1, directory_format: str = ""global_step_{}""):
    
    if save_limit <= 0:
        return

    if not os.path.exists(path):
        return

    pattern = re.escape(directory_format).replace(r""\{\}"", r""(\d+)"")
    ckpt_folders = []
    for folder in os.listdir(path):
        if match := re.match(pattern, folder):
            step = int(match.group(1))
            if step < global_step:
                ckpt_folders.append((step, folder))

    ckpt_folders.sort(reverse=True)
    for _, folder in ckpt_folders[save_limit - 1 :]:
        folder_path = os.path.join(path, folder)
        shutil.rmtree(folder_path, ignore_errors=True)
        print(f""Removed obsolete checkpoint: {folder_path}"")",Remove the obsolete checkpoints that exceed the save_limit.,"def remove_obsolete_ckpt ( path : str , global_step : int , save_limit : int = - 1 , directory_format : str = ""global_step_{}"" ) : if save_limit <= 0 : return if not os . path . exists ( path ) : return pattern = re . escape ( directory_format ) . replace ( r""\{\}"" , r""(\d+)"" ) ckpt_folders = [ ] for folder in os . listdir ( path ) : if match := re . match ( pattern , folder ) : step = int ( match . group ( 1 ) ) if step < global_step : ckpt_folders . append ( ( step , folder ) ) ckpt_folders . sort ( reverse = True ) for _ , folder in ckpt_folders [ save_limit - 1 : ] : folder_path = os . path . join ( path , folder ) shutil . rmtree ( folder_path , ignore_errors = True ) print ( f""Removed obsolete checkpoint: {folder_path}"" )",Remove the obsolete checkpoints that exceed the save_limit.
/NLWeb/code/retrieval/qdrant.py,_get_endpoint_config,"def _get_endpoint_config(self):
        """"""Get the Qdrant endpoint configuration from CONFIG""""""
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is a Qdrant endpoint
        if endpoint_config.db_type != ""qdrant"":
            error_msg = f""Endpoint {self.endpoint_name} is not a Qdrant endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config","def _get_endpoint_config(self):
        """"""Get the Qdrant endpoint configuration from CONFIG""""""
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is a Qdrant endpoint
        if endpoint_config.db_type != ""qdrant"":
            error_msg = f""Endpoint {self.endpoint_name} is not a Qdrant endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config",Get the Qdrant endpoint configuration from CONFIG,Get the Qdrant endpoint configuration from CONFIG,"def _get_endpoint_config(self):
        
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is a Qdrant endpoint
        if endpoint_config.db_type != ""qdrant"":
            error_msg = f""Endpoint {self.endpoint_name} is not a Qdrant endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config",Get the Qdrant endpoint configuration from CONFIG,"def _get_endpoint_config ( self ) : endpoint_config = CONFIG . retrieval_endpoints . get ( self . endpoint_name ) if not endpoint_config : error_msg = f""No configuration found for endpoint {self.endpoint_name}"" logger . error ( error_msg ) raise ValueError ( error_msg ) # Verify this is a Qdrant endpoint if endpoint_config . db_type != ""qdrant"" : error_msg = f""Endpoint {self.endpoint_name} is not a Qdrant endpoint (type: {endpoint_config.db_type})"" logger . error ( error_msg ) raise ValueError ( error_msg ) return endpoint_config",Get the Qdrant endpoint configuration from CONFIG
/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/train/trainer.py,_safe_save,"def _safe_save(self, output_dir: str):
        """"""Collects the state dict and dump to disk.""""""
        if self.deepspeed:
            torch.cuda.synchronize()
            self.save_model(output_dir)
            return
    
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)","def _safe_save(self, output_dir: str):
        """"""Collects the state dict and dump to disk.""""""
        if self.deepspeed:
            torch.cuda.synchronize()
            self.save_model(output_dir)
            return
    
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,Collects the state dict and dump to disk.,"def _safe_save(self, output_dir: str):
        
        if self.deepspeed:
            torch.cuda.synchronize()
            self.save_model(output_dir)
            return
    
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)",Collects the state dict and dump to disk.,"def _safe_save ( self , output_dir : str ) : if self . deepspeed : torch . cuda . synchronize ( ) self . save_model ( output_dir ) return state_dict = self . model . state_dict ( ) if self . args . should_save : cpu_state_dict = { key : value . cpu ( ) for key , value in state_dict . items ( ) } del state_dict self . _save ( output_dir , state_dict = cpu_state_dict )",Collects the state dict and dump to disk.
/alphafold3/src/alphafold3/data/featurisation.py,validate_fold_input,"def validate_fold_input(fold_input: folding_input.Input):
  """"""Validates the fold input contains MSA and templates for featurisation.""""""
  for i, chain in enumerate(fold_input.protein_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing unpaired MSA.')
    if chain.paired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing paired MSA.')
    if chain.templates is None:
      raise ValueError(f'Protein chain {i + 1} is missing Templates.')
  for i, chain in enumerate(fold_input.rna_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'RNA chain {i + 1} is missing unpaired MSA.')","def validate_fold_input(fold_input: folding_input.Input):
  """"""Validates the fold input contains MSA and templates for featurisation.""""""
  for i, chain in enumerate(fold_input.protein_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing unpaired MSA.')
    if chain.paired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing paired MSA.')
    if chain.templates is None:
      raise ValueError(f'Protein chain {i + 1} is missing Templates.')
  for i, chain in enumerate(fold_input.rna_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'RNA chain {i + 1} is missing unpaired MSA.')",Validates the fold input contains MSA and templates for featurisation.,Validates the fold input contains MSA and templates for featurisation.,"def validate_fold_input(fold_input: folding_input.Input):
  
  for i, chain in enumerate(fold_input.protein_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing unpaired MSA.')
    if chain.paired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing paired MSA.')
    if chain.templates is None:
      raise ValueError(f'Protein chain {i + 1} is missing Templates.')
  for i, chain in enumerate(fold_input.rna_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'RNA chain {i + 1} is missing unpaired MSA.')",Validates the fold input contains MSA and templates for featurisation.,"def validate_fold_input ( fold_input : folding_input . Input ) : for i , chain in enumerate ( fold_input . protein_chains ) : if chain . unpaired_msa is None : raise ValueError ( f'Protein chain {i + 1} is missing unpaired MSA.' ) if chain . paired_msa is None : raise ValueError ( f'Protein chain {i + 1} is missing paired MSA.' ) if chain . templates is None : raise ValueError ( f'Protein chain {i + 1} is missing Templates.' ) for i , chain in enumerate ( fold_input . rna_chains ) : if chain . unpaired_msa is None : raise ValueError ( f'RNA chain {i + 1} is missing unpaired MSA.' )",Validates the fold input contains MSA and templates for featurisation.
/intentkit/skills/unrealspeech/__init__.py,get_unrealspeech_skill,"def get_unrealspeech_skill(
    name: str,
    store: SkillStoreABC,
) -> UnrealSpeechBaseTool:
    """"""Get an UnrealSpeech skill by name.""""""
    if name == ""text_to_speech"":
        if name not in _cache:
            _cache[name] = TextToSpeech(
                skill_store=store,
            )
        return _cache[name]
    else:
        raise ValueError(f""Unknown UnrealSpeech skill: {name}"")","def get_unrealspeech_skill(
    name: str,
    store: SkillStoreABC,
) -> UnrealSpeechBaseTool:
    """"""Get an UnrealSpeech skill by name.""""""
    if name == ""text_to_speech"":
        if name not in _cache:
            _cache[name] = TextToSpeech(
                skill_store=store,
            )
        return _cache[name]
    else:
        raise ValueError(f""Unknown UnrealSpeech skill: {name}"")",Get an UnrealSpeech skill by name.,Get an UnrealSpeech skill by name.,"def get_unrealspeech_skill(
    name: str,
    store: SkillStoreABC,
) -> UnrealSpeechBaseTool:
    
    if name == ""text_to_speech"":
        if name not in _cache:
            _cache[name] = TextToSpeech(
                skill_store=store,
            )
        return _cache[name]
    else:
        raise ValueError(f""Unknown UnrealSpeech skill: {name}"")",Get an UnrealSpeech skill by name.,"def get_unrealspeech_skill ( name : str , store : SkillStoreABC , ) -> UnrealSpeechBaseTool : if name == ""text_to_speech"" : if name not in _cache : _cache [ name ] = TextToSpeech ( skill_store = store , ) return _cache [ name ] else : raise ValueError ( f""Unknown UnrealSpeech skill: {name}"" )",Get an UnrealSpeech skill by name.
/Kokoro-FastAPI/dev/Test num.py,handle_money,"def handle_money(m: re.Match[str]) -> str:
    """"""Convert money expressions to spoken form""""""

    bill = ""dollar"" if m.group(2) == ""$"" else ""pound""
    coin = ""cent"" if m.group(2) == ""$"" else ""pence""
    number = m.group(3)

    multiplier = m.group(4)
    try:
        number = float(number)
    except:
        return m.group()

    if m.group(1) == ""-"":
        number *= -1

    if number % 1 == 0 or multiplier != """":
        text_number = f""{INFLECT_ENGINE.number_to_words(conditional_int(number))}{multiplier} {INFLECT_ENGINE.plural(bill, count=number)}""
    else:
        sub_number = int(str(number).split(""."")[-1].ljust(2, ""0""))

        text_number = f""{INFLECT_ENGINE.number_to_words(int(round(number)))} {INFLECT_ENGINE.plural(bill, count=number)} and {INFLECT_ENGINE.number_to_words(sub_number)} {INFLECT_ENGINE.plural(coin, count=sub_number)}""

    return text_number","def handle_money(m: re.Match[str]) -> str:
    """"""Convert money expressions to spoken form""""""

    bill = ""dollar"" if m.group(2) == ""$"" else ""pound""
    coin = ""cent"" if m.group(2) == ""$"" else ""pence""
    number = m.group(3)

    multiplier = m.group(4)
    try:
        number = float(number)
    except:
        return m.group()

    if m.group(1) == ""-"":
        number *= -1

    if number % 1 == 0 or multiplier != """":
        text_number = f""{INFLECT_ENGINE.number_to_words(conditional_int(number))}{multiplier} {INFLECT_ENGINE.plural(bill, count=number)}""
    else:
        sub_number = int(str(number).split(""."")[-1].ljust(2, ""0""))

        text_number = f""{INFLECT_ENGINE.number_to_words(int(round(number)))} {INFLECT_ENGINE.plural(bill, count=number)} and {INFLECT_ENGINE.number_to_words(sub_number)} {INFLECT_ENGINE.plural(coin, count=sub_number)}""

    return text_number",Convert money expressions to spoken form,Convert money expressions to spoken form,"def handle_money(m: re.Match[str]) -> str:
    

    bill = ""dollar"" if m.group(2) == ""$"" else ""pound""
    coin = ""cent"" if m.group(2) == ""$"" else ""pence""
    number = m.group(3)

    multiplier = m.group(4)
    try:
        number = float(number)
    except:
        return m.group()

    if m.group(1) == ""-"":
        number *= -1

    if number % 1 == 0 or multiplier != """":
        text_number = f""{INFLECT_ENGINE.number_to_words(conditional_int(number))}{multiplier} {INFLECT_ENGINE.plural(bill, count=number)}""
    else:
        sub_number = int(str(number).split(""."")[-1].ljust(2, ""0""))

        text_number = f""{INFLECT_ENGINE.number_to_words(int(round(number)))} {INFLECT_ENGINE.plural(bill, count=number)} and {INFLECT_ENGINE.number_to_words(sub_number)} {INFLECT_ENGINE.plural(coin, count=sub_number)}""

    return text_number",Convert money expressions to spoken form,"def handle_money ( m : re . Match [ str ] ) -> str : bill = ""dollar"" if m . group ( 2 ) == ""$"" else ""pound"" coin = ""cent"" if m . group ( 2 ) == ""$"" else ""pence"" number = m . group ( 3 ) multiplier = m . group ( 4 ) try : number = float ( number ) except : return m . group ( ) if m . group ( 1 ) == ""-"" : number *= - 1 if number % 1 == 0 or multiplier != """" : text_number = f""{INFLECT_ENGINE.number_to_words(conditional_int(number))}{multiplier} {INFLECT_ENGINE.plural(bill, count=number)}"" else : sub_number = int ( str ( number ) . split ( ""."" ) [ - 1 ] . ljust ( 2 , ""0"" ) ) text_number = f""{INFLECT_ENGINE.number_to_words(int(round(number)))} {INFLECT_ENGINE.plural(bill, count=number)} and {INFLECT_ENGINE.number_to_words(sub_number)} {INFLECT_ENGINE.plural(coin, count=sub_number)}"" return text_number",Convert money expressions to spoken form
/Agent-S/gui_agents/s1/aci/MacOSACI.py,hotkey,"def hotkey(self, keys: List):
        """"""Press a hotkey combination
        Args:
            keys:List the keys to press in combination in a list format (e.g. ['shift', 'c'])
        """"""
        # Normalize any 'cmd' to 'command'
        keys = [_normalize_key(k) for k in keys]
        # add quotes around the keys
        keys = [f""'{key}'"" for key in keys]
        return f""import pyautogui; pyautogui.hotkey({', '.join(keys)}, interval=1)""","def hotkey(self, keys: List):
        """"""Press a hotkey combination
        Args:
            keys:List the keys to press in combination in a list format (e.g. ['shift', 'c'])
        """"""
        # Normalize any 'cmd' to 'command'
        keys = [_normalize_key(k) for k in keys]
        # add quotes around the keys
        keys = [f""'{key}'"" for key in keys]
        return f""import pyautogui; pyautogui.hotkey({', '.join(keys)}, interval=1)""","Press a hotkey combination
Args:
    keys:List the keys to press in combination in a list format (e.g. ['shift', 'c'])",Press a hotkey combination,"def hotkey(self, keys: List):
        
        # Normalize any 'cmd' to 'command'
        keys = [_normalize_key(k) for k in keys]
        # add quotes around the keys
        keys = [f""'{key}'"" for key in keys]
        return f""import pyautogui; pyautogui.hotkey({', '.join(keys)}, interval=1)""",Press a hotkey combination,"def hotkey ( self , keys : List ) : # Normalize any 'cmd' to 'command' keys = [ _normalize_key ( k ) for k in keys ] # add quotes around the keys keys = [ f""'{key}'"" for key in keys ] return f""import pyautogui; pyautogui.hotkey({', '.join(keys)}, interval=1)""",Press a hotkey combination
/EasyR1/verl/utils/model_utils.py,print_model_size,"def print_model_size(model: nn.Module, name: Optional[str] = None) -> None:
    """"""Print the model size.""""""
    if is_rank0():
        n_params, scale = _get_model_size(model, scale=""auto"")
        if name is None:
            name = model.__class__.__name__

        print(f""{name} contains {n_params:.2f}{scale} parameters."")","def print_model_size(model: nn.Module, name: Optional[str] = None) -> None:
    """"""Print the model size.""""""
    if is_rank0():
        n_params, scale = _get_model_size(model, scale=""auto"")
        if name is None:
            name = model.__class__.__name__

        print(f""{name} contains {n_params:.2f}{scale} parameters."")",Print the model size.,Print the model size.,"def print_model_size(model: nn.Module, name: Optional[str] = None) -> None:
    
    if is_rank0():
        n_params, scale = _get_model_size(model, scale=""auto"")
        if name is None:
            name = model.__class__.__name__

        print(f""{name} contains {n_params:.2f}{scale} parameters."")",Print the model size.,"def print_model_size ( model : nn . Module , name : Optional [ str ] = None ) -> None : if is_rank0 ( ) : n_params , scale = _get_model_size ( model , scale = ""auto"" ) if name is None : name = model . __class__ . __name__ print ( f""{name} contains {n_params:.2f}{scale} parameters."" )",Print the model size.
/nexa-sdk/examples/local_file_organization/file_utils.py,read_pdf_file,"def read_pdf_file(file_path):
    """"""Read text content from a PDF file.""""""
    try:
        doc = fitz.open(file_path)
        # Read only the first few pages to speed up processing
        num_pages_to_read = 3  # Adjust as needed
        full_text = []
        for page_num in range(min(num_pages_to_read, len(doc))):
            page = doc.load_page(page_num)
            full_text.append(page.get_text())
        pdf_content = '\n'.join(full_text)
        return pdf_content
    except Exception as e:
        print(f""Error reading PDF file {file_path}: {e}"")
        return None","def read_pdf_file(file_path):
    """"""Read text content from a PDF file.""""""
    try:
        doc = fitz.open(file_path)
        # Read only the first few pages to speed up processing
        num_pages_to_read = 3  # Adjust as needed
        full_text = []
        for page_num in range(min(num_pages_to_read, len(doc))):
            page = doc.load_page(page_num)
            full_text.append(page.get_text())
        pdf_content = '\n'.join(full_text)
        return pdf_content
    except Exception as e:
        print(f""Error reading PDF file {file_path}: {e}"")
        return None",Read text content from a PDF file.,Read text content from a PDF file.,"def read_pdf_file(file_path):
    
    try:
        doc = fitz.open(file_path)
        # Read only the first few pages to speed up processing
        num_pages_to_read = 3  # Adjust as needed
        full_text = []
        for page_num in range(min(num_pages_to_read, len(doc))):
            page = doc.load_page(page_num)
            full_text.append(page.get_text())
        pdf_content = '\n'.join(full_text)
        return pdf_content
    except Exception as e:
        print(f""Error reading PDF file {file_path}: {e}"")
        return None",Read text content from a PDF file.,"def read_pdf_file ( file_path ) : try : doc = fitz . open ( file_path ) # Read only the first few pages to speed up processing num_pages_to_read = 3 # Adjust as needed full_text = [ ] for page_num in range ( min ( num_pages_to_read , len ( doc ) ) ) : page = doc . load_page ( page_num ) full_text . append ( page . get_text ( ) ) pdf_content = '\n' . join ( full_text ) return pdf_content except Exception as e : print ( f""Error reading PDF file {file_path}: {e}"" ) return None",Read text content from a PDF file.
/ragaai-catalyst/ragaai_catalyst/tracers/agentic_tracing/tracers/custom_tracer.py,_sanitize_input,"def _sanitize_input(self, args: tuple, kwargs: dict) -> dict:
            """"""Sanitize and format input data, including handling of nested lists and dictionaries.""""""

            def sanitize_value(value):
                if isinstance(value, (int, float, bool, str)):
                    return value
                elif isinstance(value, list):
                    return [sanitize_value(item) for item in value]
                elif isinstance(value, dict):
                    return {key: sanitize_value(val) for key, val in value.items()}
                else:
                    return str(value)  # Convert non-standard types to string

            return {
                ""args"": [sanitize_value(arg) for arg in args],
                ""kwargs"": {key: sanitize_value(val) for key, val in kwargs.items()},
            }","def _sanitize_input(self, args: tuple, kwargs: dict) -> dict:
            """"""Sanitize and format input data, including handling of nested lists and dictionaries.""""""

            def sanitize_value(value):
                if isinstance(value, (int, float, bool, str)):
                    return value
                elif isinstance(value, list):
                    return [sanitize_value(item) for item in value]
                elif isinstance(value, dict):
                    return {key: sanitize_value(val) for key, val in value.items()}
                else:
                    return str(value)  # Convert non-standard types to string

            return {
                ""args"": [sanitize_value(arg) for arg in args],
                ""kwargs"": {key: sanitize_value(val) for key, val in kwargs.items()},
            }","Sanitize and format input data, including handling of nested lists and dictionaries.","Sanitize and format input data, including handling of nested lists and dictionaries.","def _sanitize_input(self, args: tuple, kwargs: dict) -> dict:
            

            def sanitize_value(value):
                if isinstance(value, (int, float, bool, str)):
                    return value
                elif isinstance(value, list):
                    return [sanitize_value(item) for item in value]
                elif isinstance(value, dict):
                    return {key: sanitize_value(val) for key, val in value.items()}
                else:
                    return str(value)  # Convert non-standard types to string

            return {
                ""args"": [sanitize_value(arg) for arg in args],
                ""kwargs"": {key: sanitize_value(val) for key, val in kwargs.items()},
            }","Sanitize and format input data, including handling of nested lists and dictionaries.","def _sanitize_input ( self , args : tuple , kwargs : dict ) -> dict : def sanitize_value ( value ) : if isinstance ( value , ( int , float , bool , str ) ) : return value elif isinstance ( value , list ) : return [ sanitize_value ( item ) for item in value ] elif isinstance ( value , dict ) : return { key : sanitize_value ( val ) for key , val in value . items ( ) } else : return str ( value ) # Convert non-standard types to string return { ""args"" : [ sanitize_value ( arg ) for arg in args ] , ""kwargs"" : { key : sanitize_value ( val ) for key , val in kwargs . items ( ) } , }","Sanitize and format input data, including handling of nested lists and dictionaries."
/olmocr/olmocr/train/dataloader.py,cache_s3_files,"def cache_s3_files(dataset: Dataset, pdf_cache_location: str, num_proc: int = 32) -> Dataset:
    """"""
    Caches all S3 paths in the dataset to the local cache directory.
    """"""

    # Define the download function to use in parallel processing
    def cache_file(example):
        s3_path = example[""s3_path""]
        if s3_path:
            # Download the file and cache it locally
            local_path = _cache_s3_file(s3_path, pdf_cache_location)
            return {""local_pdf_path"": local_path}
        return {""local_pdf_path"": None}

    # Map the caching function to the dataset (with parallelism if needed)
    dataset = dataset.map(cache_file, num_proc=num_proc, load_from_cache_file=False)

    return dataset","def cache_s3_files(dataset: Dataset, pdf_cache_location: str, num_proc: int = 32) -> Dataset:
    """"""
    Caches all S3 paths in the dataset to the local cache directory.
    """"""

    # Define the download function to use in parallel processing
    def cache_file(example):
        s3_path = example[""s3_path""]
        if s3_path:
            # Download the file and cache it locally
            local_path = _cache_s3_file(s3_path, pdf_cache_location)
            return {""local_pdf_path"": local_path}
        return {""local_pdf_path"": None}

    # Map the caching function to the dataset (with parallelism if needed)
    dataset = dataset.map(cache_file, num_proc=num_proc, load_from_cache_file=False)

    return dataset",Caches all S3 paths in the dataset to the local cache directory.,Caches all S3 paths in the dataset to the local cache directory.,"def cache_s3_files(dataset: Dataset, pdf_cache_location: str, num_proc: int = 32) -> Dataset:
    

    # Define the download function to use in parallel processing
    def cache_file(example):
        s3_path = example[""s3_path""]
        if s3_path:
            # Download the file and cache it locally
            local_path = _cache_s3_file(s3_path, pdf_cache_location)
            return {""local_pdf_path"": local_path}
        return {""local_pdf_path"": None}

    # Map the caching function to the dataset (with parallelism if needed)
    dataset = dataset.map(cache_file, num_proc=num_proc, load_from_cache_file=False)

    return dataset",Caches all S3 paths in the dataset to the local cache directory.,"def cache_s3_files ( dataset : Dataset , pdf_cache_location : str , num_proc : int = 32 ) -> Dataset : # Define the download function to use in parallel processing def cache_file ( example ) : s3_path = example [ ""s3_path"" ] if s3_path : # Download the file and cache it locally local_path = _cache_s3_file ( s3_path , pdf_cache_location ) return { ""local_pdf_path"" : local_path } return { ""local_pdf_path"" : None } # Map the caching function to the dataset (with parallelism if needed) dataset = dataset . map ( cache_file , num_proc = num_proc , load_from_cache_file = False ) return dataset",Caches all S3 paths in the dataset to the local cache directory.
/ag2/test/tools/experimental/duckduckgo/test_duckduckgo.py,mock_response,"def mock_response(self) -> list[dict[str, Any]]:
        """"""
        Provide a mock response fixture for testing.
        """"""
        return [
            {
                ""title"": ""Test Result"",
                ""href"": ""https://example.com"",  # Use 'href' as per duckduckgo-search
                ""body"": ""This is a test snippet."",  # Use 'body' as per duckduckgo-search
            }
        ]","def mock_response(self) -> list[dict[str, Any]]:
        """"""
        Provide a mock response fixture for testing.
        """"""
        return [
            {
                ""title"": ""Test Result"",
                ""body"": ""This is a test snippet."",  # Use 'body' as per duckduckgo-search
            }
        ]",Provide a mock response fixture for testing.,Provide a mock response fixture for testing.,"def mock_response(self) -> list[dict[str, Any]]:
        
        return [
            {
                ""title"": ""Test Result"",
                ""body"": ""This is a test snippet."",  # Use 'body' as per duckduckgo-search
            }
        ]",Provide a mock response fixture for testing.,"def mock_response ( self ) -> list [ dict [ str , Any ] ] : return [ { ""title"" : ""Test Result"" , ""body"" : ""This is a test snippet."" , # Use 'body' as per duckduckgo-search } ]",Provide a mock response fixture for testing.
/alphafold3/src/alphafold3/data/tools/subprocess_utils.py,create_query_fasta_file,"def create_query_fasta_file(sequence: str, path: str, linewidth: int = 80):
  """"""Creates a fasta file with the sequence with line width limit.""""""
  with open(path, 'w') as f:
    f.write('>query\n')

    i = 0
    while i < len(sequence):
      f.write(f'{sequence[i:(i + linewidth)]}\n')
      i += linewidth","def create_query_fasta_file(sequence: str, path: str, linewidth: int = 80):
  """"""Creates a fasta file with the sequence with line width limit.""""""
  with open(path, 'w') as f:
    f.write('>query\n')

    i = 0
    while i < len(sequence):
      f.write(f'{sequence[i:(i + linewidth)]}\n')
      i += linewidth",Creates a fasta file with the sequence with line width limit.,Creates a fasta file with the sequence with line width limit.,"def create_query_fasta_file(sequence: str, path: str, linewidth: int = 80):
  
  with open(path, 'w') as f:
    f.write('>query\n')

    i = 0
    while i < len(sequence):
      f.write(f'{sequence[i:(i + linewidth)]}\n')
      i += linewidth",Creates a fasta file with the sequence with line width limit.,"def create_query_fasta_file ( sequence : str , path : str , linewidth : int = 80 ) : with open ( path , 'w' ) as f : f . write ( '>query\n' ) i = 0 while i < len ( sequence ) : f . write ( f'{sequence[i:(i + linewidth)]}\n' ) i += linewidth",Creates a fasta file with the sequence with line width limit.
/ag2/autogen/logger/file_logger.py,start,"def start(self) -> str:
        """"""Start the logger and return the session_id.""""""
        try:
            self.logger.info(f""Started new session with Session ID: {self.session_id}"")
        except Exception as e:
            logger.error(f""[file_logger] Failed to create logging file: {e}"")
        finally:
            return self.session_id","def start(self) -> str:
        """"""Start the logger and return the session_id.""""""
        try:
            self.logger.info(f""Started new session with Session ID: {self.session_id}"")
        except Exception as e:
            logger.error(f""[file_logger] Failed to create logging file: {e}"")
        finally:
            return self.session_id",Start the logger and return the session_id.,Start the logger and return the session_id.,"def start(self) -> str:
        
        try:
            self.logger.info(f""Started new session with Session ID: {self.session_id}"")
        except Exception as e:
            logger.error(f""[file_logger] Failed to create logging file: {e}"")
        finally:
            return self.session_id",Start the logger and return the session_id.,"def start ( self ) -> str : try : self . logger . info ( f""Started new session with Session ID: {self.session_id}"" ) except Exception as e : logger . error ( f""[file_logger] Failed to create logging file: {e}"" ) finally : return self . session_id",Start the logger and return the session_id.
/nesa/demo/nesa/settings.py,load_configs,"def load_configs(prefix: str) -> Dict[str, Any]:
    """"""
    load configurations based on the given prefix.
    """"""
    keys = [key for key in os.environ.keys() if key.startswith(f""{prefix}_"")]
    configs = {}
    for key in keys:
        config_key = key.removeprefix(f""{prefix}_"").lower()
        value = os.getenv(key)
        try:
            configs[config_key] = json.loads(value) if value.startswith(""["") else value
        except json.JSONDecodeError:
            configs[config_key] = value
    return configs","def load_configs(prefix: str) -> Dict[str, Any]:
    """"""
    load configurations based on the given prefix.
    """"""
    keys = [key for key in os.environ.keys() if key.startswith(f""{prefix}_"")]
    configs = {}
    for key in keys:
        config_key = key.removeprefix(f""{prefix}_"").lower()
        value = os.getenv(key)
        try:
            configs[config_key] = json.loads(value) if value.startswith(""["") else value
        except json.JSONDecodeError:
            configs[config_key] = value
    return configs",load configurations based on the given prefix.,load configurations based on the given prefix.,"def load_configs(prefix: str) -> Dict[str, Any]:
    
    keys = [key for key in os.environ.keys() if key.startswith(f""{prefix}_"")]
    configs = {}
    for key in keys:
        config_key = key.removeprefix(f""{prefix}_"").lower()
        value = os.getenv(key)
        try:
            configs[config_key] = json.loads(value) if value.startswith(""["") else value
        except json.JSONDecodeError:
            configs[config_key] = value
    return configs",load configurations based on the given prefix.,"def load_configs ( prefix : str ) -> Dict [ str , Any ] : keys = [ key for key in os . environ . keys ( ) if key . startswith ( f""{prefix}_"" ) ] configs = { } for key in keys : config_key = key . removeprefix ( f""{prefix}_"" ) . lower ( ) value = os . getenv ( key ) try : configs [ config_key ] = json . loads ( value ) if value . startswith ( ""["" ) else value except json . JSONDecodeError : configs [ config_key ] = value return configs",load configurations based on the given prefix.
/ragaai-catalyst/tests/examples/crewai/scifi_writer/scifi_writer.py,write_to_file,"def write_to_file(filename: str, content: str) -> str:
    """"""Write content to a file with the specified filename.""""""
    with open(filename, ""w"") as f:
        f.write(content)
    return f""Content successfully written to {filename}""","def write_to_file(filename: str, content: str) -> str:
    """"""Write content to a file with the specified filename.""""""
    with open(filename, ""w"") as f:
        f.write(content)
    return f""Content successfully written to {filename}""",Write content to a file with the specified filename.,Write content to a file with the specified filename.,"def write_to_file(filename: str, content: str) -> str:
    
    with open(filename, ""w"") as f:
        f.write(content)
    return f""Content successfully written to {filename}""",Write content to a file with the specified filename.,"def write_to_file ( filename : str , content : str ) -> str : with open ( filename , ""w"" ) as f : f . write ( content ) return f""Content successfully written to {filename}""",Write content to a file with the specified filename.
/ag2/autogen/oai/gemini.py,get_usage,"def get_usage(response) -> dict:
        """"""Return usage summary of the response using RESPONSE_USAGE_KEYS.""""""
        # ...  # pragma: no cover
        return {
            ""prompt_tokens"": response.usage.prompt_tokens,
            ""completion_tokens"": response.usage.completion_tokens,
            ""total_tokens"": response.usage.total_tokens,
            ""cost"": response.cost,
            ""model"": response.model,
        }","def get_usage(response) -> dict:
        """"""Return usage summary of the response using RESPONSE_USAGE_KEYS.""""""
        # ...  # pragma: no cover
        return {
            ""prompt_tokens"": response.usage.prompt_tokens,
            ""completion_tokens"": response.usage.completion_tokens,
            ""total_tokens"": response.usage.total_tokens,
            ""cost"": response.cost,
            ""model"": response.model,
        }",Return usage summary of the response using RESPONSE_USAGE_KEYS.,Return usage summary of the response using RESPONSE_USAGE_KEYS.,"def get_usage(response) -> dict:
        
        # ...  # pragma: no cover
        return {
            ""prompt_tokens"": response.usage.prompt_tokens,
            ""completion_tokens"": response.usage.completion_tokens,
            ""total_tokens"": response.usage.total_tokens,
            ""cost"": response.cost,
            ""model"": response.model,
        }",Return usage summary of the response using RESPONSE_USAGE_KEYS.,"def get_usage ( response ) -> dict : # ...  # pragma: no cover return { ""prompt_tokens"" : response . usage . prompt_tokens , ""completion_tokens"" : response . usage . completion_tokens , ""total_tokens"" : response . usage . total_tokens , ""cost"" : response . cost , ""model"" : response . model , }",Return usage summary of the response using RESPONSE_USAGE_KEYS.
/airweave/backend/airweave/models/connection.py,delete_integration_credential,"def delete_integration_credential(mapper, connection, target):
    """"""When a Connection is deleted, also delete its IntegrationCredential if present.""""""
    if target.integration_credential_id:
        # Get the session
        session = Session.object_session(target)
        if session:
            # If we're in a session, use the session to delete the IntegrationCredential
            from airweave.models.integration_credential import IntegrationCredential

            credential = session.get(IntegrationCredential, target.integration_credential_id)
            if credential:
                session.delete(credential)
        else:
            # If we're not in a session, use the connection directly
            connection.execute(
                f""DELETE FROM integration_credential WHERE id = '{target.integration_credential_id}'""  # noqa: E501
            )","def delete_integration_credential(mapper, connection, target):
    """"""When a Connection is deleted, also delete its IntegrationCredential if present.""""""
    if target.integration_credential_id:
        # Get the session
        session = Session.object_session(target)
        if session:
            # If we're in a session, use the session to delete the IntegrationCredential
            from airweave.models.integration_credential import IntegrationCredential

            credential = session.get(IntegrationCredential, target.integration_credential_id)
            if credential:
                session.delete(credential)
        else:
            # If we're not in a session, use the connection directly
            connection.execute(
                f""DELETE FROM integration_credential WHERE id = '{target.integration_credential_id}'""  # noqa: E501
            )","When a Connection is deleted, also delete its IntegrationCredential if present.","When a Connection is deleted, also delete its IntegrationCredential if present.","def delete_integration_credential(mapper, connection, target):
    
    if target.integration_credential_id:
        # Get the session
        session = Session.object_session(target)
        if session:
            # If we're in a session, use the session to delete the IntegrationCredential
            from airweave.models.integration_credential import IntegrationCredential

            credential = session.get(IntegrationCredential, target.integration_credential_id)
            if credential:
                session.delete(credential)
        else:
            # If we're not in a session, use the connection directly
            connection.execute(
                f""DELETE FROM integration_credential WHERE id = '{target.integration_credential_id}'""  # noqa: E501
            )","When a Connection is deleted, also delete its IntegrationCredential if present.","def delete_integration_credential ( mapper , connection , target ) : if target . integration_credential_id : # Get the session session = Session . object_session ( target ) if session : # If we're in a session, use the session to delete the IntegrationCredential from airweave . models . integration_credential import IntegrationCredential credential = session . get ( IntegrationCredential , target . integration_credential_id ) if credential : session . delete ( credential ) else : # If we're not in a session, use the connection directly connection . execute ( f""DELETE FROM integration_credential WHERE id = '{target.integration_credential_id}'"" # noqa: E501 )","When a Connection is deleted, also delete its IntegrationCredential if present."
/NLWeb/code/tools/json_analysis.py,analyze_schema_types,"def analyze_schema_types(filename: str) -> Counter:
    """"""Analyze a JSONL file containing schema.org markup and return all types found with counts""""""
    all_types = Counter()
    
    with open(filename) as f:
        for line in f:
            items = line.strip().split('\t')
            if (len(items) < 2):
                continue
            js = json.loads(items[1])
            try:
                all_types.update(extract_types(js))
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON line: {line[:100]}..."")
                continue
            
    return all_types","def analyze_schema_types(filename: str) -> Counter:
    """"""Analyze a JSONL file containing schema.org markup and return all types found with counts""""""
    all_types = Counter()
    
    with open(filename) as f:
        for line in f:
            items = line.strip().split('\t')
            if (len(items) < 2):
                continue
            js = json.loads(items[1])
            try:
                all_types.update(extract_types(js))
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON line: {line[:100]}..."")
                continue
            
    return all_types",Analyze a JSONL file containing schema.org markup and return all types found with counts,Analyze a JSONL file containing schema.org markup and return all types found with counts,"def analyze_schema_types(filename: str) -> Counter:
    
    all_types = Counter()
    
    with open(filename) as f:
        for line in f:
            items = line.strip().split('\t')
            if (len(items) < 2):
                continue
            js = json.loads(items[1])
            try:
                all_types.update(extract_types(js))
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON line: {line[:100]}..."")
                continue
            
    return all_types",Analyze a JSONL file containing  markup and return all types found with counts,"def analyze_schema_types ( filename : str ) -> Counter : all_types = Counter ( ) with open ( filename ) as f : for line in f : items = line . strip ( ) . split ( '\t' ) if ( len ( items ) < 2 ) : continue js = json . loads ( items [ 1 ] ) try : all_types . update ( extract_types ( js ) ) except json . JSONDecodeError : print ( f""Warning: Could not parse JSON line: {line[:100]}..."" ) continue return all_types",Analyze a JSONL file containing markup and return all types found with counts
/optillm/optillm/plugins/mcp_plugin.py,create_default_config,"def create_default_config(self) -> bool:
        """"""Create a default configuration file if none exists""""""
        try:
            if self.config_path.exists():
                return True
                
            default_config = {
                ""mcpServers"": {},
                ""log_level"": ""INFO""
            }
            
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w') as f:
                json.dump(default_config, f, indent=2)
                
            logger.info(f""Created default configuration at {self.config_path}"")
            return True
            
        except Exception as e:
            logger.error(f""Error creating default configuration: {e}"")
            return False","def create_default_config(self) -> bool:
        """"""Create a default configuration file if none exists""""""
        try:
            if self.config_path.exists():
                return True
                
            default_config = {
                ""mcpServers"": {},
                ""log_level"": ""INFO""
            }
            
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w') as f:
                json.dump(default_config, f, indent=2)
                
            logger.info(f""Created default configuration at {self.config_path}"")
            return True
            
        except Exception as e:
            logger.error(f""Error creating default configuration: {e}"")
            return False",Create a default configuration file if none exists,Create a default configuration file if none exists,"def create_default_config(self) -> bool:
        
        try:
            if self.config_path.exists():
                return True
                
            default_config = {
                ""mcpServers"": {},
                ""log_level"": ""INFO""
            }
            
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w') as f:
                json.dump(default_config, f, indent=2)
                
            logger.info(f""Created default configuration at {self.config_path}"")
            return True
            
        except Exception as e:
            logger.error(f""Error creating default configuration: {e}"")
            return False",Create a default configuration file if none exists,"def create_default_config ( self ) -> bool : try : if self . config_path . exists ( ) : return True default_config = { ""mcpServers"" : { } , ""log_level"" : ""INFO"" } self . config_path . parent . mkdir ( parents = True , exist_ok = True ) with open ( self . config_path , 'w' ) as f : json . dump ( default_config , f , indent = 2 ) logger . info ( f""Created default configuration at {self.config_path}"" ) return True except Exception as e : logger . error ( f""Error creating default configuration: {e}"" ) return False",Create a default configuration file if none exists
/airweave/backend/tests/e2e/runner.py,teardown,"def teardown(self) -> None:
        """"""Stop all services after tests.""""""
        logger.info(""Stopping services"")
        try:
            # Don't remove volumes by default to enable reuse
            self.docker.stop(remove_volumes=False)
        except Exception as e:
            logger.error(f""Error during service teardown: {e}"")
            raise","def teardown(self) -> None:
        """"""Stop all services after tests.""""""
        logger.info(""Stopping services"")
        try:
            # Don't remove volumes by default to enable reuse
            self.docker.stop(remove_volumes=False)
        except Exception as e:
            logger.error(f""Error during service teardown: {e}"")
            raise",Stop all services after tests.,Stop all services after tests.,"def teardown(self) -> None:
        
        logger.info(""Stopping services"")
        try:
            # Don't remove volumes by default to enable reuse
            self.docker.stop(remove_volumes=False)
        except Exception as e:
            logger.error(f""Error during service teardown: {e}"")
            raise",Stop all services after tests.,"def teardown ( self ) -> None : logger . info ( ""Stopping services"" ) try : # Don't remove volumes by default to enable reuse self . docker . stop ( remove_volumes = False ) except Exception as e : logger . error ( f""Error during service teardown: {e}"" ) raise",Stop all services after tests.
/Sana/tools/metrics/pytorch-fid/compute_fid.py,save_fid_stats,"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    """"""Calculates the FID of two paths""""""
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers, flag=""ref"")
    np.savez_compressed(paths[1], mu=m1, sigma=s1)","def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    """"""Calculates the FID of two paths""""""
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers, flag=""ref"")
    np.savez_compressed(paths[1], mu=m1, sigma=s1)",Calculates the FID of two paths,Calculates the FID of two paths,"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers, flag=""ref"")
    np.savez_compressed(paths[1], mu=m1, sigma=s1)",Calculates the FID of two paths,"def save_fid_stats ( paths , batch_size , device , dims , num_workers = 1 ) : if not os . path . exists ( paths [ 0 ] ) : raise RuntimeError ( ""Invalid path: %s"" % paths [ 0 ] ) if os . path . exists ( paths [ 1 ] ) : raise RuntimeError ( ""Existing output file: %s"" % paths [ 1 ] ) block_idx = InceptionV3 . BLOCK_INDEX_BY_DIM [ dims ] model = InceptionV3 ( [ block_idx ] ) . to ( device ) print ( f""Saving statistics for {paths[0]}"" ) m1 , s1 = compute_statistics_of_path ( paths [ 0 ] , model , batch_size , dims , device , num_workers , flag = ""ref"" ) np . savez_compressed ( paths [ 1 ] , mu = m1 , sigma = s1 )",Calculates the FID of two paths
/fastapi_mcp/tests/conftest.py,pytest_sessionfinish,"def pytest_sessionfinish(session, exitstatus):
    """"""Combine coverage data from subprocesses at the end of the test session.""""""
    cov_dir = os.path.abspath(""."")
    if exitstatus == 0 and os.environ.get(""COVERAGE_PROCESS_START""):
        try:
            cov = coverage.Coverage()
            cov.combine(data_paths=[cov_dir], strict=True)
            cov.save()
        except Exception as e:
            print(f""Error combining coverage data: {e}"", file=sys.stderr)","def pytest_sessionfinish(session, exitstatus):
    """"""Combine coverage data from subprocesses at the end of the test session.""""""
    cov_dir = os.path.abspath(""."")
    if exitstatus == 0 and os.environ.get(""COVERAGE_PROCESS_START""):
        try:
            cov = coverage.Coverage()
            cov.combine(data_paths=[cov_dir], strict=True)
            cov.save()
        except Exception as e:
            print(f""Error combining coverage data: {e}"", file=sys.stderr)",Combine coverage data from subprocesses at the end of the test session.,Combine coverage data from subprocesses at the end of the test session.,"def pytest_sessionfinish(session, exitstatus):
    
    cov_dir = os.path.abspath(""."")
    if exitstatus == 0 and os.environ.get(""COVERAGE_PROCESS_START""):
        try:
            cov = coverage.Coverage()
            cov.combine(data_paths=[cov_dir], strict=True)
            cov.save()
        except Exception as e:
            print(f""Error combining coverage data: {e}"", file=sys.stderr)",Combine coverage data from subprocesses at the end of the test session.,"def pytest_sessionfinish ( session , exitstatus ) : cov_dir = os . path . abspath ( ""."" ) if exitstatus == 0 and os . environ . get ( ""COVERAGE_PROCESS_START"" ) : try : cov = coverage . Coverage ( ) cov . combine ( data_paths = [ cov_dir ] , strict = True ) cov . save ( ) except Exception as e : print ( f""Error combining coverage data: {e}"" , file = sys . stderr )",Combine coverage data from subprocesses at the end of the test session.
/verl/verl/workers/sharding_manager/megatron_vllm.py,_build_param_buffer,"def _build_param_buffer(self, pp_rank):
        """"""Build the parameter buffer in each pp rank""""""
        if pp_rank == self._pp_rank:
            from verl.utils.memory_buffer import MemoryBuffer

            # The code here is very hard-coded, based on the following assumptions:
            # 1. `len(_this_rank_models) == 1`
            # 2. `_this_rank_models[0]` is a instance of `DistributedDataParallel` and `use_distributed_optimizer=True`
            # 3. Only bfloat16 data type is used in parameters
            source = self._this_rank_models[0].buffers[0].param_data
            self.memory_buffers[pp_rank] = {torch.bfloat16: MemoryBuffer(source.numel(), source.numel(), torch.bfloat16, source)}
        else:
            model = self.pp_models[pp_rank]
            weight_buffer_meta = get_weight_buffer_meta_from_module(model)
            self.memory_buffers[pp_rank] = build_memory_buffer(weight_buffer_meta)","def _build_param_buffer(self, pp_rank):
        """"""Build the parameter buffer in each pp rank""""""
        if pp_rank == self._pp_rank:
            from verl.utils.memory_buffer import MemoryBuffer

            # The code here is very hard-coded, based on the following assumptions:
            # 1. `len(_this_rank_models) == 1`
            # 2. `_this_rank_models[0]` is a instance of `DistributedDataParallel` and `use_distributed_optimizer=True`
            # 3. Only bfloat16 data type is used in parameters
            source = self._this_rank_models[0].buffers[0].param_data
            self.memory_buffers[pp_rank] = {torch.bfloat16: MemoryBuffer(source.numel(), source.numel(), torch.bfloat16, source)}
        else:
            model = self.pp_models[pp_rank]
            weight_buffer_meta = get_weight_buffer_meta_from_module(model)
            self.memory_buffers[pp_rank] = build_memory_buffer(weight_buffer_meta)",Build the parameter buffer in each pp rank,Build the parameter buffer in each pp rank,"def _build_param_buffer(self, pp_rank):
        
        if pp_rank == self._pp_rank:
            from verl.utils.memory_buffer import MemoryBuffer

            # The code here is very hard-coded, based on the following assumptions:
            # 1. `len(_this_rank_models) == 1`
            # 2. `_this_rank_models[0]` is a instance of `DistributedDataParallel` and `use_distributed_optimizer=True`
            # 3. Only bfloat16 data type is used in parameters
            source = self._this_rank_models[0].buffers[0].param_data
            self.memory_buffers[pp_rank] = {torch.bfloat16: MemoryBuffer(source.numel(), source.numel(), torch.bfloat16, source)}
        else:
            model = self.pp_models[pp_rank]
            weight_buffer_meta = get_weight_buffer_meta_from_module(model)
            self.memory_buffers[pp_rank] = build_memory_buffer(weight_buffer_meta)",Build the parameter buffer in each pp rank,"def _build_param_buffer ( self , pp_rank ) : if pp_rank == self . _pp_rank : from verl . utils . memory_buffer import MemoryBuffer # The code here is very hard-coded, based on the following assumptions: # 1. `len(_this_rank_models) == 1` # 2. `_this_rank_models[0]` is a instance of `DistributedDataParallel` and `use_distributed_optimizer=True` # 3. Only bfloat16 data type is used in parameters source = self . _this_rank_models [ 0 ] . buffers [ 0 ] . param_data self . memory_buffers [ pp_rank ] = { torch . bfloat16 : MemoryBuffer ( source . numel ( ) , source . numel ( ) , torch . bfloat16 , source ) } else : model = self . pp_models [ pp_rank ] weight_buffer_meta = get_weight_buffer_meta_from_module ( model ) self . memory_buffers [ pp_rank ] = build_memory_buffer ( weight_buffer_meta )",Build the parameter buffer in each pp rank
/Qwen2-VL/evaluation/mmmu/dataset_utils.py,MMMU_preproc,"def MMMU_preproc(data):
    """"""
    Preprocess MMMU dataset to reformulate open questions to multi-choice ones.
    This aligns with the implementation in multiple_choice.py
    """"""
    print(""Preprocessing MMMU dataset..."")
    cnt = 0
    As, Bs, Ans = list(data['A']), list(data['B']), list(data['answer'])
    lt = len(data)
    for i in range(lt):
        if pd.isna(As[i]):
            As[i] = Ans[i]
            Bs[i] = 'Other Answers'
            cnt += 1
    print(f'During MMMU_preproc in Evaluation, {cnt} open questions are re-formulated to multi-choice ones.')
    data['A'] = As
    data['B'] = Bs
    return data","def MMMU_preproc(data):
    """"""
    Preprocess MMMU dataset to reformulate open questions to multi-choice ones.
    This aligns with the implementation in multiple_choice.py
    """"""
    print(""Preprocessing MMMU dataset..."")
    cnt = 0
    As, Bs, Ans = list(data['A']), list(data['B']), list(data['answer'])
    lt = len(data)
    for i in range(lt):
        if pd.isna(As[i]):
            As[i] = Ans[i]
            Bs[i] = 'Other Answers'
            cnt += 1
    print(f'During MMMU_preproc in Evaluation, {cnt} open questions are re-formulated to multi-choice ones.')
    data['A'] = As
    data['B'] = Bs
    return data","Preprocess MMMU dataset to reformulate open questions to multi-choice ones.
This aligns with the implementation in multiple_choice.py",Preprocess MMMU dataset to reformulate open questions to multi-choice ones.,"def MMMU_preproc(data):
    
    print(""Preprocessing MMMU dataset..."")
    cnt = 0
    As, Bs, Ans = list(data['A']), list(data['B']), list(data['answer'])
    lt = len(data)
    for i in range(lt):
        if pd.isna(As[i]):
            As[i] = Ans[i]
            Bs[i] = 'Other Answers'
            cnt += 1
    print(f'During MMMU_preproc in Evaluation, {cnt} open questions are re-formulated to multi-choice ones.')
    data['A'] = As
    data['B'] = Bs
    return data",Preprocess MMMU dataset to reformulate open questions to multi-choice ones.,"def MMMU_preproc ( data ) : print ( ""Preprocessing MMMU dataset..."" ) cnt = 0 As , Bs , Ans = list ( data [ 'A' ] ) , list ( data [ 'B' ] ) , list ( data [ 'answer' ] ) lt = len ( data ) for i in range ( lt ) : if pd . isna ( As [ i ] ) : As [ i ] = Ans [ i ] Bs [ i ] = 'Other Answers' cnt += 1 print ( f'During MMMU_preproc in Evaluation, {cnt} open questions are re-formulated to multi-choice ones.' ) data [ 'A' ] = As data [ 'B' ] = Bs return data",Preprocess MMMU dataset to reformulate open questions to multi-choice ones.
/potpie/app/modules/intelligence/agents/custom_agents/runtime_agent.py,parse_expected_output,"def parse_expected_output(cls, v):
        """"""Ensure expected_output is a dictionary""""""
        if isinstance(v, str):
            try:
                return json.loads(v)
            except json.JSONDecodeError as e:
                logger.error(f""Error parsing expected_output: {str(e)}"")
                raise ValueError(""Invalid JSON format for expected_output"")
        return v","def parse_expected_output(cls, v):
        """"""Ensure expected_output is a dictionary""""""
        if isinstance(v, str):
            try:
                return json.loads(v)
            except json.JSONDecodeError as e:
                logger.error(f""Error parsing expected_output: {str(e)}"")
                raise ValueError(""Invalid JSON format for expected_output"")
        return v",Ensure expected_output is a dictionary,Ensure expected_output is a dictionary,"def parse_expected_output(cls, v):
        
        if isinstance(v, str):
            try:
                return json.loads(v)
            except json.JSONDecodeError as e:
                logger.error(f""Error parsing expected_output: {str(e)}"")
                raise ValueError(""Invalid JSON format for expected_output"")
        return v",Ensure expected_output is a dictionary,"def parse_expected_output ( cls , v ) : if isinstance ( v , str ) : try : return json . loads ( v ) except json . JSONDecodeError as e : logger . error ( f""Error parsing expected_output: {str(e)}"" ) raise ValueError ( ""Invalid JSON format for expected_output"" ) return v",Ensure expected_output is a dictionary
/web-ui/src/utils/utils.py,get_latest_files,"def get_latest_files(directory: str, file_types: list = ['.webm', '.zip']) -> Dict[str, Optional[str]]:
    """"""Get the latest recording and trace files""""""
    latest_files: Dict[str, Optional[str]] = {ext: None for ext in file_types}

    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        return latest_files

    for file_type in file_types:
        try:
            matches = list(Path(directory).rglob(f""*{file_type}""))
            if matches:
                latest = max(matches, key=lambda p: p.stat().st_mtime)
                # Only return files that are complete (not being written)
                if time.time() - latest.stat().st_mtime > 1.0:
                    latest_files[file_type] = str(latest)
        except Exception as e:
            print(f""Error getting latest {file_type} file: {e}"")

    return latest_files","def get_latest_files(directory: str, file_types: list = ['.webm', '.zip']) -> Dict[str, Optional[str]]:
    """"""Get the latest recording and trace files""""""
    latest_files: Dict[str, Optional[str]] = {ext: None for ext in file_types}

    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        return latest_files

    for file_type in file_types:
        try:
            matches = list(Path(directory).rglob(f""*{file_type}""))
            if matches:
                latest = max(matches, key=lambda p: p.stat().st_mtime)
                # Only return files that are complete (not being written)
                if time.time() - latest.stat().st_mtime > 1.0:
                    latest_files[file_type] = str(latest)
        except Exception as e:
            print(f""Error getting latest {file_type} file: {e}"")

    return latest_files",Get the latest recording and trace files,Get the latest recording and trace files,"def get_latest_files(directory: str, file_types: list = ['.webm', '.zip']) -> Dict[str, Optional[str]]:
    
    latest_files: Dict[str, Optional[str]] = {ext: None for ext in file_types}

    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        return latest_files

    for file_type in file_types:
        try:
            matches = list(Path(directory).rglob(f""*{file_type}""))
            if matches:
                latest = max(matches, key=lambda p: p.stat().st_mtime)
                # Only return files that are complete (not being written)
                if time.time() - latest.stat().st_mtime > 1.0:
                    latest_files[file_type] = str(latest)
        except Exception as e:
            print(f""Error getting latest {file_type} file: {e}"")

    return latest_files",Get the latest recording and trace files,"def get_latest_files ( directory : str , file_types : list = [ '.webm' , '.zip' ] ) -> Dict [ str , Optional [ str ] ] : latest_files : Dict [ str , Optional [ str ] ] = { ext : None for ext in file_types } if not os . path . exists ( directory ) : os . makedirs ( directory , exist_ok = True ) return latest_files for file_type in file_types : try : matches = list ( Path ( directory ) . rglob ( f""*{file_type}"" ) ) if matches : latest = max ( matches , key = lambda p : p . stat ( ) . st_mtime ) # Only return files that are complete (not being written) if time . time ( ) - latest . stat ( ) . st_mtime > 1.0 : latest_files [ file_type ] = str ( latest ) except Exception as e : print ( f""Error getting latest {file_type} file: {e}"" ) return latest_files",Get the latest recording and trace files
/aci/backend/aci/server/app_connectors/e2b.py,run_code,"def run_code(
        self,
        code: str,
    ) -> dict[str, Any]:
        """"""
        Execute code in E2B sandbox and return the result.
        """"""
        with Sandbox(api_key=self.api_key) as sandbox:
            execution = sandbox.run_code(code)
            return {""text"": execution.text}","def run_code(
        self,
        code: str,
    ) -> dict[str, Any]:
        """"""
        Execute code in E2B sandbox and return the result.
        """"""
        with Sandbox(api_key=self.api_key) as sandbox:
            execution = sandbox.run_code(code)
            return {""text"": execution.text}",Execute code in E2B sandbox and return the result.,Execute code in E2B sandbox and return the result.,"def run_code(
        self,
        code: str,
    ) -> dict[str, Any]:
        
        with Sandbox(api_key=self.api_key) as sandbox:
            execution = sandbox.run_code(code)
            return {""text"": execution.text}",Execute code in E2B sandbox and return the result.,"def run_code ( self , code : str , ) -> dict [ str , Any ] : with Sandbox ( api_key = self . api_key ) as sandbox : execution = sandbox . run_code ( code ) return { ""text"" : execution . text }",Execute code in E2B sandbox and return the result.
/OpenManus-RL/openmanus_rl/utils/visualization.py,_trajectory_to_text,"def _trajectory_to_text(trajectory: Dict[str, Any]) -> str:
    """"""Convert a trajectory to formatted text.""""""
    text = ""AGENT TRAJECTORY\n"" + ""=""*50 + ""\n\n""
    
    # Add answers/responses
    if ""answer"" in trajectory and isinstance(trajectory[""answer""], list):
        for i, answer in enumerate(trajectory[""answer""]):
            text += f""STEP {i+1}:\n""
            text += f""Agent Response:\n{answer}\n\n""
    
    # Add parsed responses if available
    if ""parsed_response"" in trajectory and isinstance(trajectory[""parsed_response""], list):
        text += ""\nPARSED RESPONSES\n"" + ""-""*50 + ""\n\n""
        for i, parsed in enumerate(trajectory[""parsed_response""]):
            text += f""Step {i+1} Parsed:\n""
            for key, value in parsed.items():
                if value:
                    text += f""  {key}: {value}\n""
            text += ""\n""
    
    return text","def _trajectory_to_text(trajectory: Dict[str, Any]) -> str:
    """"""Convert a trajectory to formatted text.""""""
    text = ""AGENT TRAJECTORY\n"" + ""=""*50 + ""\n\n""
    
    # Add answers/responses
    if ""answer"" in trajectory and isinstance(trajectory[""answer""], list):
        for i, answer in enumerate(trajectory[""answer""]):
            text += f""STEP {i+1}:\n""
            text += f""Agent Response:\n{answer}\n\n""
    
    # Add parsed responses if available
    if ""parsed_response"" in trajectory and isinstance(trajectory[""parsed_response""], list):
        text += ""\nPARSED RESPONSES\n"" + ""-""*50 + ""\n\n""
        for i, parsed in enumerate(trajectory[""parsed_response""]):
            text += f""Step {i+1} Parsed:\n""
            for key, value in parsed.items():
                if value:
                    text += f""  {key}: {value}\n""
            text += ""\n""
    
    return text",Convert a trajectory to formatted text.,Convert a trajectory to formatted text.,"def _trajectory_to_text(trajectory: Dict[str, Any]) -> str:
    
    text = ""AGENT TRAJECTORY\n"" + ""=""*50 + ""\n\n""
    
    # Add answers/responses
    if ""answer"" in trajectory and isinstance(trajectory[""answer""], list):
        for i, answer in enumerate(trajectory[""answer""]):
            text += f""STEP {i+1}:\n""
            text += f""Agent Response:\n{answer}\n\n""
    
    # Add parsed responses if available
    if ""parsed_response"" in trajectory and isinstance(trajectory[""parsed_response""], list):
        text += ""\nPARSED RESPONSES\n"" + ""-""*50 + ""\n\n""
        for i, parsed in enumerate(trajectory[""parsed_response""]):
            text += f""Step {i+1} Parsed:\n""
            for key, value in parsed.items():
                if value:
                    text += f""  {key}: {value}\n""
            text += ""\n""
    
    return text",Convert a trajectory to formatted text.,"def _trajectory_to_text ( trajectory : Dict [ str , Any ] ) -> str : text = ""AGENT TRAJECTORY\n"" + ""="" * 50 + ""\n\n"" # Add answers/responses if ""answer"" in trajectory and isinstance ( trajectory [ ""answer"" ] , list ) : for i , answer in enumerate ( trajectory [ ""answer"" ] ) : text += f""STEP {i+1}:\n"" text += f""Agent Response:\n{answer}\n\n"" # Add parsed responses if available if ""parsed_response"" in trajectory and isinstance ( trajectory [ ""parsed_response"" ] , list ) : text += ""\nPARSED RESPONSES\n"" + ""-"" * 50 + ""\n\n"" for i , parsed in enumerate ( trajectory [ ""parsed_response"" ] ) : text += f""Step {i+1} Parsed:\n"" for key , value in parsed . items ( ) : if value : text += f""  {key}: {value}\n"" text += ""\n"" return text",Convert a trajectory to formatted text.
/local-deep-research/src/local_deep_research/benchmarks/datasets/browsecomp.py,get_dataset_info,"def get_dataset_info(cls) -> Dict[str, str]:
        """"""Get basic information about the dataset.""""""
        return {
            ""id"": ""browsecomp"",
            ""name"": ""BrowseComp"",
            ""description"": ""Web browsing comprehension evaluation dataset"",
            ""url"": cls.get_default_dataset_path(),
        }","def get_dataset_info(cls) -> Dict[str, str]:
        """"""Get basic information about the dataset.""""""
        return {
            ""id"": ""browsecomp"",
            ""name"": ""BrowseComp"",
            ""description"": ""Web browsing comprehension evaluation dataset"",
            ""url"": cls.get_default_dataset_path(),
        }",Get basic information about the dataset.,Get basic information about the dataset.,"def get_dataset_info(cls) -> Dict[str, str]:
        
        return {
            ""id"": ""browsecomp"",
            ""name"": ""BrowseComp"",
            ""description"": ""Web browsing comprehension evaluation dataset"",
            ""url"": cls.get_default_dataset_path(),
        }",Get basic information about the dataset.,"def get_dataset_info ( cls ) -> Dict [ str , str ] : return { ""id"" : ""browsecomp"" , ""name"" : ""BrowseComp"" , ""description"" : ""Web browsing comprehension evaluation dataset"" , ""url"" : cls . get_default_dataset_path ( ) , }",Get basic information about the dataset.
/mcp-use/mcp_use/managers/tools/disconnect_server.py,_run,"def _run(self, **kwargs) -> str:
        """"""Disconnect from the currently active MCP server.""""""
        if not self.server_manager.active_server:
            return ""No MCP server is currently active, so there's nothing to disconnect from.""

        server_name = self.server_manager.active_server
        try:
            # Clear the active server
            self.server_manager.active_server = None

            # Note: We're not actually closing the session here, just 'deactivating'
            # This way we keep the session cache without requiring reconnection if needed again

            return f""Successfully disconnected from MCP server '{server_name}'.""
        except Exception as e:
            logger.error(f""Error disconnecting from server '{server_name}': {e}"")
            return f""Failed to disconnect from server '{server_name}': {str(e)}""","def _run(self, **kwargs) -> str:
        """"""Disconnect from the currently active MCP server.""""""
        if not self.server_manager.active_server:
            return ""No MCP server is currently active, so there's nothing to disconnect from.""

        server_name = self.server_manager.active_server
        try:
            # Clear the active server
            self.server_manager.active_server = None

            # Note: We're not actually closing the session here, just 'deactivating'
            # This way we keep the session cache without requiring reconnection if needed again

            return f""Successfully disconnected from MCP server '{server_name}'.""
        except Exception as e:
            logger.error(f""Error disconnecting from server '{server_name}': {e}"")
            return f""Failed to disconnect from server '{server_name}': {str(e)}""",Disconnect from the currently active MCP server.,Disconnect from the currently active MCP server.,"def _run(self, **kwargs) -> str:
        
        if not self.server_manager.active_server:
            return ""No MCP server is currently active, so there's nothing to disconnect from.""

        server_name = self.server_manager.active_server
        try:
            # Clear the active server
            self.server_manager.active_server = None

            # Note: We're not actually closing the session here, just 'deactivating'
            # This way we keep the session cache without requiring reconnection if needed again

            return f""Successfully disconnected from MCP server '{server_name}'.""
        except Exception as e:
            logger.error(f""Error disconnecting from server '{server_name}': {e}"")
            return f""Failed to disconnect from server '{server_name}': {str(e)}""",Disconnect from the currently active MCP server.,"def _run ( self , ** kwargs ) -> str : if not self . server_manager . active_server : return ""No MCP server is currently active, so there's nothing to disconnect from."" server_name = self . server_manager . active_server try : # Clear the active server self . server_manager . active_server = None # Note: We're not actually closing the session here, just 'deactivating' # This way we keep the session cache without requiring reconnection if needed again return f""Successfully disconnected from MCP server '{server_name}'."" except Exception as e : logger . error ( f""Error disconnecting from server '{server_name}': {e}"" ) return f""Failed to disconnect from server '{server_name}': {str(e)}""",Disconnect from the currently active MCP server.
/mcp-agent/src/mcp_agent/executor/workflow_signal.py,on_signal,"def on_signal(self, signal_name: str) -> Callable:
        """"""Register a handler for a signal.""""""

        def decorator(func: Callable) -> Callable:
            unique_name = f""{signal_name}_{uuid.uuid4()}""

            async def wrapped(value: SignalValueT):
                try:
                    if asyncio.iscoroutinefunction(func):
                        await func(value)
                    else:
                        func(value)
                except Exception as e:
                    # Log the error but don't fail the entire signal handling
                    print(f""Error in signal handler {signal_name}: {str(e)}"")

            self._handlers.setdefault(signal_name, []).append((unique_name, wrapped))
            return wrapped

        return decorator","def on_signal(self, signal_name: str) -> Callable:
        """"""Register a handler for a signal.""""""

        def decorator(func: Callable) -> Callable:
            unique_name = f""{signal_name}_{uuid.uuid4()}""

            async def wrapped(value: SignalValueT):
                try:
                    if asyncio.iscoroutinefunction(func):
                        await func(value)
                    else:
                        func(value)
                except Exception as e:
                    # Log the error but don't fail the entire signal handling
                    print(f""Error in signal handler {signal_name}: {str(e)}"")

            self._handlers.setdefault(signal_name, []).append((unique_name, wrapped))
            return wrapped

        return decorator",Register a handler for a signal.,Register a handler for a signal.,"def on_signal(self, signal_name: str) -> Callable:
        

        def decorator(func: Callable) -> Callable:
            unique_name = f""{signal_name}_{uuid.uuid4()}""

            async def wrapped(value: SignalValueT):
                try:
                    if asyncio.iscoroutinefunction(func):
                        await func(value)
                    else:
                        func(value)
                except Exception as e:
                    # Log the error but don't fail the entire signal handling
                    print(f""Error in signal handler {signal_name}: {str(e)}"")

            self._handlers.setdefault(signal_name, []).append((unique_name, wrapped))
            return wrapped

        return decorator",Register a handler for a signal.,"def on_signal ( self , signal_name : str ) -> Callable : def decorator ( func : Callable ) -> Callable : unique_name = f""{signal_name}_{uuid.uuid4()}"" async def wrapped ( value : SignalValueT ) : try : if asyncio . iscoroutinefunction ( func ) : await func ( value ) else : func ( value ) except Exception as e : # Log the error but don't fail the entire signal handling print ( f""Error in signal handler {signal_name}: {str(e)}"" ) self . _handlers . setdefault ( signal_name , [ ] ) . append ( ( unique_name , wrapped ) ) return wrapped return decorator",Register a handler for a signal.
/Sana/tools/download.py,download_model,"def download_model(model_name):
    """"""
    Downloads a pre-trained Sana model from the web.
    """"""
    assert model_name in pretrained_models
    local_path = f""output/pretrained_models/{model_name}""
    if not os.path.isfile(local_path):
        hf_endpoint = os.environ.get(""HF_ENDPOINT"")
        if hf_endpoint is None:
            hf_endpoint = ""https://huggingface.co""
        os.makedirs(""output/pretrained_models"", exist_ok=True)
        web_path = f""""
        download_url(web_path, ""output/pretrained_models/"")
    model = torch.load(local_path, map_location=lambda storage, loc: storage)
    return model","def download_model(model_name):
    """"""
    Downloads a pre-trained Sana model from the web.
    """"""
    assert model_name in pretrained_models
    local_path = f""output/pretrained_models/{model_name}""
    if not os.path.isfile(local_path):
        hf_endpoint = os.environ.get(""HF_ENDPOINT"")
        if hf_endpoint is None:
        os.makedirs(""output/pretrained_models"", exist_ok=True)
        web_path = f""""
        download_url(web_path, ""output/pretrained_models/"")
    model = torch.load(local_path, map_location=lambda storage, loc: storage)
    return model",Downloads a pre-trained Sana model from the web.,Downloads a pre-trained Sana model from the web.,"def download_model(model_name):
    
    assert model_name in pretrained_models
    local_path = f""output/pretrained_models/{model_name}""
    if not os.path.isfile(local_path):
        hf_endpoint = os.environ.get(""HF_ENDPOINT"")
        if hf_endpoint is None:
        os.makedirs(""output/pretrained_models"", exist_ok=True)
        web_path = f""""
        download_url(web_path, ""output/pretrained_models/"")
    model = torch.load(local_path, map_location=lambda storage, loc: storage)
    return model",Downloads a pre-trained Sana model from the web.,"def download_model ( model_name ) : assert model_name in pretrained_models local_path = f""output/pretrained_models/{model_name}"" if not os . path . isfile ( local_path ) : hf_endpoint = os . environ . get ( ""HF_ENDPOINT"" ) if hf_endpoint is None : os . makedirs ( ""output/pretrained_models"" , exist_ok = True ) web_path = f"""" download_url ( web_path , ""output/pretrained_models/"" ) model = torch . load ( local_path , map_location = lambda storage , loc : storage ) return model",Downloads a pre-trained Sana model from the web.
/web-ui/src/agent/deep_research/deep_research_agent.py,_save_search_results_to_json,"def _save_search_results_to_json(results: List[Dict[str, Any]], output_dir: str):
    """"""Appends or overwrites search results to a JSON file.""""""
    search_file = os.path.join(output_dir, SEARCH_INFO_FILENAME)
    try:
        # Simple overwrite for now, could be append
        with open(search_file, ""w"", encoding=""utf-8"") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f""Search results saved to {search_file}"")
    except Exception as e:
        logger.error(f""Failed to save search results to {search_file}: {e}"")","def _save_search_results_to_json(results: List[Dict[str, Any]], output_dir: str):
    """"""Appends or overwrites search results to a JSON file.""""""
    search_file = os.path.join(output_dir, SEARCH_INFO_FILENAME)
    try:
        # Simple overwrite for now, could be append
        with open(search_file, ""w"", encoding=""utf-8"") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f""Search results saved to {search_file}"")
    except Exception as e:
        logger.error(f""Failed to save search results to {search_file}: {e}"")",Appends or overwrites search results to a JSON file.,Appends or overwrites search results to a JSON file.,"def _save_search_results_to_json(results: List[Dict[str, Any]], output_dir: str):
    
    search_file = os.path.join(output_dir, SEARCH_INFO_FILENAME)
    try:
        # Simple overwrite for now, could be append
        with open(search_file, ""w"", encoding=""utf-8"") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f""Search results saved to {search_file}"")
    except Exception as e:
        logger.error(f""Failed to save search results to {search_file}: {e}"")",Appends or overwrites search results to a JSON file.,"def _save_search_results_to_json ( results : List [ Dict [ str , Any ] ] , output_dir : str ) : search_file = os . path . join ( output_dir , SEARCH_INFO_FILENAME ) try : # Simple overwrite for now, could be append with open ( search_file , ""w"" , encoding = ""utf-8"" ) as f : json . dump ( results , f , indent = 2 , ensure_ascii = False ) logger . info ( f""Search results saved to {search_file}"" ) except Exception as e : logger . error ( f""Failed to save search results to {search_file}: {e}"" )",Appends or overwrites search results to a JSON file.
/mcp/samples/mcp-integration-with-kb/clients/client_server.py,add_knowledge_base,"def add_knowledge_base(request: KnowledgeBaseRequest):
    """"""Add a new knowledge base ID.""""""
    # In a real implementation, this might validate the KB ID with AWS
    return {
        'message': f'Knowledge base {request.kb_id} added successfully',
        'kb_id': request.kb_id,
    }","def add_knowledge_base(request: KnowledgeBaseRequest):
    """"""Add a new knowledge base ID.""""""
    # In a real implementation, this might validate the KB ID with AWS
    return {
        'message': f'Knowledge base {request.kb_id} added successfully',
        'kb_id': request.kb_id,
    }",Add a new knowledge base ID.,Add a new knowledge base ID.,"def add_knowledge_base(request: KnowledgeBaseRequest):
    
    # In a real implementation, this might validate the KB ID with AWS
    return {
        'message': f'Knowledge base {request.kb_id} added successfully',
        'kb_id': request.kb_id,
    }",Add a new knowledge base ID.,"def add_knowledge_base ( request : KnowledgeBaseRequest ) : # In a real implementation, this might validate the KB ID with AWS return { 'message' : f'Knowledge base {request.kb_id} added successfully' , 'kb_id' : request . kb_id , }",Add a new knowledge base ID.
/preswald/preswald/engine/managers/branding.py,get_branding_config_with_data_urls,"def get_branding_config_with_data_urls(
        self, script_path: str | None = None
    ) -> dict[str, Any]:
        """"""Get branding config with logo and favicon as data URLs""""""
        branding = self.get_branding_config(script_path)

        # Convert logo and favicon to data URLs
        branding[""logo""] = self._convert_to_data_url(branding[""logo""])
        branding[""favicon""] = self._convert_to_data_url(
            branding[""favicon""].split(""?"")[0]
        )  # Remove timestamp query param

        logger.info(f""Actual final branding configuration: {branding}"")
        return branding","def get_branding_config_with_data_urls(
        self, script_path: str | None = None
    ) -> dict[str, Any]:
        """"""Get branding config with logo and favicon as data URLs""""""
        branding = self.get_branding_config(script_path)

        # Convert logo and favicon to data URLs
        branding[""logo""] = self._convert_to_data_url(branding[""logo""])
        branding[""favicon""] = self._convert_to_data_url(
            branding[""favicon""].split(""?"")[0]
        )  # Remove timestamp query param

        logger.info(f""Actual final branding configuration: {branding}"")
        return branding",Get branding config with logo and favicon as data URLs,Get branding config with logo and favicon as data URLs,"def get_branding_config_with_data_urls(
        self, script_path: str | None = None
    ) -> dict[str, Any]:
        
        branding = self.get_branding_config(script_path)

        # Convert logo and favicon to data URLs
        branding[""logo""] = self._convert_to_data_url(branding[""logo""])
        branding[""favicon""] = self._convert_to_data_url(
            branding[""favicon""].split(""?"")[0]
        )  # Remove timestamp query param

        logger.info(f""Actual final branding configuration: {branding}"")
        return branding",Get branding config with logo and favicon as data URLs,"def get_branding_config_with_data_urls ( self , script_path : str | None = None ) -> dict [ str , Any ] : branding = self . get_branding_config ( script_path ) # Convert logo and favicon to data URLs branding [ ""logo"" ] = self . _convert_to_data_url ( branding [ ""logo"" ] ) branding [ ""favicon"" ] = self . _convert_to_data_url ( branding [ ""favicon"" ] . split ( ""?"" ) [ 0 ] ) # Remove timestamp query param logger . info ( f""Actual final branding configuration: {branding}"" ) return branding",Get branding config with logo and favicon as data URLs
/Kokoro-FastAPI/examples/assorted_checks/test_voices/analyze_voice_dimensions.py,analyze_voice_file,"def analyze_voice_file(file_path):
    """"""Analyze dimensions and statistics of a voice tensor.""""""
    try:
        tensor = torch.load(file_path, map_location=""cpu"")
        logger.info(f""\nAnalyzing {os.path.basename(file_path)}:"")
        logger.info(f""Shape: {tensor.shape}"")
        logger.info(f""Mean: {tensor.mean().item():.4f}"")
        logger.info(f""Std: {tensor.std().item():.4f}"")
        logger.info(f""Min: {tensor.min().item():.4f}"")
        logger.info(f""Max: {tensor.max().item():.4f}"")
        return tensor.shape
    except Exception as e:
        logger.error(f""Error analyzing {file_path}: {e}"")
        return None","def analyze_voice_file(file_path):
    """"""Analyze dimensions and statistics of a voice tensor.""""""
    try:
        tensor = torch.load(file_path, map_location=""cpu"")
        logger.info(f""\nAnalyzing {os.path.basename(file_path)}:"")
        logger.info(f""Shape: {tensor.shape}"")
        logger.info(f""Mean: {tensor.mean().item():.4f}"")
        logger.info(f""Std: {tensor.std().item():.4f}"")
        logger.info(f""Min: {tensor.min().item():.4f}"")
        logger.info(f""Max: {tensor.max().item():.4f}"")
        return tensor.shape
    except Exception as e:
        logger.error(f""Error analyzing {file_path}: {e}"")
        return None",Analyze dimensions and statistics of a voice tensor.,Analyze dimensions and statistics of a voice tensor.,"def analyze_voice_file(file_path):
    
    try:
        tensor = torch.load(file_path, map_location=""cpu"")
        logger.info(f""\nAnalyzing {os.path.basename(file_path)}:"")
        logger.info(f""Shape: {tensor.shape}"")
        logger.info(f""Mean: {tensor.mean().item():.4f}"")
        logger.info(f""Std: {tensor.std().item():.4f}"")
        logger.info(f""Min: {tensor.min().item():.4f}"")
        logger.info(f""Max: {tensor.max().item():.4f}"")
        return tensor.shape
    except Exception as e:
        logger.error(f""Error analyzing {file_path}: {e}"")
        return None",Analyze dimensions and statistics of a voice tensor.,"def analyze_voice_file ( file_path ) : try : tensor = torch . load ( file_path , map_location = ""cpu"" ) logger . info ( f""\nAnalyzing {os.path.basename(file_path)}:"" ) logger . info ( f""Shape: {tensor.shape}"" ) logger . info ( f""Mean: {tensor.mean().item():.4f}"" ) logger . info ( f""Std: {tensor.std().item():.4f}"" ) logger . info ( f""Min: {tensor.min().item():.4f}"" ) logger . info ( f""Max: {tensor.max().item():.4f}"" ) return tensor . shape except Exception as e : logger . error ( f""Error analyzing {file_path}: {e}"" ) return None",Analyze dimensions and statistics of a voice tensor.
/morphik-core/core/tests/integration/test_colpali_integrate_multivector.py,pdf_path,"def pdf_path():
    """"""Fixture to provide the path to the test PDF file""""""
    pdf_path = Path(PDF_FILE_PATH)
    if not pdf_path.exists():
        pytest.skip(f""Test PDF file not found at {pdf_path}"")
    return pdf_path","def pdf_path():
    """"""Fixture to provide the path to the test PDF file""""""
    pdf_path = Path(PDF_FILE_PATH)
    if not pdf_path.exists():
        pytest.skip(f""Test PDF file not found at {pdf_path}"")
    return pdf_path",Fixture to provide the path to the test PDF file,Fixture to provide the path to the test PDF file,"def pdf_path():
    
    pdf_path = Path(PDF_FILE_PATH)
    if not pdf_path.exists():
        pytest.skip(f""Test PDF file not found at {pdf_path}"")
    return pdf_path",Fixture to provide the path to the test PDF file,"def pdf_path ( ) : pdf_path = Path ( PDF_FILE_PATH ) if not pdf_path . exists ( ) : pytest . skip ( f""Test PDF file not found at {pdf_path}"" ) return pdf_path",Fixture to provide the path to the test PDF file
/magentic-ui/src/magentic_ui/eval/benchmarks/webgames/webgames.py,download_dataset,"def download_dataset(self) -> None:
        """"""
        Download the dataset into self.data_dir using huggingface datasets.
        """"""
        assert self.data_dir is not None
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir, exist_ok=True)

        logging.info(f""[WebGames] Downloading dataset into '{self.data_dir}'..."")

        # Use pandas to download and save the dataset
        df = pd.read_json(  # type: ignore
            f""hf://datasets/{self.DATASET_REPO_ID}/{self.TEST_FILE}"", lines=True
        )
        output_path = os.path.join(self.data_dir, self.TEST_FILE)
        df.to_json(output_path, orient=""records"", lines=True)  # type: ignore

        logging.info(""[WebGames] Dataset downloaded."")","def download_dataset(self) -> None:
        """"""
        Download the dataset into self.data_dir using huggingface datasets.
        """"""
        assert self.data_dir is not None
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir, exist_ok=True)

        logging.info(f""[WebGames] Downloading dataset into '{self.data_dir}'..."")

        # Use pandas to download and save the dataset
        df = pd.read_json(  # type: ignore
        )
        output_path = os.path.join(self.data_dir, self.TEST_FILE)
        df.to_json(output_path, orient=""records"", lines=True)  # type: ignore

        logging.info(""[WebGames] Dataset downloaded."")",Download the dataset into self.data_dir using huggingface datasets.,Download the dataset into self.data_dir using huggingface datasets.,"def download_dataset(self) -> None:
        
        assert self.data_dir is not None
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir, exist_ok=True)

        logging.info(f""[WebGames] Downloading dataset into '{self.data_dir}'..."")

        # Use pandas to download and save the dataset
        df = pd.read_json(  # type: ignore
        )
        output_path = os.path.join(self.data_dir, self.TEST_FILE)
        df.to_json(output_path, orient=""records"", lines=True)  # type: ignore

        logging.info(""[WebGames] Dataset downloaded."")",Download the dataset into self.data_dir using huggingface datasets.,"def download_dataset ( self ) -> None : assert self . data_dir is not None if not os . path . exists ( self . data_dir ) : os . makedirs ( self . data_dir , exist_ok = True ) logging . info ( f""[WebGames] Downloading dataset into '{self.data_dir}'..."" ) # Use pandas to download and save the dataset df = pd . read_json ( # type: ignore ) output_path = os . path . join ( self . data_dir , self . TEST_FILE ) df . to_json ( output_path , orient = ""records"" , lines = True ) # type: ignore logging . info ( ""[WebGames] Dataset downloaded."" )",Download the dataset into self.data_dir using huggingface datasets.
/PocketFlow/cookbook/pocketflow-fastapi-hitl/utils/process_task.py,process_task,"def process_task(input_data):
    """"""Minimal simulation of processing the input data.""""""
    print(f""Processing: '{input_data[:50]}...'"")
    
    # Simulate work
    time.sleep(2)

    processed_result = f""Processed: {input_data}""
    print(f""Finished processing."")
    return processed_result","def process_task(input_data):
    """"""Minimal simulation of processing the input data.""""""
    print(f""Processing: '{input_data[:50]}...'"")
    
    # Simulate work
    time.sleep(2)

    processed_result = f""Processed: {input_data}""
    print(f""Finished processing."")
    return processed_result",Minimal simulation of processing the input data.,Minimal simulation of processing the input data.,"def process_task(input_data):
    
    print(f""Processing: '{input_data[:50]}...'"")
    
    # Simulate work
    time.sleep(2)

    processed_result = f""Processed: {input_data}""
    print(f""Finished processing."")
    return processed_result",Minimal simulation of processing the input data.,"def process_task ( input_data ) : print ( f""Processing: '{input_data[:50]}...'"" ) # Simulate work time . sleep ( 2 ) processed_result = f""Processed: {input_data}"" print ( f""Finished processing."" ) return processed_result",Minimal simulation of processing the input data.
/aci/backend/aci/server/tests/security/test_rate_limiting.py,get_ratelimit_middleware_instance,"def get_ratelimit_middleware_instance(fastapi_app: FastAPI) -> RateLimitMiddleware:
    """"""find the RateLimitMiddleware instance in the middleware stack""""""
    layer = fastapi_app.middleware_stack
    while layer is not None:
        if layer.__class__.__name__ == RateLimitMiddleware.__name__:
            return cast(RateLimitMiddleware, layer)
        layer = getattr(layer, ""app"", None)

    assert False, f""{RateLimitMiddleware.__name__} instance not found""","def get_ratelimit_middleware_instance(fastapi_app: FastAPI) -> RateLimitMiddleware:
    """"""find the RateLimitMiddleware instance in the middleware stack""""""
    layer = fastapi_app.middleware_stack
    while layer is not None:
        if layer.__class__.__name__ == RateLimitMiddleware.__name__:
            return cast(RateLimitMiddleware, layer)
        layer = getattr(layer, ""app"", None)

    assert False, f""{RateLimitMiddleware.__name__} instance not found""",find the RateLimitMiddleware instance in the middleware stack,find the RateLimitMiddleware instance in the middleware stack,"def get_ratelimit_middleware_instance(fastapi_app: FastAPI) -> RateLimitMiddleware:
    
    layer = fastapi_app.middleware_stack
    while layer is not None:
        if layer.__class__.__name__ == RateLimitMiddleware.__name__:
            return cast(RateLimitMiddleware, layer)
        layer = getattr(layer, ""app"", None)

    assert False, f""{RateLimitMiddleware.__name__} instance not found""",find the RateLimitMiddleware instance in the middleware stack,"def get_ratelimit_middleware_instance ( fastapi_app : FastAPI ) -> RateLimitMiddleware : layer = fastapi_app . middleware_stack while layer is not None : if layer . __class__ . __name__ == RateLimitMiddleware . __name__ : return cast ( RateLimitMiddleware , layer ) layer = getattr ( layer , ""app"" , None ) assert False , f""{RateLimitMiddleware.__name__} instance not found""",find the RateLimitMiddleware instance in the middleware stack
/PocketFlow/cookbook/pocketflow-mcp/main.py,post,"def post(self, shared, prep_res, exec_res):
        """"""Store tools and process to decision node""""""
        tools = exec_res
        shared[""tools""] = tools
        
        # Format tool information for later use
        tool_info = []
        for i, tool in enumerate(tools, 1):
            properties = tool.inputSchema.get('properties', {})
            required = tool.inputSchema.get('required', [])
            
            params = []
            for param_name, param_info in properties.items():
                param_type = param_info.get('type', 'unknown')
                req_status = ""(Required)"" if param_name in required else ""(Optional)""
                params.append(f""    - {param_name} ({param_type}): {req_status}"")
            
            tool_info.append(f""[{i}] {tool.name}\n  Description: {tool.description}\n  Parameters:\n"" + ""\n"".join(params))
        
        shared[""tool_info""] = ""\n"".join(tool_info)
        return ""decide""","def post(self, shared, prep_res, exec_res):
        """"""Store tools and process to decision node""""""
        tools = exec_res
        shared[""tools""] = tools
        
        # Format tool information for later use
        tool_info = []
        for i, tool in enumerate(tools, 1):
            properties = tool.inputSchema.get('properties', {})
            required = tool.inputSchema.get('required', [])
            
            params = []
            for param_name, param_info in properties.items():
                param_type = param_info.get('type', 'unknown')
                req_status = ""(Required)"" if param_name in required else ""(Optional)""
                params.append(f""    - {param_name} ({param_type}): {req_status}"")
            
            tool_info.append(f""[{i}] {tool.name}\n  Description: {tool.description}\n  Parameters:\n"" + ""\n"".join(params))
        
        shared[""tool_info""] = ""\n"".join(tool_info)
        return ""decide""",Store tools and process to decision node,Store tools and process to decision node,"def post(self, shared, prep_res, exec_res):
        
        tools = exec_res
        shared[""tools""] = tools
        
        # Format tool information for later use
        tool_info = []
        for i, tool in enumerate(tools, 1):
            properties = tool.inputSchema.get('properties', {})
            required = tool.inputSchema.get('required', [])
            
            params = []
            for param_name, param_info in properties.items():
                param_type = param_info.get('type', 'unknown')
                req_status = ""(Required)"" if param_name in required else ""(Optional)""
                params.append(f""    - {param_name} ({param_type}): {req_status}"")
            
            tool_info.append(f""[{i}] {tool.name}\n  Description: {tool.description}\n  Parameters:\n"" + ""\n"".join(params))
        
        shared[""tool_info""] = ""\n"".join(tool_info)
        return ""decide""",Store tools and process to decision node,"def post ( self , shared , prep_res , exec_res ) : tools = exec_res shared [ ""tools"" ] = tools # Format tool information for later use tool_info = [ ] for i , tool in enumerate ( tools , 1 ) : properties = tool . inputSchema . get ( 'properties' , { } ) required = tool . inputSchema . get ( 'required' , [ ] ) params = [ ] for param_name , param_info in properties . items ( ) : param_type = param_info . get ( 'type' , 'unknown' ) req_status = ""(Required)"" if param_name in required else ""(Optional)"" params . append ( f""    - {param_name} ({param_type}): {req_status}"" ) tool_info . append ( f""[{i}] {tool.name}\n  Description: {tool.description}\n  Parameters:\n"" + ""\n"" . join ( params ) ) shared [ ""tool_info"" ] = ""\n"" . join ( tool_info ) return ""decide""",Store tools and process to decision node
/NLWeb/code/benchmark/run_speed_benchmark.py,plot_total_conversation_time,"def plot_total_conversation_time(results, title, filename):
    """"""Plot total conversation time per provider and save to file.""""""
    df = pd.DataFrame(results)
    df = df[(df['elapsed'].notnull()) & (df['turn'] == 'ALL')]
    if df.empty:
        print(f""No successful results to plot for {title}."")
        return
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    df.boxplot(column='elapsed', by=['provider'], ax=ax)
    plt.title(title)
    plt.suptitle("""")
    plt.ylabel('Total Conversation Time (s)')
    plt.xlabel('Provider')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(filename)
    plt.show()","def plot_total_conversation_time(results, title, filename):
    """"""Plot total conversation time per provider and save to file.""""""
    df = pd.DataFrame(results)
    df = df[(df['elapsed'].notnull()) & (df['turn'] == 'ALL')]
    if df.empty:
        print(f""No successful results to plot for {title}."")
        return
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    df.boxplot(column='elapsed', by=['provider'], ax=ax)
    plt.title(title)
    plt.suptitle("""")
    plt.ylabel('Total Conversation Time (s)')
    plt.xlabel('Provider')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(filename)
    plt.show()",Plot total conversation time per provider and save to file.,Plot total conversation time per provider and save to file.,"def plot_total_conversation_time(results, title, filename):
    
    df = pd.DataFrame(results)
    df = df[(df['elapsed'].notnull()) & (df['turn'] == 'ALL')]
    if df.empty:
        print(f""No successful results to plot for {title}."")
        return
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    df.boxplot(column='elapsed', by=['provider'], ax=ax)
    plt.title(title)
    plt.suptitle("""")
    plt.ylabel('Total Conversation Time (s)')
    plt.xlabel('Provider')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(filename)
    plt.show()",Plot total conversation time per provider and save to file.,"def plot_total_conversation_time ( results , title , filename ) : df = pd . DataFrame ( results ) df = df [ ( df [ 'elapsed' ] . notnull ( ) ) & ( df [ 'turn' ] == 'ALL' ) ] if df . empty : print ( f""No successful results to plot for {title}."" ) return plt . figure ( figsize = ( 10 , 6 ) ) ax = plt . gca ( ) df . boxplot ( column = 'elapsed' , by = [ 'provider' ] , ax = ax ) plt . title ( title ) plt . suptitle ( """" ) plt . ylabel ( 'Total Conversation Time (s)' ) plt . xlabel ( 'Provider' ) plt . xticks ( rotation = 45 , ha = 'right' ) plt . tight_layout ( ) plt . savefig ( filename ) plt . show ( )",Plot total conversation time per provider and save to file.
/OpenManus-RL/openmanus_rl/agentgym/agentenv-gaia/tests/test_trajectories.py,_extract_tool_calls,"def _extract_tool_calls(self, trajectory: dict) -> list:
        """"""Extract tool calls from a trajectory""""""
        tool_calls = []
        for message in trajectory.get(""messages"", []):
            if message.get(""role"") == ""assistant"" and message.get(""tool_calls""):
                for tool_call in message.get(""tool_calls"", []):
                    if tool_call.get(""function"", {}).get(""name"") and tool_call.get(""function"", {}).get(""arguments""):
                        tool_calls.append({
                            ""name"": tool_call[""function""][""name""],
                            ""arguments"": json.loads(tool_call[""function""][""arguments""])
                        })
        return tool_calls","def _extract_tool_calls(self, trajectory: dict) -> list:
        """"""Extract tool calls from a trajectory""""""
        tool_calls = []
        for message in trajectory.get(""messages"", []):
            if message.get(""role"") == ""assistant"" and message.get(""tool_calls""):
                for tool_call in message.get(""tool_calls"", []):
                    if tool_call.get(""function"", {}).get(""name"") and tool_call.get(""function"", {}).get(""arguments""):
                        tool_calls.append({
                            ""name"": tool_call[""function""][""name""],
                            ""arguments"": json.loads(tool_call[""function""][""arguments""])
                        })
        return tool_calls",Extract tool calls from a trajectory,Extract tool calls from a trajectory,"def _extract_tool_calls(self, trajectory: dict) -> list:
        
        tool_calls = []
        for message in trajectory.get(""messages"", []):
            if message.get(""role"") == ""assistant"" and message.get(""tool_calls""):
                for tool_call in message.get(""tool_calls"", []):
                    if tool_call.get(""function"", {}).get(""name"") and tool_call.get(""function"", {}).get(""arguments""):
                        tool_calls.append({
                            ""name"": tool_call[""function""][""name""],
                            ""arguments"": json.loads(tool_call[""function""][""arguments""])
                        })
        return tool_calls",Extract tool calls from a trajectory,"def _extract_tool_calls ( self , trajectory : dict ) -> list : tool_calls = [ ] for message in trajectory . get ( ""messages"" , [ ] ) : if message . get ( ""role"" ) == ""assistant"" and message . get ( ""tool_calls"" ) : for tool_call in message . get ( ""tool_calls"" , [ ] ) : if tool_call . get ( ""function"" , { } ) . get ( ""name"" ) and tool_call . get ( ""function"" , { } ) . get ( ""arguments"" ) : tool_calls . append ( { ""name"" : tool_call [ ""function"" ] [ ""name"" ] , ""arguments"" : json . loads ( tool_call [ ""function"" ] [ ""arguments"" ] ) } ) return tool_calls",Extract tool calls from a trajectory
/morphik-core/core/vector_store/milvus_multivector_store.py,close,"def close(self):
        """"""Close the Milvus connection.""""""
        try:
            # Milvus client doesn't have an explicit close method, but we can release the collection
            if hasattr(self.client, ""release_collection""):
                self.client.release_collection(collection_name=self.collection_name)
        except Exception as e:
            logger.error(f""Error closing Milvus connection: {e}"")","def close(self):
        """"""Close the Milvus connection.""""""
        try:
            # Milvus client doesn't have an explicit close method, but we can release the collection
            if hasattr(self.client, ""release_collection""):
                self.client.release_collection(collection_name=self.collection_name)
        except Exception as e:
            logger.error(f""Error closing Milvus connection: {e}"")",Close the Milvus connection.,Close the Milvus connection.,"def close(self):
        
        try:
            # Milvus client doesn't have an explicit close method, but we can release the collection
            if hasattr(self.client, ""release_collection""):
                self.client.release_collection(collection_name=self.collection_name)
        except Exception as e:
            logger.error(f""Error closing Milvus connection: {e}"")",Close the Milvus connection.,"def close ( self ) : try : # Milvus client doesn't have an explicit close method, but we can release the collection if hasattr ( self . client , ""release_collection"" ) : self . client . release_collection ( collection_name = self . collection_name ) except Exception as e : logger . error ( f""Error closing Milvus connection: {e}"" )",Close the Milvus connection.
/verl/verl/utils/tracking.py,log_generations_to_mlflow,"def log_generations_to_mlflow(self, samples, step):
        """"""Log validation generation to mlflow as artifacts""""""
        # https://mlflow.org/docs/latest/api_reference/python_api/mlflow.html?highlight=log_artifact#mlflow.log_artifact

        import json
        import tempfile

        import mlflow

        try:
            with tempfile.TemporaryDirectory() as tmp_dir:
                validation_gen_step_file = Path(tmp_dir, f""val_step{step}.json"")
                row_data = []
                for sample in samples:
                    data = {""input"": sample[0], ""output"": sample[1], ""score"": sample[2]}
                    row_data.append(data)
                with open(validation_gen_step_file, ""w"") as file:
                    json.dump(row_data, file)
                mlflow.log_artifact(validation_gen_step_file)
        except Exception as e:
            print(f""WARNING: save validation generation file to mlflow failed with error {e}"")","def log_generations_to_mlflow(self, samples, step):
        """"""Log validation generation to mlflow as artifacts""""""

        import json
        import tempfile

        import mlflow

        try:
            with tempfile.TemporaryDirectory() as tmp_dir:
                validation_gen_step_file = Path(tmp_dir, f""val_step{step}.json"")
                row_data = []
                for sample in samples:
                    data = {""input"": sample[0], ""output"": sample[1], ""score"": sample[2]}
                    row_data.append(data)
                with open(validation_gen_step_file, ""w"") as file:
                    json.dump(row_data, file)
                mlflow.log_artifact(validation_gen_step_file)
        except Exception as e:
            print(f""WARNING: save validation generation file to mlflow failed with error {e}"")",Log validation generation to mlflow as artifacts,Log validation generation to mlflow as artifacts,"def log_generations_to_mlflow(self, samples, step):
        

        import json
        import tempfile

        import mlflow

        try:
            with tempfile.TemporaryDirectory() as tmp_dir:
                validation_gen_step_file = Path(tmp_dir, f""val_step{step}.json"")
                row_data = []
                for sample in samples:
                    data = {""input"": sample[0], ""output"": sample[1], ""score"": sample[2]}
                    row_data.append(data)
                with open(validation_gen_step_file, ""w"") as file:
                    json.dump(row_data, file)
                mlflow.log_artifact(validation_gen_step_file)
        except Exception as e:
            print(f""WARNING: save validation generation file to mlflow failed with error {e}"")",Log validation generation to mlflow as artifacts,"def log_generations_to_mlflow ( self , samples , step ) : import json import tempfile import mlflow try : with tempfile . TemporaryDirectory ( ) as tmp_dir : validation_gen_step_file = Path ( tmp_dir , f""val_step{step}.json"" ) row_data = [ ] for sample in samples : data = { ""input"" : sample [ 0 ] , ""output"" : sample [ 1 ] , ""score"" : sample [ 2 ] } row_data . append ( data ) with open ( validation_gen_step_file , ""w"" ) as file : json . dump ( row_data , file ) mlflow . log_artifact ( validation_gen_step_file ) except Exception as e : print ( f""WARNING: save validation generation file to mlflow failed with error {e}"" )",Log validation generation to mlflow as artifacts
/local-deep-research/src/local_deep_research/web/app_factory.py,register_error_handlers,"def register_error_handlers(app):
    """"""Register error handlers with the Flask app.""""""

    @app.errorhandler(404)
    def not_found(error):
        return make_response(jsonify({""error"": ""Not found""}), 404)

    @app.errorhandler(500)
    def server_error(error):
        return make_response(jsonify({""error"": ""Server error""}), 500)","def register_error_handlers(app):
    """"""Register error handlers with the Flask app.""""""

    @app.errorhandler(404)
    def not_found(error):
        return make_response(jsonify({""error"": ""Not found""}), 404)

    @app.errorhandler(500)
    def server_error(error):
        return make_response(jsonify({""error"": ""Server error""}), 500)",Register error handlers with the Flask app.,Register error handlers with the Flask app.,"def register_error_handlers(app):
    

    @app.errorhandler(404)
    def not_found(error):
        return make_response(jsonify({""error"": ""Not found""}), 404)

    @app.errorhandler(500)
    def server_error(error):
        return make_response(jsonify({""error"": ""Server error""}), 500)",Register error handlers with the Flask app.,"def register_error_handlers ( app ) : @ app . errorhandler ( 404 ) def not_found ( error ) : return make_response ( jsonify ( { ""error"" : ""Not found"" } ) , 404 ) @ app . errorhandler ( 500 ) def server_error ( error ) : return make_response ( jsonify ( { ""error"" : ""Server error"" } ) , 500 )",Register error handlers with the Flask app.
/local-deep-research/src/local_deep_research/benchmarks/efficiency/speed_profiler.py,print_summary,"def print_summary(self):
        """"""Print a formatted summary of timing information.""""""
        summary = self.get_summary()
        total = summary.get(""total_duration"", 0)
        
        print(""\n===== SPEED PROFILE SUMMARY ====="")
        print(f""Total execution time: {total:.2f} seconds"")
        print(""\n--- Component Breakdown ---"")
        
        # Print each component's timing
        for name, data in self.timings.items():
            if name != ""total"":
                percent = data[""total""] / total * 100 if total > 0 else 0
                print(f""{name}: {data['total']:.2f}s ({percent:.1f}%) - ""
                      f""{data['count']} calls, avg {data['total'] / data['count']:.3f}s per call"")
        
        print(""\n=============================="")","def print_summary(self):
        """"""Print a formatted summary of timing information.""""""
        summary = self.get_summary()
        total = summary.get(""total_duration"", 0)
        
        print(""\n===== SPEED PROFILE SUMMARY ====="")
        print(f""Total execution time: {total:.2f} seconds"")
        print(""\n--- Component Breakdown ---"")
        
        # Print each component's timing
        for name, data in self.timings.items():
            if name != ""total"":
                percent = data[""total""] / total * 100 if total > 0 else 0
                print(f""{name}: {data['total']:.2f}s ({percent:.1f}%) - ""
                      f""{data['count']} calls, avg {data['total'] / data['count']:.3f}s per call"")
        
        print(""\n=============================="")",Print a formatted summary of timing information.,Print a formatted summary of timing information.,"def print_summary(self):
        
        summary = self.get_summary()
        total = summary.get(""total_duration"", 0)
        
        print(""\n===== SPEED PROFILE SUMMARY ====="")
        print(f""Total execution time: {total:.2f} seconds"")
        print(""\n--- Component Breakdown ---"")
        
        # Print each component's timing
        for name, data in self.timings.items():
            if name != ""total"":
                percent = data[""total""] / total * 100 if total > 0 else 0
                print(f""{name}: {data['total']:.2f}s ({percent:.1f}%) - ""
                      f""{data['count']} calls, avg {data['total'] / data['count']:.3f}s per call"")
        
        print(""\n=============================="")",Print a formatted summary of timing information.,"def print_summary ( self ) : summary = self . get_summary ( ) total = summary . get ( ""total_duration"" , 0 ) print ( ""\n===== SPEED PROFILE SUMMARY ====="" ) print ( f""Total execution time: {total:.2f} seconds"" ) print ( ""\n--- Component Breakdown ---"" ) # Print each component's timing for name , data in self . timings . items ( ) : if name != ""total"" : percent = data [ ""total"" ] / total * 100 if total > 0 else 0 print ( f""{name}: {data['total']:.2f}s ({percent:.1f}%) - "" f""{data['count']} calls, avg {data['total'] / data['count']:.3f}s per call"" ) print ( ""\n=============================="" )",Print a formatted summary of timing information.
/cursor-free-vip/check_user_authorized.py,generate_cursor_checksum,"def generate_cursor_checksum(token: str, translator=None) -> str:
    """"""Generate Cursor checksum from token using the algorithm""""""
    try:
        # Clean the token
        clean_token = token.strip()
        
        # Generate machineId and macMachineId
        machine_id = generate_hashed64_hex(clean_token, 'machineId')
        mac_machine_id = generate_hashed64_hex(clean_token, 'macMachineId')
        
        # Get timestamp and convert to byte array
        timestamp = int(time.time() * 1000) // 1000000
        byte_array = bytearray(struct.pack('>Q', timestamp)[-6:])  # Take last 6 bytes
        
        # Obfuscate bytes and encode as base64
        obfuscated_bytes = obfuscate_bytes(byte_array)
        encoded_checksum = base64.b64encode(obfuscated_bytes).decode('utf-8')
        
        # Combine final checksum
        return f""{encoded_checksum}{machine_id}/{mac_machine_id}""
    except Exception as e:
        print(f""{Fore.RED}{EMOJI['ERROR']} {translator.get('auth_check.error_generating_checksum', error=str(e)) if translator else f'Error generating checksum: {str(e)}'}{Style.RESET_ALL}"")
        return """"","def generate_cursor_checksum(token: str, translator=None) -> str:
    """"""Generate Cursor checksum from token using the algorithm""""""
    try:
        # Clean the token
        clean_token = token.strip()
        
        # Generate machineId and macMachineId
        machine_id = generate_hashed64_hex(clean_token, 'machineId')
        mac_machine_id = generate_hashed64_hex(clean_token, 'macMachineId')
        
        # Get timestamp and convert to byte array
        byte_array = bytearray(struct.pack('>Q', timestamp)[-6:])  # Take last 6 bytes
        
        # Obfuscate bytes and encode as base64
        obfuscated_bytes = obfuscate_bytes(byte_array)
        encoded_checksum = base64.b64encode(obfuscated_bytes).decode('utf-8')
        
        # Combine final checksum
        return f""{encoded_checksum}{machine_id}/{mac_machine_id}""
    except Exception as e:
        print(f""{Fore.RED}{EMOJI['ERROR']} {translator.get('auth_check.error_generating_checksum', error=str(e)) if translator else f'Error generating checksum: {str(e)}'}{Style.RESET_ALL}"")
        return """"",Generate Cursor checksum from token using the algorithm,Generate Cursor checksum from token using the algorithm,"def generate_cursor_checksum(token: str, translator=None) -> str:
    
    try:
        # Clean the token
        clean_token = token.strip()
        
        # Generate machineId and macMachineId
        machine_id = generate_hashed64_hex(clean_token, 'machineId')
        mac_machine_id = generate_hashed64_hex(clean_token, 'macMachineId')
        
        # Get timestamp and convert to byte array
        byte_array = bytearray(struct.pack('>Q', timestamp)[-6:])  # Take last 6 bytes
        
        # Obfuscate bytes and encode as base64
        obfuscated_bytes = obfuscate_bytes(byte_array)
        encoded_checksum = base64.b64encode(obfuscated_bytes).decode('utf-8')
        
        # Combine final checksum
        return f""{encoded_checksum}{machine_id}/{mac_machine_id}""
    except Exception as e:
        print(f""{Fore.RED}{EMOJI['ERROR']} {translator.get('auth_check.error_generating_checksum', error=str(e)) if translator else f'Error generating checksum: {str(e)}'}{Style.RESET_ALL}"")
        return """"",Generate Cursor checksum from token using the algorithm,"def generate_cursor_checksum ( token : str , translator = None ) -> str : try : # Clean the token clean_token = token . strip ( ) # Generate machineId and macMachineId machine_id = generate_hashed64_hex ( clean_token , 'machineId' ) mac_machine_id = generate_hashed64_hex ( clean_token , 'macMachineId' ) # Get timestamp and convert to byte array byte_array = bytearray ( struct . pack ( '>Q' , timestamp ) [ - 6 : ] ) # Take last 6 bytes # Obfuscate bytes and encode as base64 obfuscated_bytes = obfuscate_bytes ( byte_array ) encoded_checksum = base64 . b64encode ( obfuscated_bytes ) . decode ( 'utf-8' ) # Combine final checksum return f""{encoded_checksum}{machine_id}/{mac_machine_id}"" except Exception as e : print ( f""{Fore.RED}{EMOJI['ERROR']} {translator.get('auth_check.error_generating_checksum', error=str(e)) if translator else f'Error generating checksum: {str(e)}'}{Style.RESET_ALL}"" ) return """"",Generate Cursor checksum from token using the algorithm
/cua/libs/computer-server/computer_server/handlers/linux.py,get_screen_size,"def get_screen_size(self) -> Tuple[int, int]:
        """"""Get the screen size.""""""
        try:
            size = pyautogui.size()
            return size.width, size.height
        except Exception as e:
            logger.warning(f""Failed to get screen size with pyautogui: {e}"")
        
        logger.info(""Getting screen size (simulated)"")
        return 1920, 1080","def get_screen_size(self) -> Tuple[int, int]:
        """"""Get the screen size.""""""
        try:
            size = pyautogui.size()
            return size.width, size.height
        except Exception as e:
            logger.warning(f""Failed to get screen size with pyautogui: {e}"")
        
        logger.info(""Getting screen size (simulated)"")
        return 1920, 1080",Get the screen size.,Get the screen size.,"def get_screen_size(self) -> Tuple[int, int]:
        
        try:
            size = pyautogui.size()
            return size.width, size.height
        except Exception as e:
            logger.warning(f""Failed to get screen size with pyautogui: {e}"")
        
        logger.info(""Getting screen size (simulated)"")
        return 1920, 1080",Get the screen size.,"def get_screen_size ( self ) -> Tuple [ int , int ] : try : size = pyautogui . size ( ) return size . width , size . height except Exception as e : logger . warning ( f""Failed to get screen size with pyautogui: {e}"" ) logger . info ( ""Getting screen size (simulated)"" ) return 1920 , 1080",Get the screen size.
/ragaai-catalyst/tests/examples/langgraph/personal_research_assistant/research_assistant.py,generate_sub_questions,"def generate_sub_questions(state: ResearchState) -> ResearchState:
    """"""Generate sub-questions based on the topic.""""""
    prompt = PromptTemplate(
        input_variables=[""topic""],
        template=""Given the topic '{topic}', generate 3 specific sub-questions to guide research.""
    )
    response = llm.invoke(prompt.format(topic=state[""topic""]))
    questions = [q.strip() for q in response.content.split(""\n"") if q.strip()]
    return {""sub_questions"": questions, ""status"": ""generated_questions""}","def generate_sub_questions(state: ResearchState) -> ResearchState:
    """"""Generate sub-questions based on the topic.""""""
    prompt = PromptTemplate(
        input_variables=[""topic""],
        template=""Given the topic '{topic}', generate 3 specific sub-questions to guide research.""
    )
    response = llm.invoke(prompt.format(topic=state[""topic""]))
    questions = [q.strip() for q in response.content.split(""\n"") if q.strip()]
    return {""sub_questions"": questions, ""status"": ""generated_questions""}",Generate sub-questions based on the topic.,Generate sub-questions based on the topic.,"def generate_sub_questions(state: ResearchState) -> ResearchState:
    
    prompt = PromptTemplate(
        input_variables=[""topic""],
        template=""Given the topic '{topic}', generate 3 specific sub-questions to guide research.""
    )
    response = llm.invoke(prompt.format(topic=state[""topic""]))
    questions = [q.strip() for q in response.content.split(""\n"") if q.strip()]
    return {""sub_questions"": questions, ""status"": ""generated_questions""}",Generate sub-questions based on the topic.,"def generate_sub_questions ( state : ResearchState ) -> ResearchState : prompt = PromptTemplate ( input_variables = [ ""topic"" ] , template = ""Given the topic '{topic}', generate 3 specific sub-questions to guide research."" ) response = llm . invoke ( prompt . format ( topic = state [ ""topic"" ] ) ) questions = [ q . strip ( ) for q in response . content . split ( ""\n"" ) if q . strip ( ) ] return { ""sub_questions"" : questions , ""status"" : ""generated_questions"" }",Generate sub-questions based on the topic.
/fastmcp/examples/smart_home/src/smart_home/hub.py,hub_status,"def hub_status() -> str:
    """"""Checks the status of the main hub and connections.""""""
    try:
        bridge = Bridge(
            ip=str(settings.hue_bridge_ip),
            username=settings.hue_bridge_username,
            save_config=False,
        )
        bridge.connect()
        return ""Hub OK. Hue Bridge Connected (via phue2).""
    except Exception as e:
        return f""Hub Warning: Hue Bridge connection failed or not attempted: {e}""","def hub_status() -> str:
    """"""Checks the status of the main hub and connections.""""""
    try:
        bridge = Bridge(
            ip=str(settings.hue_bridge_ip),
            username=settings.hue_bridge_username,
            save_config=False,
        )
        bridge.connect()
        return ""Hub OK. Hue Bridge Connected (via phue2).""
    except Exception as e:
        return f""Hub Warning: Hue Bridge connection failed or not attempted: {e}""",Checks the status of the main hub and connections.,Checks the status of the main hub and connections.,"def hub_status() -> str:
    
    try:
        bridge = Bridge(
            ip=str(settings.hue_bridge_ip),
            username=settings.hue_bridge_username,
            save_config=False,
        )
        bridge.connect()
        return ""Hub OK. Hue Bridge Connected (via phue2).""
    except Exception as e:
        return f""Hub Warning: Hue Bridge connection failed or not attempted: {e}""",Checks the status of the main hub and connections.,"def hub_status ( ) -> str : try : bridge = Bridge ( ip = str ( settings . hue_bridge_ip ) , username = settings . hue_bridge_username , save_config = False , ) bridge . connect ( ) return ""Hub OK. Hue Bridge Connected (via phue2)."" except Exception as e : return f""Hub Warning: Hue Bridge connection failed or not attempted: {e}""",Checks the status of the main hub and connections.
/mcp-agent/tests/workflows/llm/test_augmented_llm_google.py,create_tool_result_message,"def create_tool_result_message(tool_result, tool_name, status=""success""):
        """"""
        Creates a tool result message for testing in Google's format.
        """"""
        from google.genai import types

        if status == ""success"":
            function_response = {""result"": tool_result}
        else:
            function_response = {""error"": tool_result}

        return types.Content(
            role=""tool"",
            parts=[
                types.Part.from_function_response(
                    name=tool_name, response=function_response
                )
            ],
        )","def create_tool_result_message(tool_result, tool_name, status=""success""):
        """"""
        Creates a tool result message for testing in Google's format.
        """"""
        from google.genai import types

        if status == ""success"":
            function_response = {""result"": tool_result}
        else:
            function_response = {""error"": tool_result}

        return types.Content(
            role=""tool"",
            parts=[
                types.Part.from_function_response(
                    name=tool_name, response=function_response
                )
            ],
        )",Creates a tool result message for testing in Google's format.,Creates a tool result message for testing in Google's format.,"def create_tool_result_message(tool_result, tool_name, status=""success""):
        
        from google.genai import types

        if status == ""success"":
            function_response = {""result"": tool_result}
        else:
            function_response = {""error"": tool_result}

        return types.Content(
            role=""tool"",
            parts=[
                types.Part.from_function_response(
                    name=tool_name, response=function_response
                )
            ],
        )",Creates a tool result message for testing in Google's format.,"def create_tool_result_message ( tool_result , tool_name , status = ""success"" ) : from google . genai import types if status == ""success"" : function_response = { ""result"" : tool_result } else : function_response = { ""error"" : tool_result } return types . Content ( role = ""tool"" , parts = [ types . Part . from_function_response ( name = tool_name , response = function_response ) ] , )",Creates a tool result message for testing in Google's format.
/verl/scripts/model_merger.py,_merge_by_placement,"def _merge_by_placement(self, tensors: list[torch.Tensor], placement: Placement) -> torch.Tensor:
        """"""Merges a list of tensors based on their DTensor placement""""""
        if placement.is_replicate():
            return tensors[0]
        elif placement.is_partial():
            raise NotImplementedError(""Partial placement is not supported yet"")
        elif placement.is_shard():
            return torch.cat(tensors, dim=placement.dim).contiguous()

        raise NotImplementedError(f""Unsupported placement: {placement}"")","def _merge_by_placement(self, tensors: list[torch.Tensor], placement: Placement) -> torch.Tensor:
        """"""Merges a list of tensors based on their DTensor placement""""""
        if placement.is_replicate():
            return tensors[0]
        elif placement.is_partial():
            raise NotImplementedError(""Partial placement is not supported yet"")
        elif placement.is_shard():
            return torch.cat(tensors, dim=placement.dim).contiguous()

        raise NotImplementedError(f""Unsupported placement: {placement}"")",Merges a list of tensors based on their DTensor placement,Merges a list of tensors based on their DTensor placement,"def _merge_by_placement(self, tensors: list[torch.Tensor], placement: Placement) -> torch.Tensor:
        
        if placement.is_replicate():
            return tensors[0]
        elif placement.is_partial():
            raise NotImplementedError(""Partial placement is not supported yet"")
        elif placement.is_shard():
            return torch.cat(tensors, dim=placement.dim).contiguous()

        raise NotImplementedError(f""Unsupported placement: {placement}"")",Merges a list of tensors based on their DTensor placement,"def _merge_by_placement ( self , tensors : list [ torch . Tensor ] , placement : Placement ) -> torch . Tensor : if placement . is_replicate ( ) : return tensors [ 0 ] elif placement . is_partial ( ) : raise NotImplementedError ( ""Partial placement is not supported yet"" ) elif placement . is_shard ( ) : return torch . cat ( tensors , dim = placement . dim ) . contiguous ( ) raise NotImplementedError ( f""Unsupported placement: {placement}"" )",Merges a list of tensors based on their DTensor placement
/ai-hedge-fund/src/agents/charlie_munger.py,analyze_news_sentiment,"def analyze_news_sentiment(news_items: list) -> str:
    """"""
    Simple qualitative analysis of recent news.
    Munger pays attention to significant news but doesn't overreact to short-term stories.
    """"""
    if not news_items or len(news_items) == 0:
        return ""No news data available""
    
    # Just return a simple count for now - in a real implementation, this would use NLP
    return f""Qualitative review of {len(news_items)} recent news items would be needed""","def analyze_news_sentiment(news_items: list) -> str:
    """"""
    Simple qualitative analysis of recent news.
    Munger pays attention to significant news but doesn't overreact to short-term stories.
    """"""
    if not news_items or len(news_items) == 0:
        return ""No news data available""
    
    # Just return a simple count for now - in a real implementation, this would use NLP
    return f""Qualitative review of {len(news_items)} recent news items would be needed""","Simple qualitative analysis of recent news.
Munger pays attention to significant news but doesn't overreact to short-term stories.",Simple qualitative analysis of recent news.,"def analyze_news_sentiment(news_items: list) -> str:
    
    if not news_items or len(news_items) == 0:
        return ""No news data available""
    
    # Just return a simple count for now - in a real implementation, this would use NLP
    return f""Qualitative review of {len(news_items)} recent news items would be needed""",Simple qualitative analysis of recent news.,"def analyze_news_sentiment ( news_items : list ) -> str : if not news_items or len ( news_items ) == 0 : return ""No news data available"" # Just return a simple count for now - in a real implementation, this would use NLP return f""Qualitative review of {len(news_items)} recent news items would be needed""",Simple qualitative analysis of recent news.
/LightRAG/lightrag/modalprocessors.py,_encode_image_to_base64,"def _encode_image_to_base64(self, image_path: str) -> str:
        """"""Encode image to base64""""""
        try:
            with open(image_path, ""rb"") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode(""utf-8"")
            return encoded_string
        except Exception as e:
            logger.error(f""Failed to encode image {image_path}: {e}"")
            return """"","def _encode_image_to_base64(self, image_path: str) -> str:
        """"""Encode image to base64""""""
        try:
            with open(image_path, ""rb"") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode(""utf-8"")
            return encoded_string
        except Exception as e:
            logger.error(f""Failed to encode image {image_path}: {e}"")
            return """"",Encode image to base64,Encode image to base64,"def _encode_image_to_base64(self, image_path: str) -> str:
        
        try:
            with open(image_path, ""rb"") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode(""utf-8"")
            return encoded_string
        except Exception as e:
            logger.error(f""Failed to encode image {image_path}: {e}"")
            return """"",Encode image to base64,"def _encode_image_to_base64 ( self , image_path : str ) -> str : try : with open ( image_path , ""rb"" ) as image_file : encoded_string = base64 . b64encode ( image_file . read ( ) ) . decode ( ""utf-8"" ) return encoded_string except Exception as e : logger . error ( f""Failed to encode image {image_path}: {e}"" ) return """"",Encode image to base64
/optillm/scripts/eval_aime_benchmark.py,save_raw_response,"def save_raw_response(filename: str, problem_id: int, response_data: Dict):
    """"""Save raw response data (including logprobs) to a separate file.""""""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # Create a timestamped ID for this response
    timestamp = int(time.time())
    response_id = f""{problem_id}_{timestamp}""
    
    # Create or update the raw responses file
    try:
        with open(filename, 'r') as f:
            raw_responses = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        raw_responses = {}
    
    # Add this response to the collection
    raw_responses[response_id] = response_data
    
    # Save the updated collection
    with open(filename, 'w') as f:
        json.dump(raw_responses, f)
    
    return response_id","def save_raw_response(filename: str, problem_id: int, response_data: Dict):
    """"""Save raw response data (including logprobs) to a separate file.""""""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # Create a timestamped ID for this response
    timestamp = int(time.time())
    response_id = f""{problem_id}_{timestamp}""
    
    # Create or update the raw responses file
    try:
        with open(filename, 'r') as f:
            raw_responses = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        raw_responses = {}
    
    # Add this response to the collection
    raw_responses[response_id] = response_data
    
    # Save the updated collection
    with open(filename, 'w') as f:
        json.dump(raw_responses, f)
    
    return response_id",Save raw response data (including logprobs) to a separate file.,Save raw response data (including logprobs) to a separate file.,"def save_raw_response(filename: str, problem_id: int, response_data: Dict):
    
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # Create a timestamped ID for this response
    timestamp = int(time.time())
    response_id = f""{problem_id}_{timestamp}""
    
    # Create or update the raw responses file
    try:
        with open(filename, 'r') as f:
            raw_responses = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        raw_responses = {}
    
    # Add this response to the collection
    raw_responses[response_id] = response_data
    
    # Save the updated collection
    with open(filename, 'w') as f:
        json.dump(raw_responses, f)
    
    return response_id",Save raw response data (including logprobs) to a separate file.,"def save_raw_response ( filename : str , problem_id : int , response_data : Dict ) : os . makedirs ( os . path . dirname ( filename ) , exist_ok = True ) # Create a timestamped ID for this response timestamp = int ( time . time ( ) ) response_id = f""{problem_id}_{timestamp}"" # Create or update the raw responses file try : with open ( filename , 'r' ) as f : raw_responses = json . load ( f ) except ( FileNotFoundError , json . JSONDecodeError ) : raw_responses = { } # Add this response to the collection raw_responses [ response_id ] = response_data # Save the updated collection with open ( filename , 'w' ) as f : json . dump ( raw_responses , f ) return response_id",Save raw response data (including logprobs) to a separate file.
/OpenManus-RL/verl/single_controller/base/decorator.py,get_predefined_execute_fn,"def get_predefined_execute_fn(execute_mode):
    """"""
    Note that here we only asks execute_all and execute_rank_zero to be implemented
    Leave the choice of how these two functions handle argument 'blocking' to users
    """"""
    predefined_execute_mode_fn = {
        Execute.ALL: {
            'execute_fn_name': 'execute_all'
        },
        Execute.RANK_ZERO: {
            'execute_fn_name': 'execute_rank_zero'
        }
    }
    return predefined_execute_mode_fn[execute_mode]","def get_predefined_execute_fn(execute_mode):
    """"""
    Note that here we only asks execute_all and execute_rank_zero to be implemented
    Leave the choice of how these two functions handle argument 'blocking' to users
    """"""
    predefined_execute_mode_fn = {
        Execute.ALL: {
            'execute_fn_name': 'execute_all'
        },
        Execute.RANK_ZERO: {
            'execute_fn_name': 'execute_rank_zero'
        }
    }
    return predefined_execute_mode_fn[execute_mode]","Note that here we only asks execute_all and execute_rank_zero to be implemented
Leave the choice of how these two functions handle argument 'blocking' to users",Note that here we only asks execute_all and execute_rank_zero to be implemented,"def get_predefined_execute_fn(execute_mode):
    
    predefined_execute_mode_fn = {
        Execute.ALL: {
            'execute_fn_name': 'execute_all'
        },
        Execute.RANK_ZERO: {
            'execute_fn_name': 'execute_rank_zero'
        }
    }
    return predefined_execute_mode_fn[execute_mode]",Note that here we only asks execute_all and execute_rank_zero to be implemented,"def get_predefined_execute_fn ( execute_mode ) : predefined_execute_mode_fn = { Execute . ALL : { 'execute_fn_name' : 'execute_all' } , Execute . RANK_ZERO : { 'execute_fn_name' : 'execute_rank_zero' } } return predefined_execute_mode_fn [ execute_mode ]",Note that here we only asks execute_all and execute_rank_zero to be implemented
/agenticSeek/sources/llm_provider.py,deepseek_fn,"def deepseek_fn(self, history, verbose=False):
        """"""
        Use deepseek api to generate text.
        """"""
        client = OpenAI(api_key=self.api_key, base_url=""https://api.deepseek.com"")
        if self.is_local:
            raise Exception(""Deepseek (API) is not available for local use. Change config.ini"")
        try:
            response = client.chat.completions.create(
                model=""deepseek-chat"",
                messages=history,
                stream=False
            )
            thought = response.choices[0].message.content
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""Deepseek API error: {str(e)}"") from e","def deepseek_fn(self, history, verbose=False):
        """"""
        Use deepseek api to generate text.
        """"""
        if self.is_local:
            raise Exception(""Deepseek (API) is not available for local use. Change config.ini"")
        try:
            response = client.chat.completions.create(
                model=""deepseek-chat"",
                messages=history,
                stream=False
            )
            thought = response.choices[0].message.content
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""Deepseek API error: {str(e)}"") from e",Use deepseek api to generate text.,Use deepseek api to generate text.,"def deepseek_fn(self, history, verbose=False):
        
        if self.is_local:
            raise Exception(""Deepseek (API) is not available for local use. Change config.ini"")
        try:
            response = client.chat.completions.create(
                model=""deepseek-chat"",
                messages=history,
                stream=False
            )
            thought = response.choices[0].message.content
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""Deepseek API error: {str(e)}"") from e",Use deepseek api to generate text.,"def deepseek_fn ( self , history , verbose = False ) : if self . is_local : raise Exception ( ""Deepseek (API) is not available for local use. Change config.ini"" ) try : response = client . chat . completions . create ( model = ""deepseek-chat"" , messages = history , stream = False ) thought = response . choices [ 0 ] . message . content if verbose : print ( thought ) return thought except Exception as e : raise Exception ( f""Deepseek API error: {str(e)}"" ) from e",Use deepseek api to generate text.
/browser-use/browser_use/browser/profile.py,copy_old_config_names_to_new,"def copy_old_config_names_to_new(self) -> Self:
		""""""Copy old config window_width & window_height to window_size.""""""
		if self.window_width or self.window_height:
			self.window_size = self.window_size or {}
			self.window_size['width'] = (self.window_size or {}).get('width') or self.window_width or 1280
			self.window_size['height'] = (self.window_size or {}).get('height') or self.window_height or 1100
		return self","def copy_old_config_names_to_new(self) -> Self:
		""""""Copy old config window_width & window_height to window_size.""""""
		if self.window_width or self.window_height:
			self.window_size = self.window_size or {}
			self.window_size['width'] = (self.window_size or {}).get('width') or self.window_width or 1280
			self.window_size['height'] = (self.window_size or {}).get('height') or self.window_height or 1100
		return self",Copy old config window_width & window_height to window_size.,Copy old config window_width & window_height to window_size.,"def copy_old_config_names_to_new(self) -> Self:
		
		if self.window_width or self.window_height:
			self.window_size = self.window_size or {}
			self.window_size['width'] = (self.window_size or {}).get('width') or self.window_width or 1280
			self.window_size['height'] = (self.window_size or {}).get('height') or self.window_height or 1100
		return self",Copy old config window_width & window_height to window_size.,def copy_old_config_names_to_new ( self ) -> Self : if self . window_width or self . window_height : self . window_size = self . window_size or { } self . window_size [ 'width' ] = ( self . window_size or { } ) . get ( 'width' ) or self . window_width or 1280 self . window_size [ 'height' ] = ( self . window_size or { } ) . get ( 'height' ) or self . window_height or 1100 return self,Copy old config window_width & window_height to window_size.
/morphik-core/sdks/python/morphik/_internal.py,_prepare_batch_get_documents_request,"def _prepare_batch_get_documents_request(
        self, document_ids: List[str], folder_name: Optional[Union[str, List[str]]], end_user_id: Optional[str]
    ) -> Dict[str, Any]:
        """"""Prepare request for batch_get_documents endpoint""""""
        if folder_name or end_user_id:
            request = {""document_ids"": document_ids}
            if folder_name:
                request[""folder_name""] = folder_name
            if end_user_id:
                request[""end_user_id""] = end_user_id
            return request
        return document_ids","def _prepare_batch_get_documents_request(
        self, document_ids: List[str], folder_name: Optional[Union[str, List[str]]], end_user_id: Optional[str]
    ) -> Dict[str, Any]:
        """"""Prepare request for batch_get_documents endpoint""""""
        if folder_name or end_user_id:
            request = {""document_ids"": document_ids}
            if folder_name:
                request[""folder_name""] = folder_name
            if end_user_id:
                request[""end_user_id""] = end_user_id
            return request
        return document_ids",Prepare request for batch_get_documents endpoint,Prepare request for batch_get_documents endpoint,"def _prepare_batch_get_documents_request(
        self, document_ids: List[str], folder_name: Optional[Union[str, List[str]]], end_user_id: Optional[str]
    ) -> Dict[str, Any]:
        
        if folder_name or end_user_id:
            request = {""document_ids"": document_ids}
            if folder_name:
                request[""folder_name""] = folder_name
            if end_user_id:
                request[""end_user_id""] = end_user_id
            return request
        return document_ids",Prepare request for batch_get_documents endpoint,"def _prepare_batch_get_documents_request ( self , document_ids : List [ str ] , folder_name : Optional [ Union [ str , List [ str ] ] ] , end_user_id : Optional [ str ] ) -> Dict [ str , Any ] : if folder_name or end_user_id : request = { ""document_ids"" : document_ids } if folder_name : request [ ""folder_name"" ] = folder_name if end_user_id : request [ ""end_user_id"" ] = end_user_id return request return document_ids",Prepare request for batch_get_documents endpoint
/OpenManus/app/sandbox/core/manager.py,start_cleanup_task,"def start_cleanup_task(self) -> None:
        """"""Starts automatic cleanup task.""""""

        async def cleanup_loop():
            while not self._is_shutting_down:
                try:
                    await self._cleanup_idle_sandboxes()
                except Exception as e:
                    logger.error(f""Error in cleanup loop: {e}"")
                await asyncio.sleep(self.cleanup_interval)

        self._cleanup_task = asyncio.create_task(cleanup_loop())","def start_cleanup_task(self) -> None:
        """"""Starts automatic cleanup task.""""""

        async def cleanup_loop():
            while not self._is_shutting_down:
                try:
                    await self._cleanup_idle_sandboxes()
                except Exception as e:
                    logger.error(f""Error in cleanup loop: {e}"")
                await asyncio.sleep(self.cleanup_interval)

        self._cleanup_task = asyncio.create_task(cleanup_loop())",Starts automatic cleanup task.,Starts automatic cleanup task.,"def start_cleanup_task(self) -> None:
        

        async def cleanup_loop():
            while not self._is_shutting_down:
                try:
                    await self._cleanup_idle_sandboxes()
                except Exception as e:
                    logger.error(f""Error in cleanup loop: {e}"")
                await asyncio.sleep(self.cleanup_interval)

        self._cleanup_task = asyncio.create_task(cleanup_loop())",Starts automatic cleanup task.,"def start_cleanup_task ( self ) -> None : async def cleanup_loop ( ) : while not self . _is_shutting_down : try : await self . _cleanup_idle_sandboxes ( ) except Exception as e : logger . error ( f""Error in cleanup loop: {e}"" ) await asyncio . sleep ( self . cleanup_interval ) self . _cleanup_task = asyncio . create_task ( cleanup_loop ( ) )",Starts automatic cleanup task.
/agenticSeek/sources/agents/file_agent.py,__init__,"def __init__(self, name, prompt_path, provider, verbose=False):
        """"""
        The file agent is a special agent for file operations.
        """"""
        super().__init__(name, prompt_path, provider, verbose, None)
        self.tools = {
            ""file_finder"": FileFinder(),
            ""bash"": BashInterpreter()
        }
        self.work_dir = self.tools[""file_finder""].get_work_dir()
        self.role = ""files""
        self.type = ""file_agent""
        self.memory = Memory(self.load_prompt(prompt_path),
                        recover_last_session=False, # session recovery in handled by the interaction class
                        memory_compression=False,
                        model_provider=provider.get_model_name())","def __init__(self, name, prompt_path, provider, verbose=False):
        """"""
        The file agent is a special agent for file operations.
        """"""
        super().__init__(name, prompt_path, provider, verbose, None)
        self.tools = {
            ""file_finder"": FileFinder(),
            ""bash"": BashInterpreter()
        }
        self.work_dir = self.tools[""file_finder""].get_work_dir()
        self.role = ""files""
        self.type = ""file_agent""
        self.memory = Memory(self.load_prompt(prompt_path),
                        recover_last_session=False, # session recovery in handled by the interaction class
                        memory_compression=False,
                        model_provider=provider.get_model_name())",The file agent is a special agent for file operations.,The file agent is a special agent for file operations.,"def __init__(self, name, prompt_path, provider, verbose=False):
        
        super().__init__(name, prompt_path, provider, verbose, None)
        self.tools = {
            ""file_finder"": FileFinder(),
            ""bash"": BashInterpreter()
        }
        self.work_dir = self.tools[""file_finder""].get_work_dir()
        self.role = ""files""
        self.type = ""file_agent""
        self.memory = Memory(self.load_prompt(prompt_path),
                        recover_last_session=False, # session recovery in handled by the interaction class
                        memory_compression=False,
                        model_provider=provider.get_model_name())",The file agent is a special agent for file operations.,"def __init__ ( self , name , prompt_path , provider , verbose = False ) : super ( ) . __init__ ( name , prompt_path , provider , verbose , None ) self . tools = { ""file_finder"" : FileFinder ( ) , ""bash"" : BashInterpreter ( ) } self . work_dir = self . tools [ ""file_finder"" ] . get_work_dir ( ) self . role = ""files"" self . type = ""file_agent"" self . memory = Memory ( self . load_prompt ( prompt_path ) , recover_last_session = False , # session recovery in handled by the interaction class memory_compression = False , model_provider = provider . get_model_name ( ) )",The file agent is a special agent for file operations.
/adk-samples/python/agents/RAG/rag/shared_libraries/prepare_corpus_and_data.py,download_pdf_from_url,"def download_pdf_from_url(url, output_path):
  """"""Downloads a PDF file from the specified URL.""""""
  print(f""Downloading PDF from {url}..."")
  response = requests.get(url, stream=True)
  response.raise_for_status()  # Raise an exception for HTTP errors
  
  with open(output_path, 'wb') as f:
    for chunk in response.iter_content(chunk_size=8192):
      f.write(chunk)
  
  print(f""PDF downloaded successfully to {output_path}"")
  return output_path","def download_pdf_from_url(url, output_path):
  """"""Downloads a PDF file from the specified URL.""""""
  print(f""Downloading PDF from {url}..."")
  response = requests.get(url, stream=True)
  response.raise_for_status()  # Raise an exception for HTTP errors
  
  with open(output_path, 'wb') as f:
    for chunk in response.iter_content(chunk_size=8192):
      f.write(chunk)
  
  print(f""PDF downloaded successfully to {output_path}"")
  return output_path",Downloads a PDF file from the specified URL.,Downloads a PDF file from the specified URL.,"def download_pdf_from_url(url, output_path):
  
  print(f""Downloading PDF from {url}..."")
  response = requests.get(url, stream=True)
  response.raise_for_status()  # Raise an exception for HTTP errors
  
  with open(output_path, 'wb') as f:
    for chunk in response.iter_content(chunk_size=8192):
      f.write(chunk)
  
  print(f""PDF downloaded successfully to {output_path}"")
  return output_path",Downloads a PDF file from the specified URL.,"def download_pdf_from_url ( url , output_path ) : print ( f""Downloading PDF from {url}..."" ) response = requests . get ( url , stream = True ) response . raise_for_status ( ) # Raise an exception for HTTP errors with open ( output_path , 'wb' ) as f : for chunk in response . iter_content ( chunk_size = 8192 ) : f . write ( chunk ) print ( f""PDF downloaded successfully to {output_path}"" ) return output_path",Downloads a PDF file from the specified URL.
/olmocr/olmocr/bench/scripts/workspace_to_bench.py,load_jsonl_files,"def load_jsonl_files(input_dir):
    """"""Load all JSONL files from the input directory.""""""
    jsonl_files = list(Path(input_dir).glob(""*.jsonl""))
    if not jsonl_files:
        print(f""No JSONL files found in {input_dir}"")
        return []

    print(f""Found {len(jsonl_files)} JSONL files: {[f.name for f in jsonl_files]}"")
    return jsonl_files","def load_jsonl_files(input_dir):
    """"""Load all JSONL files from the input directory.""""""
    jsonl_files = list(Path(input_dir).glob(""*.jsonl""))
    if not jsonl_files:
        print(f""No JSONL files found in {input_dir}"")
        return []

    print(f""Found {len(jsonl_files)} JSONL files: {[f.name for f in jsonl_files]}"")
    return jsonl_files",Load all JSONL files from the input directory.,Load all JSONL files from the input directory.,"def load_jsonl_files(input_dir):
    
    jsonl_files = list(Path(input_dir).glob(""*.jsonl""))
    if not jsonl_files:
        print(f""No JSONL files found in {input_dir}"")
        return []

    print(f""Found {len(jsonl_files)} JSONL files: {[f.name for f in jsonl_files]}"")
    return jsonl_files",Load all JSONL files from the input directory.,"def load_jsonl_files ( input_dir ) : jsonl_files = list ( Path ( input_dir ) . glob ( ""*.jsonl"" ) ) if not jsonl_files : print ( f""No JSONL files found in {input_dir}"" ) return [ ] print ( f""Found {len(jsonl_files)} JSONL files: {[f.name for f in jsonl_files]}"" ) return jsonl_files",Load all JSONL files from the input directory.
/fastmcp/tests/client/test_client.py,tagged_resources_server,"def tagged_resources_server():
    """"""Fixture that creates a FastMCP server with tagged resources and templates.""""""
    server = FastMCP(""TaggedResourcesServer"")

    # Add a resource with tags
    @server.resource(
        uri=""data://tagged"", tags={""test"", ""metadata""}, description=""A tagged resource""
    )
    async def get_tagged_data():
        return {""type"": ""tagged_data""}

    # Add a resource template with tags
    @server.resource(
        uri=""template://{id}"",
        tags={""template"", ""parameterized""},
        description=""A tagged template"",
    )
    async def get_template_data(id: str):
        return {""id"": id, ""type"": ""template_data""}

    return server","def tagged_resources_server():
    """"""Fixture that creates a FastMCP server with tagged resources and templates.""""""
    server = FastMCP(""TaggedResourcesServer"")

    # Add a resource with tags
    @server.resource(
    )
    async def get_tagged_data():
        return {""type"": ""tagged_data""}

    # Add a resource template with tags
    @server.resource(
        tags={""template"", ""parameterized""},
        description=""A tagged template"",
    )
    async def get_template_data(id: str):
        return {""id"": id, ""type"": ""template_data""}

    return server",Fixture that creates a FastMCP server with tagged resources and templates.,Fixture that creates a FastMCP server with tagged resources and templates.,"def tagged_resources_server():
    
    server = FastMCP(""TaggedResourcesServer"")

    # Add a resource with tags
    @server.resource(
    )
    async def get_tagged_data():
        return {""type"": ""tagged_data""}

    # Add a resource template with tags
    @server.resource(
        tags={""template"", ""parameterized""},
        description=""A tagged template"",
    )
    async def get_template_data(id: str):
        return {""id"": id, ""type"": ""template_data""}

    return server",Fixture that creates a FastMCP server with tagged resources and templates.,"def tagged_resources_server ( ) : server = FastMCP ( ""TaggedResourcesServer"" ) # Add a resource with tags @ server . resource ( ) async def get_tagged_data ( ) : return { ""type"" : ""tagged_data"" } # Add a resource template with tags @ server . resource ( tags = { ""template"" , ""parameterized"" } , description = ""A tagged template"" , ) async def get_template_data ( id : str ) : return { ""id"" : id , ""type"" : ""template_data"" } return server",Fixture that creates a FastMCP server with tagged resources and templates.
/magentic-ui/src/magentic_ui/backend/web/initialization.py,_load_environment,"def _load_environment(self) -> None:
        """"""Load environment variables from .env file if it exists""""""
        env_file = self.app_root / "".env""
        if env_file.exists():
            # logger.info(f""Loading environment variables from {env_file}"")
            load_dotenv(str(env_file))","def _load_environment(self) -> None:
        """"""Load environment variables from .env file if it exists""""""
        env_file = self.app_root / "".env""
        if env_file.exists():
            # logger.info(f""Loading environment variables from {env_file}"")
            load_dotenv(str(env_file))",Load environment variables from .env file if it exists,Load environment variables from .env file if it exists,"def _load_environment(self) -> None:
        
        env_file = self.app_root / "".env""
        if env_file.exists():
            # logger.info(f""Loading environment variables from {env_file}"")
            load_dotenv(str(env_file))",Load environment variables from .env file if it exists,"def _load_environment ( self ) -> None : env_file = self . app_root / "".env"" if env_file . exists ( ) : # logger.info(f""Loading environment variables from {env_file}"") load_dotenv ( str ( env_file ) )",Load environment variables from .env file if it exists
/OpenManus/app/tool/tool_collection.py,add_tool,"def add_tool(self, tool: BaseTool):
        """"""Add a single tool to the collection.

        If a tool with the same name already exists, it will be skipped and a warning will be logged.
        """"""
        if tool.name in self.tool_map:
            logger.warning(f""Tool {tool.name} already exists in collection, skipping"")
            return self

        self.tools += (tool,)
        self.tool_map[tool.name] = tool
        return self","def add_tool(self, tool: BaseTool):
        """"""Add a single tool to the collection.

        If a tool with the same name already exists, it will be skipped and a warning will be logged.
        """"""
        if tool.name in self.tool_map:
            logger.warning(f""Tool {tool.name} already exists in collection, skipping"")
            return self

        self.tools += (tool,)
        self.tool_map[tool.name] = tool
        return self","Add a single tool to the collection.

If a tool with the same name already exists, it will be skipped and a warning will be logged.",Add a single tool to the collection.,"def add_tool(self, tool: BaseTool):
        
        if tool.name in self.tool_map:
            logger.warning(f""Tool {tool.name} already exists in collection, skipping"")
            return self

        self.tools += (tool,)
        self.tool_map[tool.name] = tool
        return self",Add a single tool to the collection.,"def add_tool ( self , tool : BaseTool ) : if tool . name in self . tool_map : logger . warning ( f""Tool {tool.name} already exists in collection, skipping"" ) return self self . tools += ( tool , ) self . tool_map [ tool . name ] = tool return self",Add a single tool to the collection.
/ragaai-catalyst/ragaai_catalyst/synthetic_data_generation.py,_generate_batch_response,"def _generate_batch_response(self, text, system_message, provider, model_config, api_key, api_base):
        """"""Generate a batch of responses using the specified provider.""""""
        MAX_RETRIES = 3
        
        for attempt in range(MAX_RETRIES):
            try:
                if provider == ""gemini"" and api_base:
                    messages = [{'role': 'user', 'content': system_message + text}]
                    response = proxy_api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    # response = proxy_call.api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    return pd.DataFrame(ast.literal_eval(response[0]))
                else:
                    return self._generate_llm_response(text, system_message, model_config, api_key)
            except (json.JSONDecodeError, ValueError) as e:
                if attempt == MAX_RETRIES - 1:
                    raise Exception(f""Failed to generate valid response after {MAX_RETRIES} attempts: {str(e)}"")
                continue","def _generate_batch_response(self, text, system_message, provider, model_config, api_key, api_base):
        """"""Generate a batch of responses using the specified provider.""""""
        MAX_RETRIES = 3
        
        for attempt in range(MAX_RETRIES):
            try:
                if provider == ""gemini"" and api_base:
                    messages = [{'role': 'user', 'content': system_message + text}]
                    response = proxy_api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    # response = proxy_call.api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    return pd.DataFrame(ast.literal_eval(response[0]))
                else:
                    return self._generate_llm_response(text, system_message, model_config, api_key)
            except (json.JSONDecodeError, ValueError) as e:
                if attempt == MAX_RETRIES - 1:
                    raise Exception(f""Failed to generate valid response after {MAX_RETRIES} attempts: {str(e)}"")
                continue",Generate a batch of responses using the specified provider.,Generate a batch of responses using the specified provider.,"def _generate_batch_response(self, text, system_message, provider, model_config, api_key, api_base):
        
        MAX_RETRIES = 3
        
        for attempt in range(MAX_RETRIES):
            try:
                if provider == ""gemini"" and api_base:
                    messages = [{'role': 'user', 'content': system_message + text}]
                    response = proxy_api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    # response = proxy_call.api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    return pd.DataFrame(ast.literal_eval(response[0]))
                else:
                    return self._generate_llm_response(text, system_message, model_config, api_key)
            except (json.JSONDecodeError, ValueError) as e:
                if attempt == MAX_RETRIES - 1:
                    raise Exception(f""Failed to generate valid response after {MAX_RETRIES} attempts: {str(e)}"")
                continue",Generate a batch of responses using the specified provider.,"def _generate_batch_response ( self , text , system_message , provider , model_config , api_key , api_base ) : MAX_RETRIES = 3 for attempt in range ( MAX_RETRIES ) : try : if provider == ""gemini"" and api_base : messages = [ { 'role' : 'user' , 'content' : system_message + text } ] response = proxy_api_completion ( messages = messages , model = model_config [ ""model"" ] , api_base = api_base ) # response = proxy_call.api_completion(messages=messages, model=model_config[""model""], api_base=api_base) return pd . DataFrame ( ast . literal_eval ( response [ 0 ] ) ) else : return self . _generate_llm_response ( text , system_message , model_config , api_key ) except ( json . JSONDecodeError , ValueError ) as e : if attempt == MAX_RETRIES - 1 : raise Exception ( f""Failed to generate valid response after {MAX_RETRIES} attempts: {str(e)}"" ) continue",Generate a batch of responses using the specified provider.
/airweave/backend/tests/platform/embedding_models/conftest.py,model_kwargs,"def model_kwargs():
    """"""Return kwargs for model initialization.""""""
    inference_url = settings.TEXT2VEC_INFERENCE_URL
    if not inference_url:
        raise ValueError(""TEXT2VEC_INFERENCE_URL environment variable is not set"")
    return {""inference_url"": inference_url}","def model_kwargs():
    """"""Return kwargs for model initialization.""""""
    inference_url = settings.TEXT2VEC_INFERENCE_URL
    if not inference_url:
        raise ValueError(""TEXT2VEC_INFERENCE_URL environment variable is not set"")
    return {""inference_url"": inference_url}",Return kwargs for model initialization.,Return kwargs for model initialization.,"def model_kwargs():
    
    inference_url = settings.TEXT2VEC_INFERENCE_URL
    if not inference_url:
        raise ValueError(""TEXT2VEC_INFERENCE_URL environment variable is not set"")
    return {""inference_url"": inference_url}",Return kwargs for model initialization.,"def model_kwargs ( ) : inference_url = settings . TEXT2VEC_INFERENCE_URL if not inference_url : raise ValueError ( ""TEXT2VEC_INFERENCE_URL environment variable is not set"" ) return { ""inference_url"" : inference_url }",Return kwargs for model initialization.
/optillm/optillm/inference.py,validate_adapter,"def validate_adapter(self, adapter_id: str) -> bool:
        """"""Validate if adapter exists and is compatible""""""
        try:
            config = PeftConfig.from_pretrained(
                adapter_id,
                trust_remote_code=True,
                use_auth_token=os.getenv(""HF_TOKEN"")
            )
            return True
        except Exception as e:
            logger.error(f""Error validating adapter {adapter_id}: {str(e)}"")
            return False","def validate_adapter(self, adapter_id: str) -> bool:
        """"""Validate if adapter exists and is compatible""""""
        try:
            config = PeftConfig.from_pretrained(
                adapter_id,
                trust_remote_code=True,
                use_auth_token=os.getenv(""HF_TOKEN"")
            )
            return True
        except Exception as e:
            logger.error(f""Error validating adapter {adapter_id}: {str(e)}"")
            return False",Validate if adapter exists and is compatible,Validate if adapter exists and is compatible,"def validate_adapter(self, adapter_id: str) -> bool:
        
        try:
            config = PeftConfig.from_pretrained(
                adapter_id,
                trust_remote_code=True,
                use_auth_token=os.getenv(""HF_TOKEN"")
            )
            return True
        except Exception as e:
            logger.error(f""Error validating adapter {adapter_id}: {str(e)}"")
            return False",Validate if adapter exists and is compatible,"def validate_adapter ( self , adapter_id : str ) -> bool : try : config = PeftConfig . from_pretrained ( adapter_id , trust_remote_code = True , use_auth_token = os . getenv ( ""HF_TOKEN"" ) ) return True except Exception as e : logger . error ( f""Error validating adapter {adapter_id}: {str(e)}"" ) return False",Validate if adapter exists and is compatible
/ragaai-catalyst/tests/examples/all_llm_provider/all_llm_provider.py,_get_litellm_response,"def _get_litellm_response(
    prompt,
    model, 
    temperature,
    max_tokens
    ):
    """"""
    Get response using LiteLLM
    """"""
    try:
        response = completion(
            model=model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f""Error with LiteLLM: {str(e)}"")
        return None","def _get_litellm_response(
    prompt,
    model, 
    temperature,
    max_tokens
    ):
    """"""
    Get response using LiteLLM
    """"""
    try:
        response = completion(
            model=model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f""Error with LiteLLM: {str(e)}"")
        return None",Get response using LiteLLM,Get response using LiteLLM,"def _get_litellm_response(
    prompt,
    model, 
    temperature,
    max_tokens
    ):
    
    try:
        response = completion(
            model=model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f""Error with LiteLLM: {str(e)}"")
        return None",Get response using LiteLLM,"def _get_litellm_response ( prompt , model , temperature , max_tokens ) : try : response = completion ( model = model , messages = [ { ""role"" : ""user"" , ""content"" : prompt } ] , temperature = temperature , max_tokens = max_tokens ) return response . choices [ 0 ] . message . content except Exception as e : print ( f""Error with LiteLLM: {str(e)}"" ) return None",Get response using LiteLLM
/optillm/optillm/plugins/executecode_plugin.py,extract_python_code,"def extract_python_code(text: str) -> List[str]:
    """"""Extract Python code blocks from text.""""""
    # print(f""Extracting code: {text}"")
    pattern = r'```python\s*(.*?)\s*```'
    return re.findall(pattern, text, re.DOTALL)","def extract_python_code(text: str) -> List[str]:
    """"""Extract Python code blocks from text.""""""
    # print(f""Extracting code: {text}"")
    pattern = r'```python\s*(.*?)\s*```'
    return re.findall(pattern, text, re.DOTALL)",Extract Python code blocks from text.,Extract Python code blocks from text.,"def extract_python_code(text: str) -> List[str]:
    
    # print(f""Extracting code: {text}"")
    pattern = r'```python\s*(.*?)\s*```'
    return re.findall(pattern, text, re.DOTALL)",Extract Python code blocks from text.,"def extract_python_code ( text : str ) -> List [ str ] : # print(f""Extracting code: {text}"") pattern = r'```python\s*(.*?)\s*```' return re . findall ( pattern , text , re . DOTALL )",Extract Python code blocks from text.
/preswald/preswald/browser/entrypoint.py,handle_js_message,"def handle_js_message(client_id, message_type, data):
        """"""Handle message from JavaScript""""""
        if _service:
            asyncio.create_task(  # noqa: RUF006
                _service.handle_client_message(
                    client_id, {""type"": message_type, ""data"": data}
                )
            )
        return True","def handle_js_message(client_id, message_type, data):
        """"""Handle message from JavaScript""""""
        if _service:
            asyncio.create_task(  # noqa: RUF006
                _service.handle_client_message(
                    client_id, {""type"": message_type, ""data"": data}
                )
            )
        return True",Handle message from JavaScript,Handle message from JavaScript,"def handle_js_message(client_id, message_type, data):
        
        if _service:
            asyncio.create_task(  # noqa: RUF006
                _service.handle_client_message(
                    client_id, {""type"": message_type, ""data"": data}
                )
            )
        return True",Handle message from JavaScript,"def handle_js_message ( client_id , message_type , data ) : if _service : asyncio . create_task ( # noqa: RUF006 _service . handle_client_message ( client_id , { ""type"" : message_type , ""data"" : data } ) ) return True",Handle message from JavaScript
/ClearerVoice-Studio/train/target_speaker_extraction/models/av_tfgridnetV3/espnet2/nets_utils.py,rename_state_dict,"def rename_state_dict(
    old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]
):
    """"""Replace keys of old prefix with new prefix in state dict.""""""
    # need this list not to break the dict iterator
    old_keys = [k for k in state_dict if k.startswith(old_prefix)]
    if len(old_keys) > 0:
        logging.warning(f""Rename: {old_prefix} -> {new_prefix}"")
    for k in old_keys:
        v = state_dict.pop(k)
        new_k = k.replace(old_prefix, new_prefix)
        state_dict[new_k] = v","def rename_state_dict(
    old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]
):
    """"""Replace keys of old prefix with new prefix in state dict.""""""
    # need this list not to break the dict iterator
    old_keys = [k for k in state_dict if k.startswith(old_prefix)]
    if len(old_keys) > 0:
        logging.warning(f""Rename: {old_prefix} -> {new_prefix}"")
    for k in old_keys:
        v = state_dict.pop(k)
        new_k = k.replace(old_prefix, new_prefix)
        state_dict[new_k] = v",Replace keys of old prefix with new prefix in state dict.,Replace keys of old prefix with new prefix in state dict.,"def rename_state_dict(
    old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]
):
    
    # need this list not to break the dict iterator
    old_keys = [k for k in state_dict if k.startswith(old_prefix)]
    if len(old_keys) > 0:
        logging.warning(f""Rename: {old_prefix} -> {new_prefix}"")
    for k in old_keys:
        v = state_dict.pop(k)
        new_k = k.replace(old_prefix, new_prefix)
        state_dict[new_k] = v",Replace keys of old prefix with new prefix in state dict.,"def rename_state_dict ( old_prefix : str , new_prefix : str , state_dict : Dict [ str , torch . Tensor ] ) : # need this list not to break the dict iterator old_keys = [ k for k in state_dict if k . startswith ( old_prefix ) ] if len ( old_keys ) > 0 : logging . warning ( f""Rename: {old_prefix} -> {new_prefix}"" ) for k in old_keys : v = state_dict . pop ( k ) new_k = k . replace ( old_prefix , new_prefix ) state_dict [ new_k ] = v",Replace keys of old prefix with new prefix in state dict.
/YuE/evals/pitch_range/raw_pitch_extracted_combined/analyze_f0.py,get_note_name,"def get_note_name(midi):
    """"""Convert MIDI note number to note name""""""
    notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    octave = (midi // 12) - 1
    note = notes[midi % 12]
    return f""{note}{octave}""","def get_note_name(midi):
    """"""Convert MIDI note number to note name""""""
    notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    note = notes[midi % 12]
    return f""{note}{octave}""",Convert MIDI note number to note name,Convert MIDI note number to note name,"def get_note_name(midi):
    
    notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    note = notes[midi % 12]
    return f""{note}{octave}""",Convert MIDI note number to note name,"def get_note_name ( midi ) : notes = [ 'C' , 'C#' , 'D' , 'D#' , 'E' , 'F' , 'F#' , 'G' , 'G#' , 'A' , 'A#' , 'B' ] note = notes [ midi % 12 ] return f""{note}{octave}""",Convert MIDI note number to note name
/nexa-sdk/nexa/eval/nexa_task/samplers.py,sample,"def sample(self, n) -> None:
        """"""
        Draw the first `n` samples in order from the specified split.
        Used for tasks with ""canonical"" ordered fewshot examples, such as MMLU and CMMLU.
        """"""
        assert (
            n <= len(self.docs)
        ), f""Error: number of fewshot samples requested exceeds the {len(self.docs)} that are available.""
        return self.docs[:n]","def sample(self, n) -> None:
        """"""
        Draw the first `n` samples in order from the specified split.
        Used for tasks with ""canonical"" ordered fewshot examples, such as MMLU and CMMLU.
        """"""
        assert (
            n <= len(self.docs)
        ), f""Error: number of fewshot samples requested exceeds the {len(self.docs)} that are available.""
        return self.docs[:n]","Draw the first `n` samples in order from the specified split.
Used for tasks with ""canonical"" ordered fewshot examples, such as MMLU and CMMLU.",Draw the first `n` samples in order from the specified split.,"def sample(self, n) -> None:
        
        assert (
            n <= len(self.docs)
        ), f""Error: number of fewshot samples requested exceeds the {len(self.docs)} that are available.""
        return self.docs[:n]",Draw the first `n` samples in order from the specified split.,"def sample ( self , n ) -> None : assert ( n <= len ( self . docs ) ) , f""Error: number of fewshot samples requested exceeds the {len(self.docs)} that are available."" return self . docs [ : n ]",Draw the first `n` samples in order from the specified split.
/cua/libs/agent/agent/providers/omni/clients/oaicompat.py,_get_loggable_messages,"def _get_loggable_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """"""Create a loggable version of messages with image data truncated.""""""
        loggable_messages = []
        for msg in messages:
            if isinstance(msg.get(""content""), list):
                new_content = []
                for content in msg[""content""]:
                    if content.get(""type"") == ""image"":
                        new_content.append(
                            {""type"": ""image"", ""image_url"": {""url"": ""[BASE64_IMAGE_DATA]""}}
                        )
                    else:
                        new_content.append(content)
                loggable_messages.append({""role"": msg[""role""], ""content"": new_content})
            else:
                loggable_messages.append(msg)
        return loggable_messages","def _get_loggable_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """"""Create a loggable version of messages with image data truncated.""""""
        loggable_messages = []
        for msg in messages:
            if isinstance(msg.get(""content""), list):
                new_content = []
                for content in msg[""content""]:
                    if content.get(""type"") == ""image"":
                        new_content.append(
                            {""type"": ""image"", ""image_url"": {""url"": ""[BASE64_IMAGE_DATA]""}}
                        )
                    else:
                        new_content.append(content)
                loggable_messages.append({""role"": msg[""role""], ""content"": new_content})
            else:
                loggable_messages.append(msg)
        return loggable_messages",Create a loggable version of messages with image data truncated.,Create a loggable version of messages with image data truncated.,"def _get_loggable_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        
        loggable_messages = []
        for msg in messages:
            if isinstance(msg.get(""content""), list):
                new_content = []
                for content in msg[""content""]:
                    if content.get(""type"") == ""image"":
                        new_content.append(
                            {""type"": ""image"", ""image_url"": {""url"": ""[BASE64_IMAGE_DATA]""}}
                        )
                    else:
                        new_content.append(content)
                loggable_messages.append({""role"": msg[""role""], ""content"": new_content})
            else:
                loggable_messages.append(msg)
        return loggable_messages",Create a loggable version of messages with image data truncated.,"def _get_loggable_messages ( self , messages : List [ Dict [ str , Any ] ] ) -> List [ Dict [ str , Any ] ] : loggable_messages = [ ] for msg in messages : if isinstance ( msg . get ( ""content"" ) , list ) : new_content = [ ] for content in msg [ ""content"" ] : if content . get ( ""type"" ) == ""image"" : new_content . append ( { ""type"" : ""image"" , ""image_url"" : { ""url"" : ""[BASE64_IMAGE_DATA]"" } } ) else : new_content . append ( content ) loggable_messages . append ( { ""role"" : msg [ ""role"" ] , ""content"" : new_content } ) else : loggable_messages . append ( msg ) return loggable_messages",Create a loggable version of messages with image data truncated.
/nv-ingest/api/src/nv_ingest_api/internal/transform/split_text.py,_build_split_documents,"def _build_split_documents(row, chunks: List[str]) -> List[dict[str, Any]]:
    """"""Build documents from text chunks""""""
    documents: List[dict] = []

    for i, text in enumerate(chunks):
        if text is None or not text.strip():
            continue

        metadata = row.metadata if hasattr(row, ""metadata"") and isinstance(row.metadata, dict) else {}
        metadata = copy.deepcopy(metadata)

        metadata[""content""] = text

        documents.append({""document_type"": ContentTypeEnum.TEXT.value, ""metadata"": metadata, ""uuid"": str(uuid.uuid4())})

    return documents","def _build_split_documents(row, chunks: List[str]) -> List[dict[str, Any]]:
    """"""Build documents from text chunks""""""
    documents: List[dict] = []

    for i, text in enumerate(chunks):
        if text is None or not text.strip():
            continue

        metadata = row.metadata if hasattr(row, ""metadata"") and isinstance(row.metadata, dict) else {}
        metadata = copy.deepcopy(metadata)

        metadata[""content""] = text

        documents.append({""document_type"": ContentTypeEnum.TEXT.value, ""metadata"": metadata, ""uuid"": str(uuid.uuid4())})

    return documents",Build documents from text chunks,Build documents from text chunks,"def _build_split_documents(row, chunks: List[str]) -> List[dict[str, Any]]:
    
    documents: List[dict] = []

    for i, text in enumerate(chunks):
        if text is None or not text.strip():
            continue

        metadata = row.metadata if hasattr(row, ""metadata"") and isinstance(row.metadata, dict) else {}
        metadata = copy.deepcopy(metadata)

        metadata[""content""] = text

        documents.append({""document_type"": ContentTypeEnum.TEXT.value, ""metadata"": metadata, ""uuid"": str(uuid.uuid4())})

    return documents",Build documents from text chunks,"def _build_split_documents ( row , chunks : List [ str ] ) -> List [ dict [ str , Any ] ] : documents : List [ dict ] = [ ] for i , text in enumerate ( chunks ) : if text is None or not text . strip ( ) : continue metadata = row . metadata if hasattr ( row , ""metadata"" ) and isinstance ( row . metadata , dict ) else { } metadata = copy . deepcopy ( metadata ) metadata [ ""content"" ] = text documents . append ( { ""document_type"" : ContentTypeEnum . TEXT . value , ""metadata"" : metadata , ""uuid"" : str ( uuid . uuid4 ( ) ) } ) return documents",Build documents from text chunks
/ag2/autogen/agentchat/group/context_expression.py,_extract_variable_names,"def _extract_variable_names(self, expr: str) -> list[str]:
        """"""Extract all variable references ${var_name} from the expression.""""""
        # Find all patterns like ${var_name}
        matches = re.findall(r""\${([^}]*)}"", expr)
        return matches","def _extract_variable_names(self, expr: str) -> list[str]:
        """"""Extract all variable references ${var_name} from the expression.""""""
        # Find all patterns like ${var_name}
        matches = re.findall(r""\${([^}]*)}"", expr)
        return matches",Extract all variable references ${var_name} from the expression.,Extract all variable references ${var_name} from the expression.,"def _extract_variable_names(self, expr: str) -> list[str]:
        
        # Find all patterns like ${var_name}
        matches = re.findall(r""\${([^}]*)}"", expr)
        return matches",Extract all variable references ${var_name} from the expression.,"def _extract_variable_names ( self , expr : str ) -> list [ str ] : # Find all patterns like ${var_name} matches = re . findall ( r""\${([^}]*)}"" , expr ) return matches",Extract all variable references ${var_name} from the expression.
/local-deep-research/src/local_deep_research/web_search_engines/engines/search_engine_github.py,_format_repository_preview,"def _format_repository_preview(self, repo: Dict[str, Any]) -> Dict[str, Any]:
        """"""Format repository search result as preview""""""
        return {
            ""id"": str(repo.get(""id"", """")),
            ""title"": repo.get(""full_name"", """"),
            ""link"": repo.get(""html_url"", """"),
            ""snippet"": repo.get(""description"", ""No description provided""),
            ""stars"": repo.get(""stargazers_count"", 0),
            ""forks"": repo.get(""forks_count"", 0),
            ""language"": repo.get(""language"", """"),
            ""updated_at"": repo.get(""updated_at"", """"),
            ""created_at"": repo.get(""created_at"", """"),
            ""topics"": repo.get(""topics"", []),
            ""owner"": repo.get(""owner"", {}).get(""login"", """"),
            ""is_fork"": repo.get(""fork"", False),
            ""search_type"": ""repository"",
            ""repo_full_name"": repo.get(""full_name"", """"),
        }","def _format_repository_preview(self, repo: Dict[str, Any]) -> Dict[str, Any]:
        """"""Format repository search result as preview""""""
        return {
            ""id"": str(repo.get(""id"", """")),
            ""title"": repo.get(""full_name"", """"),
            ""link"": repo.get(""html_url"", """"),
            ""snippet"": repo.get(""description"", ""No description provided""),
            ""stars"": repo.get(""stargazers_count"", 0),
            ""forks"": repo.get(""forks_count"", 0),
            ""language"": repo.get(""language"", """"),
            ""updated_at"": repo.get(""updated_at"", """"),
            ""created_at"": repo.get(""created_at"", """"),
            ""topics"": repo.get(""topics"", []),
            ""owner"": repo.get(""owner"", {}).get(""login"", """"),
            ""is_fork"": repo.get(""fork"", False),
            ""search_type"": ""repository"",
            ""repo_full_name"": repo.get(""full_name"", """"),
        }",Format repository search result as preview,Format repository search result as preview,"def _format_repository_preview(self, repo: Dict[str, Any]) -> Dict[str, Any]:
        
        return {
            ""id"": str(repo.get(""id"", """")),
            ""title"": repo.get(""full_name"", """"),
            ""link"": repo.get(""html_url"", """"),
            ""snippet"": repo.get(""description"", ""No description provided""),
            ""stars"": repo.get(""stargazers_count"", 0),
            ""forks"": repo.get(""forks_count"", 0),
            ""language"": repo.get(""language"", """"),
            ""updated_at"": repo.get(""updated_at"", """"),
            ""created_at"": repo.get(""created_at"", """"),
            ""topics"": repo.get(""topics"", []),
            ""owner"": repo.get(""owner"", {}).get(""login"", """"),
            ""is_fork"": repo.get(""fork"", False),
            ""search_type"": ""repository"",
            ""repo_full_name"": repo.get(""full_name"", """"),
        }",Format repository search result as preview,"def _format_repository_preview ( self , repo : Dict [ str , Any ] ) -> Dict [ str , Any ] : return { ""id"" : str ( repo . get ( ""id"" , """" ) ) , ""title"" : repo . get ( ""full_name"" , """" ) , ""link"" : repo . get ( ""html_url"" , """" ) , ""snippet"" : repo . get ( ""description"" , ""No description provided"" ) , ""stars"" : repo . get ( ""stargazers_count"" , 0 ) , ""forks"" : repo . get ( ""forks_count"" , 0 ) , ""language"" : repo . get ( ""language"" , """" ) , ""updated_at"" : repo . get ( ""updated_at"" , """" ) , ""created_at"" : repo . get ( ""created_at"" , """" ) , ""topics"" : repo . get ( ""topics"" , [ ] ) , ""owner"" : repo . get ( ""owner"" , { } ) . get ( ""login"" , """" ) , ""is_fork"" : repo . get ( ""fork"" , False ) , ""search_type"" : ""repository"" , ""repo_full_name"" : repo . get ( ""full_name"" , """" ) , }",Format repository search result as preview
/owl/community_usecase/stock-analysis/run.py,save_chat_history,"def save_chat_history(chat_history: List[dict[Any, Any]], company_name: str) -> None:
    """"""Analyze chat history and extract tool call information.""""""

    # 保存聊天历史记录到文件
    chat_history_file = os.path.join(LOG_DIR, f""{company_name}_chat_history.json"")
    with open(chat_history_file, ""w"", encoding=""utf-8"") as f:
        json.dump(chat_history, f, ensure_ascii=False, indent=2)

    print(f""{Fore.GREEN}Records saved to {chat_history_file}{Style.RESET_ALL}"")","def save_chat_history(chat_history: List[dict[Any, Any]], company_name: str) -> None:
    """"""Analyze chat history and extract tool call information.""""""

    # 保存聊天历史记录到文件
    chat_history_file = os.path.join(LOG_DIR, f""{company_name}_chat_history.json"")
    with open(chat_history_file, ""w"", encoding=""utf-8"") as f:
        json.dump(chat_history, f, ensure_ascii=False, indent=2)

    print(f""{Fore.GREEN}Records saved to {chat_history_file}{Style.RESET_ALL}"")",Analyze chat history and extract tool call information.,Analyze chat history and extract tool call information.,"def save_chat_history(chat_history: List[dict[Any, Any]], company_name: str) -> None:
    

    # 保存聊天历史记录到文件
    chat_history_file = os.path.join(LOG_DIR, f""{company_name}_chat_history.json"")
    with open(chat_history_file, ""w"", encoding=""utf-8"") as f:
        json.dump(chat_history, f, ensure_ascii=False, indent=2)

    print(f""{Fore.GREEN}Records saved to {chat_history_file}{Style.RESET_ALL}"")",Analyze chat history and extract tool call information.,"def save_chat_history ( chat_history : List [ dict [ Any , Any ] ] , company_name : str ) -> None : # 保存聊天历史记录到文件 chat_history_file = os . path . join ( LOG_DIR , f""{company_name}_chat_history.json"" ) with open ( chat_history_file , ""w"" , encoding = ""utf-8"" ) as f : json . dump ( chat_history , f , ensure_ascii = False , indent = 2 ) print ( f""{Fore.GREEN}Records saved to {chat_history_file}{Style.RESET_ALL}"" )",Analyze chat history and extract tool call information.
/OpenManus-RL/openmanus_rl/agentgym/agentenv-tool/Toolusage/toolusage/utils/tool/helpers.py,save_log,"def save_log(logger_name, task_name, output_dir):
    """"""Creates a log file and logging object for the corresponding task Name""""""
    log_dir = os.path.join(output_dir, 'trajectory')
    os.makedirs(log_dir, exist_ok=True)
    log_file_name = f'{task_name}.log'
    log_file_path = os.path.join(log_dir, log_file_name)
    logger = AgentLogger(logger_name, filepath=log_file_path)
    return logger","def save_log(logger_name, task_name, output_dir):
    """"""Creates a log file and logging object for the corresponding task Name""""""
    log_dir = os.path.join(output_dir, 'trajectory')
    os.makedirs(log_dir, exist_ok=True)
    log_file_name = f'{task_name}.log'
    log_file_path = os.path.join(log_dir, log_file_name)
    logger = AgentLogger(logger_name, filepath=log_file_path)
    return logger",Creates a log file and logging object for the corresponding task Name,Creates a log file and logging object for the corresponding task Name,"def save_log(logger_name, task_name, output_dir):
    
    log_dir = os.path.join(output_dir, 'trajectory')
    os.makedirs(log_dir, exist_ok=True)
    log_file_name = f'{task_name}.log'
    log_file_path = os.path.join(log_dir, log_file_name)
    logger = AgentLogger(logger_name, filepath=log_file_path)
    return logger",Creates a log file and logging object for the corresponding task Name,"def save_log ( logger_name , task_name , output_dir ) : log_dir = os . path . join ( output_dir , 'trajectory' ) os . makedirs ( log_dir , exist_ok = True ) log_file_name = f'{task_name}.log' log_file_path = os . path . join ( log_dir , log_file_name ) logger = AgentLogger ( logger_name , filepath = log_file_path ) return logger",Creates a log file and logging object for the corresponding task Name
/olmocr/olmocr/viewer/dolmaviewer.py,read_jsonl,"def read_jsonl(paths):
    """"""
    Generator that yields lines from multiple JSONL files.
    Supports both local and S3 paths.
    """"""
    for path in paths:
        try:
            with smart_open.smart_open(path, ""r"", encoding=""utf-8"") as f:
                for line in f:
                    yield line.strip()
        except Exception as e:
            print(f""Error reading {path}: {e}"")","def read_jsonl(paths):
    """"""
    Generator that yields lines from multiple JSONL files.
    Supports both local and S3 paths.
    """"""
    for path in paths:
        try:
            with smart_open.smart_open(path, ""r"", encoding=""utf-8"") as f:
                for line in f:
                    yield line.strip()
        except Exception as e:
            print(f""Error reading {path}: {e}"")","Generator that yields lines from multiple JSONL files.
Supports both local and S3 paths.",Generator that yields lines from multiple JSONL files.,"def read_jsonl(paths):
    
    for path in paths:
        try:
            with smart_open.smart_open(path, ""r"", encoding=""utf-8"") as f:
                for line in f:
                    yield line.strip()
        except Exception as e:
            print(f""Error reading {path}: {e}"")",Generator that yields lines from multiple JSONL files.,"def read_jsonl ( paths ) : for path in paths : try : with smart_open . smart_open ( path , ""r"" , encoding = ""utf-8"" ) as f : for line in f : yield line . strip ( ) except Exception as e : print ( f""Error reading {path}: {e}"" )",Generator that yields lines from multiple JSONL files.
/nexa-sdk/nexa/gguf/lib_utils.py,try_add_cuda_lib_path,"def try_add_cuda_lib_path():
    """"""Try to add the CUDA library paths to the system PATH.""""""
    required_submodules = [""cuda_runtime"", ""cublas""]
    cuda_versions = [""11"", ""12""]

    module_spec = find_spec(""nvidia"")
    if not module_spec:
        return

    nvidia_lib_root = Path(module_spec.submodule_search_locations[0])

    for submodule in required_submodules:
        for ver in cuda_versions:
            try:
                package_name = f""nvidia_{submodule}_cu{ver}""
                _ = distribution(package_name)

                lib_path = nvidia_lib_root / submodule / ""bin""
                os.add_dll_directory(str(lib_path))
                os.environ[""PATH""] = str(lib_path) + \
                    os.pathsep + os.environ[""PATH""]
                logging.debug(f""Added {lib_path} to PATH"")
            except PackageNotFoundError:
                logging.debug(f""{package_name} not found"")","def try_add_cuda_lib_path():
    """"""Try to add the CUDA library paths to the system PATH.""""""
    required_submodules = [""cuda_runtime"", ""cublas""]
    cuda_versions = [""11"", ""12""]

    module_spec = find_spec(""nvidia"")
    if not module_spec:
        return

    nvidia_lib_root = Path(module_spec.submodule_search_locations[0])

    for submodule in required_submodules:
        for ver in cuda_versions:
            try:
                package_name = f""nvidia_{submodule}_cu{ver}""
                _ = distribution(package_name)

                lib_path = nvidia_lib_root / submodule / ""bin""
                os.add_dll_directory(str(lib_path))
                os.environ[""PATH""] = str(lib_path) + \
                    os.pathsep + os.environ[""PATH""]
                logging.debug(f""Added {lib_path} to PATH"")
            except PackageNotFoundError:
                logging.debug(f""{package_name} not found"")",Try to add the CUDA library paths to the system PATH.,Try to add the CUDA library paths to the system PATH.,"def try_add_cuda_lib_path():
    
    required_submodules = [""cuda_runtime"", ""cublas""]
    cuda_versions = [""11"", ""12""]

    module_spec = find_spec(""nvidia"")
    if not module_spec:
        return

    nvidia_lib_root = Path(module_spec.submodule_search_locations[0])

    for submodule in required_submodules:
        for ver in cuda_versions:
            try:
                package_name = f""nvidia_{submodule}_cu{ver}""
                _ = distribution(package_name)

                lib_path = nvidia_lib_root / submodule / ""bin""
                os.add_dll_directory(str(lib_path))
                os.environ[""PATH""] = str(lib_path) + \
                    os.pathsep + os.environ[""PATH""]
                logging.debug(f""Added {lib_path} to PATH"")
            except PackageNotFoundError:
                logging.debug(f""{package_name} not found"")",Try to add the CUDA library paths to the system PATH.,"def try_add_cuda_lib_path ( ) : required_submodules = [ ""cuda_runtime"" , ""cublas"" ] cuda_versions = [ ""11"" , ""12"" ] module_spec = find_spec ( ""nvidia"" ) if not module_spec : return nvidia_lib_root = Path ( module_spec . submodule_search_locations [ 0 ] ) for submodule in required_submodules : for ver in cuda_versions : try : package_name = f""nvidia_{submodule}_cu{ver}"" _ = distribution ( package_name ) lib_path = nvidia_lib_root / submodule / ""bin"" os . add_dll_directory ( str ( lib_path ) ) os . environ [ ""PATH"" ] = str ( lib_path ) + os . pathsep + os . environ [ ""PATH"" ] logging . debug ( f""Added {lib_path} to PATH"" ) except PackageNotFoundError : logging . debug ( f""{package_name} not found"" )",Try to add the CUDA library paths to the system PATH.
/ag2/autogen/_website/generate_mkdocs.py,format_page_entry,"def format_page_entry(page_loc: str, indent: str, keywords: dict[str, str], mkdocs_docs_dir: Path) -> str:
    """"""Format a single page entry as either a parenthesized path or a markdown link.""""""
    file_path_str = f""{page_loc}.md""
    title = format_title(file_path_str, keywords, mkdocs_docs_dir)
    return f""{indent}    - [{title}]({file_path_str})""","def format_page_entry(page_loc: str, indent: str, keywords: dict[str, str], mkdocs_docs_dir: Path) -> str:
    """"""Format a single page entry as either a parenthesized path or a markdown link.""""""
    file_path_str = f""{page_loc}.md""
    title = format_title(file_path_str, keywords, mkdocs_docs_dir)
    return f""{indent}    - [{title}]({file_path_str})""",Format a single page entry as either a parenthesized path or a markdown link.,Format a single page entry as either a parenthesized path or a markdown link.,"def format_page_entry(page_loc: str, indent: str, keywords: dict[str, str], mkdocs_docs_dir: Path) -> str:
    
    file_path_str = f""{page_loc}.md""
    title = format_title(file_path_str, keywords, mkdocs_docs_dir)
    return f""{indent}    - [{title}]({file_path_str})""",Format a single page entry as either a parenthesized path or a markdown link.,"def format_page_entry ( page_loc : str , indent : str , keywords : dict [ str , str ] , mkdocs_docs_dir : Path ) -> str : file_path_str = f""{page_loc}.md"" title = format_title ( file_path_str , keywords , mkdocs_docs_dir ) return f""{indent}    - [{title}]({file_path_str})""",Format a single page entry as either a parenthesized path or a markdown link.
/nv-ingest/src/util/gen_dataset.py,parse_size,"def parse_size(size):
    """"""Parse the size string with optional suffix (KB, MB) into bytes.""""""
    units = {""KB"": 1024, ""MB"": 1024**2, ""GB"": 1024**3}
    if size.isdigit():
        return int(size)
    unit = size[-2:].upper()
    number = size[:-2]
    if unit in units and number.isdigit():
        return int(number) * units[unit]
    raise ValueError(""Size must be in the format <number>[KB|MB|GB]."")","def parse_size(size):
    """"""Parse the size string with optional suffix (KB, MB) into bytes.""""""
    units = {""KB"": 1024, ""MB"": 1024**2, ""GB"": 1024**3}
    if size.isdigit():
        return int(size)
    unit = size[-2:].upper()
    number = size[:-2]
    if unit in units and number.isdigit():
        return int(number) * units[unit]
    raise ValueError(""Size must be in the format <number>[KB|MB|GB]."")","Parse the size string with optional suffix (KB, MB) into bytes.","Parse the size string with optional suffix (KB, MB) into bytes.","def parse_size(size):
    
    units = {""KB"": 1024, ""MB"": 1024**2, ""GB"": 1024**3}
    if size.isdigit():
        return int(size)
    unit = size[-2:].upper()
    number = size[:-2]
    if unit in units and number.isdigit():
        return int(number) * units[unit]
    raise ValueError(""Size must be in the format <number>[KB|MB|GB]."")","Parse the size string with optional suffix (KB, MB) into bytes.","def parse_size ( size ) : units = { ""KB"" : 1024 , ""MB"" : 1024 ** 2 , ""GB"" : 1024 ** 3 } if size . isdigit ( ) : return int ( size ) unit = size [ - 2 : ] . upper ( ) number = size [ : - 2 ] if unit in units and number . isdigit ( ) : return int ( number ) * units [ unit ] raise ValueError ( ""Size must be in the format <number>[KB|MB|GB]."" )","Parse the size string with optional suffix (KB, MB) into bytes."
/nv-ingest/src/nv_ingest/framework/orchestration/morpheus/util/pipeline/pipeline_runners.py,signal_handler,"def signal_handler(signum, frame):
            """"""Handle signals to ensure clean subprocess termination.""""""
            logger.info(f""Received signal {signum}. Terminating pipeline subprocess group..."")
            _terminate_subprocess(process)
            sys.exit(0)","def signal_handler(signum, frame):
            """"""Handle signals to ensure clean subprocess termination.""""""
            logger.info(f""Received signal {signum}. Terminating pipeline subprocess group..."")
            _terminate_subprocess(process)
            sys.exit(0)",Handle signals to ensure clean subprocess termination.,Handle signals to ensure clean subprocess termination.,"def signal_handler(signum, frame):
            
            logger.info(f""Received signal {signum}. Terminating pipeline subprocess group..."")
            _terminate_subprocess(process)
            sys.exit(0)",Handle signals to ensure clean subprocess termination.,"def signal_handler ( signum , frame ) : logger . info ( f""Received signal {signum}. Terminating pipeline subprocess group..."" ) _terminate_subprocess ( process ) sys . exit ( 0 )",Handle signals to ensure clean subprocess termination.
/local-deep-research/examples/optimization/run_multi_benchmark.py,print_optimization_results,"def print_optimization_results(params: Dict[str, Any], score: float):
    """"""Print optimization results in a nicely formatted way.""""""
    print(""\n"" + ""="" * 50)
    print("" OPTIMIZATION RESULTS "")
    print(""="" * 50)
    print(f""SCORE: {score:.4f}"")
    print(""\nBest Parameters:"")
    for param, value in params.items():
        print(f""  {param}: {value}"")
    print(""="" * 50 + ""\n"")","def print_optimization_results(params: Dict[str, Any], score: float):
    """"""Print optimization results in a nicely formatted way.""""""
    print(""\n"" + ""="" * 50)
    print("" OPTIMIZATION RESULTS "")
    print(""="" * 50)
    print(f""SCORE: {score:.4f}"")
    print(""\nBest Parameters:"")
    for param, value in params.items():
        print(f""  {param}: {value}"")
    print(""="" * 50 + ""\n"")",Print optimization results in a nicely formatted way.,Print optimization results in a nicely formatted way.,"def print_optimization_results(params: Dict[str, Any], score: float):
    
    print(""\n"" + ""="" * 50)
    print("" OPTIMIZATION RESULTS "")
    print(""="" * 50)
    print(f""SCORE: {score:.4f}"")
    print(""\nBest Parameters:"")
    for param, value in params.items():
        print(f""  {param}: {value}"")
    print(""="" * 50 + ""\n"")",Print optimization results in a nicely formatted way.,"def print_optimization_results ( params : Dict [ str , Any ] , score : float ) : print ( ""\n"" + ""="" * 50 ) print ( "" OPTIMIZATION RESULTS "" ) print ( ""="" * 50 ) print ( f""SCORE: {score:.4f}"" ) print ( ""\nBest Parameters:"" ) for param , value in params . items ( ) : print ( f""  {param}: {value}"" ) print ( ""="" * 50 + ""\n"" )",Print optimization results in a nicely formatted way.
/python-sdk/tests/server/fastmcp/test_integration.py,run_stateless_http_server,"def run_stateless_http_server(server_port: int) -> None:
    """"""Run the stateless StreamableHTTP server.""""""
    _, app = make_fastmcp_stateless_http_app()
    server = uvicorn.Server(
        config=uvicorn.Config(
            app=app, host=""127.0.0.1"", port=server_port, log_level=""error""
        )
    )
    print(f""Starting stateless StreamableHTTP server on port {server_port}"")
    server.run()","def run_stateless_http_server(server_port: int) -> None:
    """"""Run the stateless StreamableHTTP server.""""""
    _, app = make_fastmcp_stateless_http_app()
    server = uvicorn.Server(
        config=uvicorn.Config(
            app=app, host=""127.0.0.1"", port=server_port, log_level=""error""
        )
    )
    print(f""Starting stateless StreamableHTTP server on port {server_port}"")
    server.run()",Run the stateless StreamableHTTP server.,Run the stateless StreamableHTTP server.,"def run_stateless_http_server(server_port: int) -> None:
    
    _, app = make_fastmcp_stateless_http_app()
    server = uvicorn.Server(
        config=uvicorn.Config(
            app=app, host=""127.0.0.1"", port=server_port, log_level=""error""
        )
    )
    print(f""Starting stateless StreamableHTTP server on port {server_port}"")
    server.run()",Run the stateless StreamableHTTP server.,"def run_stateless_http_server ( server_port : int ) -> None : _ , app = make_fastmcp_stateless_http_app ( ) server = uvicorn . Server ( config = uvicorn . Config ( app = app , host = ""127.0.0.1"" , port = server_port , log_level = ""error"" ) ) print ( f""Starting stateless StreamableHTTP server on port {server_port}"" ) server . run ( )",Run the stateless StreamableHTTP server.
/alphafold3/src/alphafold3/data/templates.py,package_template_features,"def package_template_features(
    *,
    hit_features: Sequence[Mapping[str, Any]],
    include_ligand_features: bool,
) -> Mapping[str, Any]:
  """"""Stacks polymer features, adds empty and keeps ligand features unstacked.""""""

  features_to_include = set(_POLYMER_FEATURES)
  if include_ligand_features:
    features_to_include.update(_LIGAND_FEATURES)

  features = {
      feat: [single_hit_features[feat] for single_hit_features in hit_features]
      for feat in features_to_include
  }

  stacked_features = {}
  for k, v in features.items():
    if k in _POLYMER_FEATURES:
      v = np.stack(v, axis=0) if v else np.array([], dtype=_POLYMER_FEATURES[k])
    stacked_features[k] = v

  return stacked_features","def package_template_features(
    *,
    hit_features: Sequence[Mapping[str, Any]],
    include_ligand_features: bool,
) -> Mapping[str, Any]:
  """"""Stacks polymer features, adds empty and keeps ligand features unstacked.""""""

  features_to_include = set(_POLYMER_FEATURES)
  if include_ligand_features:
    features_to_include.update(_LIGAND_FEATURES)

  features = {
      feat: [single_hit_features[feat] for single_hit_features in hit_features]
      for feat in features_to_include
  }

  stacked_features = {}
  for k, v in features.items():
    if k in _POLYMER_FEATURES:
      v = np.stack(v, axis=0) if v else np.array([], dtype=_POLYMER_FEATURES[k])
    stacked_features[k] = v

  return stacked_features","Stacks polymer features, adds empty and keeps ligand features unstacked.","Stacks polymer features, adds empty and keeps ligand features unstacked.","def package_template_features(
    *,
    hit_features: Sequence[Mapping[str, Any]],
    include_ligand_features: bool,
) -> Mapping[str, Any]:
  

  features_to_include = set(_POLYMER_FEATURES)
  if include_ligand_features:
    features_to_include.update(_LIGAND_FEATURES)

  features = {
      feat: [single_hit_features[feat] for single_hit_features in hit_features]
      for feat in features_to_include
  }

  stacked_features = {}
  for k, v in features.items():
    if k in _POLYMER_FEATURES:
      v = np.stack(v, axis=0) if v else np.array([], dtype=_POLYMER_FEATURES[k])
    stacked_features[k] = v

  return stacked_features","Stacks polymer features, adds empty and keeps ligand features unstacked.","def package_template_features ( * , hit_features : Sequence [ Mapping [ str , Any ] ] , include_ligand_features : bool , ) -> Mapping [ str , Any ] : features_to_include = set ( _POLYMER_FEATURES ) if include_ligand_features : features_to_include . update ( _LIGAND_FEATURES ) features = { feat : [ single_hit_features [ feat ] for single_hit_features in hit_features ] for feat in features_to_include } stacked_features = { } for k , v in features . items ( ) : if k in _POLYMER_FEATURES : v = np . stack ( v , axis = 0 ) if v else np . array ( [ ] , dtype = _POLYMER_FEATURES [ k ] ) stacked_features [ k ] = v return stacked_features","Stacks polymer features, adds empty and keeps ligand features unstacked."
/adk-samples/python/agents/data-science/data_science/sub_agents/bigquery/chase_sql/sql_postprocessor/sql_translator.py,rewrite_schema_for_sqlglot,"def rewrite_schema_for_sqlglot(
        cls, schema: str | SQLGlotSchemaType | BirdSampleType
    ) -> SQLGlotSchemaType:
        """"""Rewrites the schema for use in SQLGlot.""""""
        schema_dict = None
        if schema:
            if isinstance(schema, str):
                schema = cls.extract_schema_from_ddls(schema)
                schema_dict = cls.format_schema(schema)
            elif _isinstance_sqlglot_schema_type(schema):
                schema_dict = schema
            elif _isinstance_bird_sample_type(schema):
                schema_dict = cls._get_schema_from_bird_sample(schema)
            elif _isinstance_ddl_schema_type(schema):
                schema_dict = cls.format_schema(schema)
            else:
                raise TypeError(f""Unsupported schema type: {type(schema)}"")
        return schema_dict","def rewrite_schema_for_sqlglot(
        cls, schema: str | SQLGlotSchemaType | BirdSampleType
    ) -> SQLGlotSchemaType:
        """"""Rewrites the schema for use in SQLGlot.""""""
        schema_dict = None
        if schema:
            if isinstance(schema, str):
                schema = cls.extract_schema_from_ddls(schema)
                schema_dict = cls.format_schema(schema)
            elif _isinstance_sqlglot_schema_type(schema):
                schema_dict = schema
            elif _isinstance_bird_sample_type(schema):
                schema_dict = cls._get_schema_from_bird_sample(schema)
            elif _isinstance_ddl_schema_type(schema):
                schema_dict = cls.format_schema(schema)
            else:
                raise TypeError(f""Unsupported schema type: {type(schema)}"")
        return schema_dict",Rewrites the schema for use in SQLGlot.,Rewrites the schema for use in SQLGlot.,"def rewrite_schema_for_sqlglot(
        cls, schema: str | SQLGlotSchemaType | BirdSampleType
    ) -> SQLGlotSchemaType:
        
        schema_dict = None
        if schema:
            if isinstance(schema, str):
                schema = cls.extract_schema_from_ddls(schema)
                schema_dict = cls.format_schema(schema)
            elif _isinstance_sqlglot_schema_type(schema):
                schema_dict = schema
            elif _isinstance_bird_sample_type(schema):
                schema_dict = cls._get_schema_from_bird_sample(schema)
            elif _isinstance_ddl_schema_type(schema):
                schema_dict = cls.format_schema(schema)
            else:
                raise TypeError(f""Unsupported schema type: {type(schema)}"")
        return schema_dict",Rewrites the schema for use in SQLGlot.,"def rewrite_schema_for_sqlglot ( cls , schema : str | SQLGlotSchemaType | BirdSampleType ) -> SQLGlotSchemaType : schema_dict = None if schema : if isinstance ( schema , str ) : schema = cls . extract_schema_from_ddls ( schema ) schema_dict = cls . format_schema ( schema ) elif _isinstance_sqlglot_schema_type ( schema ) : schema_dict = schema elif _isinstance_bird_sample_type ( schema ) : schema_dict = cls . _get_schema_from_bird_sample ( schema ) elif _isinstance_ddl_schema_type ( schema ) : schema_dict = cls . format_schema ( schema ) else : raise TypeError ( f""Unsupported schema type: {type(schema)}"" ) return schema_dict",Rewrites the schema for use in SQLGlot.
/NLWeb/code/testing/run_tests.py,get_config_defaults,"def get_config_defaults() -> Dict[str, Any]:
    """"""Get default values from config files.""""""
    defaults = {
        'site': 'all',
        'model': 'gpt-4o-mini',
        'generate_mode': 'list', 
        'retrieval_backend': CONFIG.preferred_retrieval_endpoint,
        'prev': []
    }
    
    # Try to get preferred model from LLM config
    if hasattr(CONFIG, 'preferred_llm_provider') and CONFIG.preferred_llm_provider:
        llm_provider = CONFIG.get_llm_provider()
        if llm_provider and llm_provider.models:
            # Use the 'low' model as default for testing
            defaults['model'] = llm_provider.models.low or defaults['model']
    
    return defaults","def get_config_defaults() -> Dict[str, Any]:
    """"""Get default values from config files.""""""
    defaults = {
        'site': 'all',
        'model': 'gpt-4o-mini',
        'generate_mode': 'list', 
        'retrieval_backend': CONFIG.preferred_retrieval_endpoint,
        'prev': []
    }
    
    # Try to get preferred model from LLM config
    if hasattr(CONFIG, 'preferred_llm_provider') and CONFIG.preferred_llm_provider:
        llm_provider = CONFIG.get_llm_provider()
        if llm_provider and llm_provider.models:
            # Use the 'low' model as default for testing
            defaults['model'] = llm_provider.models.low or defaults['model']
    
    return defaults",Get default values from config files.,Get default values from config files.,"def get_config_defaults() -> Dict[str, Any]:
    
    defaults = {
        'site': 'all',
        'model': 'gpt-4o-mini',
        'generate_mode': 'list', 
        'retrieval_backend': CONFIG.preferred_retrieval_endpoint,
        'prev': []
    }
    
    # Try to get preferred model from LLM config
    if hasattr(CONFIG, 'preferred_llm_provider') and CONFIG.preferred_llm_provider:
        llm_provider = CONFIG.get_llm_provider()
        if llm_provider and llm_provider.models:
            # Use the 'low' model as default for testing
            defaults['model'] = llm_provider.models.low or defaults['model']
    
    return defaults",Get default values from config files.,"def get_config_defaults ( ) -> Dict [ str , Any ] : defaults = { 'site' : 'all' , 'model' : 'gpt-4o-mini' , 'generate_mode' : 'list' , 'retrieval_backend' : CONFIG . preferred_retrieval_endpoint , 'prev' : [ ] } # Try to get preferred model from LLM config if hasattr ( CONFIG , 'preferred_llm_provider' ) and CONFIG . preferred_llm_provider : llm_provider = CONFIG . get_llm_provider ( ) if llm_provider and llm_provider . models : # Use the 'low' model as default for testing defaults [ 'model' ] = llm_provider . models . low or defaults [ 'model' ] return defaults",Get default values from config files.
/alphafold3/run_alphafold.py,write_fold_input_json,"def write_fold_input_json(
    fold_input: folding_input.Input,
    output_dir: os.PathLike[str] | str,
) -> None:
  """"""Writes the input JSON to the output directory.""""""
  os.makedirs(output_dir, exist_ok=True)
  path = os.path.join(output_dir, f'{fold_input.sanitised_name()}_data.json')
  print(f'Writing model input JSON to {path}')
  with open(path, 'wt') as f:
    f.write(fold_input.to_json())","def write_fold_input_json(
    fold_input: folding_input.Input,
    output_dir: os.PathLike[str] | str,
) -> None:
  """"""Writes the input JSON to the output directory.""""""
  os.makedirs(output_dir, exist_ok=True)
  path = os.path.join(output_dir, f'{fold_input.sanitised_name()}_data.json')
  print(f'Writing model input JSON to {path}')
  with open(path, 'wt') as f:
    f.write(fold_input.to_json())",Writes the input JSON to the output directory.,Writes the input JSON to the output directory.,"def write_fold_input_json(
    fold_input: folding_input.Input,
    output_dir: os.PathLike[str] | str,
) -> None:
  
  os.makedirs(output_dir, exist_ok=True)
  path = os.path.join(output_dir, f'{fold_input.sanitised_name()}_data.json')
  print(f'Writing model input JSON to {path}')
  with open(path, 'wt') as f:
    f.write(fold_input.to_json())",Writes the input JSON to the output directory.,"def write_fold_input_json ( fold_input : folding_input . Input , output_dir : os . PathLike [ str ] | str , ) -> None : os . makedirs ( output_dir , exist_ok = True ) path = os . path . join ( output_dir , f'{fold_input.sanitised_name()}_data.json' ) print ( f'Writing model input JSON to {path}' ) with open ( path , 'wt' ) as f : f . write ( fold_input . to_json ( ) )",Writes the input JSON to the output directory.
/podcastfy/podcastfy/text_to_speech.py,_generate_audio_segments,"def _generate_audio_segments(self, text: str, temp_dir: str) -> List[str]:
        """"""Generate audio segments for each Q&A pair.""""""
        qa_pairs = self.provider.split_qa(
            text, self.ending_message, self.provider.get_supported_tags()
        )
        audio_files = []
        provider_config = self._get_provider_config()

        for idx, (question, answer) in enumerate(qa_pairs, 1):
            for speaker_type, content in [(""question"", question), (""answer"", answer)]:
                temp_file = os.path.join(
                    temp_dir, f""{idx}_{speaker_type}.{self.audio_format}""
                )
                voice = provider_config.get(""default_voices"", {}).get(speaker_type)
                model = provider_config.get(""model"")

                audio_data = self.provider.generate_audio(content, voice, model)
                with open(temp_file, ""wb"") as f:
                    f.write(audio_data)
                audio_files.append(temp_file)

        return audio_files","def _generate_audio_segments(self, text: str, temp_dir: str) -> List[str]:
        """"""Generate audio segments for each Q&A pair.""""""
        qa_pairs = self.provider.split_qa(
            text, self.ending_message, self.provider.get_supported_tags()
        )
        audio_files = []
        provider_config = self._get_provider_config()

        for idx, (question, answer) in enumerate(qa_pairs, 1):
            for speaker_type, content in [(""question"", question), (""answer"", answer)]:
                temp_file = os.path.join(
                    temp_dir, f""{idx}_{speaker_type}.{self.audio_format}""
                )
                voice = provider_config.get(""default_voices"", {}).get(speaker_type)
                model = provider_config.get(""model"")

                audio_data = self.provider.generate_audio(content, voice, model)
                with open(temp_file, ""wb"") as f:
                    f.write(audio_data)
                audio_files.append(temp_file)

        return audio_files",Generate audio segments for each Q&A pair.,Generate audio segments for each Q&A pair.,"def _generate_audio_segments(self, text: str, temp_dir: str) -> List[str]:
        
        qa_pairs = self.provider.split_qa(
            text, self.ending_message, self.provider.get_supported_tags()
        )
        audio_files = []
        provider_config = self._get_provider_config()

        for idx, (question, answer) in enumerate(qa_pairs, 1):
            for speaker_type, content in [(""question"", question), (""answer"", answer)]:
                temp_file = os.path.join(
                    temp_dir, f""{idx}_{speaker_type}.{self.audio_format}""
                )
                voice = provider_config.get(""default_voices"", {}).get(speaker_type)
                model = provider_config.get(""model"")

                audio_data = self.provider.generate_audio(content, voice, model)
                with open(temp_file, ""wb"") as f:
                    f.write(audio_data)
                audio_files.append(temp_file)

        return audio_files",Generate audio segments for each Q&A pair.,"def _generate_audio_segments ( self , text : str , temp_dir : str ) -> List [ str ] : qa_pairs = self . provider . split_qa ( text , self . ending_message , self . provider . get_supported_tags ( ) ) audio_files = [ ] provider_config = self . _get_provider_config ( ) for idx , ( question , answer ) in enumerate ( qa_pairs , 1 ) : for speaker_type , content in [ ( ""question"" , question ) , ( ""answer"" , answer ) ] : temp_file = os . path . join ( temp_dir , f""{idx}_{speaker_type}.{self.audio_format}"" ) voice = provider_config . get ( ""default_voices"" , { } ) . get ( speaker_type ) model = provider_config . get ( ""model"" ) audio_data = self . provider . generate_audio ( content , voice , model ) with open ( temp_file , ""wb"" ) as f : f . write ( audio_data ) audio_files . append ( temp_file ) return audio_files",Generate audio segments for each Q&A pair.
/ag2/autogen/agents/experimental/document_agent/document_agent.py,format,"def format(self) -> str:
        """"""Format the DocumentTask as a string for the TaskManager to work with.""""""
        if len(self.ingestions) == 0 and len(self.queries) == 0:
            return ""There were no ingestion or query tasks detected.""

        instructions = ""Tasks:\n\n""
        order = 1

        if len(self.ingestions) > 0:
            instructions += ""Ingestions:\n""
            for ingestion in self.ingestions:
                instructions += f""{order}: {ingestion.path_or_url}\n""
                order += 1

            instructions += ""\n""

        if len(self.queries) > 0:
            instructions += ""Queries:\n""
            for query in self.queries:
                instructions += f""{order}: {query.query}\n""
                order += 1

        return instructions","def format(self) -> str:
        """"""Format the DocumentTask as a string for the TaskManager to work with.""""""
        if len(self.ingestions) == 0 and len(self.queries) == 0:
            return ""There were no ingestion or query tasks detected.""

        instructions = ""Tasks:\n\n""
        order = 1

        if len(self.ingestions) > 0:
            instructions += ""Ingestions:\n""
            for ingestion in self.ingestions:
                instructions += f""{order}: {ingestion.path_or_url}\n""
                order += 1

            instructions += ""\n""

        if len(self.queries) > 0:
            instructions += ""Queries:\n""
            for query in self.queries:
                instructions += f""{order}: {query.query}\n""
                order += 1

        return instructions",Format the DocumentTask as a string for the TaskManager to work with.,Format the DocumentTask as a string for the TaskManager to work with.,"def format(self) -> str:
        
        if len(self.ingestions) == 0 and len(self.queries) == 0:
            return ""There were no ingestion or query tasks detected.""

        instructions = ""Tasks:\n\n""
        order = 1

        if len(self.ingestions) > 0:
            instructions += ""Ingestions:\n""
            for ingestion in self.ingestions:
                instructions += f""{order}: {ingestion.path_or_url}\n""
                order += 1

            instructions += ""\n""

        if len(self.queries) > 0:
            instructions += ""Queries:\n""
            for query in self.queries:
                instructions += f""{order}: {query.query}\n""
                order += 1

        return instructions",Format the DocumentTask as a string for the TaskManager to work with.,"def format ( self ) -> str : if len ( self . ingestions ) == 0 and len ( self . queries ) == 0 : return ""There were no ingestion or query tasks detected."" instructions = ""Tasks:\n\n"" order = 1 if len ( self . ingestions ) > 0 : instructions += ""Ingestions:\n"" for ingestion in self . ingestions : instructions += f""{order}: {ingestion.path_or_url}\n"" order += 1 instructions += ""\n"" if len ( self . queries ) > 0 : instructions += ""Queries:\n"" for query in self . queries : instructions += f""{order}: {query.query}\n"" order += 1 return instructions",Format the DocumentTask as a string for the TaskManager to work with.
/ai-hedge-fund/src/utils/display.py,sort_agent_signals,"def sort_agent_signals(signals):
    """"""Sort agent signals in a consistent order.""""""
    # Create order mapping from ANALYST_ORDER
    analyst_order = {display: idx for idx, (display, _) in enumerate(ANALYST_ORDER)}
    analyst_order[""Risk Management""] = len(ANALYST_ORDER)  # Add Risk Management at the end

    return sorted(signals, key=lambda x: analyst_order.get(x[0], 999))","def sort_agent_signals(signals):
    """"""Sort agent signals in a consistent order.""""""
    # Create order mapping from ANALYST_ORDER
    analyst_order = {display: idx for idx, (display, _) in enumerate(ANALYST_ORDER)}
    analyst_order[""Risk Management""] = len(ANALYST_ORDER)  # Add Risk Management at the end

    return sorted(signals, key=lambda x: analyst_order.get(x[0], 999))",Sort agent signals in a consistent order.,Sort agent signals in a consistent order.,"def sort_agent_signals(signals):
    
    # Create order mapping from ANALYST_ORDER
    analyst_order = {display: idx for idx, (display, _) in enumerate(ANALYST_ORDER)}
    analyst_order[""Risk Management""] = len(ANALYST_ORDER)  # Add Risk Management at the end

    return sorted(signals, key=lambda x: analyst_order.get(x[0], 999))",Sort agent signals in a consistent order.,"def sort_agent_signals ( signals ) : # Create order mapping from ANALYST_ORDER analyst_order = { display : idx for idx , ( display , _ ) in enumerate ( ANALYST_ORDER ) } analyst_order [ ""Risk Management"" ] = len ( ANALYST_ORDER ) # Add Risk Management at the end return sorted ( signals , key = lambda x : analyst_order . get ( x [ 0 ] , 999 ) )",Sort agent signals in a consistent order.
/cua/libs/pylume/pylume/pylume.py,_log_debug,"def _log_debug(self, message: str, **kwargs) -> None:
        """"""Log debug information if debug mode is enabled.""""""
        if self.server.debug:
            print(f""DEBUG: {message}"")
            if kwargs:
                print(json.dumps(kwargs, indent=2))","def _log_debug(self, message: str, **kwargs) -> None:
        """"""Log debug information if debug mode is enabled.""""""
        if self.server.debug:
            print(f""DEBUG: {message}"")
            if kwargs:
                print(json.dumps(kwargs, indent=2))",Log debug information if debug mode is enabled.,Log debug information if debug mode is enabled.,"def _log_debug(self, message: str, **kwargs) -> None:
        
        if self.server.debug:
            print(f""DEBUG: {message}"")
            if kwargs:
                print(json.dumps(kwargs, indent=2))",Log debug information if debug mode is enabled.,"def _log_debug ( self , message : str , ** kwargs ) -> None : if self . server . debug : print ( f""DEBUG: {message}"" ) if kwargs : print ( json . dumps ( kwargs , indent = 2 ) )",Log debug information if debug mode is enabled.
/nv-ingest/src/util/image_model_validation/util.py,load_and_preprocess_image,"def load_and_preprocess_image(image_path, target_img_size):
    """"""
    Load and preprocess an image from the specified path, resizing and padding it to the target size.
    """"""
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f""Failed to load image from path: {image_path}"")

    # Resize and pad the image to the target size
    resized_image = resize_image(image, target_img_size)

    # Normalize the image (assuming model expects normalized input)
    normalized_image = resized_image.astype(np.float32) / 255.0

    # Expand dimensions to match the model's input shape
    input_data = np.expand_dims(normalized_image, axis=0)  # Add batch dimension
    return resized_image, input_data","def load_and_preprocess_image(image_path, target_img_size):
    """"""
    Load and preprocess an image from the specified path, resizing and padding it to the target size.
    """"""
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f""Failed to load image from path: {image_path}"")

    # Resize and pad the image to the target size
    resized_image = resize_image(image, target_img_size)

    # Normalize the image (assuming model expects normalized input)
    normalized_image = resized_image.astype(np.float32) / 255.0

    # Expand dimensions to match the model's input shape
    input_data = np.expand_dims(normalized_image, axis=0)  # Add batch dimension
    return resized_image, input_data","Load and preprocess an image from the specified path, resizing and padding it to the target size.","Load and preprocess an image from the specified path, resizing and padding it to the target size.","def load_and_preprocess_image(image_path, target_img_size):
    
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f""Failed to load image from path: {image_path}"")

    # Resize and pad the image to the target size
    resized_image = resize_image(image, target_img_size)

    # Normalize the image (assuming model expects normalized input)
    normalized_image = resized_image.astype(np.float32) / 255.0

    # Expand dimensions to match the model's input shape
    input_data = np.expand_dims(normalized_image, axis=0)  # Add batch dimension
    return resized_image, input_data","Load and preprocess an image from the specified path, resizing and padding it to the target size.","def load_and_preprocess_image ( image_path , target_img_size ) : image = cv2 . imread ( image_path ) if image is None : raise ValueError ( f""Failed to load image from path: {image_path}"" ) # Resize and pad the image to the target size resized_image = resize_image ( image , target_img_size ) # Normalize the image (assuming model expects normalized input) normalized_image = resized_image . astype ( np . float32 ) / 255.0 # Expand dimensions to match the model's input shape input_data = np . expand_dims ( normalized_image , axis = 0 ) # Add batch dimension return resized_image , input_data","Load and preprocess an image from the specified path, resizing and padding it to the target size."
/optillm/optillm/plugins/deepthink/uncertainty_cot.py,_generate_greedy_sample,"def _generate_greedy_sample(self, prompt: str) -> str:
        """"""Generate a single greedy sample with temperature=0.""""""
        logger.debug(""Generating greedy sample"")
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            max_tokens=self.max_tokens,
            temperature=0.0  # Greedy decoding
        )
        
        self.completion_tokens += response.usage.completion_tokens
        
        return response.choices[0].message.content.strip()","def _generate_greedy_sample(self, prompt: str) -> str:
        """"""Generate a single greedy sample with temperature=0.""""""
        logger.debug(""Generating greedy sample"")
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            max_tokens=self.max_tokens,
            temperature=0.0  # Greedy decoding
        )
        
        self.completion_tokens += response.usage.completion_tokens
        
        return response.choices[0].message.content.strip()",Generate a single greedy sample with temperature=0.,Generate a single greedy sample with temperature=0.,"def _generate_greedy_sample(self, prompt: str) -> str:
        
        logger.debug(""Generating greedy sample"")
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{""role"": ""user"", ""content"": prompt}],
            max_tokens=self.max_tokens,
            temperature=0.0  # Greedy decoding
        )
        
        self.completion_tokens += response.usage.completion_tokens
        
        return response.choices[0].message.content.strip()",Generate a single greedy sample with temperature=0.,"def _generate_greedy_sample ( self , prompt : str ) -> str : logger . debug ( ""Generating greedy sample"" ) response = self . client . chat . completions . create ( model = self . model , messages = [ { ""role"" : ""user"" , ""content"" : prompt } ] , max_tokens = self . max_tokens , temperature = 0.0 # Greedy decoding ) self . completion_tokens += response . usage . completion_tokens return response . choices [ 0 ] . message . content . strip ( )",Generate a single greedy sample with temperature=0.
/adk-python/src/google/adk/tools/apihub_tool/apihub_toolset.py,_prepare_toolset,"def _prepare_toolset(self) -> None:
    """"""Fetches the spec from API Hub and generates the toolset.""""""
    # For each API, get the first version and the first spec of that version.
    spec_str = self._apihub_client.get_spec_content(self._apihub_resource_name)
    spec_dict = yaml.safe_load(spec_str)
    if not spec_dict:
      return

    self.name = self.name or _to_snake_case(
        spec_dict.get('info', {}).get('title', 'unnamed')
    )
    self.description = self.description or spec_dict.get('info', {}).get(
        'description', ''
    )
    self._openapi_toolset = OpenAPIToolset(
        spec_dict=spec_dict,
        auth_credential=self._auth_credential,
        auth_scheme=self._auth_scheme,
        tool_filter=self.tool_filter,
    )","def _prepare_toolset(self) -> None:
    """"""Fetches the spec from API Hub and generates the toolset.""""""
    # For each API, get the first version and the first spec of that version.
    spec_str = self._apihub_client.get_spec_content(self._apihub_resource_name)
    spec_dict = yaml.safe_load(spec_str)
    if not spec_dict:
      return

    self.name = self.name or _to_snake_case(
        spec_dict.get('info', {}).get('title', 'unnamed')
    )
    self.description = self.description or spec_dict.get('info', {}).get(
        'description', ''
    )
    self._openapi_toolset = OpenAPIToolset(
        spec_dict=spec_dict,
        auth_credential=self._auth_credential,
        auth_scheme=self._auth_scheme,
        tool_filter=self.tool_filter,
    )",Fetches the spec from API Hub and generates the toolset.,Fetches the spec from API Hub and generates the toolset.,"def _prepare_toolset(self) -> None:
    
    # For each API, get the first version and the first spec of that version.
    spec_str = self._apihub_client.get_spec_content(self._apihub_resource_name)
    spec_dict = yaml.safe_load(spec_str)
    if not spec_dict:
      return

    self.name = self.name or _to_snake_case(
        spec_dict.get('info', {}).get('title', 'unnamed')
    )
    self.description = self.description or spec_dict.get('info', {}).get(
        'description', ''
    )
    self._openapi_toolset = OpenAPIToolset(
        spec_dict=spec_dict,
        auth_credential=self._auth_credential,
        auth_scheme=self._auth_scheme,
        tool_filter=self.tool_filter,
    )",Fetches the spec from API Hub and generates the toolset.,"def _prepare_toolset ( self ) -> None : # For each API, get the first version and the first spec of that version. spec_str = self . _apihub_client . get_spec_content ( self . _apihub_resource_name ) spec_dict = yaml . safe_load ( spec_str ) if not spec_dict : return self . name = self . name or _to_snake_case ( spec_dict . get ( 'info' , { } ) . get ( 'title' , 'unnamed' ) ) self . description = self . description or spec_dict . get ( 'info' , { } ) . get ( 'description' , '' ) self . _openapi_toolset = OpenAPIToolset ( spec_dict = spec_dict , auth_credential = self . _auth_credential , auth_scheme = self . _auth_scheme , tool_filter = self . tool_filter , )",Fetches the spec from API Hub and generates the toolset.
/agenticSeek/llm_server/sources/cache.py,add_message_pair,"def add_message_pair(self, user_message: str, assistant_message: str):
        """"""Add a user/assistant pair to the cache if not present.""""""
        if not any(entry[""user""] == user_message for entry in self.cache):
            self.cache.append({""user"": user_message, ""assistant"": assistant_message})
            self._save()","def add_message_pair(self, user_message: str, assistant_message: str):
        """"""Add a user/assistant pair to the cache if not present.""""""
        if not any(entry[""user""] == user_message for entry in self.cache):
            self.cache.append({""user"": user_message, ""assistant"": assistant_message})
            self._save()",Add a user/assistant pair to the cache if not present.,Add a user/assistant pair to the cache if not present.,"def add_message_pair(self, user_message: str, assistant_message: str):
        
        if not any(entry[""user""] == user_message for entry in self.cache):
            self.cache.append({""user"": user_message, ""assistant"": assistant_message})
            self._save()",Add a user/assistant pair to the cache if not present.,"def add_message_pair ( self , user_message : str , assistant_message : str ) : if not any ( entry [ ""user"" ] == user_message for entry in self . cache ) : self . cache . append ( { ""user"" : user_message , ""assistant"" : assistant_message } ) self . _save ( )",Add a user/assistant pair to the cache if not present.
/NLWeb/code/llm/snowflake.py,clean_response,"def clean_response(cls, content: str) -> Dict[str, Any]:
        """"""
        Strip markdown fences and extract the first JSON object.
        """"""
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            return {}
        return json.loads(match.group(1))","def clean_response(cls, content: str) -> Dict[str, Any]:
        """"""
        Strip markdown fences and extract the first JSON object.
        """"""
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            return {}
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,Strip markdown fences and extract the first JSON object.,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            return {}
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,"def clean_response ( cls , content : str ) -> Dict [ str , Any ] : cleaned = re . sub ( r""```(?:json)?\s*"" , """" , content ) . strip ( ) match = re . search ( r""(\{.*\})"" , cleaned , re . S ) if not match : logger . error ( ""Failed to parse JSON from content: %r"" , content ) return { } return json . loads ( match . group ( 1 ) )",Strip markdown fences and extract the first JSON object.
/local-deep-research/src/local_deep_research/web_search_engines/engines/search_engine_semantic_scholar.py,_create_session,"def _create_session(self) -> requests.Session:
        """"""Create and configure a requests session with retry capabilities""""""
        session = requests.Session()

        # Configure automatic retries with exponential backoff
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=self.retry_backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods={""HEAD"", ""GET"", ""POST"", ""OPTIONS""},
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount(""https://"", adapter)

        # Set up headers
        headers = {""Accept"": ""application/json""}
        if self.api_key:
            headers[""x-api-key""] = self.api_key

        session.headers.update(headers)

        return session","def _create_session(self) -> requests.Session:
        """"""Create and configure a requests session with retry capabilities""""""
        session = requests.Session()

        # Configure automatic retries with exponential backoff
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=self.retry_backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods={""HEAD"", ""GET"", ""POST"", ""OPTIONS""},
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)

        # Set up headers
        headers = {""Accept"": ""application/json""}
        if self.api_key:
            headers[""x-api-key""] = self.api_key

        session.headers.update(headers)

        return session",Create and configure a requests session with retry capabilities,Create and configure a requests session with retry capabilities,"def _create_session(self) -> requests.Session:
        
        session = requests.Session()

        # Configure automatic retries with exponential backoff
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=self.retry_backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods={""HEAD"", ""GET"", ""POST"", ""OPTIONS""},
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)

        # Set up headers
        headers = {""Accept"": ""application/json""}
        if self.api_key:
            headers[""x-api-key""] = self.api_key

        session.headers.update(headers)

        return session",Create and configure a requests session with retry capabilities,"def _create_session ( self ) -> requests . Session : session = requests . Session ( ) # Configure automatic retries with exponential backoff retry_strategy = Retry ( total = self . max_retries , backoff_factor = self . retry_backoff_factor , status_forcelist = [ 429 , 500 , 502 , 503 , 504 ] , allowed_methods = { ""HEAD"" , ""GET"" , ""POST"" , ""OPTIONS"" } , ) adapter = HTTPAdapter ( max_retries = retry_strategy ) # Set up headers headers = { ""Accept"" : ""application/json"" } if self . api_key : headers [ ""x-api-key"" ] = self . api_key session . headers . update ( headers ) return session",Create and configure a requests session with retry capabilities
/ag2/autogen/tools/experimental/messageplatform/discord/discord.py,_snowflake_to_iso,"def _snowflake_to_iso(snowflake: str) -> str:
        """"""Convert a Discord snowflake ID to ISO timestamp string.""""""
        if not DiscordRetrieveTool._is_snowflake(snowflake):
            raise ValueError(f""Invalid snowflake ID: {snowflake}"")

        # Discord epoch (2015-01-01)
        discord_epoch = 1420070400000

        # Convert ID to int and shift right 22 bits to get timestamp
        timestamp_ms = (int(snowflake) >> 22) + discord_epoch

        # Convert to datetime and format as ISO string
        dt = datetime.fromtimestamp(timestamp_ms / 1000.0, tz=timezone.utc)
        return dt.isoformat()","def _snowflake_to_iso(snowflake: str) -> str:
        """"""Convert a Discord snowflake ID to ISO timestamp string.""""""
        if not DiscordRetrieveTool._is_snowflake(snowflake):
            raise ValueError(f""Invalid snowflake ID: {snowflake}"")

        # Discord epoch (2015-01-01)
        discord_epoch = 1420070400000

        # Convert ID to int and shift right 22 bits to get timestamp
        timestamp_ms = (int(snowflake) >> 22) + discord_epoch

        # Convert to datetime and format as ISO string
        dt = datetime.fromtimestamp(timestamp_ms / 1000.0, tz=timezone.utc)
        return dt.isoformat()",Convert a Discord snowflake ID to ISO timestamp string.,Convert a Discord snowflake ID to ISO timestamp string.,"def _snowflake_to_iso(snowflake: str) -> str:
        
        if not DiscordRetrieveTool._is_snowflake(snowflake):
            raise ValueError(f""Invalid snowflake ID: {snowflake}"")

        # Discord epoch (2015-01-01)
        discord_epoch = 1420070400000

        # Convert ID to int and shift right 22 bits to get timestamp
        timestamp_ms = (int(snowflake) >> 22) + discord_epoch

        # Convert to datetime and format as ISO string
        dt = datetime.fromtimestamp(timestamp_ms / 1000.0, tz=timezone.utc)
        return dt.isoformat()",Convert a Discord snowflake ID to ISO timestamp string.,"def _snowflake_to_iso ( snowflake : str ) -> str : if not DiscordRetrieveTool . _is_snowflake ( snowflake ) : raise ValueError ( f""Invalid snowflake ID: {snowflake}"" ) # Discord epoch (2015-01-01) discord_epoch = 1420070400000 # Convert ID to int and shift right 22 bits to get timestamp timestamp_ms = ( int ( snowflake ) >> 22 ) + discord_epoch # Convert to datetime and format as ISO string dt = datetime . fromtimestamp ( timestamp_ms / 1000.0 , tz = timezone . utc ) return dt . isoformat ( )",Convert a Discord snowflake ID to ISO timestamp string.
/airweave/backend/tests/e2e/smoke/test_public_api.py,wait_for_health,"def wait_for_health(url: str, timeout: int = 300) -> bool:
    """"""Wait for service to be healthy.""""""
    print(f""Waiting for {url} to be healthy..."")
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            response = requests.get(f""{url}/health"", timeout=5)
            if response.status_code == 200:
                print(""✓ Service is healthy"")
                return True
        except requests.exceptions.RequestException:
            pass

        time.sleep(2)
        print(""."", end="""", flush=True)

    print(""\n✗ Service health check timed out"")
    return False","def wait_for_health(url: str, timeout: int = 300) -> bool:
    """"""Wait for service to be healthy.""""""
    print(f""Waiting for {url} to be healthy..."")
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            response = requests.get(f""{url}/health"", timeout=5)
            if response.status_code == 200:
                print(""✓ Service is healthy"")
                return True
        except requests.exceptions.RequestException:
            pass

        time.sleep(2)
        print(""."", end="""", flush=True)

    print(""\n✗ Service health check timed out"")
    return False",Wait for service to be healthy.,Wait for service to be healthy.,"def wait_for_health(url: str, timeout: int = 300) -> bool:
    
    print(f""Waiting for {url} to be healthy..."")
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            response = requests.get(f""{url}/health"", timeout=5)
            if response.status_code == 200:
                print(""✓ Service is healthy"")
                return True
        except requests.exceptions.RequestException:
            pass

        time.sleep(2)
        print(""."", end="""", flush=True)

    print(""\n✗ Service health check timed out"")
    return False",Wait for service to be healthy.,"def wait_for_health ( url : str , timeout : int = 300 ) -> bool : print ( f""Waiting for {url} to be healthy..."" ) start_time = time . time ( ) while time . time ( ) - start_time < timeout : try : response = requests . get ( f""{url}/health"" , timeout = 5 ) if response . status_code == 200 : print ( ""✓ Service is healthy"" ) return True except requests . exceptions . RequestException : pass time . sleep ( 2 ) print ( ""."" , end = """" , flush = True ) print ( ""\n✗ Service health check timed out"" ) return False",Wait for service to be healthy.
/adk-samples/python/agents/academic-research/deployment/deploy.py,create,"def create() -> None:
    """"""Creates an agent engine for Academic Research.""""""
    adk_app = AdkApp(agent=root_agent, enable_tracing=True)

    remote_agent = agent_engines.create(
        adk_app,
        display_name=root_agent.name,
        requirements=[
            ""google-adk (>=0.0.2)"",
            ""google-cloud-aiplatform[agent_engines] (>=1.91.0,!=1.92.0)"",
            ""google-genai (>=1.5.0,<2.0.0)"",
            ""pydantic (>=2.10.6,<3.0.0)"",
            ""absl-py (>=2.2.1,<3.0.0)"",
        ],
        #        extra_packages=[""""],
    )
    print(f""Created remote agent: {remote_agent.resource_name}"")","def create() -> None:
    """"""Creates an agent engine for Academic Research.""""""
    adk_app = AdkApp(agent=root_agent, enable_tracing=True)

    remote_agent = agent_engines.create(
        adk_app,
        display_name=root_agent.name,
        requirements=[
            ""google-adk (>=0.0.2)"",
            ""google-cloud-aiplatform[agent_engines] (>=1.91.0,!=1.92.0)"",
            ""google-genai (>=1.5.0,<2.0.0)"",
            ""pydantic (>=2.10.6,<3.0.0)"",
            ""absl-py (>=2.2.1,<3.0.0)"",
        ],
        #        extra_packages=[""""],
    )
    print(f""Created remote agent: {remote_agent.resource_name}"")",Creates an agent engine for Academic Research.,Creates an agent engine for Academic Research.,"def create() -> None:
    
    adk_app = AdkApp(agent=root_agent, enable_tracing=True)

    remote_agent = agent_engines.create(
        adk_app,
        display_name=root_agent.name,
        requirements=[
            ""google-adk (>=0.0.2)"",
            ""google-cloud-aiplatform[agent_engines] (>=1.91.0,!=1.92.0)"",
            ""google-genai (>=1.5.0,<2.0.0)"",
            ""pydantic (>=2.10.6,<3.0.0)"",
            ""absl-py (>=2.2.1,<3.0.0)"",
        ],
        #        extra_packages=[""""],
    )
    print(f""Created remote agent: {remote_agent.resource_name}"")",Creates an agent engine for Academic Research.,"def create ( ) -> None : adk_app = AdkApp ( agent = root_agent , enable_tracing = True ) remote_agent = agent_engines . create ( adk_app , display_name = root_agent . name , requirements = [ ""google-adk (>=0.0.2)"" , ""google-cloud-aiplatform[agent_engines] (>=1.91.0,!=1.92.0)"" , ""google-genai (>=1.5.0,<2.0.0)"" , ""pydantic (>=2.10.6,<3.0.0)"" , ""absl-py (>=2.2.1,<3.0.0)"" , ] , #        extra_packages=[""""], ) print ( f""Created remote agent: {remote_agent.resource_name}"" )",Creates an agent engine for Academic Research.
/Scrapling/scrapling/engines/toolbelt/custom.py,configure,"def configure(cls, **kwargs):
        """"""Set multiple arguments for the parser at once globally

        :param kwargs: The keywords can be any arguments of the following: huge_tree, keep_comments, keep_cdata, auto_match, storage, storage_args, automatch_domain
        """"""
        for key, value in kwargs.items():
            key = key.strip().lower()
            if hasattr(cls, key):
                if key in cls.parser_keywords:
                    setattr(cls, key, value)
                else:
                    # Yup, no fun allowed LOL
                    raise AttributeError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')
            else:
                raise ValueError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')

        if not kwargs:
            raise AttributeError(f'You must pass a keyword to configure, current keywords: {cls.parser_keywords}?')","def configure(cls, **kwargs):
        """"""Set multiple arguments for the parser at once globally

        :param kwargs: The keywords can be any arguments of the following: huge_tree, keep_comments, keep_cdata, auto_match, storage, storage_args, automatch_domain
        """"""
        for key, value in kwargs.items():
            key = key.strip().lower()
            if hasattr(cls, key):
                if key in cls.parser_keywords:
                    setattr(cls, key, value)
                else:
                    # Yup, no fun allowed LOL
                    raise AttributeError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')
            else:
                raise ValueError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')

        if not kwargs:
            raise AttributeError(f'You must pass a keyword to configure, current keywords: {cls.parser_keywords}?')","Set multiple arguments for the parser at once globally

:param kwargs: The keywords can be any arguments of the following: huge_tree, keep_comments, keep_cdata, auto_match, storage, storage_args, automatch_domain","Set multiple arguments for the parser at once globally :param kwargs: The keywords can be any arguments of the following: huge_tree, keep_comments, keep_cdata, auto_match, storage, storage_args, automatch_domain","def configure(cls, **kwargs):
        
        for key, value in kwargs.items():
            key = key.strip().lower()
            if hasattr(cls, key):
                if key in cls.parser_keywords:
                    setattr(cls, key, value)
                else:
                    # Yup, no fun allowed LOL
                    raise AttributeError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')
            else:
                raise ValueError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')

        if not kwargs:
            raise AttributeError(f'You must pass a keyword to configure, current keywords: {cls.parser_keywords}?')","Set multiple arguments for the parser at once globally :param kwargs: The keywords can be any arguments of the following: huge_tree, keep_comments, keep_cdata, auto_match, storage, storage_args, automatch_domain","def configure ( cls , ** kwargs ) : for key , value in kwargs . items ( ) : key = key . strip ( ) . lower ( ) if hasattr ( cls , key ) : if key in cls . parser_keywords : setattr ( cls , key , value ) else : # Yup, no fun allowed LOL raise AttributeError ( f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?' ) else : raise ValueError ( f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?' ) if not kwargs : raise AttributeError ( f'You must pass a keyword to configure, current keywords: {cls.parser_keywords}?' )","Set multiple arguments for the parser at once globally :param kwargs: The keywords can be any arguments of the following: huge_tree, keep_comments, keep_cdata, auto_match, storage, storage_args, automatch_domain"
/TinyZero/verl/workers/actor/dp_actor.py,__init__,"def __init__(
        self,
        config,
        actor_module: nn.Module,
        actor_optimizer: torch.optim.Optimizer = None,
    ):
        """"""When optimizer is None, it is Reference Policy""""""
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer
        self.use_remove_padding = self.config.get('use_remove_padding', False)
        print(f'Actor use_remove_padding={self.use_remove_padding}')
        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = torch.compile(verl_F.entropy_from_logits, dynamic=True)","def __init__(
        self,
        config,
        actor_module: nn.Module,
        actor_optimizer: torch.optim.Optimizer = None,
    ):
        """"""When optimizer is None, it is Reference Policy""""""
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer
        self.use_remove_padding = self.config.get('use_remove_padding', False)
        print(f'Actor use_remove_padding={self.use_remove_padding}')
        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = torch.compile(verl_F.entropy_from_logits, dynamic=True)","When optimizer is None, it is Reference Policy","When optimizer is None, it is Reference Policy","def __init__(
        self,
        config,
        actor_module: nn.Module,
        actor_optimizer: torch.optim.Optimizer = None,
    ):
        
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer
        self.use_remove_padding = self.config.get('use_remove_padding', False)
        print(f'Actor use_remove_padding={self.use_remove_padding}')
        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = torch.compile(verl_F.entropy_from_logits, dynamic=True)","When optimizer is None, it is Reference Policy","def __init__ ( self , config , actor_module : nn . Module , actor_optimizer : torch . optim . Optimizer = None , ) : super ( ) . __init__ ( config ) self . actor_module = actor_module self . actor_optimizer = actor_optimizer self . use_remove_padding = self . config . get ( 'use_remove_padding' , False ) print ( f'Actor use_remove_padding={self.use_remove_padding}' ) self . ulysses_sequence_parallel_size = self . config . ulysses_sequence_parallel_size self . use_ulysses_sp = self . ulysses_sequence_parallel_size > 1 self . compute_entropy_from_logits = torch . compile ( verl_F . entropy_from_logits , dynamic = True )","When optimizer is None, it is Reference Policy"
/KAG/kag/common/registry/registrable.py,list_available,"def list_available(cls) -> List[str]:
        """"""List default first if it exists""""""
        keys = list(Registrable._registry[cls].keys())
        default = cls.default_implementation

        if default is None:
            return keys
        elif default not in keys:
            raise ConfigurationError(
                f""Default implementation {default} is not registered""
            )
        else:
            return [default] + [k for k in keys if k != default]","def list_available(cls) -> List[str]:
        """"""List default first if it exists""""""
        keys = list(Registrable._registry[cls].keys())
        default = cls.default_implementation

        if default is None:
            return keys
        elif default not in keys:
            raise ConfigurationError(
                f""Default implementation {default} is not registered""
            )
        else:
            return [default] + [k for k in keys if k != default]",List default first if it exists,List default first if it exists,"def list_available(cls) -> List[str]:
        
        keys = list(Registrable._registry[cls].keys())
        default = cls.default_implementation

        if default is None:
            return keys
        elif default not in keys:
            raise ConfigurationError(
                f""Default implementation {default} is not registered""
            )
        else:
            return [default] + [k for k in keys if k != default]",List default first if it exists,"def list_available ( cls ) -> List [ str ] : keys = list ( Registrable . _registry [ cls ] . keys ( ) ) default = cls . default_implementation if default is None : return keys elif default not in keys : raise ConfigurationError ( f""Default implementation {default} is not registered"" ) else : return [ default ] + [ k for k in keys if k != default ]",List default first if it exists
/mcp-use/tests/conftest.py,mock_session,"def mock_session():
    """"""Return a mock session object for testing.""""""
    from unittest.mock import AsyncMock, MagicMock

    # Create mock connector
    connector = MagicMock()
    connector.connect = AsyncMock()
    connector.disconnect = AsyncMock()
    connector.initialize = AsyncMock(return_value={""session_id"": ""test_session""})
    connector.tools = [{""name"": ""test_tool""}]
    connector.call_tool = AsyncMock(return_value={""result"": ""success""})

    return connector","def mock_session():
    """"""Return a mock session object for testing.""""""
    from unittest.mock import AsyncMock, MagicMock

    # Create mock connector
    connector = MagicMock()
    connector.connect = AsyncMock()
    connector.disconnect = AsyncMock()
    connector.initialize = AsyncMock(return_value={""session_id"": ""test_session""})
    connector.tools = [{""name"": ""test_tool""}]
    connector.call_tool = AsyncMock(return_value={""result"": ""success""})

    return connector",Return a mock session object for testing.,Return a mock session object for testing.,"def mock_session():
    
    from unittest.mock import AsyncMock, MagicMock

    # Create mock connector
    connector = MagicMock()
    connector.connect = AsyncMock()
    connector.disconnect = AsyncMock()
    connector.initialize = AsyncMock(return_value={""session_id"": ""test_session""})
    connector.tools = [{""name"": ""test_tool""}]
    connector.call_tool = AsyncMock(return_value={""result"": ""success""})

    return connector",Return a mock session object for testing.,"def mock_session ( ) : from unittest . mock import AsyncMock , MagicMock # Create mock connector connector = MagicMock ( ) connector . connect = AsyncMock ( ) connector . disconnect = AsyncMock ( ) connector . initialize = AsyncMock ( return_value = { ""session_id"" : ""test_session"" } ) connector . tools = [ { ""name"" : ""test_tool"" } ] connector . call_tool = AsyncMock ( return_value = { ""result"" : ""success"" } ) return connector",Return a mock session object for testing.
/agenticSeek/sources/memory.py,load_json_file,"def load_json_file(self, path: str) -> dict:
        """"""Load a JSON file.""""""
        json_memory = {}
        try:
            with open(path, 'r') as f:
                json_memory = json.load(f)
        except FileNotFoundError:
            self.logger.warning(f""File not found: {path}"")
            return {}
        except json.JSONDecodeError:
            self.logger.warning(f""Error decoding JSON from file: {path}"")
            return {}
        except Exception as e:
            self.logger.warning(f""Error loading file {path}: {e}"")
            return {}
        return json_memory","def load_json_file(self, path: str) -> dict:
        """"""Load a JSON file.""""""
        json_memory = {}
        try:
            with open(path, 'r') as f:
                json_memory = json.load(f)
        except FileNotFoundError:
            self.logger.warning(f""File not found: {path}"")
            return {}
        except json.JSONDecodeError:
            self.logger.warning(f""Error decoding JSON from file: {path}"")
            return {}
        except Exception as e:
            self.logger.warning(f""Error loading file {path}: {e}"")
            return {}
        return json_memory",Load a JSON file.,Load a JSON file.,"def load_json_file(self, path: str) -> dict:
        
        json_memory = {}
        try:
            with open(path, 'r') as f:
                json_memory = json.load(f)
        except FileNotFoundError:
            self.logger.warning(f""File not found: {path}"")
            return {}
        except json.JSONDecodeError:
            self.logger.warning(f""Error decoding JSON from file: {path}"")
            return {}
        except Exception as e:
            self.logger.warning(f""Error loading file {path}: {e}"")
            return {}
        return json_memory",Load a JSON file.,"def load_json_file ( self , path : str ) -> dict : json_memory = { } try : with open ( path , 'r' ) as f : json_memory = json . load ( f ) except FileNotFoundError : self . logger . warning ( f""File not found: {path}"" ) return { } except json . JSONDecodeError : self . logger . warning ( f""Error decoding JSON from file: {path}"" ) return { } except Exception as e : self . logger . warning ( f""Error loading file {path}: {e}"" ) return { } return json_memory",Load a JSON file.
/magentic-ui/src/magentic_ui/types.py,__str__,"def __str__(self) -> str:
        """"""Return the string representation of the plan.""""""
        plan_str = """"
        if self.task is not None:
            plan_str += f""Task: {self.task}\n""
        for i, step in enumerate(self.steps):
            plan_str += f""{i}. {step.agent_name}: {step.title}\n   {step.details}\n""
        return plan_str","def __str__(self) -> str:
        """"""Return the string representation of the plan.""""""
        plan_str = """"
        if self.task is not None:
            plan_str += f""Task: {self.task}\n""
        for i, step in enumerate(self.steps):
            plan_str += f""{i}. {step.agent_name}: {step.title}\n   {step.details}\n""
        return plan_str",Return the string representation of the plan.,Return the string representation of the plan.,"def __str__(self) -> str:
        
        plan_str = """"
        if self.task is not None:
            plan_str += f""Task: {self.task}\n""
        for i, step in enumerate(self.steps):
            plan_str += f""{i}. {step.agent_name}: {step.title}\n   {step.details}\n""
        return plan_str",Return the string representation of the plan.,"def __str__ ( self ) -> str : plan_str = """" if self . task is not None : plan_str += f""Task: {self.task}\n"" for i , step in enumerate ( self . steps ) : plan_str += f""{i}. {step.agent_name}: {step.title}\n   {step.details}\n"" return plan_str",Return the string representation of the plan.
/adk-samples/python/agents/software-bug-assistant/software_bug_assistant/tools/tools.py,get_current_date,"def get_current_date() -> dict:
    """"""
    Get the current date in the format YYYY-MM-DD
    """"""
    return {""current_date"": datetime.now().strftime(""%Y-%m-%d"")}","def get_current_date() -> dict:
    """"""
    Get the current date in the format YYYY-MM-DD
    """"""
    return {""current_date"": datetime.now().strftime(""%Y-%m-%d"")}",Get the current date in the format YYYY-MM-DD,Get the current date in the format YYYY-MM-DD,"def get_current_date() -> dict:
    
    return {""current_date"": datetime.now().strftime(""%Y-%m-%d"")}",Get the current date in the format YYYY-MM-DD,"def get_current_date ( ) -> dict : return { ""current_date"" : datetime . now ( ) . strftime ( ""%Y-%m-%d"" ) }",Get the current date in the format YYYY-MM-DD
/olmocr/olmocr/eval/scoreelo.py,fetch_presigned_datastore,"def fetch_presigned_datastore(presigned_url):
    """"""
    Fetch the JSON datastore from the presigned URL.
    Returns a dict. If any error or no content, returns {}.
    """"""
    try:
        # Clean up the presigned URL (sometimes the signature may need re-encoding)
        url_parts = urlsplit(presigned_url)
        query_params = parse_qs(url_parts.query)
        encoded_query = urlencode(query_params, doseq=True)
        cleaned_url = urlunsplit((url_parts.scheme, url_parts.netloc, url_parts.path, encoded_query, url_parts.fragment))

        resp = requests.get(cleaned_url, headers={""Host"": url_parts.netloc, ""User-Agent"": ""Mozilla/5.0""})
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        print(f""Error fetching datastore from {presigned_url}: {e}"")
        return {}","def fetch_presigned_datastore(presigned_url):
    """"""
    Fetch the JSON datastore from the presigned URL.
    Returns a dict. If any error or no content, returns {}.
    """"""
    try:
        # Clean up the presigned URL (sometimes the signature may need re-encoding)
        url_parts = urlsplit(presigned_url)
        query_params = parse_qs(url_parts.query)
        encoded_query = urlencode(query_params, doseq=True)
        cleaned_url = urlunsplit((url_parts.scheme, url_parts.netloc, url_parts.path, encoded_query, url_parts.fragment))

        resp = requests.get(cleaned_url, headers={""Host"": url_parts.netloc, ""User-Agent"": ""Mozilla/5.0""})
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        print(f""Error fetching datastore from {presigned_url}: {e}"")
        return {}","Fetch the JSON datastore from the presigned URL.
Returns a dict. If any error or no content, returns {}.",Fetch the JSON datastore from the presigned URL.,"def fetch_presigned_datastore(presigned_url):
    
    try:
        # Clean up the presigned URL (sometimes the signature may need re-encoding)
        url_parts = urlsplit(presigned_url)
        query_params = parse_qs(url_parts.query)
        encoded_query = urlencode(query_params, doseq=True)
        cleaned_url = urlunsplit((url_parts.scheme, url_parts.netloc, url_parts.path, encoded_query, url_parts.fragment))

        resp = requests.get(cleaned_url, headers={""Host"": url_parts.netloc, ""User-Agent"": ""Mozilla/5.0""})
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        print(f""Error fetching datastore from {presigned_url}: {e}"")
        return {}",Fetch the JSON datastore from the presigned URL.,"def fetch_presigned_datastore ( presigned_url ) : try : # Clean up the presigned URL (sometimes the signature may need re-encoding) url_parts = urlsplit ( presigned_url ) query_params = parse_qs ( url_parts . query ) encoded_query = urlencode ( query_params , doseq = True ) cleaned_url = urlunsplit ( ( url_parts . scheme , url_parts . netloc , url_parts . path , encoded_query , url_parts . fragment ) ) resp = requests . get ( cleaned_url , headers = { ""Host"" : url_parts . netloc , ""User-Agent"" : ""Mozilla/5.0"" } ) resp . raise_for_status ( ) return resp . json ( ) except Exception as e : print ( f""Error fetching datastore from {presigned_url}: {e}"" ) return { }",Fetch the JSON datastore from the presigned URL.
/Second-Me/lpm_kernel/L2/memory_manager.py,cleanup_memory,"def cleanup_memory(self, force: bool = False) -> None:
        """"""Free up memory by garbage collection and emptying CUDA cache.""""""
        # Run Python garbage collection
        gc.collect()
        
        # Empty CUDA cache if available
        if self.cuda_available:
            torch.cuda.empty_cache()
            
        # Log memory status after cleanup
        if force:
            info = self.get_memory_info()
            logger.info(
                f""Memory after cleanup: RAM: {info['ram_used_gb']:.2f}GB / {info['ram_total_gb']:.2f}GB, ""
                f""VRAM: {info.get('vram_used_gb', 0):.2f}GB / {info.get('vram_total_gb', 0):.2f}GB""
            )","def cleanup_memory(self, force: bool = False) -> None:
        """"""Free up memory by garbage collection and emptying CUDA cache.""""""
        # Run Python garbage collection
        gc.collect()
        
        # Empty CUDA cache if available
        if self.cuda_available:
            torch.cuda.empty_cache()
            
        # Log memory status after cleanup
        if force:
            info = self.get_memory_info()
            logger.info(
                f""Memory after cleanup: RAM: {info['ram_used_gb']:.2f}GB / {info['ram_total_gb']:.2f}GB, ""
                f""VRAM: {info.get('vram_used_gb', 0):.2f}GB / {info.get('vram_total_gb', 0):.2f}GB""
            )",Free up memory by garbage collection and emptying CUDA cache.,Free up memory by garbage collection and emptying CUDA cache.,"def cleanup_memory(self, force: bool = False) -> None:
        
        # Run Python garbage collection
        gc.collect()
        
        # Empty CUDA cache if available
        if self.cuda_available:
            torch.cuda.empty_cache()
            
        # Log memory status after cleanup
        if force:
            info = self.get_memory_info()
            logger.info(
                f""Memory after cleanup: RAM: {info['ram_used_gb']:.2f}GB / {info['ram_total_gb']:.2f}GB, ""
                f""VRAM: {info.get('vram_used_gb', 0):.2f}GB / {info.get('vram_total_gb', 0):.2f}GB""
            )",Free up memory by garbage collection and emptying CUDA cache.,"def cleanup_memory ( self , force : bool = False ) -> None : # Run Python garbage collection gc . collect ( ) # Empty CUDA cache if available if self . cuda_available : torch . cuda . empty_cache ( ) # Log memory status after cleanup if force : info = self . get_memory_info ( ) logger . info ( f""Memory after cleanup: RAM: {info['ram_used_gb']:.2f}GB / {info['ram_total_gb']:.2f}GB, "" f""VRAM: {info.get('vram_used_gb', 0):.2f}GB / {info.get('vram_total_gb', 0):.2f}GB"" )",Free up memory by garbage collection and emptying CUDA cache.
/python-sdk/src/mcp/server/fastmcp/prompts/manager.py,add_prompt,"def add_prompt(
        self,
        prompt: Prompt,
    ) -> Prompt:
        """"""Add a prompt to the manager.""""""

        # Check for duplicates
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing

        self._prompts[prompt.name] = prompt
        return prompt","def add_prompt(
        self,
        prompt: Prompt,
    ) -> Prompt:
        """"""Add a prompt to the manager.""""""

        # Check for duplicates
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing

        self._prompts[prompt.name] = prompt
        return prompt",Add a prompt to the manager.,Add a prompt to the manager.,"def add_prompt(
        self,
        prompt: Prompt,
    ) -> Prompt:
        

        # Check for duplicates
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing

        self._prompts[prompt.name] = prompt
        return prompt",Add a prompt to the manager.,"def add_prompt ( self , prompt : Prompt , ) -> Prompt : # Check for duplicates existing = self . _prompts . get ( prompt . name ) if existing : if self . warn_on_duplicate_prompts : logger . warning ( f""Prompt already exists: {prompt.name}"" ) return existing self . _prompts [ prompt . name ] = prompt return prompt",Add a prompt to the manager.
/nesa/demo/modules/models_settings.py,save_model_settings,"def save_model_settings(model, state):
    '''
    Save the settings for this model to models/config-user.yaml
    '''
    if model == 'None':
        yield (""Not saving the settings because no model is selected in the menu."")
        return

    user_config = shared.load_user_config()
    model_regex = model + '$'  # For exact matches
    if model_regex not in user_config:
        user_config[model_regex] = {}

    for k in ui.list_model_elements():
        if k == 'loader' or k in loaders.loaders_and_params[state['loader']]:
            user_config[model_regex][k] = state[k]

    shared.user_config = user_config

    output = yaml.dump(user_config, sort_keys=False)
    p = Path(f'{shared.args.model_dir}/config-user.yaml')
    with open(p, 'w') as f:
        f.write(output)

    yield (f""Settings for `{model}` saved to `{p}`."")","def save_model_settings(model, state):
    '''
    Save the settings for this model to models/config-user.yaml
    '''
    if model == 'None':
        yield (""Not saving the settings because no model is selected in the menu."")
        return

    user_config = shared.load_user_config()
    model_regex = model + '$'  # For exact matches
    if model_regex not in user_config:
        user_config[model_regex] = {}

    for k in ui.list_model_elements():
        if k == 'loader' or k in loaders.loaders_and_params[state['loader']]:
            user_config[model_regex][k] = state[k]

    shared.user_config = user_config

    output = yaml.dump(user_config, sort_keys=False)
    p = Path(f'{shared.args.model_dir}/config-user.yaml')
    with open(p, 'w') as f:
        f.write(output)

    yield (f""Settings for `{model}` saved to `{p}`."")",Save the settings for this model to models/config-user.yaml,Save the settings for this model to models/config-user.yaml,"def save_model_settings(model, state):
    
    if model == 'None':
        yield (""Not saving the settings because no model is selected in the menu."")
        return

    user_config = shared.load_user_config()
    model_regex = model + '$'  # For exact matches
    if model_regex not in user_config:
        user_config[model_regex] = {}

    for k in ui.list_model_elements():
        if k == 'loader' or k in loaders.loaders_and_params[state['loader']]:
            user_config[model_regex][k] = state[k]

    shared.user_config = user_config

    output = yaml.dump(user_config, sort_keys=False)
    p = Path(f'{shared.args.model_dir}/config-user.yaml')
    with open(p, 'w') as f:
        f.write(output)

    yield (f""Settings for `{model}` saved to `{p}`."")",Save the settings for this model to models/config-user.yaml,"def save_model_settings ( model , state ) : if model == 'None' : yield ( ""Not saving the settings because no model is selected in the menu."" ) return user_config = shared . load_user_config ( ) model_regex = model + '$' # For exact matches if model_regex not in user_config : user_config [ model_regex ] = { } for k in ui . list_model_elements ( ) : if k == 'loader' or k in loaders . loaders_and_params [ state [ 'loader' ] ] : user_config [ model_regex ] [ k ] = state [ k ] shared . user_config = user_config output = yaml . dump ( user_config , sort_keys = False ) p = Path ( f'{shared.args.model_dir}/config-user.yaml' ) with open ( p , 'w' ) as f : f . write ( output ) yield ( f""Settings for `{model}` saved to `{p}`."" )",Save the settings for this model to models/config-user.yaml
/mcp-agent/tests/workflows/llm/test_augmented_llm_azure.py,default_usage,"def default_usage(self):
        """"""
        Returns a default usage object for testing.
        """"""
        return {
            ""completion_tokens"": 100,
            ""prompt_tokens"": 150,
            ""total_tokens"": 250,
        }","def default_usage(self):
        """"""
        Returns a default usage object for testing.
        """"""
        return {
            ""completion_tokens"": 100,
            ""prompt_tokens"": 150,
            ""total_tokens"": 250,
        }",Returns a default usage object for testing.,Returns a default usage object for testing.,"def default_usage(self):
        
        return {
            ""completion_tokens"": 100,
            ""prompt_tokens"": 150,
            ""total_tokens"": 250,
        }",Returns a default usage object for testing.,"def default_usage ( self ) : return { ""completion_tokens"" : 100 , ""prompt_tokens"" : 150 , ""total_tokens"" : 250 , }",Returns a default usage object for testing.
/podcastfy/podcastfy/tts/providers/openai.py,generate_audio,"def generate_audio(self, text: str, voice: str, model: str, voice2: str = None) -> bytes:
        """"""Generate audio using OpenAI API.""""""
        self.validate_parameters(text, voice, model)
        
        try:
            response = openai.audio.speech.create(
                model=model,
                voice=voice,
                input=text
            )
            return response.content
        except Exception as e:
            raise RuntimeError(f""Failed to generate audio: {str(e)}"") from e","def generate_audio(self, text: str, voice: str, model: str, voice2: str = None) -> bytes:
        """"""Generate audio using OpenAI API.""""""
        self.validate_parameters(text, voice, model)
        
        try:
            response = openai.audio.speech.create(
                model=model,
                voice=voice,
                input=text
            )
            return response.content
        except Exception as e:
            raise RuntimeError(f""Failed to generate audio: {str(e)}"") from e",Generate audio using OpenAI API.,Generate audio using OpenAI API.,"def generate_audio(self, text: str, voice: str, model: str, voice2: str = None) -> bytes:
        
        self.validate_parameters(text, voice, model)
        
        try:
            response = openai.audio.speech.create(
                model=model,
                voice=voice,
                input=text
            )
            return response.content
        except Exception as e:
            raise RuntimeError(f""Failed to generate audio: {str(e)}"") from e",Generate audio using OpenAI API.,"def generate_audio ( self , text : str , voice : str , model : str , voice2 : str = None ) -> bytes : self . validate_parameters ( text , voice , model ) try : response = openai . audio . speech . create ( model = model , voice = voice , input = text ) return response . content except Exception as e : raise RuntimeError ( f""Failed to generate audio: {str(e)}"" ) from e",Generate audio using OpenAI API.
/ag2/autogen/doc_utils.py,get_target_module,"def get_target_module(obj: object) -> Optional[str]:
    """"""Get the target module where an object should be documented.""""""
    if not hasattr(obj, ""__module__""):
        return None

    fqn = f""{obj.__module__}.{obj.__name__}""
    return _PDOC_MODULE_EXPORT_MAPPINGS.get(fqn)","def get_target_module(obj: object) -> Optional[str]:
    """"""Get the target module where an object should be documented.""""""
    if not hasattr(obj, ""__module__""):
        return None

    fqn = f""{obj.__module__}.{obj.__name__}""
    return _PDOC_MODULE_EXPORT_MAPPINGS.get(fqn)",Get the target module where an object should be documented.,Get the target module where an object should be documented.,"def get_target_module(obj: object) -> Optional[str]:
    
    if not hasattr(obj, ""__module__""):
        return None

    fqn = f""{obj.__module__}.{obj.__name__}""
    return _PDOC_MODULE_EXPORT_MAPPINGS.get(fqn)",Get the target module where an object should be documented.,"def get_target_module ( obj : object ) -> Optional [ str ] : if not hasattr ( obj , ""__module__"" ) : return None fqn = f""{obj.__module__}.{obj.__name__}"" return _PDOC_MODULE_EXPORT_MAPPINGS . get ( fqn )",Get the target module where an object should be documented.
/morphik-core/sdks/python/morphik/models.py,validate_graph_fields,"def validate_graph_fields(self) -> ""GraphPromptOverrides"":
        """"""Ensure only graph-related fields are present.""""""
        allowed_fields = {""entity_extraction"", ""entity_resolution""}
        for field in self.model_fields:
            if field not in allowed_fields and getattr(self, field, None) is not None:
                raise ValueError(f""Field '{field}' is not allowed in graph prompt overrides"")
        return self","def validate_graph_fields(self) -> ""GraphPromptOverrides"":
        """"""Ensure only graph-related fields are present.""""""
        allowed_fields = {""entity_extraction"", ""entity_resolution""}
        for field in self.model_fields:
            if field not in allowed_fields and getattr(self, field, None) is not None:
                raise ValueError(f""Field '{field}' is not allowed in graph prompt overrides"")
        return self",Ensure only graph-related fields are present.,Ensure only graph-related fields are present.,"def validate_graph_fields(self) -> ""GraphPromptOverrides"":
        
        allowed_fields = {""entity_extraction"", ""entity_resolution""}
        for field in self.model_fields:
            if field not in allowed_fields and getattr(self, field, None) is not None:
                raise ValueError(f""Field '{field}' is not allowed in graph prompt overrides"")
        return self",Ensure only graph-related fields are present.,"def validate_graph_fields ( self ) -> ""GraphPromptOverrides"" : allowed_fields = { ""entity_extraction"" , ""entity_resolution"" } for field in self . model_fields : if field not in allowed_fields and getattr ( self , field , None ) is not None : raise ValueError ( f""Field '{field}' is not allowed in graph prompt overrides"" ) return self",Ensure only graph-related fields are present.
/airweave/backend/airweave/platform/sync/entity_processor.py,_log_vectorization_start,"def _log_vectorization_start(
        self, processed_entities: List[BaseEntity], sync_context: SyncContext, entity_context: str
    ) -> None:
        """"""Log vectorization startup information.""""""
        embedding_model = sync_context.embedding_model
        entity_count = len(processed_entities)

        sync_context.logger.info(
            f""Computing vectors for {entity_count} entities using {embedding_model.model_name}""
        )","def _log_vectorization_start(
        self, processed_entities: List[BaseEntity], sync_context: SyncContext, entity_context: str
    ) -> None:
        """"""Log vectorization startup information.""""""
        embedding_model = sync_context.embedding_model
        entity_count = len(processed_entities)

        sync_context.logger.info(
            f""Computing vectors for {entity_count} entities using {embedding_model.model_name}""
        )",Log vectorization startup information.,Log vectorization startup information.,"def _log_vectorization_start(
        self, processed_entities: List[BaseEntity], sync_context: SyncContext, entity_context: str
    ) -> None:
        
        embedding_model = sync_context.embedding_model
        entity_count = len(processed_entities)

        sync_context.logger.info(
            f""Computing vectors for {entity_count} entities using {embedding_model.model_name}""
        )",Log vectorization startup information.,"def _log_vectorization_start ( self , processed_entities : List [ BaseEntity ] , sync_context : SyncContext , entity_context : str ) -> None : embedding_model = sync_context . embedding_model entity_count = len ( processed_entities ) sync_context . logger . info ( f""Computing vectors for {entity_count} entities using {embedding_model.model_name}"" )",Log vectorization startup information.
/ag2/autogen/agentchat/contrib/capabilities/transforms.py,_truncate_multimodal_text,"def _truncate_multimodal_text(self, contents: list[dict[str, Any]], n_tokens: int) -> list[dict[str, Any]]:
        """"""Truncates text content within a list of multimodal elements, preserving the overall structure.""""""
        tmp_contents = []
        for content in contents:
            if content[""type""] == ""text"":
                truncated_text = self._truncate_tokens(content[""text""], n_tokens)
                tmp_contents.append({""type"": ""text"", ""text"": truncated_text})
            else:
                tmp_contents.append(content)
        return tmp_contents","def _truncate_multimodal_text(self, contents: list[dict[str, Any]], n_tokens: int) -> list[dict[str, Any]]:
        """"""Truncates text content within a list of multimodal elements, preserving the overall structure.""""""
        tmp_contents = []
        for content in contents:
            if content[""type""] == ""text"":
                truncated_text = self._truncate_tokens(content[""text""], n_tokens)
                tmp_contents.append({""type"": ""text"", ""text"": truncated_text})
            else:
                tmp_contents.append(content)
        return tmp_contents","Truncates text content within a list of multimodal elements, preserving the overall structure.","Truncates text content within a list of multimodal elements, preserving the overall structure.","def _truncate_multimodal_text(self, contents: list[dict[str, Any]], n_tokens: int) -> list[dict[str, Any]]:
        
        tmp_contents = []
        for content in contents:
            if content[""type""] == ""text"":
                truncated_text = self._truncate_tokens(content[""text""], n_tokens)
                tmp_contents.append({""type"": ""text"", ""text"": truncated_text})
            else:
                tmp_contents.append(content)
        return tmp_contents","Truncates text content within a list of multimodal elements, preserving the overall structure.","def _truncate_multimodal_text ( self , contents : list [ dict [ str , Any ] ] , n_tokens : int ) -> list [ dict [ str , Any ] ] : tmp_contents = [ ] for content in contents : if content [ ""type"" ] == ""text"" : truncated_text = self . _truncate_tokens ( content [ ""text"" ] , n_tokens ) tmp_contents . append ( { ""type"" : ""text"" , ""text"" : truncated_text } ) else : tmp_contents . append ( content ) return tmp_contents","Truncates text content within a list of multimodal elements, preserving the overall structure."
/intentkit/utils/error.py,format_validation_errors,"def format_validation_errors(errors: list) -> str:
    """"""Format validation errors into a more readable string.""""""
    formatted_errors = []

    for error in errors:
        loc = error.get(""loc"", [])
        msg = error.get(""msg"", """")
        error_type = error.get(""type"", """")

        # Build field path
        field_path = "" -> "".join(str(part) for part in loc if part != ""body"")

        # Format the error message with type information
        if field_path:
            if error_type:
                formatted_error = f""Field '{field_path}' ({error_type}): {msg}""
            else:
                formatted_error = f""Field '{field_path}': {msg}""
        else:
            formatted_error = msg

        formatted_errors.append(formatted_error)

    return ""; "".join(formatted_errors)","def format_validation_errors(errors: list) -> str:
    """"""Format validation errors into a more readable string.""""""
    formatted_errors = []

    for error in errors:
        loc = error.get(""loc"", [])
        msg = error.get(""msg"", """")
        error_type = error.get(""type"", """")

        # Build field path
        field_path = "" -> "".join(str(part) for part in loc if part != ""body"")

        # Format the error message with type information
        if field_path:
            if error_type:
                formatted_error = f""Field '{field_path}' ({error_type}): {msg}""
            else:
                formatted_error = f""Field '{field_path}': {msg}""
        else:
            formatted_error = msg

        formatted_errors.append(formatted_error)

    return ""; "".join(formatted_errors)",Format validation errors into a more readable string.,Format validation errors into a more readable string.,"def format_validation_errors(errors: list) -> str:
    
    formatted_errors = []

    for error in errors:
        loc = error.get(""loc"", [])
        msg = error.get(""msg"", """")
        error_type = error.get(""type"", """")

        # Build field path
        field_path = "" -> "".join(str(part) for part in loc if part != ""body"")

        # Format the error message with type information
        if field_path:
            if error_type:
                formatted_error = f""Field '{field_path}' ({error_type}): {msg}""
            else:
                formatted_error = f""Field '{field_path}': {msg}""
        else:
            formatted_error = msg

        formatted_errors.append(formatted_error)

    return ""; "".join(formatted_errors)",Format validation errors into a more readable string.,"def format_validation_errors ( errors : list ) -> str : formatted_errors = [ ] for error in errors : loc = error . get ( ""loc"" , [ ] ) msg = error . get ( ""msg"" , """" ) error_type = error . get ( ""type"" , """" ) # Build field path field_path = "" -> "" . join ( str ( part ) for part in loc if part != ""body"" ) # Format the error message with type information if field_path : if error_type : formatted_error = f""Field '{field_path}' ({error_type}): {msg}"" else : formatted_error = f""Field '{field_path}': {msg}"" else : formatted_error = msg formatted_errors . append ( formatted_error ) return ""; "" . join ( formatted_errors )",Format validation errors into a more readable string.
/VLM-R1/src/open-r1-multimodal/src/open_r1/utils/hub.py,check_hub_revision_exists,"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    """"""Checks if a given Hub revision exists.""""""
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id, revision=training_args.hub_model_revision
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )","def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    """"""Checks if a given Hub revision exists.""""""
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id, revision=training_args.hub_model_revision
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )",Checks if a given Hub revision exists.,Checks if a given Hub revision exists.,"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id, revision=training_args.hub_model_revision
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )",Checks if a given Hub revision exists.,"def check_hub_revision_exists ( training_args : SFTConfig | GRPOConfig ) : if repo_exists ( training_args . hub_model_id ) : if training_args . push_to_hub_revision is True : # First check if the revision exists revisions = [ rev . name for rev in list_repo_refs ( training_args . hub_model_id ) . branches ] # If the revision exists, we next check it has a README file if training_args . hub_model_revision in revisions : repo_files = list_repo_files ( repo_id = training_args . hub_model_id , revision = training_args . hub_model_revision ) if ""README.md"" in repo_files and training_args . overwrite_hub_revision is False : raise ValueError ( f""Revision {training_args.hub_model_revision} already exists. "" ""Use --overwrite_hub_revision to overwrite it."" )",Checks if a given Hub revision exists.
/OpenManus-RL/openmanus_rl/agentbench/src/assigner.py,save_partial_results,"def save_partial_results(self, agent, task):
        """"""Calculate and save partial results for a specific agent-task pair""""""
        try:
            if task in self.tasks and agent in self.completions and task in self.completions[agent]:
                task_client = self.tasks[task]
                overall = task_client.calculate_overall(self.completions[agent][task])
                output_dir = self.get_output_dir(agent, task)
                os.makedirs(output_dir, exist_ok=True)
                with open(os.path.join(output_dir, ""overall.json""), ""w"") as f:
                    f.write(json.dumps(overall, indent=4, ensure_ascii=False))
                return overall
        except Exception as e:
            print(ColorMessage.yellow(f""Warning: Failed to save partial results for {agent}/{task}: {str(e)}""))
        return None","def save_partial_results(self, agent, task):
        """"""Calculate and save partial results for a specific agent-task pair""""""
        try:
            if task in self.tasks and agent in self.completions and task in self.completions[agent]:
                task_client = self.tasks[task]
                overall = task_client.calculate_overall(self.completions[agent][task])
                output_dir = self.get_output_dir(agent, task)
                os.makedirs(output_dir, exist_ok=True)
                with open(os.path.join(output_dir, ""overall.json""), ""w"") as f:
                    f.write(json.dumps(overall, indent=4, ensure_ascii=False))
                return overall
        except Exception as e:
            print(ColorMessage.yellow(f""Warning: Failed to save partial results for {agent}/{task}: {str(e)}""))
        return None",Calculate and save partial results for a specific agent-task pair,Calculate and save partial results for a specific agent-task pair,"def save_partial_results(self, agent, task):
        
        try:
            if task in self.tasks and agent in self.completions and task in self.completions[agent]:
                task_client = self.tasks[task]
                overall = task_client.calculate_overall(self.completions[agent][task])
                output_dir = self.get_output_dir(agent, task)
                os.makedirs(output_dir, exist_ok=True)
                with open(os.path.join(output_dir, ""overall.json""), ""w"") as f:
                    f.write(json.dumps(overall, indent=4, ensure_ascii=False))
                return overall
        except Exception as e:
            print(ColorMessage.yellow(f""Warning: Failed to save partial results for {agent}/{task}: {str(e)}""))
        return None",Calculate and save partial results for a specific agent-task pair,"def save_partial_results ( self , agent , task ) : try : if task in self . tasks and agent in self . completions and task in self . completions [ agent ] : task_client = self . tasks [ task ] overall = task_client . calculate_overall ( self . completions [ agent ] [ task ] ) output_dir = self . get_output_dir ( agent , task ) os . makedirs ( output_dir , exist_ok = True ) with open ( os . path . join ( output_dir , ""overall.json"" ) , ""w"" ) as f : f . write ( json . dumps ( overall , indent = 4 , ensure_ascii = False ) ) return overall except Exception as e : print ( ColorMessage . yellow ( f""Warning: Failed to save partial results for {agent}/{task}: {str(e)}"" ) ) return None",Calculate and save partial results for a specific agent-task pair
/BabelDOC/babeldoc/pdfminer/pdfinterp.py,do_Tm,"def do_Tm(
        self,
        a: PDFStackT,
        b: PDFStackT,
        c: PDFStackT,
        d: PDFStackT,
        e: PDFStackT,
        f: PDFStackT,
    ) -> None:
        """"""Set text matrix and text line matrix""""""
        values = (a, b, c, d, e, f)
        matrix = safe_matrix(*values)

        if matrix is None:
            log.warning(
                f""Could not set text matrix because not all values in {values!r} can be parsed as floats""
            )
        else:
            self.textstate.matrix = matrix
            self.textstate.linematrix = (0, 0)","def do_Tm(
        self,
        a: PDFStackT,
        b: PDFStackT,
        c: PDFStackT,
        d: PDFStackT,
        e: PDFStackT,
        f: PDFStackT,
    ) -> None:
        """"""Set text matrix and text line matrix""""""
        values = (a, b, c, d, e, f)
        matrix = safe_matrix(*values)

        if matrix is None:
            log.warning(
                f""Could not set text matrix because not all values in {values!r} can be parsed as floats""
            )
        else:
            self.textstate.matrix = matrix
            self.textstate.linematrix = (0, 0)",Set text matrix and text line matrix,Set text matrix and text line matrix,"def do_Tm(
        self,
        a: PDFStackT,
        b: PDFStackT,
        c: PDFStackT,
        d: PDFStackT,
        e: PDFStackT,
        f: PDFStackT,
    ) -> None:
        
        values = (a, b, c, d, e, f)
        matrix = safe_matrix(*values)

        if matrix is None:
            log.warning(
                f""Could not set text matrix because not all values in {values!r} can be parsed as floats""
            )
        else:
            self.textstate.matrix = matrix
            self.textstate.linematrix = (0, 0)",Set text matrix and text line matrix,"def do_Tm ( self , a : PDFStackT , b : PDFStackT , c : PDFStackT , d : PDFStackT , e : PDFStackT , f : PDFStackT , ) -> None : values = ( a , b , c , d , e , f ) matrix = safe_matrix ( * values ) if matrix is None : log . warning ( f""Could not set text matrix because not all values in {values!r} can be parsed as floats"" ) else : self . textstate . matrix = matrix self . textstate . linematrix = ( 0 , 0 )",Set text matrix and text line matrix
/magentic-ui/src/magentic_ui/tools/tool_metadata.py,get_tool_metadata,"def get_tool_metadata(tool_name_or_schema: str | ToolSchema) -> ToolMetadata:
    """"""Get the metadata for a tool by its name.""""""
    tool_name: str | None = (
        tool_name_or_schema
        if isinstance(tool_name_or_schema, str)
        else tool_name_or_schema.get(""name"")
    )

    metadata = _tool_metadata.get(tool_name)

    if metadata is None:
        raise ValueError(f""Tool {tool_name} not found in metadata."")

    return metadata","def get_tool_metadata(tool_name_or_schema: str | ToolSchema) -> ToolMetadata:
    """"""Get the metadata for a tool by its name.""""""
    tool_name: str | None = (
        tool_name_or_schema
        if isinstance(tool_name_or_schema, str)
        else tool_name_or_schema.get(""name"")
    )

    metadata = _tool_metadata.get(tool_name)

    if metadata is None:
        raise ValueError(f""Tool {tool_name} not found in metadata."")

    return metadata",Get the metadata for a tool by its name.,Get the metadata for a tool by its name.,"def get_tool_metadata(tool_name_or_schema: str | ToolSchema) -> ToolMetadata:
    
    tool_name: str | None = (
        tool_name_or_schema
        if isinstance(tool_name_or_schema, str)
        else tool_name_or_schema.get(""name"")
    )

    metadata = _tool_metadata.get(tool_name)

    if metadata is None:
        raise ValueError(f""Tool {tool_name} not found in metadata."")

    return metadata",Get the metadata for a tool by its name.,"def get_tool_metadata ( tool_name_or_schema : str | ToolSchema ) -> ToolMetadata : tool_name : str | None = ( tool_name_or_schema if isinstance ( tool_name_or_schema , str ) else tool_name_or_schema . get ( ""name"" ) ) metadata = _tool_metadata . get ( tool_name ) if metadata is None : raise ValueError ( f""Tool {tool_name} not found in metadata."" ) return metadata",Get the metadata for a tool by its name.
/PocketFlow-Tutorial-Codebase-Knowledge/utils/crawl_github_files.py,fetch_branches,"def fetch_branches(owner: str, repo: str):
        """"""Get brancshes of the repository""""""

        url = f""https://api.github.com/repos/{owner}/{repo}/branches""
        response = requests.get(url, headers=headers)

        if response.status_code == 404:
            if not token:
                print(f""Error 404: Repository not found or is private.\n""
                      f""If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable."")
            else:
                print(f""Error 404: Repository not found or insufficient permissions with the provided token.\n""
                      f""Please verify the repository exists and the token has access to this repository."")
            return []
            
        if response.status_code != 200:
            print(f""Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}"")
            return []

        return response.json()","def fetch_branches(owner: str, repo: str):
        """"""Get brancshes of the repository""""""

        response = requests.get(url, headers=headers)

        if response.status_code == 404:
            if not token:
                print(f""Error 404: Repository not found or is private.\n""
                      f""If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable."")
            else:
                print(f""Error 404: Repository not found or insufficient permissions with the provided token.\n""
                      f""Please verify the repository exists and the token has access to this repository."")
            return []
            
        if response.status_code != 200:
            print(f""Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}"")
            return []

        return response.json()",Get brancshes of the repository,Get brancshes of the repository,"def fetch_branches(owner: str, repo: str):
        

        response = requests.get(url, headers=headers)

        if response.status_code == 404:
            if not token:
                print(f""Error 404: Repository not found or is private.\n""
                      f""If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable."")
            else:
                print(f""Error 404: Repository not found or insufficient permissions with the provided token.\n""
                      f""Please verify the repository exists and the token has access to this repository."")
            return []
            
        if response.status_code != 200:
            print(f""Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}"")
            return []

        return response.json()",Get brancshes of the repository,"def fetch_branches ( owner : str , repo : str ) : response = requests . get ( url , headers = headers ) if response . status_code == 404 : if not token : print ( f""Error 404: Repository not found or is private.\n"" f""If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable."" ) else : print ( f""Error 404: Repository not found or insufficient permissions with the provided token.\n"" f""Please verify the repository exists and the token has access to this repository."" ) return [ ] if response . status_code != 200 : print ( f""Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}"" ) return [ ] return response . json ( )",Get brancshes of the repository
/local-deep-researcher/src/ollama_deep_researcher/configuration.py,from_runnable_config,"def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        """"""Create a Configuration instance from a RunnableConfig.""""""
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        
        # Get raw values from environment or config
        raw_values: dict[str, Any] = {
            name: os.environ.get(name.upper(), configurable.get(name))
            for name in cls.model_fields.keys()
        }
        
        # Filter out None values
        values = {k: v for k, v in raw_values.items() if v is not None}
        
        return cls(**values)","def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        """"""Create a Configuration instance from a RunnableConfig.""""""
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        
        # Get raw values from environment or config
        raw_values: dict[str, Any] = {
            name: os.environ.get(name.upper(), configurable.get(name))
            for name in cls.model_fields.keys()
        }
        
        # Filter out None values
        values = {k: v for k, v in raw_values.items() if v is not None}
        
        return cls(**values)",Create a Configuration instance from a RunnableConfig.,Create a Configuration instance from a RunnableConfig.,"def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        
        # Get raw values from environment or config
        raw_values: dict[str, Any] = {
            name: os.environ.get(name.upper(), configurable.get(name))
            for name in cls.model_fields.keys()
        }
        
        # Filter out None values
        values = {k: v for k, v in raw_values.items() if v is not None}
        
        return cls(**values)",Create a Configuration instance from a RunnableConfig.,"def from_runnable_config ( cls , config : Optional [ RunnableConfig ] = None ) -> ""Configuration"" : configurable = ( config [ ""configurable"" ] if config and ""configurable"" in config else { } ) # Get raw values from environment or config raw_values : dict [ str , Any ] = { name : os . environ . get ( name . upper ( ) , configurable . get ( name ) ) for name in cls . model_fields . keys ( ) } # Filter out None values values = { k : v for k , v in raw_values . items ( ) if v is not None } return cls ( ** values )",Create a Configuration instance from a RunnableConfig.
/nesa/demo/nesa/backend/registry.py,get_model,"def get_model(cls, base_id, task_type):
        """"""Get a model class, first by model name, then by task type.""""""
        if base_id in cls.registry[""model_specific""]:
            return cls.registry[""model_specific""][base_id]
        elif task_type in cls.registry[""task_type""]:
            return cls.registry[""task_type""][task_type]
        else:
            raise ValueError(f""No model class found for model '{base_id}' or task type '{task_type}'"")","def get_model(cls, base_id, task_type):
        """"""Get a model class, first by model name, then by task type.""""""
        if base_id in cls.registry[""model_specific""]:
            return cls.registry[""model_specific""][base_id]
        elif task_type in cls.registry[""task_type""]:
            return cls.registry[""task_type""][task_type]
        else:
            raise ValueError(f""No model class found for model '{base_id}' or task type '{task_type}'"")","Get a model class, first by model name, then by task type.","Get a model class, first by model name, then by task type.","def get_model(cls, base_id, task_type):
        
        if base_id in cls.registry[""model_specific""]:
            return cls.registry[""model_specific""][base_id]
        elif task_type in cls.registry[""task_type""]:
            return cls.registry[""task_type""][task_type]
        else:
            raise ValueError(f""No model class found for model '{base_id}' or task type '{task_type}'"")","Get a model class, first by model name, then by task type.","def get_model ( cls , base_id , task_type ) : if base_id in cls . registry [ ""model_specific"" ] : return cls . registry [ ""model_specific"" ] [ base_id ] elif task_type in cls . registry [ ""task_type"" ] : return cls . registry [ ""task_type"" ] [ task_type ] else : raise ValueError ( f""No model class found for model '{base_id}' or task type '{task_type}'"" )","Get a model class, first by model name, then by task type."
/local-deep-research/src/local_deep_research/citation_handler.py,_format_sources,"def _format_sources(self, documents: List[Document]) -> str:
        """"""Format sources with numbers for citation.""""""
        sources = []
        for doc in documents:
            source_id = doc.metadata[""index""]
            sources.append(f""[{source_id}] {doc.page_content}"")
        return ""\n\n"".join(sources)","def _format_sources(self, documents: List[Document]) -> str:
        """"""Format sources with numbers for citation.""""""
        sources = []
        for doc in documents:
            source_id = doc.metadata[""index""]
            sources.append(f""[{source_id}] {doc.page_content}"")
        return ""\n\n"".join(sources)",Format sources with numbers for citation.,Format sources with numbers for citation.,"def _format_sources(self, documents: List[Document]) -> str:
        
        sources = []
        for doc in documents:
            source_id = doc.metadata[""index""]
            sources.append(f""[{source_id}] {doc.page_content}"")
        return ""\n\n"".join(sources)",Format sources with numbers for citation.,"def _format_sources ( self , documents : List [ Document ] ) -> str : sources = [ ] for doc in documents : source_id = doc . metadata [ ""index"" ] sources . append ( f""[{source_id}] {doc.page_content}"" ) return ""\n\n"" . join ( sources )",Format sources with numbers for citation.
/YuE/finetune/scripts/train_lora.py,_compile_dependencies,"def _compile_dependencies():
    """"""Compile dataset C++ code.""""""
    if torch.distributed.get_rank() == 0:
        start_time = time.time()
        logger.info(""> Compiling dataset index builder..."")
        try:
            from core.datasets.utils import compile_helpers
            compile_helpers()
            logger.info(
                f"">>> Done with dataset index builder. Compilation time: {time.time() - start_time:.3f} seconds""
            )
        except Exception as e:
            logger.error(f""Failed to compile helpers: {e}"")
            raise","def _compile_dependencies():
    """"""Compile dataset C++ code.""""""
    if torch.distributed.get_rank() == 0:
        start_time = time.time()
        logger.info(""> Compiling dataset index builder..."")
        try:
            from core.datasets.utils import compile_helpers
            compile_helpers()
            logger.info(
                f"">>> Done with dataset index builder. Compilation time: {time.time() - start_time:.3f} seconds""
            )
        except Exception as e:
            logger.error(f""Failed to compile helpers: {e}"")
            raise",Compile dataset C++ code.,Compile dataset C++ code.,"def _compile_dependencies():
    
    if torch.distributed.get_rank() == 0:
        start_time = time.time()
        logger.info(""> Compiling dataset index builder..."")
        try:
            from core.datasets.utils import compile_helpers
            compile_helpers()
            logger.info(
                f"">>> Done with dataset index builder. Compilation time: {time.time() - start_time:.3f} seconds""
            )
        except Exception as e:
            logger.error(f""Failed to compile helpers: {e}"")
            raise",Compile dataset C++ code.,"def _compile_dependencies ( ) : if torch . distributed . get_rank ( ) == 0 : start_time = time . time ( ) logger . info ( ""> Compiling dataset index builder..."" ) try : from core . datasets . utils import compile_helpers compile_helpers ( ) logger . info ( f"">>> Done with dataset index builder. Compilation time: {time.time() - start_time:.3f} seconds"" ) except Exception as e : logger . error ( f""Failed to compile helpers: {e}"" ) raise",Compile dataset C++ code.
/PocketFlow/cookbook/pocketflow-tool-crawler/tools/crawler.py,crawl,"def crawl(self) -> List[Dict]:
        """"""Crawl website starting from base_url""""""
        to_visit = [self.base_url]
        results = []
        
        while to_visit and len(self.visited) < self.max_pages:
            url = to_visit.pop(0)
            
            if url in self.visited:
                continue
                
            print(f""Crawling: {url}"")
            content = self.extract_page_content(url)
            
            if content:
                self.visited.add(url)
                results.append(content)
                
                # Add new URLs to visit
                new_urls = [url for url in content[""links""] 
                          if url not in self.visited 
                          and url not in to_visit]
                to_visit.extend(new_urls)
        
        return results","def crawl(self) -> List[Dict]:
        """"""Crawl website starting from base_url""""""
        to_visit = [self.base_url]
        results = []
        
        while to_visit and len(self.visited) < self.max_pages:
            url = to_visit.pop(0)
            
            if url in self.visited:
                continue
                
            print(f""Crawling: {url}"")
            content = self.extract_page_content(url)
            
            if content:
                self.visited.add(url)
                results.append(content)
                
                # Add new URLs to visit
                new_urls = [url for url in content[""links""] 
                          if url not in self.visited 
                          and url not in to_visit]
                to_visit.extend(new_urls)
        
        return results",Crawl website starting from base_url,Crawl website starting from base_url,"def crawl(self) -> List[Dict]:
        
        to_visit = [self.base_url]
        results = []
        
        while to_visit and len(self.visited) < self.max_pages:
            url = to_visit.pop(0)
            
            if url in self.visited:
                continue
                
            print(f""Crawling: {url}"")
            content = self.extract_page_content(url)
            
            if content:
                self.visited.add(url)
                results.append(content)
                
                # Add new URLs to visit
                new_urls = [url for url in content[""links""] 
                          if url not in self.visited 
                          and url not in to_visit]
                to_visit.extend(new_urls)
        
        return results",Crawl website starting from base_url,"def crawl ( self ) -> List [ Dict ] : to_visit = [ self . base_url ] results = [ ] while to_visit and len ( self . visited ) < self . max_pages : url = to_visit . pop ( 0 ) if url in self . visited : continue print ( f""Crawling: {url}"" ) content = self . extract_page_content ( url ) if content : self . visited . add ( url ) results . append ( content ) # Add new URLs to visit new_urls = [ url for url in content [ ""links"" ] if url not in self . visited and url not in to_visit ] to_visit . extend ( new_urls ) return results",Crawl website starting from base_url
/NLWeb/code/llm/inception.py,clean_response,"def clean_response(cls, content: str) -> Dict[str, Any]:
        """"""
        Strip markdown fences and extract the first JSON object.
        """"""
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            return {}
        return json.loads(match.group(1))","def clean_response(cls, content: str) -> Dict[str, Any]:
        """"""
        Strip markdown fences and extract the first JSON object.
        """"""
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            return {}
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,Strip markdown fences and extract the first JSON object.,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            return {}
        return json.loads(match.group(1))",Strip markdown fences and extract the first JSON object.,"def clean_response ( cls , content : str ) -> Dict [ str , Any ] : cleaned = re . sub ( r""```(?:json)?\s*"" , """" , content ) . strip ( ) match = re . search ( r""(\{.*\})"" , cleaned , re . S ) if not match : return { } return json . loads ( match . group ( 1 ) )",Strip markdown fences and extract the first JSON object.
/Agent-S/gui_agents/s1/mllm/MultimodalAgent.py,replace_message_at,"def replace_message_at(
        self, index, text_content, image_content=None, image_detail=""high""
    ):
        """"""Replace a message at a given index""""""
        if index < len(self.messages):
            self.messages[index] = {
                ""role"": self.messages[index][""role""],
                ""content"": [{""type"": ""text"", ""text"": text_content}],
            }
            if image_content:
                base64_image = self.encode_image(image_content)
                self.messages[index][""content""].append(
                    {
                        ""type"": ""image_url"",
                        ""image_url"": {
                            ""url"": f""data:image/png;base64,{base64_image}"",
                            ""detail"": image_detail,
                        },
                    }
                )","def replace_message_at(
        self, index, text_content, image_content=None, image_detail=""high""
    ):
        """"""Replace a message at a given index""""""
        if index < len(self.messages):
            self.messages[index] = {
                ""role"": self.messages[index][""role""],
                ""content"": [{""type"": ""text"", ""text"": text_content}],
            }
            if image_content:
                base64_image = self.encode_image(image_content)
                self.messages[index][""content""].append(
                    {
                        ""type"": ""image_url"",
                        ""image_url"": {
                            ""url"": f""data:image/png;base64,{base64_image}"",
                            ""detail"": image_detail,
                        },
                    }
                )",Replace a message at a given index,Replace a message at a given index,"def replace_message_at(
        self, index, text_content, image_content=None, image_detail=""high""
    ):
        
        if index < len(self.messages):
            self.messages[index] = {
                ""role"": self.messages[index][""role""],
                ""content"": [{""type"": ""text"", ""text"": text_content}],
            }
            if image_content:
                base64_image = self.encode_image(image_content)
                self.messages[index][""content""].append(
                    {
                        ""type"": ""image_url"",
                        ""image_url"": {
                            ""url"": f""data:image/png;base64,{base64_image}"",
                            ""detail"": image_detail,
                        },
                    }
                )",Replace a message at a given index,"def replace_message_at ( self , index , text_content , image_content = None , image_detail = ""high"" ) : if index < len ( self . messages ) : self . messages [ index ] = { ""role"" : self . messages [ index ] [ ""role"" ] , ""content"" : [ { ""type"" : ""text"" , ""text"" : text_content } ] , } if image_content : base64_image = self . encode_image ( image_content ) self . messages [ index ] [ ""content"" ] . append ( { ""type"" : ""image_url"" , ""image_url"" : { ""url"" : f""data:image/png;base64,{base64_image}"" , ""detail"" : image_detail , } , } )",Replace a message at a given index
/cua/libs/agent/agent/providers/anthropic/tools/edit.py,_make_output,"def _make_output(
        self,
        file_content: str,
        file_descriptor: str,
        init_line: int = 1,
        expand_tabs: bool = True,
    ) -> str:
        """"""Generate output for the CLI based on the content of a file.""""""
        file_content = maybe_truncate(file_content)
        if expand_tabs:
            file_content = file_content.expandtabs()
        file_content = ""\n"".join(
            [f""{i + init_line:6}\t{line}"" for i, line in enumerate(file_content.split(""\n""))]
        )
        return (
            f""Here's the result of running `cat -n` on {file_descriptor}:\n"" + file_content + ""\n""
        )","def _make_output(
        self,
        file_content: str,
        file_descriptor: str,
        init_line: int = 1,
        expand_tabs: bool = True,
    ) -> str:
        """"""Generate output for the CLI based on the content of a file.""""""
        file_content = maybe_truncate(file_content)
        if expand_tabs:
            file_content = file_content.expandtabs()
        file_content = ""\n"".join(
            [f""{i + init_line:6}\t{line}"" for i, line in enumerate(file_content.split(""\n""))]
        )
        return (
            f""Here's the result of running `cat -n` on {file_descriptor}:\n"" + file_content + ""\n""
        )",Generate output for the CLI based on the content of a file.,Generate output for the CLI based on the content of a file.,"def _make_output(
        self,
        file_content: str,
        file_descriptor: str,
        init_line: int = 1,
        expand_tabs: bool = True,
    ) -> str:
        
        file_content = maybe_truncate(file_content)
        if expand_tabs:
            file_content = file_content.expandtabs()
        file_content = ""\n"".join(
            [f""{i + init_line:6}\t{line}"" for i, line in enumerate(file_content.split(""\n""))]
        )
        return (
            f""Here's the result of running `cat -n` on {file_descriptor}:\n"" + file_content + ""\n""
        )",Generate output for the CLI based on the content of a file.,"def _make_output ( self , file_content : str , file_descriptor : str , init_line : int = 1 , expand_tabs : bool = True , ) -> str : file_content = maybe_truncate ( file_content ) if expand_tabs : file_content = file_content . expandtabs ( ) file_content = ""\n"" . join ( [ f""{i + init_line:6}\t{line}"" for i , line in enumerate ( file_content . split ( ""\n"" ) ) ] ) return ( f""Here's the result of running `cat -n` on {file_descriptor}:\n"" + file_content + ""\n"" )",Generate output for the CLI based on the content of a file.
/morphik-core/core/parser/video/parse_video.py,get_transcript_object,"def get_transcript_object(self) -> aai.Transcript:
        """"""
        Get the transcript object from AssemblyAI
        """"""
        logger.info(""Starting video transcription"")
        transcript = self.transcriber.transcribe(self.video_path)
        if transcript.status == ""error"":
            logger.error(f""Transcription failed: {transcript.error}"")
            raise ValueError(f""Transcription failed: {transcript.error}"")
        if not transcript.words:
            logger.warning(""No words found in transcript"")
        logger.info(""Transcription completed successfully!"")

        return transcript","def get_transcript_object(self) -> aai.Transcript:
        """"""
        Get the transcript object from AssemblyAI
        """"""
        logger.info(""Starting video transcription"")
        transcript = self.transcriber.transcribe(self.video_path)
        if transcript.status == ""error"":
            logger.error(f""Transcription failed: {transcript.error}"")
            raise ValueError(f""Transcription failed: {transcript.error}"")
        if not transcript.words:
            logger.warning(""No words found in transcript"")
        logger.info(""Transcription completed successfully!"")

        return transcript",Get the transcript object from AssemblyAI,Get the transcript object from AssemblyAI,"def get_transcript_object(self) -> aai.Transcript:
        
        logger.info(""Starting video transcription"")
        transcript = self.transcriber.transcribe(self.video_path)
        if transcript.status == ""error"":
            logger.error(f""Transcription failed: {transcript.error}"")
            raise ValueError(f""Transcription failed: {transcript.error}"")
        if not transcript.words:
            logger.warning(""No words found in transcript"")
        logger.info(""Transcription completed successfully!"")

        return transcript",Get the transcript object from AssemblyAI,"def get_transcript_object ( self ) -> aai . Transcript : logger . info ( ""Starting video transcription"" ) transcript = self . transcriber . transcribe ( self . video_path ) if transcript . status == ""error"" : logger . error ( f""Transcription failed: {transcript.error}"" ) raise ValueError ( f""Transcription failed: {transcript.error}"" ) if not transcript . words : logger . warning ( ""No words found in transcript"" ) logger . info ( ""Transcription completed successfully!"" ) return transcript",Get the transcript object from AssemblyAI
/ai-hedge-fund/src/utils/progress.py,update_status,"def update_status(self, agent_name: str, ticker: Optional[str] = None, status: str = """", analysis: Optional[str] = None):
        """"""Update the status of an agent.""""""
        if agent_name not in self.agent_status:
            self.agent_status[agent_name] = {""status"": """", ""ticker"": None}

        if ticker:
            self.agent_status[agent_name][""ticker""] = ticker
        if status:
            self.agent_status[agent_name][""status""] = status
        if analysis:
            self.agent_status[agent_name][""analysis""] = analysis
        
        # Set the timestamp as UTC datetime
        timestamp = datetime.now(timezone.utc).isoformat()
        self.agent_status[agent_name][""timestamp""] = timestamp

        # Notify all registered handlers
        for handler in self.update_handlers:
            handler(agent_name, ticker, status, analysis, timestamp)

        self._refresh_display()","def update_status(self, agent_name: str, ticker: Optional[str] = None, status: str = """", analysis: Optional[str] = None):
        """"""Update the status of an agent.""""""
        if agent_name not in self.agent_status:
            self.agent_status[agent_name] = {""status"": """", ""ticker"": None}

        if ticker:
            self.agent_status[agent_name][""ticker""] = ticker
        if status:
            self.agent_status[agent_name][""status""] = status
        if analysis:
            self.agent_status[agent_name][""analysis""] = analysis
        
        # Set the timestamp as UTC datetime
        timestamp = datetime.now(timezone.utc).isoformat()
        self.agent_status[agent_name][""timestamp""] = timestamp

        # Notify all registered handlers
        for handler in self.update_handlers:
            handler(agent_name, ticker, status, analysis, timestamp)

        self._refresh_display()",Update the status of an agent.,Update the status of an agent.,"def update_status(self, agent_name: str, ticker: Optional[str] = None, status: str = """", analysis: Optional[str] = None):
        
        if agent_name not in self.agent_status:
            self.agent_status[agent_name] = {""status"": """", ""ticker"": None}

        if ticker:
            self.agent_status[agent_name][""ticker""] = ticker
        if status:
            self.agent_status[agent_name][""status""] = status
        if analysis:
            self.agent_status[agent_name][""analysis""] = analysis
        
        # Set the timestamp as UTC datetime
        timestamp = datetime.now(timezone.utc).isoformat()
        self.agent_status[agent_name][""timestamp""] = timestamp

        # Notify all registered handlers
        for handler in self.update_handlers:
            handler(agent_name, ticker, status, analysis, timestamp)

        self._refresh_display()",Update the status of an agent.,"def update_status ( self , agent_name : str , ticker : Optional [ str ] = None , status : str = """" , analysis : Optional [ str ] = None ) : if agent_name not in self . agent_status : self . agent_status [ agent_name ] = { ""status"" : """" , ""ticker"" : None } if ticker : self . agent_status [ agent_name ] [ ""ticker"" ] = ticker if status : self . agent_status [ agent_name ] [ ""status"" ] = status if analysis : self . agent_status [ agent_name ] [ ""analysis"" ] = analysis # Set the timestamp as UTC datetime timestamp = datetime . now ( timezone . utc ) . isoformat ( ) self . agent_status [ agent_name ] [ ""timestamp"" ] = timestamp # Notify all registered handlers for handler in self . update_handlers : handler ( agent_name , ticker , status , analysis , timestamp ) self . _refresh_display ( )",Update the status of an agent.
/EasyR1/verl/protocol.py,reorder,"def reorder(self, indices: torch.Tensor) -> None:
        """"""
        Note that this operation is in-place
        """"""
        indices_np = indices.detach().numpy()
        self.batch = self.batch[indices]
        self.non_tensor_batch = {key: value[indices_np] for key, value in self.non_tensor_batch.items()}","def reorder(self, indices: torch.Tensor) -> None:
        """"""
        Note that this operation is in-place
        """"""
        indices_np = indices.detach().numpy()
        self.batch = self.batch[indices]
        self.non_tensor_batch = {key: value[indices_np] for key, value in self.non_tensor_batch.items()}",Note that this operation is in-place,Note that this operation is in-place,"def reorder(self, indices: torch.Tensor) -> None:
        
        indices_np = indices.detach().numpy()
        self.batch = self.batch[indices]
        self.non_tensor_batch = {key: value[indices_np] for key, value in self.non_tensor_batch.items()}",Note that this operation is in-place,"def reorder ( self , indices : torch . Tensor ) -> None : indices_np = indices . detach ( ) . numpy ( ) self . batch = self . batch [ indices ] self . non_tensor_batch = { key : value [ indices_np ] for key , value in self . non_tensor_batch . items ( ) }",Note that this operation is in-place
/preswald/preswald/interfaces/render/registry.py,register_mimetype_component_type,"def register_mimetype_component_type(mimetype: str, component_type: Optional[str] = None):
    """"""
    Register a component type string to handle a given mimetype.

    If no component_type is provided, defaults to 'generic'.
    """"""
    if not re.match(r""^[^/]+/[^/]+$"", mimetype):
        logger.warning(f""[registry] Suspicious mimetype format: {mimetype}"")
    _mimetype_to_component_type[mimetype] = component_type or ""generic""","def register_mimetype_component_type(mimetype: str, component_type: Optional[str] = None):
    """"""
    Register a component type string to handle a given mimetype.

    If no component_type is provided, defaults to 'generic'.
    """"""
    if not re.match(r""^[^/]+/[^/]+$"", mimetype):
        logger.warning(f""[registry] Suspicious mimetype format: {mimetype}"")
    _mimetype_to_component_type[mimetype] = component_type or ""generic""","Register a component type string to handle a given mimetype.

If no component_type is provided, defaults to 'generic'.",Register a component type string to handle a given mimetype.,"def register_mimetype_component_type(mimetype: str, component_type: Optional[str] = None):
    
    if not re.match(r""^[^/]+/[^/]+$"", mimetype):
        logger.warning(f""[registry] Suspicious mimetype format: {mimetype}"")
    _mimetype_to_component_type[mimetype] = component_type or ""generic""",Register a component type string to handle a given mimetype.,"def register_mimetype_component_type ( mimetype : str , component_type : Optional [ str ] = None ) : if not re . match ( r""^[^/]+/[^/]+$"" , mimetype ) : logger . warning ( f""[registry] Suspicious mimetype format: {mimetype}"" ) _mimetype_to_component_type [ mimetype ] = component_type or ""generic""",Register a component type string to handle a given mimetype.
/mcp/src/ecs-mcp-server/awslabs/ecs_mcp_server/api/troubleshooting_tools/get_ecs_troubleshooting_guidance.py,is_ecr_image,"def is_ecr_image(image_uri: str) -> bool:
    """"""Determine if an image is from ECR.""""""
    import re

    try:
        if not (image_uri.startswith(""http://"") or image_uri.startswith(""https://"")):
            parse_uri = urlparse(f""https://{image_uri}"")
        else:
            parse_uri = urlparse(image_uri)

        hostname = parse_uri.netloc.lower()

        # Check for malformed hostnames (double dots, etc.)
        if "".."" in hostname or hostname.startswith(""."") or hostname.endswith("".""):
            return False

        # Ensure the hostname ends with amazonaws.com (proper domain validation)
        if not hostname.endswith("".amazonaws.com""):
            return False

        # Check for proper ECR hostname structure: account-id.dkr.ecr.region.amazonaws.com
        ecr_pattern = r""^\d{12}\.dkr\.ecr\.[a-z0-9-]+\.amazonaws\.com$""

        return bool(re.match(ecr_pattern, hostname))

    except Exception:
        return False","def is_ecr_image(image_uri: str) -> bool:
    """"""Determine if an image is from ECR.""""""
    import re

    try:
        else:
            parse_uri = urlparse(image_uri)

        hostname = parse_uri.netloc.lower()

        # Check for malformed hostnames (double dots, etc.)
        if "".."" in hostname or hostname.startswith(""."") or hostname.endswith("".""):
            return False

        # Ensure the hostname ends with amazonaws.com (proper domain validation)
        if not hostname.endswith("".amazonaws.com""):
            return False

        # Check for proper ECR hostname structure: account-id.dkr.ecr.region.amazonaws.com
        ecr_pattern = r""^\d{12}\.dkr\.ecr\.[a-z0-9-]+\.amazonaws\.com$""

        return bool(re.match(ecr_pattern, hostname))

    except Exception:
        return False",Determine if an image is from ECR.,Determine if an image is from ECR.,"def is_ecr_image(image_uri: str) -> bool:
    
    import re

    try:
        else:
            parse_uri = urlparse(image_uri)

        hostname = parse_uri.netloc.lower()

        # Check for malformed hostnames (double dots, etc.)
        if "".."" in hostname or hostname.startswith(""."") or hostname.endswith("".""):
            return False

        # Ensure the hostname ends with amazonaws.com (proper domain validation)
        if not hostname.endswith("".amazonaws.com""):
            return False

        # Check for proper ECR hostname structure: account-id.dkr.ecr.region.amazonaws.com
        ecr_pattern = r""^\d{12}\.dkr\.ecr\.[a-z0-9-]+\.amazonaws\.com$""

        return bool(re.match(ecr_pattern, hostname))

    except Exception:
        return False",Determine if an image is from ECR.,"def is_ecr_image ( image_uri : str ) -> bool : import re try : else : parse_uri = urlparse ( image_uri ) hostname = parse_uri . netloc . lower ( ) # Check for malformed hostnames (double dots, etc.) if "".."" in hostname or hostname . startswith ( ""."" ) or hostname . endswith ( ""."" ) : return False # Ensure the hostname ends with amazonaws.com (proper domain validation) if not hostname . endswith ( "".amazonaws.com"" ) : return False # Check for proper ECR hostname structure: account-id.dkr.ecr.region.amazonaws.com ecr_pattern = r""^\d{12}\.dkr\.ecr\.[a-z0-9-]+\.amazonaws\.com$"" return bool ( re . match ( ecr_pattern , hostname ) ) except Exception : return False",Determine if an image is from ECR.
/browser-use/examples/use-cases/play_chess.py,parse_transform,"def parse_transform(style: str) -> tuple[float, float] | None:
	""""""Extracts x and y pixel coordinates from a CSS transform string.""""""
	try:
		parts = style.split('(')[1].split(')')[0].split(',')
		x_px_str = float(parts[0].strip().replace('px', ''))
		y_px_str = float(parts[1].strip().replace('px', ''))
		return x_px_str, y_px_str
	except Exception as e:
		logger.error(f'Error parsing transform style: {e}')
		return None, None","def parse_transform(style: str) -> tuple[float, float] | None:
	""""""Extracts x and y pixel coordinates from a CSS transform string.""""""
	try:
		parts = style.split('(')[1].split(')')[0].split(',')
		x_px_str = float(parts[0].strip().replace('px', ''))
		y_px_str = float(parts[1].strip().replace('px', ''))
		return x_px_str, y_px_str
	except Exception as e:
		logger.error(f'Error parsing transform style: {e}')
		return None, None",Extracts x and y pixel coordinates from a CSS transform string.,Extracts x and y pixel coordinates from a CSS transform string.,"def parse_transform(style: str) -> tuple[float, float] | None:
	
	try:
		parts = style.split('(')[1].split(')')[0].split(',')
		x_px_str = float(parts[0].strip().replace('px', ''))
		y_px_str = float(parts[1].strip().replace('px', ''))
		return x_px_str, y_px_str
	except Exception as e:
		logger.error(f'Error parsing transform style: {e}')
		return None, None",Extracts x and y pixel coordinates from a CSS transform string.,"def parse_transform ( style : str ) -> tuple [ float , float ] | None : try : parts = style . split ( '(' ) [ 1 ] . split ( ')' ) [ 0 ] . split ( ',' ) x_px_str = float ( parts [ 0 ] . strip ( ) . replace ( 'px' , '' ) ) y_px_str = float ( parts [ 1 ] . strip ( ) . replace ( 'px' , '' ) ) return x_px_str , y_px_str except Exception as e : logger . error ( f'Error parsing transform style: {e}' ) return None , None",Extracts x and y pixel coordinates from a CSS transform string.
/morphik-core/start_server.py,check_ollama_running,"def check_ollama_running(base_url):
    """"""Check if Ollama is running and accessible at the given URL.""""""
    try:
        api_url = f""{base_url}/api/tags""
        response = requests.get(api_url, timeout=2)
        return response.status_code == 200
    except requests.RequestException:
        return False","def check_ollama_running(base_url):
    """"""Check if Ollama is running and accessible at the given URL.""""""
    try:
        api_url = f""{base_url}/api/tags""
        response = requests.get(api_url, timeout=2)
        return response.status_code == 200
    except requests.RequestException:
        return False",Check if Ollama is running and accessible at the given URL.,Check if Ollama is running and accessible at the given URL.,"def check_ollama_running(base_url):
    
    try:
        api_url = f""{base_url}/api/tags""
        response = requests.get(api_url, timeout=2)
        return response.status_code == 200
    except requests.RequestException:
        return False",Check if Ollama is running and accessible at the given URL.,"def check_ollama_running ( base_url ) : try : api_url = f""{base_url}/api/tags"" response = requests . get ( api_url , timeout = 2 ) return response . status_code == 200 except requests . RequestException : return False",Check if Ollama is running and accessible at the given URL.
/OpenManus-RL/openmanus_rl/agentbench/src/server/tasks/knowledgegraph/api.py,count,"def count(variable: Variable, sparql_executor):
    """"""
    Count the number of a variable
    :param variable: a variable
    :return: the number of a variable
    """"""
    rtn_str = f""Observation: variable ##, which is a number""
    new_variable = Variable(""type.int"", f""(COUNT {variable.program})"")
    return new_variable, rtn_str","def count(variable: Variable, sparql_executor):
    """"""
    Count the number of a variable
    :param variable: a variable
    :return: the number of a variable
    """"""
    rtn_str = f""Observation: variable ##, which is a number""
    new_variable = Variable(""type.int"", f""(COUNT {variable.program})"")
    return new_variable, rtn_str","Count the number of a variable
:param variable: a variable
:return: the number of a variable",Count the number of a variable :param variable: a variable :return: the number of a variable,"def count(variable: Variable, sparql_executor):
    
    rtn_str = f""Observation: variable ##, which is a number""
    new_variable = Variable(""type.int"", f""(COUNT {variable.program})"")
    return new_variable, rtn_str",Count the number of a variable :param variable: a variable :return: the number of a variable,"def count ( variable : Variable , sparql_executor ) : rtn_str = f""Observation: variable ##, which is a number"" new_variable = Variable ( ""type.int"" , f""(COUNT {variable.program})"" ) return new_variable , rtn_str",Count the number of a variable :param variable: a variable :return: the number of a variable
/ag2/notebook/mcp/mcp_wikipedia.py,download_article,"def download_article(title: str) -> str:
    """"""Download a Wikipedia article and store it as a text file.""""""
    try:
        page = wikipedia.page(title)
        file_path = STORAGE_PATH / f""{title.replace(' ', '_')}.txt""
        file_path.write_text(page.content, encoding=""utf-8"")
        return f""Downloaded '{title}' to {file_path.name}""
    except wikipedia.exceptions.DisambiguationError as e:
        return f""Ambiguous title '{title}'. Possible options: {', '.join(e.options[:5])}...""
    except wikipedia.exceptions.PageError:
        return f""Article '{title}' not found.""","def download_article(title: str) -> str:
    """"""Download a Wikipedia article and store it as a text file.""""""
    try:
        page = wikipedia.page(title)
        file_path = STORAGE_PATH / f""{title.replace(' ', '_')}.txt""
        file_path.write_text(page.content, encoding=""utf-8"")
        return f""Downloaded '{title}' to {file_path.name}""
    except wikipedia.exceptions.DisambiguationError as e:
        return f""Ambiguous title '{title}'. Possible options: {', '.join(e.options[:5])}...""
    except wikipedia.exceptions.PageError:
        return f""Article '{title}' not found.""",Download a Wikipedia article and store it as a text file.,Download a Wikipedia article and store it as a text file.,"def download_article(title: str) -> str:
    
    try:
        page = wikipedia.page(title)
        file_path = STORAGE_PATH / f""{title.replace(' ', '_')}.txt""
        file_path.write_text(page.content, encoding=""utf-8"")
        return f""Downloaded '{title}' to {file_path.name}""
    except wikipedia.exceptions.DisambiguationError as e:
        return f""Ambiguous title '{title}'. Possible options: {', '.join(e.options[:5])}...""
    except wikipedia.exceptions.PageError:
        return f""Article '{title}' not found.""",Download a Wikipedia article and store it as a text file.,"def download_article ( title : str ) -> str : try : page = wikipedia . page ( title ) file_path = STORAGE_PATH / f""{title.replace(' ', '_')}.txt"" file_path . write_text ( page . content , encoding = ""utf-8"" ) return f""Downloaded '{title}' to {file_path.name}"" except wikipedia . exceptions . DisambiguationError as e : return f""Ambiguous title '{title}'. Possible options: {', '.join(e.options[:5])}..."" except wikipedia . exceptions . PageError : return f""Article '{title}' not found.""",Download a Wikipedia article and store it as a text file.
/mcp/src/amazon-sns-sqs-mcp-server/awslabs/amazon_sns_sqs_mcp_server/sns.py,is_unsubscribe_allowed,"def is_unsubscribe_allowed(
    mcp: FastMCP, sns_client: Any, kwargs: Dict[str, Any]
) -> Tuple[bool, str]:
    """"""Check if the SNS subscription being unsubscribed is from a tagged topic.""""""
    subscription_arn = kwargs.get('SubscriptionArn')

    if subscription_arn is None or subscription_arn == '':
        return False, 'SubscriptionArn is not passed to the tool'

    try:
        # Get subscription attributes to find the TopicArn
        attributes = sns_client.get_subscription_attributes(SubscriptionArn=subscription_arn)
        topic_arn = attributes.get('Attributes', {}).get('TopicArn')

        return is_mutative_action_allowed(mcp, sns_client, {'TopicArn': topic_arn})

    except Exception as e:
        return False, str(e)","def is_unsubscribe_allowed(
    mcp: FastMCP, sns_client: Any, kwargs: Dict[str, Any]
) -> Tuple[bool, str]:
    """"""Check if the SNS subscription being unsubscribed is from a tagged topic.""""""
    subscription_arn = kwargs.get('SubscriptionArn')

    if subscription_arn is None or subscription_arn == '':
        return False, 'SubscriptionArn is not passed to the tool'

    try:
        # Get subscription attributes to find the TopicArn
        attributes = sns_client.get_subscription_attributes(SubscriptionArn=subscription_arn)
        topic_arn = attributes.get('Attributes', {}).get('TopicArn')

        return is_mutative_action_allowed(mcp, sns_client, {'TopicArn': topic_arn})

    except Exception as e:
        return False, str(e)",Check if the SNS subscription being unsubscribed is from a tagged topic.,Check if the SNS subscription being unsubscribed is from a tagged topic.,"def is_unsubscribe_allowed(
    mcp: FastMCP, sns_client: Any, kwargs: Dict[str, Any]
) -> Tuple[bool, str]:
    
    subscription_arn = kwargs.get('SubscriptionArn')

    if subscription_arn is None or subscription_arn == '':
        return False, 'SubscriptionArn is not passed to the tool'

    try:
        # Get subscription attributes to find the TopicArn
        attributes = sns_client.get_subscription_attributes(SubscriptionArn=subscription_arn)
        topic_arn = attributes.get('Attributes', {}).get('TopicArn')

        return is_mutative_action_allowed(mcp, sns_client, {'TopicArn': topic_arn})

    except Exception as e:
        return False, str(e)",Check if the SNS subscription being unsubscribed is from a tagged topic.,"def is_unsubscribe_allowed ( mcp : FastMCP , sns_client : Any , kwargs : Dict [ str , Any ] ) -> Tuple [ bool , str ] : subscription_arn = kwargs . get ( 'SubscriptionArn' ) if subscription_arn is None or subscription_arn == '' : return False , 'SubscriptionArn is not passed to the tool' try : # Get subscription attributes to find the TopicArn attributes = sns_client . get_subscription_attributes ( SubscriptionArn = subscription_arn ) topic_arn = attributes . get ( 'Attributes' , { } ) . get ( 'TopicArn' ) return is_mutative_action_allowed ( mcp , sns_client , { 'TopicArn' : topic_arn } ) except Exception as e : return False , str ( e )",Check if the SNS subscription being unsubscribed is from a tagged topic.
/airweave/backend/airweave/platform/file_handling/conversion/converters/pdf_converter.py,_cleanup_temp_files,"def _cleanup_temp_files(self, temp_dir: str) -> None:
        """"""Clean up temporary files.""""""
        try:
            for file in os.listdir(temp_dir):
                os.remove(os.path.join(temp_dir, file))
            os.rmdir(temp_dir)
        except Exception as e:
            logger.error(f""Error cleaning up temporary files: {str(e)}"")","def _cleanup_temp_files(self, temp_dir: str) -> None:
        """"""Clean up temporary files.""""""
        try:
            for file in os.listdir(temp_dir):
                os.remove(os.path.join(temp_dir, file))
            os.rmdir(temp_dir)
        except Exception as e:
            logger.error(f""Error cleaning up temporary files: {str(e)}"")",Clean up temporary files.,Clean up temporary files.,"def _cleanup_temp_files(self, temp_dir: str) -> None:
        
        try:
            for file in os.listdir(temp_dir):
                os.remove(os.path.join(temp_dir, file))
            os.rmdir(temp_dir)
        except Exception as e:
            logger.error(f""Error cleaning up temporary files: {str(e)}"")",Clean up temporary files.,"def _cleanup_temp_files ( self , temp_dir : str ) -> None : try : for file in os . listdir ( temp_dir ) : os . remove ( os . path . join ( temp_dir , file ) ) os . rmdir ( temp_dir ) except Exception as e : logger . error ( f""Error cleaning up temporary files: {str(e)}"" )",Clean up temporary files.
/ag2/autogen/oai/anthropic.py,get_usage,"def get_usage(response: ChatCompletion) -> dict:
        """"""Get the usage of tokens and their cost information.""""""
        return {
            ""prompt_tokens"": response.usage.prompt_tokens if response.usage is not None else 0,
            ""completion_tokens"": response.usage.completion_tokens if response.usage is not None else 0,
            ""total_tokens"": response.usage.total_tokens if response.usage is not None else 0,
            ""cost"": response.cost if hasattr(response, ""cost"") else 0.0,
            ""model"": response.model,
        }","def get_usage(response: ChatCompletion) -> dict:
        """"""Get the usage of tokens and their cost information.""""""
        return {
            ""prompt_tokens"": response.usage.prompt_tokens if response.usage is not None else 0,
            ""completion_tokens"": response.usage.completion_tokens if response.usage is not None else 0,
            ""total_tokens"": response.usage.total_tokens if response.usage is not None else 0,
            ""cost"": response.cost if hasattr(response, ""cost"") else 0.0,
            ""model"": response.model,
        }",Get the usage of tokens and their cost information.,Get the usage of tokens and their cost information.,"def get_usage(response: ChatCompletion) -> dict:
        
        return {
            ""prompt_tokens"": response.usage.prompt_tokens if response.usage is not None else 0,
            ""completion_tokens"": response.usage.completion_tokens if response.usage is not None else 0,
            ""total_tokens"": response.usage.total_tokens if response.usage is not None else 0,
            ""cost"": response.cost if hasattr(response, ""cost"") else 0.0,
            ""model"": response.model,
        }",Get the usage of tokens and their cost information.,"def get_usage ( response : ChatCompletion ) -> dict : return { ""prompt_tokens"" : response . usage . prompt_tokens if response . usage is not None else 0 , ""completion_tokens"" : response . usage . completion_tokens if response . usage is not None else 0 , ""total_tokens"" : response . usage . total_tokens if response . usage is not None else 0 , ""cost"" : response . cost if hasattr ( response , ""cost"" ) else 0.0 , ""model"" : response . model , }",Get the usage of tokens and their cost information.
/adk-python/src/google/adk/agents/invocation_context.py,increment_and_enforce_llm_calls_limit,"def increment_and_enforce_llm_calls_limit(
      self, run_config: Optional[RunConfig]
  ):
    """"""Increments _number_of_llm_calls and enforces the limit.""""""
    # We first increment the counter and then check the conditions.
    self._number_of_llm_calls += 1

    if (
        run_config
        and run_config.max_llm_calls > 0
        and self._number_of_llm_calls > run_config.max_llm_calls
    ):
      # We only enforce the limit if the limit is a positive number.
      raise LlmCallsLimitExceededError(
          ""Max number of llm calls limit of""
          f"" `{run_config.max_llm_calls}` exceeded""
      )","def increment_and_enforce_llm_calls_limit(
      self, run_config: Optional[RunConfig]
  ):
    """"""Increments _number_of_llm_calls and enforces the limit.""""""
    # We first increment the counter and then check the conditions.
    self._number_of_llm_calls += 1

    if (
        run_config
        and run_config.max_llm_calls > 0
        and self._number_of_llm_calls > run_config.max_llm_calls
    ):
      # We only enforce the limit if the limit is a positive number.
      raise LlmCallsLimitExceededError(
          ""Max number of llm calls limit of""
          f"" `{run_config.max_llm_calls}` exceeded""
      )",Increments _number_of_llm_calls and enforces the limit.,Increments _number_of_llm_calls and enforces the limit.,"def increment_and_enforce_llm_calls_limit(
      self, run_config: Optional[RunConfig]
  ):
    
    # We first increment the counter and then check the conditions.
    self._number_of_llm_calls += 1

    if (
        run_config
        and run_config.max_llm_calls > 0
        and self._number_of_llm_calls > run_config.max_llm_calls
    ):
      # We only enforce the limit if the limit is a positive number.
      raise LlmCallsLimitExceededError(
          ""Max number of llm calls limit of""
          f"" `{run_config.max_llm_calls}` exceeded""
      )",Increments _number_of_llm_calls and enforces the limit.,"def increment_and_enforce_llm_calls_limit ( self , run_config : Optional [ RunConfig ] ) : # We first increment the counter and then check the conditions. self . _number_of_llm_calls += 1 if ( run_config and run_config . max_llm_calls > 0 and self . _number_of_llm_calls > run_config . max_llm_calls ) : # We only enforce the limit if the limit is a positive number. raise LlmCallsLimitExceededError ( ""Max number of llm calls limit of"" f"" `{run_config.max_llm_calls}` exceeded"" )",Increments _number_of_llm_calls and enforces the limit.
/adk-python/src/google/adk/code_executors/built_in_code_executor.py,process_llm_request,"def process_llm_request(self, llm_request: LlmRequest) -> None:
    """"""Pre-process the LLM request for Gemini 2.0+ models to use the code execution tool.""""""
    if llm_request.model and llm_request.model.startswith(""gemini-2""):
      llm_request.config = llm_request.config or types.GenerateContentConfig()
      llm_request.config.tools = llm_request.config.tools or []
      llm_request.config.tools.append(
          types.Tool(code_execution=types.ToolCodeExecution())
      )
      return
    raise ValueError(
        ""Gemini code execution tool is not supported for model""
        f"" {llm_request.model}""
    )","def process_llm_request(self, llm_request: LlmRequest) -> None:
    """"""Pre-process the LLM request for Gemini 2.0+ models to use the code execution tool.""""""
    if llm_request.model and llm_request.model.startswith(""gemini-2""):
      llm_request.config = llm_request.config or types.GenerateContentConfig()
      llm_request.config.tools = llm_request.config.tools or []
      llm_request.config.tools.append(
          types.Tool(code_execution=types.ToolCodeExecution())
      )
      return
    raise ValueError(
        ""Gemini code execution tool is not supported for model""
        f"" {llm_request.model}""
    )",Pre-process the LLM request for Gemini 2.0+ models to use the code execution tool.,Pre-process the LLM request for Gemini 2.0+ models to use the code execution tool.,"def process_llm_request(self, llm_request: LlmRequest) -> None:
    
    if llm_request.model and llm_request.model.startswith(""gemini-2""):
      llm_request.config = llm_request.config or types.GenerateContentConfig()
      llm_request.config.tools = llm_request.config.tools or []
      llm_request.config.tools.append(
          types.Tool(code_execution=types.ToolCodeExecution())
      )
      return
    raise ValueError(
        ""Gemini code execution tool is not supported for model""
        f"" {llm_request.model}""
    )",Pre-process the LLM request for Gemini 2.0+ models to use the code execution tool.,"def process_llm_request ( self , llm_request : LlmRequest ) -> None : if llm_request . model and llm_request . model . startswith ( ""gemini-2"" ) : llm_request . config = llm_request . config or types . GenerateContentConfig ( ) llm_request . config . tools = llm_request . config . tools or [ ] llm_request . config . tools . append ( types . Tool ( code_execution = types . ToolCodeExecution ( ) ) ) return raise ValueError ( ""Gemini code execution tool is not supported for model"" f"" {llm_request.model}"" )",Pre-process the LLM request for Gemini 2.0+ models to use the code execution tool.
/TinyZero/verl/models/llama/megatron/checkpoint_utils/llama_saver.py,_megatron_calc_global_rank,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    """"""given TP,DP,PP rank to get the global rank.""""""

    args = get_args()
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert (tp_size * dp_size * pp_size == torch.distributed.get_world_size()
           ), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    if args.switch_dp_and_pp_grouping:
        # TP-PP-DP grouping
        return (dp_rank * pp_size + pp_rank) * tp_size + tp_rank
    else:
        # TP-DP-PP grouping
        return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    """"""given TP,DP,PP rank to get the global rank.""""""

    args = get_args()
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert (tp_size * dp_size * pp_size == torch.distributed.get_world_size()
           ), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    if args.switch_dp_and_pp_grouping:
        # TP-PP-DP grouping
        return (dp_rank * pp_size + pp_rank) * tp_size + tp_rank
    else:
        # TP-DP-PP grouping
        return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","given TP,DP,PP rank to get the global rank.","given TP,DP,PP rank to get the global rank.","def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    

    args = get_args()
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert (tp_size * dp_size * pp_size == torch.distributed.get_world_size()
           ), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    if args.switch_dp_and_pp_grouping:
        # TP-PP-DP grouping
        return (dp_rank * pp_size + pp_rank) * tp_size + tp_rank
    else:
        # TP-DP-PP grouping
        return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","given TP,DP,PP rank to get the global rank.","def _megatron_calc_global_rank ( tp_rank : int = 0 , dp_rank : int = 0 , pp_rank : int = 0 ) : args = get_args ( ) tp_size = mpu . get_tensor_model_parallel_world_size ( ) dp_size = mpu . get_data_parallel_world_size ( ) pp_size = mpu . get_pipeline_model_parallel_world_size ( ) assert ( tp_size * dp_size * pp_size == torch . distributed . get_world_size ( ) ) , f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}"" if args . switch_dp_and_pp_grouping : # TP-PP-DP grouping return ( dp_rank * pp_size + pp_rank ) * tp_size + tp_rank else : # TP-DP-PP grouping return ( pp_rank * dp_size + dp_rank ) * tp_size + tp_rank","given TP,DP,PP rank to get the global rank."
/fastmcp/src/fastmcp/utilities/mcp_config.py,infer_transport_type_from_url,"def infer_transport_type_from_url(
    url: str | AnyUrl,
) -> Literal[""streamable-http"", ""sse""]:
    """"""
    Infer the appropriate transport type from the given URL.
    """"""
    url = str(url)
    if not url.startswith(""http""):
        raise ValueError(f""Invalid URL: {url}"")

    parsed_url = urlparse(url)
    path = parsed_url.path

    if ""/sse/"" in path or path.rstrip(""/"").endswith(""/sse""):
        return ""sse""
    else:
        return ""streamable-http""","def infer_transport_type_from_url(
    url: str | AnyUrl,
) -> Literal[""streamable-http"", ""sse""]:
    """"""
    Infer the appropriate transport type from the given URL.
    """"""
    url = str(url)
    if not url.startswith(""http""):
        raise ValueError(f""Invalid URL: {url}"")

    parsed_url = urlparse(url)
    path = parsed_url.path

    if ""/sse/"" in path or path.rstrip(""/"").endswith(""/sse""):
        return ""sse""
    else:
        return ""streamable-http""",Infer the appropriate transport type from the given URL.,Infer the appropriate transport type from the given URL.,"def infer_transport_type_from_url(
    url: str | AnyUrl,
) -> Literal[""streamable-http"", ""sse""]:
    
    url = str(url)
    if not url.startswith(""http""):
        raise ValueError(f""Invalid URL: {url}"")

    parsed_url = urlparse(url)
    path = parsed_url.path

    if ""/sse/"" in path or path.rstrip(""/"").endswith(""/sse""):
        return ""sse""
    else:
        return ""streamable-http""",Infer the appropriate transport type from the given URL.,"def infer_transport_type_from_url ( url : str | AnyUrl , ) -> Literal [ ""streamable-http"" , ""sse"" ] : url = str ( url ) if not url . startswith ( ""http"" ) : raise ValueError ( f""Invalid URL: {url}"" ) parsed_url = urlparse ( url ) path = parsed_url . path if ""/sse/"" in path or path . rstrip ( ""/"" ) . endswith ( ""/sse"" ) : return ""sse"" else : return ""streamable-http""",Infer the appropriate transport type from the given URL.
/nexa-sdk/nexa/gguf/streamlit/streamlit_text_chat.py,load_local_model,"def load_local_model(local_path: str):
    """"""Load local model with default parameters.""""""
    try:
        st.session_state.messages = []
        nexa_model = NexaTextInference(
            model_path=""local_model"",
            local_path=local_path,
            **DEFAULT_PARAMS
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None","def load_local_model(local_path: str):
    """"""Load local model with default parameters.""""""
    try:
        st.session_state.messages = []
        nexa_model = NexaTextInference(
            model_path=""local_model"",
            local_path=local_path,
            **DEFAULT_PARAMS
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None",Load local model with default parameters.,Load local model with default parameters.,"def load_local_model(local_path: str):
    
    try:
        st.session_state.messages = []
        nexa_model = NexaTextInference(
            model_path=""local_model"",
            local_path=local_path,
            **DEFAULT_PARAMS
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None",Load local model with default parameters.,"def load_local_model ( local_path : str ) : try : st . session_state . messages = [ ] nexa_model = NexaTextInference ( model_path = ""local_model"" , local_path = local_path , ** DEFAULT_PARAMS ) # update options after successful local model load update_model_options ( specified_run_type , model_map ) return nexa_model except Exception as e : st . error ( f""Error loading local model: {str(e)}"" ) return None",Load local model with default parameters.
/fastmcp/examples/config_server.py,get_status,"def get_status() -> dict[str, str | bool]:
    """"""Get the current server configuration and status.""""""
    return {
        ""server_name"": server_name,
        ""debug_mode"": args.debug,
        ""original_name"": args.name,
    }","def get_status() -> dict[str, str | bool]:
    """"""Get the current server configuration and status.""""""
    return {
        ""server_name"": server_name,
        ""debug_mode"": args.debug,
        ""original_name"": args.name,
    }",Get the current server configuration and status.,Get the current server configuration and status.,"def get_status() -> dict[str, str | bool]:
    
    return {
        ""server_name"": server_name,
        ""debug_mode"": args.debug,
        ""original_name"": args.name,
    }",Get the current server configuration and status.,"def get_status ( ) -> dict [ str , str | bool ] : return { ""server_name"" : server_name , ""debug_mode"" : args . debug , ""original_name"" : args . name , }",Get the current server configuration and status.
/olmocr/olmocr/train/utils.py,log_trainable_parameters,"def log_trainable_parameters(model: torch.nn.Module, logger: Optional[Logger] = None):
    """"""
    Prints the number of trainable parameters in the model.
    """"""
    trainable_params = 0
    all_param = 0
    for name, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            (logger or get_logger(__name__)).info(f""training with {name}"")
            trainable_params += param.numel()

    (logger or get_logger(__name__)).info(
        ""trainable params: %s || all params: %s || trainable%%: %s"",
        f""{trainable_params:,}"",
        f""{all_param:,}"",
        f""{trainable_params / all_param:.2%}"",
    )","def log_trainable_parameters(model: torch.nn.Module, logger: Optional[Logger] = None):
    """"""
    Prints the number of trainable parameters in the model.
    """"""
    trainable_params = 0
    all_param = 0
    for name, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            (logger or get_logger(__name__)).info(f""training with {name}"")
            trainable_params += param.numel()

    (logger or get_logger(__name__)).info(
        ""trainable params: %s || all params: %s || trainable%%: %s"",
        f""{trainable_params:,}"",
        f""{all_param:,}"",
        f""{trainable_params / all_param:.2%}"",
    )",Prints the number of trainable parameters in the model.,Prints the number of trainable parameters in the model.,"def log_trainable_parameters(model: torch.nn.Module, logger: Optional[Logger] = None):
    
    trainable_params = 0
    all_param = 0
    for name, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            (logger or get_logger(__name__)).info(f""training with {name}"")
            trainable_params += param.numel()

    (logger or get_logger(__name__)).info(
        ""trainable params: %s || all params: %s || trainable%%: %s"",
        f""{trainable_params:,}"",
        f""{all_param:,}"",
        f""{trainable_params / all_param:.2%}"",
    )",Prints the number of trainable parameters in the model.,"def log_trainable_parameters ( model : torch . nn . Module , logger : Optional [ Logger ] = None ) : trainable_params = 0 all_param = 0 for name , param in model . named_parameters ( ) : all_param += param . numel ( ) if param . requires_grad : ( logger or get_logger ( __name__ ) ) . info ( f""training with {name}"" ) trainable_params += param . numel ( ) ( logger or get_logger ( __name__ ) ) . info ( ""trainable params: %s || all params: %s || trainable%%: %s"" , f""{trainable_params:,}"" , f""{all_param:,}"" , f""{trainable_params / all_param:.2%}"" , )",Prints the number of trainable parameters in the model.
/browser-use/browser_use/agent/message_manager/service.py,_log_get_message_emoji,"def _log_get_message_emoji(message_type: str) -> str:
	""""""Get emoji for a message type - used only for logging display""""""
	emoji_map = {
		'HumanMessage': '💬',
		'AIMessage': '🧠',
		'ToolMessage': '🔨',
	}
	return emoji_map.get(message_type, '🎮')","def _log_get_message_emoji(message_type: str) -> str:
	""""""Get emoji for a message type - used only for logging display""""""
	emoji_map = {
		'HumanMessage': '💬',
		'AIMessage': '🧠',
		'ToolMessage': '🔨',
	}
	return emoji_map.get(message_type, '🎮')",Get emoji for a message type - used only for logging display,Get emoji for a message type - used only for logging display,"def _log_get_message_emoji(message_type: str) -> str:
	
	emoji_map = {
		'HumanMessage': '💬',
		'AIMessage': '🧠',
		'ToolMessage': '🔨',
	}
	return emoji_map.get(message_type, '🎮')",Get emoji for a message type - used only for logging display,"def _log_get_message_emoji ( message_type : str ) -> str : emoji_map = { 'HumanMessage' : '💬' , 'AIMessage' : '🧠' , 'ToolMessage' : '🔨' , } return emoji_map . get ( message_type , '🎮' )",Get emoji for a message type - used only for logging display
/cua/examples/agent_examples.py,main,"def main():
    """"""Run the Anthropic agent example.""""""
    try:
        load_dotenv_files()

        # Register signal handler for graceful exit
        signal.signal(signal.SIGINT, handle_sigint)

        asyncio.run(run_agent_example())
    except Exception as e:
        print(f""Error running example: {e}"")
        traceback.print_exc()","def main():
    """"""Run the Anthropic agent example.""""""
    try:
        load_dotenv_files()

        # Register signal handler for graceful exit
        signal.signal(signal.SIGINT, handle_sigint)

        asyncio.run(run_agent_example())
    except Exception as e:
        print(f""Error running example: {e}"")
        traceback.print_exc()",Run the Anthropic agent example.,Run the Anthropic agent example.,"def main():
    
    try:
        load_dotenv_files()

        # Register signal handler for graceful exit
        signal.signal(signal.SIGINT, handle_sigint)

        asyncio.run(run_agent_example())
    except Exception as e:
        print(f""Error running example: {e}"")
        traceback.print_exc()",Run the Anthropic agent example.,"def main ( ) : try : load_dotenv_files ( ) # Register signal handler for graceful exit signal . signal ( signal . SIGINT , handle_sigint ) asyncio . run ( run_agent_example ( ) ) except Exception as e : print ( f""Error running example: {e}"" ) traceback . print_exc ( )",Run the Anthropic agent example.
/PocketFlow/cookbook/pocketflow-a2a/task_manager.py,_get_user_query,"def _get_user_query(self, task_send_params: TaskSendParams) -> str | None:
        """"""Extracts the first text part from the user message.""""""
        if not task_send_params.message or not task_send_params.message.parts:
            logger.warning(f""No message parts found for task {task_send_params.id}"")
            return None
        for part in task_send_params.message.parts:
            # Ensure part is treated as a dictionary if it came from JSON
            part_dict = part if isinstance(part, dict) else part.model_dump()
            if part_dict.get(""type"") == ""text"" and ""text"" in part_dict:
                 return part_dict[""text""]
        logger.warning(f""No text part found in message for task {task_send_params.id}"")
        return None","def _get_user_query(self, task_send_params: TaskSendParams) -> str | None:
        """"""Extracts the first text part from the user message.""""""
        if not task_send_params.message or not task_send_params.message.parts:
            logger.warning(f""No message parts found for task {task_send_params.id}"")
            return None
        for part in task_send_params.message.parts:
            # Ensure part is treated as a dictionary if it came from JSON
            part_dict = part if isinstance(part, dict) else part.model_dump()
            if part_dict.get(""type"") == ""text"" and ""text"" in part_dict:
                 return part_dict[""text""]
        logger.warning(f""No text part found in message for task {task_send_params.id}"")
        return None",Extracts the first text part from the user message.,Extracts the first text part from the user message.,"def _get_user_query(self, task_send_params: TaskSendParams) -> str | None:
        
        if not task_send_params.message or not task_send_params.message.parts:
            logger.warning(f""No message parts found for task {task_send_params.id}"")
            return None
        for part in task_send_params.message.parts:
            # Ensure part is treated as a dictionary if it came from JSON
            part_dict = part if isinstance(part, dict) else part.model_dump()
            if part_dict.get(""type"") == ""text"" and ""text"" in part_dict:
                 return part_dict[""text""]
        logger.warning(f""No text part found in message for task {task_send_params.id}"")
        return None",Extracts the first text part from the user message.,"def _get_user_query ( self , task_send_params : TaskSendParams ) -> str | None : if not task_send_params . message or not task_send_params . message . parts : logger . warning ( f""No message parts found for task {task_send_params.id}"" ) return None for part in task_send_params . message . parts : # Ensure part is treated as a dictionary if it came from JSON part_dict = part if isinstance ( part , dict ) else part . model_dump ( ) if part_dict . get ( ""type"" ) == ""text"" and ""text"" in part_dict : return part_dict [ ""text"" ] logger . warning ( f""No text part found in message for task {task_send_params.id}"" ) return None",Extracts the first text part from the user message.
/ag2/autogen/oai/bedrock.py,calculate_cost,"def calculate_cost(input_tokens: int, output_tokens: int, model_id: str) -> float:
    """"""Calculate the cost of the completion using the Bedrock pricing.""""""
    if model_id in PRICES_PER_K_TOKENS:
        input_cost_per_k, output_cost_per_k = PRICES_PER_K_TOKENS[model_id]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        return input_cost + output_cost
    else:
        warnings.warn(
            f'Cannot get the costs for {model_id}. The cost will be 0. In your config_list, add field {{""price"" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.',
            UserWarning,
        )
        return 0","def calculate_cost(input_tokens: int, output_tokens: int, model_id: str) -> float:
    """"""Calculate the cost of the completion using the Bedrock pricing.""""""
    if model_id in PRICES_PER_K_TOKENS:
        input_cost_per_k, output_cost_per_k = PRICES_PER_K_TOKENS[model_id]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        return input_cost + output_cost
    else:
        warnings.warn(
            f'Cannot get the costs for {model_id}. The cost will be 0. In your config_list, add field {{""price"" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.',
            UserWarning,
        )
        return 0",Calculate the cost of the completion using the Bedrock pricing.,Calculate the cost of the completion using the Bedrock pricing.,"def calculate_cost(input_tokens: int, output_tokens: int, model_id: str) -> float:
    
    if model_id in PRICES_PER_K_TOKENS:
        input_cost_per_k, output_cost_per_k = PRICES_PER_K_TOKENS[model_id]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        return input_cost + output_cost
    else:
        warnings.warn(
            f'Cannot get the costs for {model_id}. The cost will be 0. In your config_list, add field {{""price"" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.',
            UserWarning,
        )
        return 0",Calculate the cost of the completion using the Bedrock pricing.,"def calculate_cost ( input_tokens : int , output_tokens : int , model_id : str ) -> float : if model_id in PRICES_PER_K_TOKENS : input_cost_per_k , output_cost_per_k = PRICES_PER_K_TOKENS [ model_id ] input_cost = ( input_tokens / 1000 ) * input_cost_per_k output_cost = ( output_tokens / 1000 ) * output_cost_per_k return input_cost + output_cost else : warnings . warn ( f'Cannot get the costs for {model_id}. The cost will be 0. In your config_list, add field {{""price"" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.' , UserWarning , ) return 0",Calculate the cost of the completion using the Bedrock pricing.
/smolagents/src/smolagents/gradio_ui.py,get_step_footnote_content,"def get_step_footnote_content(step_log: ActionStep | PlanningStep, step_name: str) -> str:
    """"""Get a footnote string for a step log with duration and token information""""""
    step_footnote = f""**{step_name}**""
    if step_log.token_usage is not None:
        step_footnote += f"" | Input tokens: {step_log.token_usage.input_tokens:,} | Output tokens: {step_log.token_usage.output_tokens:,}""
    step_footnote += f"" | Duration: {round(float(step_log.timing.duration), 2)}s"" if step_log.timing.duration else """"
    step_footnote_content = f""""""<span style=""color: #bbbbc2; font-size: 12px;"">{step_footnote}</span> """"""
    return step_footnote_content","def get_step_footnote_content(step_log: ActionStep | PlanningStep, step_name: str) -> str:
    """"""Get a footnote string for a step log with duration and token information""""""
    step_footnote = f""**{step_name}**""
    if step_log.token_usage is not None:
        step_footnote += f"" | Input tokens: {step_log.token_usage.input_tokens:,} | Output tokens: {step_log.token_usage.output_tokens:,}""
    step_footnote += f"" | Duration: {round(float(step_log.timing.duration), 2)}s"" if step_log.timing.duration else """"
    step_footnote_content = f""""""<span style=""color: #bbbbc2; font-size: 12px;"">{step_footnote}</span> """"""
    return step_footnote_content",Get a footnote string for a step log with duration and token information,Get a footnote string for a step log with duration and token information,"def get_step_footnote_content(step_log: ActionStep | PlanningStep, step_name: str) -> str:
    
    step_footnote = f""**{step_name}**""
    if step_log.token_usage is not None:
        step_footnote += f"" | Input tokens: {step_log.token_usage.input_tokens:,} | Output tokens: {step_log.token_usage.output_tokens:,}""
    step_footnote += f"" | Duration: {round(float(step_log.timing.duration), 2)}s"" if step_log.timing.duration else """"
    step_footnote_content = f
    return step_footnote_content",Get a footnote string for a step log with duration and token information,"def get_step_footnote_content ( step_log : ActionStep | PlanningStep , step_name : str ) -> str : step_footnote = f""**{step_name}**"" if step_log . token_usage is not None : step_footnote += f"" | Input tokens: {step_log.token_usage.input_tokens:,} | Output tokens: {step_log.token_usage.output_tokens:,}"" step_footnote += f"" | Duration: {round(float(step_log.timing.duration), 2)}s"" if step_log . timing . duration else """" step_footnote_content = f return step_footnote_content",Get a footnote string for a step log with duration and token information
/browser-use/browser_use/utils.py,_cancel_interruptible_tasks,"def _cancel_interruptible_tasks(self) -> None:
		""""""Cancel current tasks that should be interruptible.""""""
		current_task = asyncio.current_task(self.loop)
		for task in asyncio.all_tasks(self.loop):
			if task != current_task and not task.done():
				task_name = task.get_name() if hasattr(task, 'get_name') else str(task)
				# Cancel tasks that match certain patterns
				if any(pattern in task_name for pattern in self.interruptible_task_patterns):
					logger.debug(f'Cancelling task: {task_name}')
					task.cancel()
					# Add exception handler to silence ""Task exception was never retrieved"" warnings
					task.add_done_callback(lambda t: t.exception() if t.cancelled() else None)

		# Also cancel the current task if it's interruptible
		if current_task and not current_task.done():
			task_name = current_task.get_name() if hasattr(current_task, 'get_name') else str(current_task)
			if any(pattern in task_name for pattern in self.interruptible_task_patterns):
				logger.debug(f'Cancelling current task: {task_name}')
				current_task.cancel()","def _cancel_interruptible_tasks(self) -> None:
		""""""Cancel current tasks that should be interruptible.""""""
		current_task = asyncio.current_task(self.loop)
		for task in asyncio.all_tasks(self.loop):
			if task != current_task and not task.done():
				task_name = task.get_name() if hasattr(task, 'get_name') else str(task)
				# Cancel tasks that match certain patterns
				if any(pattern in task_name for pattern in self.interruptible_task_patterns):
					logger.debug(f'Cancelling task: {task_name}')
					task.cancel()
					# Add exception handler to silence ""Task exception was never retrieved"" warnings
					task.add_done_callback(lambda t: t.exception() if t.cancelled() else None)

		# Also cancel the current task if it's interruptible
		if current_task and not current_task.done():
			task_name = current_task.get_name() if hasattr(current_task, 'get_name') else str(current_task)
			if any(pattern in task_name for pattern in self.interruptible_task_patterns):
				logger.debug(f'Cancelling current task: {task_name}')
				current_task.cancel()",Cancel current tasks that should be interruptible.,Cancel current tasks that should be interruptible.,"def _cancel_interruptible_tasks(self) -> None:
		
		current_task = asyncio.current_task(self.loop)
		for task in asyncio.all_tasks(self.loop):
			if task != current_task and not task.done():
				task_name = task.get_name() if hasattr(task, 'get_name') else str(task)
				# Cancel tasks that match certain patterns
				if any(pattern in task_name for pattern in self.interruptible_task_patterns):
					logger.debug(f'Cancelling task: {task_name}')
					task.cancel()
					# Add exception handler to silence ""Task exception was never retrieved"" warnings
					task.add_done_callback(lambda t: t.exception() if t.cancelled() else None)

		# Also cancel the current task if it's interruptible
		if current_task and not current_task.done():
			task_name = current_task.get_name() if hasattr(current_task, 'get_name') else str(current_task)
			if any(pattern in task_name for pattern in self.interruptible_task_patterns):
				logger.debug(f'Cancelling current task: {task_name}')
				current_task.cancel()",Cancel current tasks that should be interruptible.,"def _cancel_interruptible_tasks ( self ) -> None : current_task = asyncio . current_task ( self . loop ) for task in asyncio . all_tasks ( self . loop ) : if task != current_task and not task . done ( ) : task_name = task . get_name ( ) if hasattr ( task , 'get_name' ) else str ( task ) # Cancel tasks that match certain patterns if any ( pattern in task_name for pattern in self . interruptible_task_patterns ) : logger . debug ( f'Cancelling task: {task_name}' ) task . cancel ( ) # Add exception handler to silence ""Task exception was never retrieved"" warnings task . add_done_callback ( lambda t : t . exception ( ) if t . cancelled ( ) else None ) # Also cancel the current task if it's interruptible if current_task and not current_task . done ( ) : task_name = current_task . get_name ( ) if hasattr ( current_task , 'get_name' ) else str ( current_task ) if any ( pattern in task_name for pattern in self . interruptible_task_patterns ) : logger . debug ( f'Cancelling current task: {task_name}' ) current_task . cancel ( )",Cancel current tasks that should be interruptible.
/olmocr/olmocr/s3_utils.py,compare_hashes_gcs,"def compare_hashes_gcs(blob, local_file_path: str) -> bool:
    """"""Compare MD5 hashes for GCS blobs.""""""
    if os.path.exists(local_file_path):
        remote_md5_base64 = blob.md5_hash
        hash_md5 = hashlib.md5()
        with open(local_file_path, ""rb"") as f:
            for chunk in iter(lambda: f.read(8192), b""""):
                hash_md5.update(chunk)
        local_md5 = hash_md5.digest()
        remote_md5 = base64.b64decode(remote_md5_base64)
        if remote_md5 == local_md5:
            logger.info(f""File '{local_file_path}' already up-to-date. Skipping download."")
            return False
        else:
            logger.info(f""File '{local_file_path}' differs from GCS. Downloading."")
            return True
    else:
        logger.info(f""File '{local_file_path}' does not exist locally. Downloading."")
        return True","def compare_hashes_gcs(blob, local_file_path: str) -> bool:
    """"""Compare MD5 hashes for GCS blobs.""""""
    if os.path.exists(local_file_path):
        remote_md5_base64 = blob.md5_hash
        hash_md5 = hashlib.md5()
        with open(local_file_path, ""rb"") as f:
            for chunk in iter(lambda: f.read(8192), b""""):
                hash_md5.update(chunk)
        local_md5 = hash_md5.digest()
        remote_md5 = base64.b64decode(remote_md5_base64)
        if remote_md5 == local_md5:
            logger.info(f""File '{local_file_path}' already up-to-date. Skipping download."")
            return False
        else:
            logger.info(f""File '{local_file_path}' differs from GCS. Downloading."")
            return True
    else:
        logger.info(f""File '{local_file_path}' does not exist locally. Downloading."")
        return True",Compare MD5 hashes for GCS blobs.,Compare MD5 hashes for GCS blobs.,"def compare_hashes_gcs(blob, local_file_path: str) -> bool:
    
    if os.path.exists(local_file_path):
        remote_md5_base64 = blob.md5_hash
        hash_md5 = hashlib.md5()
        with open(local_file_path, ""rb"") as f:
            for chunk in iter(lambda: f.read(8192), b""""):
                hash_md5.update(chunk)
        local_md5 = hash_md5.digest()
        remote_md5 = base64.b64decode(remote_md5_base64)
        if remote_md5 == local_md5:
            logger.info(f""File '{local_file_path}' already up-to-date. Skipping download."")
            return False
        else:
            logger.info(f""File '{local_file_path}' differs from GCS. Downloading."")
            return True
    else:
        logger.info(f""File '{local_file_path}' does not exist locally. Downloading."")
        return True",Compare MD5 hashes for GCS blobs.,"def compare_hashes_gcs ( blob , local_file_path : str ) -> bool : if os . path . exists ( local_file_path ) : remote_md5_base64 = blob . md5_hash hash_md5 = hashlib . md5 ( ) with open ( local_file_path , ""rb"" ) as f : for chunk in iter ( lambda : f . read ( 8192 ) , b"""" ) : hash_md5 . update ( chunk ) local_md5 = hash_md5 . digest ( ) remote_md5 = base64 . b64decode ( remote_md5_base64 ) if remote_md5 == local_md5 : logger . info ( f""File '{local_file_path}' already up-to-date. Skipping download."" ) return False else : logger . info ( f""File '{local_file_path}' differs from GCS. Downloading."" ) return True else : logger . info ( f""File '{local_file_path}' does not exist locally. Downloading."" ) return True",Compare MD5 hashes for GCS blobs.
/magentic-ui/src/magentic_ui/_cli.py,setup_llm_logging,"def setup_llm_logging(log_dir: str) -> None:
    """"""Set up logging to capture only LLM calls to a directory.""""""
    log_file = os.path.join(
        log_dir, f""llm_calls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log""
    )
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        ""%(message)s""
    )  # Only log the message since it contains the JSON
    file_handler.setFormatter(formatter)
    file_handler.addFilter(LLMCallFilter())
    for handler in logger_llm.handlers[:]:  # Remove any existing handlers
        logger_llm.removeHandler(handler)
    logger_llm.addHandler(file_handler)","def setup_llm_logging(log_dir: str) -> None:
    """"""Set up logging to capture only LLM calls to a directory.""""""
    log_file = os.path.join(
        log_dir, f""llm_calls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log""
    )
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        ""%(message)s""
    )  # Only log the message since it contains the JSON
    file_handler.setFormatter(formatter)
    file_handler.addFilter(LLMCallFilter())
    for handler in logger_llm.handlers[:]:  # Remove any existing handlers
        logger_llm.removeHandler(handler)
    logger_llm.addHandler(file_handler)",Set up logging to capture only LLM calls to a directory.,Set up logging to capture only LLM calls to a directory.,"def setup_llm_logging(log_dir: str) -> None:
    
    log_file = os.path.join(
        log_dir, f""llm_calls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log""
    )
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        ""%(message)s""
    )  # Only log the message since it contains the JSON
    file_handler.setFormatter(formatter)
    file_handler.addFilter(LLMCallFilter())
    for handler in logger_llm.handlers[:]:  # Remove any existing handlers
        logger_llm.removeHandler(handler)
    logger_llm.addHandler(file_handler)",Set up logging to capture only LLM calls to a directory.,"def setup_llm_logging ( log_dir : str ) -> None : log_file = os . path . join ( log_dir , f""llm_calls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"" ) file_handler = logging . FileHandler ( log_file ) file_handler . setLevel ( logging . INFO ) formatter = logging . Formatter ( ""%(message)s"" ) # Only log the message since it contains the JSON file_handler . setFormatter ( formatter ) file_handler . addFilter ( LLMCallFilter ( ) ) for handler in logger_llm . handlers [ : ] : # Remove any existing handlers logger_llm . removeHandler ( handler ) logger_llm . addHandler ( file_handler )",Set up logging to capture only LLM calls to a directory.
/verl/verl/models/mcore/saver.py,_megatron_calc_global_rank,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0, cp_rank: int = 0, ep_rank: int = 0):
    """"""Calculate global rank with support for CP/EP parallelism""""""

    # Get parallel sizes for each dimension
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    cp_size = mpu.get_context_parallel_world_size()
    # ep_size = mpu.get_expert_model_parallel_world_size()

    # Verify total GPU count matches (must be consistent with parallel_state.py)
    total_size = tp_size * dp_size * pp_size * cp_size
    assert total_size == torch.distributed.get_world_size(), f""{tp_size}x{dp_size}x{pp_size}x{cp_size} != {torch.distributed.get_world_size()}""

    # Core calculation logic (corresponds to RankGenerator order parameter)
    # Assumes default order is ""tp-cp-ep-dp-pp""
    return ((pp_rank * dp_size + dp_rank) * cp_size + cp_rank) * tp_size + tp_rank","def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0, cp_rank: int = 0, ep_rank: int = 0):
    """"""Calculate global rank with support for CP/EP parallelism""""""

    # Get parallel sizes for each dimension
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    cp_size = mpu.get_context_parallel_world_size()
    # ep_size = mpu.get_expert_model_parallel_world_size()

    # Verify total GPU count matches (must be consistent with parallel_state.py)
    total_size = tp_size * dp_size * pp_size * cp_size
    assert total_size == torch.distributed.get_world_size(), f""{tp_size}x{dp_size}x{pp_size}x{cp_size} != {torch.distributed.get_world_size()}""

    # Core calculation logic (corresponds to RankGenerator order parameter)
    # Assumes default order is ""tp-cp-ep-dp-pp""
    return ((pp_rank * dp_size + dp_rank) * cp_size + cp_rank) * tp_size + tp_rank",Calculate global rank with support for CP/EP parallelism,Calculate global rank with support for CP/EP parallelism,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0, cp_rank: int = 0, ep_rank: int = 0):
    

    # Get parallel sizes for each dimension
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    cp_size = mpu.get_context_parallel_world_size()
    # ep_size = mpu.get_expert_model_parallel_world_size()

    # Verify total GPU count matches (must be consistent with parallel_state.py)
    total_size = tp_size * dp_size * pp_size * cp_size
    assert total_size == torch.distributed.get_world_size(), f""{tp_size}x{dp_size}x{pp_size}x{cp_size} != {torch.distributed.get_world_size()}""

    # Core calculation logic (corresponds to RankGenerator order parameter)
    # Assumes default order is ""tp-cp-ep-dp-pp""
    return ((pp_rank * dp_size + dp_rank) * cp_size + cp_rank) * tp_size + tp_rank",Calculate global rank with support for CP/EP parallelism,"def _megatron_calc_global_rank ( tp_rank : int = 0 , dp_rank : int = 0 , pp_rank : int = 0 , cp_rank : int = 0 , ep_rank : int = 0 ) : # Get parallel sizes for each dimension tp_size = mpu . get_tensor_model_parallel_world_size ( ) dp_size = mpu . get_data_parallel_world_size ( ) pp_size = mpu . get_pipeline_model_parallel_world_size ( ) cp_size = mpu . get_context_parallel_world_size ( ) # ep_size = mpu.get_expert_model_parallel_world_size() # Verify total GPU count matches (must be consistent with parallel_state.py) total_size = tp_size * dp_size * pp_size * cp_size assert total_size == torch . distributed . get_world_size ( ) , f""{tp_size}x{dp_size}x{pp_size}x{cp_size} != {torch.distributed.get_world_size()}"" # Core calculation logic (corresponds to RankGenerator order parameter) # Assumes default order is ""tp-cp-ep-dp-pp"" return ( ( pp_rank * dp_size + dp_rank ) * cp_size + cp_rank ) * tp_size + tp_rank",Calculate global rank with support for CP/EP parallelism
/nesa/demo/nesa/backend/hf_models.py,load_model_tokenizer,"def load_model_tokenizer(cls,model_name):
        """"""
        load model and tokenizer using configuration and local files.
        """"""
        model_dir = os.path.join(shared.args.model_dir,model_name)

        tokenizer = AutoTokenizer.from_pretrained(model_dir,
                                                  local_files_only=True)
        
        model = AutoModelForSequenceClassification.from_pretrained(
            model_dir,
            trust_remote_code=True,
            local_files_only=True
        )
        
        print(f""Model '{model_name}' loaded successfully "")
        return tokenizer, model","def load_model_tokenizer(cls,model_name):
        """"""
        load model and tokenizer using configuration and local files.
        """"""
        model_dir = os.path.join(shared.args.model_dir,model_name)

        tokenizer = AutoTokenizer.from_pretrained(model_dir,
                                                  local_files_only=True)
        
        model = AutoModelForSequenceClassification.from_pretrained(
            model_dir,
            trust_remote_code=True,
            local_files_only=True
        )
        
        print(f""Model '{model_name}' loaded successfully "")
        return tokenizer, model",load model and tokenizer using configuration and local files.,load model and tokenizer using configuration and local files.,"def load_model_tokenizer(cls,model_name):
        
        model_dir = os.path.join(shared.args.model_dir,model_name)

        tokenizer = AutoTokenizer.from_pretrained(model_dir,
                                                  local_files_only=True)
        
        model = AutoModelForSequenceClassification.from_pretrained(
            model_dir,
            trust_remote_code=True,
            local_files_only=True
        )
        
        print(f""Model '{model_name}' loaded successfully "")
        return tokenizer, model",load model and tokenizer using configuration and local files.,"def load_model_tokenizer ( cls , model_name ) : model_dir = os . path . join ( shared . args . model_dir , model_name ) tokenizer = AutoTokenizer . from_pretrained ( model_dir , local_files_only = True ) model = AutoModelForSequenceClassification . from_pretrained ( model_dir , trust_remote_code = True , local_files_only = True ) print ( f""Model '{model_name}' loaded successfully "" ) return tokenizer , model",load model and tokenizer using configuration and local files.
/ragaai-catalyst/ragaai_catalyst/tracers/exporters/dynamic_trace_exporter.py,shutdown,"def shutdown(self):
        """"""
        Shutdown the exporter by forwarding to the underlying exporter.
        Before shutting down, update the exporter's properties with the current values.
        """"""
        try:
            # Update the exporter's properties
            self._update_exporter_properties()
        except Exception as e:
            raise Exception(f""Error updating exporter properties: {e}"")

        try:
            # Forward the call to the underlying exporter
            return self._exporter.shutdown()
        except Exception as e:
            raise Exception(f""Error shutting down exporter: {e}"")","def shutdown(self):
        """"""
        Shutdown the exporter by forwarding to the underlying exporter.
        Before shutting down, update the exporter's properties with the current values.
        """"""
        try:
            # Update the exporter's properties
            self._update_exporter_properties()
        except Exception as e:
            raise Exception(f""Error updating exporter properties: {e}"")

        try:
            # Forward the call to the underlying exporter
            return self._exporter.shutdown()
        except Exception as e:
            raise Exception(f""Error shutting down exporter: {e}"")","Shutdown the exporter by forwarding to the underlying exporter.
Before shutting down, update the exporter's properties with the current values.",Shutdown the exporter by forwarding to the underlying exporter.,"def shutdown(self):
        
        try:
            # Update the exporter's properties
            self._update_exporter_properties()
        except Exception as e:
            raise Exception(f""Error updating exporter properties: {e}"")

        try:
            # Forward the call to the underlying exporter
            return self._exporter.shutdown()
        except Exception as e:
            raise Exception(f""Error shutting down exporter: {e}"")",Shutdown the exporter by forwarding to the underlying exporter.,"def shutdown ( self ) : try : # Update the exporter's properties self . _update_exporter_properties ( ) except Exception as e : raise Exception ( f""Error updating exporter properties: {e}"" ) try : # Forward the call to the underlying exporter return self . _exporter . shutdown ( ) except Exception as e : raise Exception ( f""Error shutting down exporter: {e}"" )",Shutdown the exporter by forwarding to the underlying exporter.
/nv-ingest/client/src/nv_ingest_client/primitives/tasks/audio_extraction.py,__str__,"def __str__(self) -> str:
        """"""
        Returns a string with the object's config and run time state
        """"""
        info = """"
        info += ""Audio Extraction Task:\n""

        if self._auth_token:
            info += ""  auth_token: [redacted]\n""
        if self._grpc_endpoint:
            info += f""  grpc_endpoint: {self._grpc_endpoint}\n""
        if self._infer_protocol:
            info += f""  infer_protocol: {self._infer_protocol}\n""
        if self._function_id:
            info += ""  function_id: [redacted]\n""
        if self._use_ssl:
            info += f""  use_ssl: {self._use_ssl}\n""
        if self._ssl_cert:
            info += ""  ssl_cert: [redacted]\n""

        return info","def __str__(self) -> str:
        """"""
        Returns a string with the object's config and run time state
        """"""
        info = """"
        info += ""Audio Extraction Task:\n""

        if self._auth_token:
            info += ""  auth_token: [redacted]\n""
        if self._grpc_endpoint:
            info += f""  grpc_endpoint: {self._grpc_endpoint}\n""
        if self._infer_protocol:
            info += f""  infer_protocol: {self._infer_protocol}\n""
        if self._function_id:
            info += ""  function_id: [redacted]\n""
        if self._use_ssl:
            info += f""  use_ssl: {self._use_ssl}\n""
        if self._ssl_cert:
            info += ""  ssl_cert: [redacted]\n""

        return info",Returns a string with the object's config and run time state,Returns a string with the object's config and run time state,"def __str__(self) -> str:
        
        info = """"
        info += ""Audio Extraction Task:\n""

        if self._auth_token:
            info += ""  auth_token: [redacted]\n""
        if self._grpc_endpoint:
            info += f""  grpc_endpoint: {self._grpc_endpoint}\n""
        if self._infer_protocol:
            info += f""  infer_protocol: {self._infer_protocol}\n""
        if self._function_id:
            info += ""  function_id: [redacted]\n""
        if self._use_ssl:
            info += f""  use_ssl: {self._use_ssl}\n""
        if self._ssl_cert:
            info += ""  ssl_cert: [redacted]\n""

        return info",Returns a string with the object's config and run time state,"def __str__ ( self ) -> str : info = """" info += ""Audio Extraction Task:\n"" if self . _auth_token : info += ""  auth_token: [redacted]\n"" if self . _grpc_endpoint : info += f""  grpc_endpoint: {self._grpc_endpoint}\n"" if self . _infer_protocol : info += f""  infer_protocol: {self._infer_protocol}\n"" if self . _function_id : info += ""  function_id: [redacted]\n"" if self . _use_ssl : info += f""  use_ssl: {self._use_ssl}\n"" if self . _ssl_cert : info += ""  ssl_cert: [redacted]\n"" return info",Returns a string with the object's config and run time state
/adk-python/tests/unittests/tools/openapi_tool/openapi_spec_parser/test_rest_api_tool.py,mock_tool_context,"def mock_tool_context(self):
    """"""Fixture for a mock OperationParser.""""""
    mock_context = MagicMock(spec=ToolContext)
    mock_context.state = State({}, {})
    mock_context.get_auth_response.return_value = {}
    mock_context.request_credential.return_value = {}
    return mock_context","def mock_tool_context(self):
    """"""Fixture for a mock OperationParser.""""""
    mock_context = MagicMock(spec=ToolContext)
    mock_context.state = State({}, {})
    mock_context.get_auth_response.return_value = {}
    mock_context.request_credential.return_value = {}
    return mock_context",Fixture for a mock OperationParser.,Fixture for a mock OperationParser.,"def mock_tool_context(self):
    
    mock_context = MagicMock(spec=ToolContext)
    mock_context.state = State({}, {})
    mock_context.get_auth_response.return_value = {}
    mock_context.request_credential.return_value = {}
    return mock_context",Fixture for a mock OperationParser.,"def mock_tool_context ( self ) : mock_context = MagicMock ( spec = ToolContext ) mock_context . state = State ( { } , { } ) mock_context . get_auth_response . return_value = { } mock_context . request_credential . return_value = { } return mock_context",Fixture for a mock OperationParser.
/Kokoro-FastAPI/examples/assorted_checks/test_voices/trim_voice_dimensions.py,trim_voice_tensor,"def trim_voice_tensor(tensor):
    """"""Trim a 511x1x256 tensor to 510x1x256 by removing the row with least impact.""""""
    if tensor.shape[0] != 511:
        raise ValueError(f""Expected tensor with first dimension 511, got {tensor.shape[0]}"")
    
    # Analyze variance contribution of each row
    variance = analyze_voice_content(tensor)
    
    # Determine which end has lower variance (less information)
    start_var = variance[:5].mean().item()
    end_var = variance[-5:].mean().item()
    
    # Remove from the end with lower variance
    if end_var < start_var:
        logger.info(""Trimming last row (lower variance at end)"")
        return tensor[:-1]
    else:
        logger.info(""Trimming first row (lower variance at start)"")
        return tensor[1:]","def trim_voice_tensor(tensor):
    """"""Trim a 511x1x256 tensor to 510x1x256 by removing the row with least impact.""""""
    if tensor.shape[0] != 511:
        raise ValueError(f""Expected tensor with first dimension 511, got {tensor.shape[0]}"")
    
    # Analyze variance contribution of each row
    variance = analyze_voice_content(tensor)
    
    # Determine which end has lower variance (less information)
    start_var = variance[:5].mean().item()
    end_var = variance[-5:].mean().item()
    
    # Remove from the end with lower variance
    if end_var < start_var:
        logger.info(""Trimming last row (lower variance at end)"")
        return tensor[:-1]
    else:
        logger.info(""Trimming first row (lower variance at start)"")
        return tensor[1:]",Trim a 511x1x256 tensor to 510x1x256 by removing the row with least impact.,Trim a 511x1x256 tensor to 510x1x256 by removing the row with least impact.,"def trim_voice_tensor(tensor):
    
    if tensor.shape[0] != 511:
        raise ValueError(f""Expected tensor with first dimension 511, got {tensor.shape[0]}"")
    
    # Analyze variance contribution of each row
    variance = analyze_voice_content(tensor)
    
    # Determine which end has lower variance (less information)
    start_var = variance[:5].mean().item()
    end_var = variance[-5:].mean().item()
    
    # Remove from the end with lower variance
    if end_var < start_var:
        logger.info(""Trimming last row (lower variance at end)"")
        return tensor[:-1]
    else:
        logger.info(""Trimming first row (lower variance at start)"")
        return tensor[1:]",Trim a 511x1x256 tensor to 510x1x256 by removing the row with least impact.,"def trim_voice_tensor ( tensor ) : if tensor . shape [ 0 ] != 511 : raise ValueError ( f""Expected tensor with first dimension 511, got {tensor.shape[0]}"" ) # Analyze variance contribution of each row variance = analyze_voice_content ( tensor ) # Determine which end has lower variance (less information) start_var = variance [ : 5 ] . mean ( ) . item ( ) end_var = variance [ - 5 : ] . mean ( ) . item ( ) # Remove from the end with lower variance if end_var < start_var : logger . info ( ""Trimming last row (lower variance at end)"" ) return tensor [ : - 1 ] else : logger . info ( ""Trimming first row (lower variance at start)"" ) return tensor [ 1 : ]",Trim a 511x1x256 tensor to 510x1x256 by removing the row with least impact.
/fastmcp/src/fastmcp/client/auth/oauth.py,clear_all,"def clear_all(cls, cache_dir: Path | None = None) -> None:
        """"""Clear all cached data for all servers.""""""
        cache_dir = cache_dir or default_cache_dir()
        if not cache_dir.exists():
            return

        file_types: list[Literal[""client_info"", ""tokens""]] = [""client_info"", ""tokens""]
        for file_type in file_types:
            for file in cache_dir.glob(f""*_{file_type}.json""):
                file.unlink(missing_ok=True)
        logger.info(""Cleared all OAuth client cache data."")","def clear_all(cls, cache_dir: Path | None = None) -> None:
        """"""Clear all cached data for all servers.""""""
        cache_dir = cache_dir or default_cache_dir()
        if not cache_dir.exists():
            return

        file_types: list[Literal[""client_info"", ""tokens""]] = [""client_info"", ""tokens""]
        for file_type in file_types:
            for file in cache_dir.glob(f""*_{file_type}.json""):
                file.unlink(missing_ok=True)
        logger.info(""Cleared all OAuth client cache data."")",Clear all cached data for all servers.,Clear all cached data for all servers.,"def clear_all(cls, cache_dir: Path | None = None) -> None:
        
        cache_dir = cache_dir or default_cache_dir()
        if not cache_dir.exists():
            return

        file_types: list[Literal[""client_info"", ""tokens""]] = [""client_info"", ""tokens""]
        for file_type in file_types:
            for file in cache_dir.glob(f""*_{file_type}.json""):
                file.unlink(missing_ok=True)
        logger.info(""Cleared all OAuth client cache data."")",Clear all cached data for all servers.,"def clear_all ( cls , cache_dir : Path | None = None ) -> None : cache_dir = cache_dir or default_cache_dir ( ) if not cache_dir . exists ( ) : return file_types : list [ Literal [ ""client_info"" , ""tokens"" ] ] = [ ""client_info"" , ""tokens"" ] for file_type in file_types : for file in cache_dir . glob ( f""*_{file_type}.json"" ) : file . unlink ( missing_ok = True ) logger . info ( ""Cleared all OAuth client cache data."" )",Clear all cached data for all servers.
/local-deep-research/examples/benchmarks/run_resumable_parallel_benchmark.py,run_simpleqa_benchmark_wrapper,"def run_simpleqa_benchmark_wrapper(args: Tuple) -> Dict[str, Any]:
    """"""Wrapper for running SimpleQA benchmark in parallel.""""""
    num_examples, output_dir, resume_from, search_config, evaluation_config = args

    logger.info(f""Starting SimpleQA benchmark with {num_examples} examples"")
    start_time = time.time()

    results = run_resumable_benchmark(
        dataset_type=""simpleqa"",
        num_examples=num_examples,
        output_dir=os.path.join(output_dir, ""simpleqa""),
        search_config=search_config,
        evaluation_config=evaluation_config,
        resume_from=resume_from,
    )

    duration = time.time() - start_time
    logger.info(f""SimpleQA benchmark completed in {duration:.1f} seconds"")

    return results","def run_simpleqa_benchmark_wrapper(args: Tuple) -> Dict[str, Any]:
    """"""Wrapper for running SimpleQA benchmark in parallel.""""""
    num_examples, output_dir, resume_from, search_config, evaluation_config = args

    logger.info(f""Starting SimpleQA benchmark with {num_examples} examples"")
    start_time = time.time()

    results = run_resumable_benchmark(
        dataset_type=""simpleqa"",
        num_examples=num_examples,
        output_dir=os.path.join(output_dir, ""simpleqa""),
        search_config=search_config,
        evaluation_config=evaluation_config,
        resume_from=resume_from,
    )

    duration = time.time() - start_time
    logger.info(f""SimpleQA benchmark completed in {duration:.1f} seconds"")

    return results",Wrapper for running SimpleQA benchmark in parallel.,Wrapper for running SimpleQA benchmark in parallel.,"def run_simpleqa_benchmark_wrapper(args: Tuple) -> Dict[str, Any]:
    
    num_examples, output_dir, resume_from, search_config, evaluation_config = args

    logger.info(f""Starting SimpleQA benchmark with {num_examples} examples"")
    start_time = time.time()

    results = run_resumable_benchmark(
        dataset_type=""simpleqa"",
        num_examples=num_examples,
        output_dir=os.path.join(output_dir, ""simpleqa""),
        search_config=search_config,
        evaluation_config=evaluation_config,
        resume_from=resume_from,
    )

    duration = time.time() - start_time
    logger.info(f""SimpleQA benchmark completed in {duration:.1f} seconds"")

    return results",Wrapper for running SimpleQA benchmark in parallel.,"def run_simpleqa_benchmark_wrapper ( args : Tuple ) -> Dict [ str , Any ] : num_examples , output_dir , resume_from , search_config , evaluation_config = args logger . info ( f""Starting SimpleQA benchmark with {num_examples} examples"" ) start_time = time . time ( ) results = run_resumable_benchmark ( dataset_type = ""simpleqa"" , num_examples = num_examples , output_dir = os . path . join ( output_dir , ""simpleqa"" ) , search_config = search_config , evaluation_config = evaluation_config , resume_from = resume_from , ) duration = time . time ( ) - start_time logger . info ( f""SimpleQA benchmark completed in {duration:.1f} seconds"" ) return results",Wrapper for running SimpleQA benchmark in parallel.
/ragaai-catalyst/ragaai_catalyst/redteaming/evaluator.py,_create_input_template,"def _create_input_template(self, input_data: EvaluationInput) -> str:
        """"""Creates the input template for the LLM.""""""
        scenarios_str = ""\n"".join(f""- {scenario}"" for scenario in input_data.scenarios)
        
        return f""""""
### AGENT DESCRIPTION
{input_data.description}

### CONVERSATION
{input_data.conversation.format()}

### SCENARIOS
{scenarios_str}
""""""","def _create_input_template(self, input_data: EvaluationInput) -> str:
        """"""Creates the input template for the LLM.""""""
        scenarios_str = ""\n"".join(f""- {scenario}"" for scenario in input_data.scenarios)
        
        return f""""""
### AGENT DESCRIPTION
{input_data.description}

### CONVERSATION
{input_data.conversation.format()}

### SCENARIOS
{scenarios_str}
""""""",Creates the input template for the LLM.,Creates the input template for the LLM.,"def _create_input_template(self, input_data: EvaluationInput) -> str:
        
        scenarios_str = ""\n"".join(f""- {scenario}"" for scenario in input_data.scenarios)
        
        return f",Creates the input template for the LLM.,"def _create_input_template ( self , input_data : EvaluationInput ) -> str : scenarios_str = ""\n"" . join ( f""- {scenario}"" for scenario in input_data . scenarios ) return f",Creates the input template for the LLM.
/OpenManus-RL/verl/trainer/fsdp_sft_trainer.py,convert_to_regular_types,"def convert_to_regular_types(obj):
    """"""Convert Hydra configs and other special types to regular Python types.""""""
    from omegaconf import ListConfig, DictConfig
    if isinstance(obj, (ListConfig, DictConfig)):
        return {k: convert_to_regular_types(v) for k, v in obj.items()} if isinstance(obj, DictConfig) else list(obj)
    elif isinstance(obj, (list, tuple)):
        return [convert_to_regular_types(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: convert_to_regular_types(v) for k, v in obj.items()}
    return obj","def convert_to_regular_types(obj):
    """"""Convert Hydra configs and other special types to regular Python types.""""""
    from omegaconf import ListConfig, DictConfig
    if isinstance(obj, (ListConfig, DictConfig)):
        return {k: convert_to_regular_types(v) for k, v in obj.items()} if isinstance(obj, DictConfig) else list(obj)
    elif isinstance(obj, (list, tuple)):
        return [convert_to_regular_types(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: convert_to_regular_types(v) for k, v in obj.items()}
    return obj",Convert Hydra configs and other special types to regular Python types.,Convert Hydra configs and other special types to regular Python types.,"def convert_to_regular_types(obj):
    
    from omegaconf import ListConfig, DictConfig
    if isinstance(obj, (ListConfig, DictConfig)):
        return {k: convert_to_regular_types(v) for k, v in obj.items()} if isinstance(obj, DictConfig) else list(obj)
    elif isinstance(obj, (list, tuple)):
        return [convert_to_regular_types(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: convert_to_regular_types(v) for k, v in obj.items()}
    return obj",Convert Hydra configs and other special types to regular Python types.,"def convert_to_regular_types ( obj ) : from omegaconf import ListConfig , DictConfig if isinstance ( obj , ( ListConfig , DictConfig ) ) : return { k : convert_to_regular_types ( v ) for k , v in obj . items ( ) } if isinstance ( obj , DictConfig ) else list ( obj ) elif isinstance ( obj , ( list , tuple ) ) : return [ convert_to_regular_types ( x ) for x in obj ] elif isinstance ( obj , dict ) : return { k : convert_to_regular_types ( v ) for k , v in obj . items ( ) } return obj",Convert Hydra configs and other special types to regular Python types.
/cursor-free-vip/account_manager.py,save_account_info,"def save_account_info(self, email, password, token, total_usage):
        """"""Save account information to file""""""
        try:
            with open(self.accounts_file, 'a', encoding='utf-8') as f:
                f.write(f""\n{'='*50}\n"")
                f.write(f""Email: {email}\n"")
                f.write(f""Password: {password}\n"")
                f.write(f""Token: {token}\n"")
                f.write(f""Usage Limit: {total_usage}\n"")
                f.write(f""{'='*50}\n"")
                
            print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('register.account_info_saved') if self.translator else 'Account information saved'}...{Style.RESET_ALL}"")
            return True
            
        except Exception as e:
            error_msg = self.translator.get('register.save_account_info_failed', error=str(e)) if self.translator else f'Failed to save account information: {str(e)}'
            print(f""{Fore.RED}{EMOJI['ERROR']} {error_msg}{Style.RESET_ALL}"")
            return False","def save_account_info(self, email, password, token, total_usage):
        """"""Save account information to file""""""
        try:
            with open(self.accounts_file, 'a', encoding='utf-8') as f:
                f.write(f""\n{'='*50}\n"")
                f.write(f""Email: {email}\n"")
                f.write(f""Password: {password}\n"")
                f.write(f""Token: {token}\n"")
                f.write(f""Usage Limit: {total_usage}\n"")
                f.write(f""{'='*50}\n"")
                
            print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('register.account_info_saved') if self.translator else 'Account information saved'}...{Style.RESET_ALL}"")
            return True
            
        except Exception as e:
            error_msg = self.translator.get('register.save_account_info_failed', error=str(e)) if self.translator else f'Failed to save account information: {str(e)}'
            print(f""{Fore.RED}{EMOJI['ERROR']} {error_msg}{Style.RESET_ALL}"")
            return False",Save account information to file,Save account information to file,"def save_account_info(self, email, password, token, total_usage):
        
        try:
            with open(self.accounts_file, 'a', encoding='utf-8') as f:
                f.write(f""\n{'='*50}\n"")
                f.write(f""Email: {email}\n"")
                f.write(f""Password: {password}\n"")
                f.write(f""Token: {token}\n"")
                f.write(f""Usage Limit: {total_usage}\n"")
                f.write(f""{'='*50}\n"")
                
            print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('register.account_info_saved') if self.translator else 'Account information saved'}...{Style.RESET_ALL}"")
            return True
            
        except Exception as e:
            error_msg = self.translator.get('register.save_account_info_failed', error=str(e)) if self.translator else f'Failed to save account information: {str(e)}'
            print(f""{Fore.RED}{EMOJI['ERROR']} {error_msg}{Style.RESET_ALL}"")
            return False",Save account information to file,"def save_account_info ( self , email , password , token , total_usage ) : try : with open ( self . accounts_file , 'a' , encoding = 'utf-8' ) as f : f . write ( f""\n{'='*50}\n"" ) f . write ( f""Email: {email}\n"" ) f . write ( f""Password: {password}\n"" ) f . write ( f""Token: {token}\n"" ) f . write ( f""Usage Limit: {total_usage}\n"" ) f . write ( f""{'='*50}\n"" ) print ( f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('register.account_info_saved') if self.translator else 'Account information saved'}...{Style.RESET_ALL}"" ) return True except Exception as e : error_msg = self . translator . get ( 'register.save_account_info_failed' , error = str ( e ) ) if self . translator else f'Failed to save account information: {str(e)}' print ( f""{Fore.RED}{EMOJI['ERROR']} {error_msg}{Style.RESET_ALL}"" ) return False",Save account information to file
/agenticSeek/sources/router.py,load_pipelines,"def load_pipelines(self) -> Dict[str, Type[pipeline]]:
        """"""
        Load the pipelines for the text classification used for routing.
        returns:
            Dict[str, Type[pipeline]]: The loaded pipelines
        """"""
        animate_thinking(""Loading zero-shot pipeline..."", color=""status"")
        return {
            ""bart"": pipeline(""zero-shot-classification"", model=""facebook/bart-large-mnli"")
        }","def load_pipelines(self) -> Dict[str, Type[pipeline]]:
        """"""
        Load the pipelines for the text classification used for routing.
        returns:
            Dict[str, Type[pipeline]]: The loaded pipelines
        """"""
        animate_thinking(""Loading zero-shot pipeline..."", color=""status"")
        return {
            ""bart"": pipeline(""zero-shot-classification"", model=""facebook/bart-large-mnli"")
        }","Load the pipelines for the text classification used for routing.
returns:
    Dict[str, Type[pipeline]]: The loaded pipelines",Load the pipelines for the text classification used for routing.,"def load_pipelines(self) -> Dict[str, Type[pipeline]]:
        
        animate_thinking(""Loading zero-shot pipeline..."", color=""status"")
        return {
            ""bart"": pipeline(""zero-shot-classification"", model=""facebook/bart-large-mnli"")
        }",Load the pipelines for the text classification used for routing.,"def load_pipelines ( self ) -> Dict [ str , Type [ pipeline ] ] : animate_thinking ( ""Loading zero-shot pipeline..."" , color = ""status"" ) return { ""bart"" : pipeline ( ""zero-shot-classification"" , model = ""facebook/bart-large-mnli"" ) }",Load the pipelines for the text classification used for routing.
/NLWeb/code/test_item_details.py,load_environment,"def load_environment():
    """"""Load environment variables from set_keys.sh""""""
    try:
        # Read and execute set_keys.sh
        with open('set_keys.sh', 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if line.startswith('export ') and '=' in line:
                # Parse export statements
                line = line.replace('export ', '')
                key, value = line.split('=', 1)
                # Remove quotes if present
                value = value.strip('""').strip(""'"")
                os.environ[key] = value
                print(f""Loaded: {key}"")
        
        print(""Environment variables loaded successfully"")
    except Exception as e:
        print(f""Error loading environment: {e}"")","def load_environment():
    """"""Load environment variables from set_keys.sh""""""
    try:
        # Read and execute set_keys.sh
        with open('set_keys.sh', 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if line.startswith('export ') and '=' in line:
                # Parse export statements
                line = line.replace('export ', '')
                key, value = line.split('=', 1)
                # Remove quotes if present
                value = value.strip('""').strip(""'"")
                os.environ[key] = value
                print(f""Loaded: {key}"")
        
        print(""Environment variables loaded successfully"")
    except Exception as e:
        print(f""Error loading environment: {e}"")",Load environment variables from set_keys.sh,Load environment variables from set_keys.sh,"def load_environment():
    
    try:
        # Read and execute set_keys.sh
        with open('set_keys.sh', 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if line.startswith('export ') and '=' in line:
                # Parse export statements
                line = line.replace('export ', '')
                key, value = line.split('=', 1)
                # Remove quotes if present
                value = value.strip('""').strip(""'"")
                os.environ[key] = value
                print(f""Loaded: {key}"")
        
        print(""Environment variables loaded successfully"")
    except Exception as e:
        print(f""Error loading environment: {e}"")",Load environment variables from set_keys.sh,"def load_environment ( ) : try : # Read and execute set_keys.sh with open ( 'set_keys.sh' , 'r' ) as f : lines = f . readlines ( ) for line in lines : line = line . strip ( ) if line . startswith ( 'export ' ) and '=' in line : # Parse export statements line = line . replace ( 'export ' , '' ) key , value = line . split ( '=' , 1 ) # Remove quotes if present value = value . strip ( '""' ) . strip ( ""'"" ) os . environ [ key ] = value print ( f""Loaded: {key}"" ) print ( ""Environment variables loaded successfully"" ) except Exception as e : print ( f""Error loading environment: {e}"" )",Load environment variables from set_keys.sh
/mcp-use/mcp_use/managers/tools/list_servers_tool.py,_run,"def _run(self, **kwargs) -> str:
        """"""List all available MCP servers along with their available tools.""""""
        servers = self.server_manager.client.get_server_names()
        if not servers:
            return ""No MCP servers are currently defined.""

        result = ""Available MCP servers:\n""
        for i, server_name in enumerate(servers):
            active_marker = "" (ACTIVE)"" if server_name == self.server_manager.active_server else """"
            result += f""{i + 1}. {server_name}{active_marker}\n""

            tools: list = []
            try:
                # Check cache first
                if server_name in self.server_manager._server_tools:
                    tools = self.server_manager._server_tools[server_name]
                    tool_count = len(tools)
                    result += f""   {tool_count} tools available for this server\n""
            except Exception as e:
                logger.error(f""Unexpected error listing tools for server '{server_name}': {e}"")

        return result","def _run(self, **kwargs) -> str:
        """"""List all available MCP servers along with their available tools.""""""
        servers = self.server_manager.client.get_server_names()
        if not servers:
            return ""No MCP servers are currently defined.""

        result = ""Available MCP servers:\n""
        for i, server_name in enumerate(servers):
            active_marker = "" (ACTIVE)"" if server_name == self.server_manager.active_server else """"
            result += f""{i + 1}. {server_name}{active_marker}\n""

            tools: list = []
            try:
                # Check cache first
                if server_name in self.server_manager._server_tools:
                    tools = self.server_manager._server_tools[server_name]
                    tool_count = len(tools)
                    result += f""   {tool_count} tools available for this server\n""
            except Exception as e:
                logger.error(f""Unexpected error listing tools for server '{server_name}': {e}"")

        return result",List all available MCP servers along with their available tools.,List all available MCP servers along with their available tools.,"def _run(self, **kwargs) -> str:
        
        servers = self.server_manager.client.get_server_names()
        if not servers:
            return ""No MCP servers are currently defined.""

        result = ""Available MCP servers:\n""
        for i, server_name in enumerate(servers):
            active_marker = "" (ACTIVE)"" if server_name == self.server_manager.active_server else """"
            result += f""{i + 1}. {server_name}{active_marker}\n""

            tools: list = []
            try:
                # Check cache first
                if server_name in self.server_manager._server_tools:
                    tools = self.server_manager._server_tools[server_name]
                    tool_count = len(tools)
                    result += f""   {tool_count} tools available for this server\n""
            except Exception as e:
                logger.error(f""Unexpected error listing tools for server '{server_name}': {e}"")

        return result",List all available MCP servers along with their available tools.,"def _run ( self , ** kwargs ) -> str : servers = self . server_manager . client . get_server_names ( ) if not servers : return ""No MCP servers are currently defined."" result = ""Available MCP servers:\n"" for i , server_name in enumerate ( servers ) : active_marker = "" (ACTIVE)"" if server_name == self . server_manager . active_server else """" result += f""{i + 1}. {server_name}{active_marker}\n"" tools : list = [ ] try : # Check cache first if server_name in self . server_manager . _server_tools : tools = self . server_manager . _server_tools [ server_name ] tool_count = len ( tools ) result += f""   {tool_count} tools available for this server\n"" except Exception as e : logger . error ( f""Unexpected error listing tools for server '{server_name}': {e}"" ) return result",List all available MCP servers along with their available tools.
/LightRAG/lightrag/kg/gremlin_impl.py,_convert_properties,"def _convert_properties(properties: Dict[str, Any]) -> str:
        """"""Create chained .property() commands from properties dict""""""
        props = []
        for k, v in properties.items():
            prop_name = GremlinStorage._to_value_map(k)
            props.append(f"".property({prop_name}, {GremlinStorage._to_value_map(v)})"")
        return """".join(props)","def _convert_properties(properties: Dict[str, Any]) -> str:
        """"""Create chained .property() commands from properties dict""""""
        props = []
        for k, v in properties.items():
            prop_name = GremlinStorage._to_value_map(k)
            props.append(f"".property({prop_name}, {GremlinStorage._to_value_map(v)})"")
        return """".join(props)",Create chained .property() commands from properties dict,Create chained .property() commands from properties dict,"def _convert_properties(properties: Dict[str, Any]) -> str:
        
        props = []
        for k, v in properties.items():
            prop_name = GremlinStorage._to_value_map(k)
            props.append(f"".property({prop_name}, {GremlinStorage._to_value_map(v)})"")
        return """".join(props)",Create chained .property() commands from properties dict,"def _convert_properties ( properties : Dict [ str , Any ] ) -> str : props = [ ] for k , v in properties . items ( ) : prop_name = GremlinStorage . _to_value_map ( k ) props . append ( f"".property({prop_name}, {GremlinStorage._to_value_map(v)})"" ) return """" . join ( props )",Create chained .property() commands from properties dict
/ragaai-catalyst/ragaai_catalyst/tracers/agentic_tracing/upload/upload_code.py,update_presigned_url,"def update_presigned_url(presigned_url, base_url):
    """"""Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL.""""""
    #To Do: If Proxy URL has domain name how do we handle such cases? Engineering Dependency.
    
    presigned_parts = urlparse(presigned_url)
    base_parts = urlparse(base_url)
    # Check if base_url contains localhost or an IP address
    if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
        new_netloc = base_parts.hostname  # Extract domain from base_url
        if base_parts.port:  # Add port if present in base_url
            new_netloc += f"":{base_parts.port}""
        updated_parts = presigned_parts._replace(netloc=new_netloc)
        return urlunparse(updated_parts)
    return presigned_url","def update_presigned_url(presigned_url, base_url):
    """"""Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL.""""""
    #To Do: If Proxy URL has domain name how do we handle such cases? Engineering Dependency.
    
    presigned_parts = urlparse(presigned_url)
    base_parts = urlparse(base_url)
    # Check if base_url contains localhost or an IP address
    if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
        new_netloc = base_parts.hostname  # Extract domain from base_url
        if base_parts.port:  # Add port if present in base_url
            new_netloc += f"":{base_parts.port}""
        updated_parts = presigned_parts._replace(netloc=new_netloc)
        return urlunparse(updated_parts)
    return presigned_url","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL.","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL.","def update_presigned_url(presigned_url, base_url):
    
    #To Do: If Proxy URL has domain name how do we handle such cases? Engineering Dependency.
    
    presigned_parts = urlparse(presigned_url)
    base_parts = urlparse(base_url)
    # Check if base_url contains localhost or an IP address
    if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
        new_netloc = base_parts.hostname  # Extract domain from base_url
        if base_parts.port:  # Add port if present in base_url
            new_netloc += f"":{base_parts.port}""
        updated_parts = presigned_parts._replace(netloc=new_netloc)
        return urlunparse(updated_parts)
    return presigned_url","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL.","def update_presigned_url ( presigned_url , base_url ) : #To Do: If Proxy URL has domain name how do we handle such cases? Engineering Dependency. presigned_parts = urlparse ( presigned_url ) base_parts = urlparse ( base_url ) # Check if base_url contains localhost or an IP address if re . match ( r'^(localhost|\d{1,3}(\.\d{1,3}){3})$' , base_parts . hostname ) : new_netloc = base_parts . hostname # Extract domain from base_url if base_parts . port : # Add port if present in base_url new_netloc += f"":{base_parts.port}"" updated_parts = presigned_parts . _replace ( netloc = new_netloc ) return urlunparse ( updated_parts ) return presigned_url","Replaces the domain (and port, if applicable) of the presigned URL with that of the base URL."
/verl/verl/utils/debug/performance.py,_get_current_mem_info,"def _get_current_mem_info(unit: str = ""GB"", precision: int = 2) -> Tuple[str]:
    """"""Get current memory usage.""""""
    assert unit in [""GB"", ""MB"", ""KB""]
    divisor = 1024**3 if unit == ""GB"" else 1024**2 if unit == ""MB"" else 1024
    mem_allocated = get_torch_device().memory_allocated()
    mem_reserved = get_torch_device().memory_reserved()
    # use get_torch_device().mem_get_info to profile device memory
    # since vllm's sleep mode works below pytorch
    # see https://github.com/vllm-project/vllm/pull/11743#issuecomment-2754338119
    mem_free, mem_total = get_torch_device().mem_get_info()
    mem_used = mem_total - mem_free
    mem_allocated = f""{mem_allocated / divisor:.{precision}f}""
    mem_reserved = f""{mem_reserved / divisor:.{precision}f}""
    mem_used = f""{mem_used / divisor:.{precision}f}""
    mem_total = f""{mem_total / divisor:.{precision}f}""
    return mem_allocated, mem_reserved, mem_used, mem_total","def _get_current_mem_info(unit: str = ""GB"", precision: int = 2) -> Tuple[str]:
    """"""Get current memory usage.""""""
    assert unit in [""GB"", ""MB"", ""KB""]
    divisor = 1024**3 if unit == ""GB"" else 1024**2 if unit == ""MB"" else 1024
    mem_allocated = get_torch_device().memory_allocated()
    mem_reserved = get_torch_device().memory_reserved()
    # use get_torch_device().mem_get_info to profile device memory
    # since vllm's sleep mode works below pytorch
    mem_free, mem_total = get_torch_device().mem_get_info()
    mem_used = mem_total - mem_free
    mem_allocated = f""{mem_allocated / divisor:.{precision}f}""
    mem_reserved = f""{mem_reserved / divisor:.{precision}f}""
    mem_used = f""{mem_used / divisor:.{precision}f}""
    mem_total = f""{mem_total / divisor:.{precision}f}""
    return mem_allocated, mem_reserved, mem_used, mem_total",Get current memory usage.,Get current memory usage.,"def _get_current_mem_info(unit: str = ""GB"", precision: int = 2) -> Tuple[str]:
    
    assert unit in [""GB"", ""MB"", ""KB""]
    divisor = 1024**3 if unit == ""GB"" else 1024**2 if unit == ""MB"" else 1024
    mem_allocated = get_torch_device().memory_allocated()
    mem_reserved = get_torch_device().memory_reserved()
    # use get_torch_device().mem_get_info to profile device memory
    # since vllm's sleep mode works below pytorch
    mem_free, mem_total = get_torch_device().mem_get_info()
    mem_used = mem_total - mem_free
    mem_allocated = f""{mem_allocated / divisor:.{precision}f}""
    mem_reserved = f""{mem_reserved / divisor:.{precision}f}""
    mem_used = f""{mem_used / divisor:.{precision}f}""
    mem_total = f""{mem_total / divisor:.{precision}f}""
    return mem_allocated, mem_reserved, mem_used, mem_total",Get current memory usage.,"def _get_current_mem_info ( unit : str = ""GB"" , precision : int = 2 ) -> Tuple [ str ] : assert unit in [ ""GB"" , ""MB"" , ""KB"" ] divisor = 1024 ** 3 if unit == ""GB"" else 1024 ** 2 if unit == ""MB"" else 1024 mem_allocated = get_torch_device ( ) . memory_allocated ( ) mem_reserved = get_torch_device ( ) . memory_reserved ( ) # use get_torch_device().mem_get_info to profile device memory # since vllm's sleep mode works below pytorch mem_free , mem_total = get_torch_device ( ) . mem_get_info ( ) mem_used = mem_total - mem_free mem_allocated = f""{mem_allocated / divisor:.{precision}f}"" mem_reserved = f""{mem_reserved / divisor:.{precision}f}"" mem_used = f""{mem_used / divisor:.{precision}f}"" mem_total = f""{mem_total / divisor:.{precision}f}"" return mem_allocated , mem_reserved , mem_used , mem_total",Get current memory usage.
/nexa-sdk/examples/local_file_organization/text_data_processing.py,summarize_text_content,"def summarize_text_content(text, text_inference):
    """"""Summarize the given text content.""""""
    prompt = f""""""Provide a concise and accurate summary of the following text, focusing on the main ideas and key details.
Limit your summary to a maximum of 150 words.

Text: {text}

Summary:""""""

    response = text_inference.create_completion(prompt)
    summary = response['choices'][0]['text'].strip()
    return summary","def summarize_text_content(text, text_inference):
    """"""Summarize the given text content.""""""
    prompt = f""""""Provide a concise and accurate summary of the following text, focusing on the main ideas and key details.
Limit your summary to a maximum of 150 words.

Text: {text}

Summary:""""""

    response = text_inference.create_completion(prompt)
    summary = response['choices'][0]['text'].strip()
    return summary",Summarize the given text content.,Summarize the given text content.,"def summarize_text_content(text, text_inference):
    
    prompt = f

    response = text_inference.create_completion(prompt)
    summary = response['choices'][0]['text'].strip()
    return summary",Summarize the given text content.,"def summarize_text_content ( text , text_inference ) : prompt = f response = text_inference . create_completion ( prompt ) summary = response [ 'choices' ] [ 0 ] [ 'text' ] . strip ( ) return summary",Summarize the given text content.
/mcp/src/aws-bedrock-data-automation-mcp-server/awslabs/aws_bedrock_data_automation_mcp_server/helpers.py,get_aws_session,"def get_aws_session(region_name=None):
    """"""Create an AWS session using AWS Profile or default credentials.""""""
    profile_name = os.environ.get('AWS_PROFILE')
    region = region_name or get_region()

    if profile_name:
        logger.debug(f'Using AWS profile: {profile_name}')
        return boto3.Session(profile_name=profile_name, region_name=region)
    else:
        logger.debug('Using default AWS credential chain')
        return boto3.Session(region_name=region)","def get_aws_session(region_name=None):
    """"""Create an AWS session using AWS Profile or default credentials.""""""
    profile_name = os.environ.get('AWS_PROFILE')
    region = region_name or get_region()

    if profile_name:
        logger.debug(f'Using AWS profile: {profile_name}')
        return boto3.Session(profile_name=profile_name, region_name=region)
    else:
        logger.debug('Using default AWS credential chain')
        return boto3.Session(region_name=region)",Create an AWS session using AWS Profile or default credentials.,Create an AWS session using AWS Profile or default credentials.,"def get_aws_session(region_name=None):
    
    profile_name = os.environ.get('AWS_PROFILE')
    region = region_name or get_region()

    if profile_name:
        logger.debug(f'Using AWS profile: {profile_name}')
        return boto3.Session(profile_name=profile_name, region_name=region)
    else:
        logger.debug('Using default AWS credential chain')
        return boto3.Session(region_name=region)",Create an AWS session using AWS Profile or default credentials.,"def get_aws_session ( region_name = None ) : profile_name = os . environ . get ( 'AWS_PROFILE' ) region = region_name or get_region ( ) if profile_name : logger . debug ( f'Using AWS profile: {profile_name}' ) return boto3 . Session ( profile_name = profile_name , region_name = region ) else : logger . debug ( 'Using default AWS credential chain' ) return boto3 . Session ( region_name = region )",Create an AWS session using AWS Profile or default credentials.
/fastmcp/examples/smart_home/src/smart_home/lights/hue_utils.py,handle_phue_error,"def handle_phue_error(
    light_or_group: str, operation: str, error: Exception
) -> dict[str, Any]:
    """"""Creates a standardized error response for phue2 operations.""""""
    base_info = {""target"": light_or_group, ""operation"": operation, ""success"": False}
    if isinstance(error, KeyError):
        base_info[""error""] = f""Target '{light_or_group}' not found""
    elif isinstance(error, PhueException):
        base_info[""error""] = f""phue2 error during {operation}: {error}""
    else:
        base_info[""error""] = f""Unexpected error during {operation}: {error}""
    return base_info","def handle_phue_error(
    light_or_group: str, operation: str, error: Exception
) -> dict[str, Any]:
    """"""Creates a standardized error response for phue2 operations.""""""
    base_info = {""target"": light_or_group, ""operation"": operation, ""success"": False}
    if isinstance(error, KeyError):
        base_info[""error""] = f""Target '{light_or_group}' not found""
    elif isinstance(error, PhueException):
        base_info[""error""] = f""phue2 error during {operation}: {error}""
    else:
        base_info[""error""] = f""Unexpected error during {operation}: {error}""
    return base_info",Creates a standardized error response for phue2 operations.,Creates a standardized error response for phue2 operations.,"def handle_phue_error(
    light_or_group: str, operation: str, error: Exception
) -> dict[str, Any]:
    
    base_info = {""target"": light_or_group, ""operation"": operation, ""success"": False}
    if isinstance(error, KeyError):
        base_info[""error""] = f""Target '{light_or_group}' not found""
    elif isinstance(error, PhueException):
        base_info[""error""] = f""phue2 error during {operation}: {error}""
    else:
        base_info[""error""] = f""Unexpected error during {operation}: {error}""
    return base_info",Creates a standardized error response for phue2 operations.,"def handle_phue_error ( light_or_group : str , operation : str , error : Exception ) -> dict [ str , Any ] : base_info = { ""target"" : light_or_group , ""operation"" : operation , ""success"" : False } if isinstance ( error , KeyError ) : base_info [ ""error"" ] = f""Target '{light_or_group}' not found"" elif isinstance ( error , PhueException ) : base_info [ ""error"" ] = f""phue2 error during {operation}: {error}"" else : base_info [ ""error"" ] = f""Unexpected error during {operation}: {error}"" return base_info",Creates a standardized error response for phue2 operations.
/fastmcp/tests/server/http/test_custom_routes.py,server_with_custom_route,"def server_with_custom_route(self):
        """"""Create a FastMCP server with a custom route.""""""
        server = FastMCP()

        @server.custom_route(""/custom-route"", methods=[""GET""])
        async def custom_route(request: Request):
            return JSONResponse({""message"": ""custom route""})

        return server","def server_with_custom_route(self):
        """"""Create a FastMCP server with a custom route.""""""
        server = FastMCP()

        @server.custom_route(""/custom-route"", methods=[""GET""])
        async def custom_route(request: Request):
            return JSONResponse({""message"": ""custom route""})

        return server",Create a FastMCP server with a custom route.,Create a FastMCP server with a custom route.,"def server_with_custom_route(self):
        
        server = FastMCP()

        @server.custom_route(""/custom-route"", methods=[""GET""])
        async def custom_route(request: Request):
            return JSONResponse({""message"": ""custom route""})

        return server",Create a FastMCP server with a custom route.,"def server_with_custom_route ( self ) : server = FastMCP ( ) @ server . custom_route ( ""/custom-route"" , methods = [ ""GET"" ] ) async def custom_route ( request : Request ) : return JSONResponse ( { ""message"" : ""custom route"" } ) return server",Create a FastMCP server with a custom route.
/nv-ingest/client/src/nv_ingest_client/primitives/tasks/caption.py,to_dict,"def to_dict(self) -> Dict:
        """"""
        Convert to a dict for submission to redis
        """"""
        task_properties = {}

        if self._api_key:
            task_properties[""api_key""] = self._api_key

        if self._endpoint_url:
            task_properties[""endpoint_url""] = self._endpoint_url

        if self._prompt:
            task_properties[""prompt""] = self._prompt

        if self._model_name:
            task_properties[""model_name""] = self._model_name

        return {""type"": ""caption"", ""task_properties"": task_properties}","def to_dict(self) -> Dict:
        """"""
        Convert to a dict for submission to redis
        """"""
        task_properties = {}

        if self._api_key:
            task_properties[""api_key""] = self._api_key

        if self._endpoint_url:
            task_properties[""endpoint_url""] = self._endpoint_url

        if self._prompt:
            task_properties[""prompt""] = self._prompt

        if self._model_name:
            task_properties[""model_name""] = self._model_name

        return {""type"": ""caption"", ""task_properties"": task_properties}",Convert to a dict for submission to redis,Convert to a dict for submission to redis,"def to_dict(self) -> Dict:
        
        task_properties = {}

        if self._api_key:
            task_properties[""api_key""] = self._api_key

        if self._endpoint_url:
            task_properties[""endpoint_url""] = self._endpoint_url

        if self._prompt:
            task_properties[""prompt""] = self._prompt

        if self._model_name:
            task_properties[""model_name""] = self._model_name

        return {""type"": ""caption"", ""task_properties"": task_properties}",Convert to a dict for submission to redis,"def to_dict ( self ) -> Dict : task_properties = { } if self . _api_key : task_properties [ ""api_key"" ] = self . _api_key if self . _endpoint_url : task_properties [ ""endpoint_url"" ] = self . _endpoint_url if self . _prompt : task_properties [ ""prompt"" ] = self . _prompt if self . _model_name : task_properties [ ""model_name"" ] = self . _model_name return { ""type"" : ""caption"" , ""task_properties"" : task_properties }",Convert to a dict for submission to redis
/TinyZero/verl/utils/hdfs_io.py,_exists,"def _exists(file_path: str):
    """""" hdfs capable to check whether a file_path is exists """"""
    if file_path.startswith(""hdfs""):
        return _run_cmd(_hdfs_cmd(f""-test -e {file_path}"")) == 0
    return os.path.exists(file_path)","def _exists(file_path: str):
    """""" hdfs capable to check whether a file_path is exists """"""
    if file_path.startswith(""hdfs""):
        return _run_cmd(_hdfs_cmd(f""-test -e {file_path}"")) == 0
    return os.path.exists(file_path)",hdfs capable to check whether a file_path is exists,hdfs capable to check whether a file_path is exists,"def _exists(file_path: str):
    
    if file_path.startswith(""hdfs""):
        return _run_cmd(_hdfs_cmd(f""-test -e {file_path}"")) == 0
    return os.path.exists(file_path)",hdfs capable to check whether a file_path is exists,"def _exists ( file_path : str ) : if file_path . startswith ( ""hdfs"" ) : return _run_cmd ( _hdfs_cmd ( f""-test -e {file_path}"" ) ) == 0 return os . path . exists ( file_path )",hdfs capable to check whether a file_path is exists
/KAG/knext/common/base/runnable.py,invoke,"def invoke(self, input: Input, **kwargs) -> List[Output]:
        """"""Transform an input into an output sequence synchronously.""""""
        raise NotImplementedError(
            f""`invoke` is not currently supported for {self.__class__.__name__}.""
        )","def invoke(self, input: Input, **kwargs) -> List[Output]:
        """"""Transform an input into an output sequence synchronously.""""""
        raise NotImplementedError(
            f""`invoke` is not currently supported for {self.__class__.__name__}.""
        )",Transform an input into an output sequence synchronously.,Transform an input into an output sequence synchronously.,"def invoke(self, input: Input, **kwargs) -> List[Output]:
        
        raise NotImplementedError(
            f""`invoke` is not currently supported for {self.__class__.__name__}.""
        )",Transform an input into an output sequence synchronously.,"def invoke ( self , input : Input , ** kwargs ) -> List [ Output ] : raise NotImplementedError ( f""`invoke` is not currently supported for {self.__class__.__name__}."" )",Transform an input into an output sequence synchronously.
/morphik-core/core/tests/unit/test_colpali_embedding.py,create_sample_chunks,"def create_sample_chunks(num_chunks=2):
    """"""Create sample chunks with base64-encoded images""""""
    chunks = []
    for i in range(num_chunks):
        img_b64 = create_sample_image()
        chunk = Chunk(content=img_b64, metadata={""filename"": f""test_image_{i}.png""})
        chunks.append(chunk)
    return chunks","def create_sample_chunks(num_chunks=2):
    """"""Create sample chunks with base64-encoded images""""""
    chunks = []
    for i in range(num_chunks):
        img_b64 = create_sample_image()
        chunk = Chunk(content=img_b64, metadata={""filename"": f""test_image_{i}.png""})
        chunks.append(chunk)
    return chunks",Create sample chunks with base64-encoded images,Create sample chunks with base64-encoded images,"def create_sample_chunks(num_chunks=2):
    
    chunks = []
    for i in range(num_chunks):
        img_b64 = create_sample_image()
        chunk = Chunk(content=img_b64, metadata={""filename"": f""test_image_{i}.png""})
        chunks.append(chunk)
    return chunks",Create sample chunks with base64-encoded images,"def create_sample_chunks ( num_chunks = 2 ) : chunks = [ ] for i in range ( num_chunks ) : img_b64 = create_sample_image ( ) chunk = Chunk ( content = img_b64 , metadata = { ""filename"" : f""test_image_{i}.png"" } ) chunks . append ( chunk ) return chunks",Create sample chunks with base64-encoded images
/PocketFlow/cookbook/pocketflow-batch-flow/flow.py,prep,"def prep(self, shared):
        """"""Generate parameters for each image-filter combination.""""""
        # List of images to process
        images = [""cat.jpg"", ""dog.jpg"", ""bird.jpg""]
        
        # List of filters to apply
        filters = [""grayscale"", ""blur"", ""sepia""]
        
        # Generate all combinations
        params = []
        for img in images:
            for f in filters:
                params.append({
                    ""input"": img,
                    ""filter"": f
                })
        
        return params","def prep(self, shared):
        """"""Generate parameters for each image-filter combination.""""""
        # List of images to process
        images = [""cat.jpg"", ""dog.jpg"", ""bird.jpg""]
        
        # List of filters to apply
        filters = [""grayscale"", ""blur"", ""sepia""]
        
        # Generate all combinations
        params = []
        for img in images:
            for f in filters:
                params.append({
                    ""input"": img,
                    ""filter"": f
                })
        
        return params",Generate parameters for each image-filter combination.,Generate parameters for each image-filter combination.,"def prep(self, shared):
        
        # List of images to process
        images = [""cat.jpg"", ""dog.jpg"", ""bird.jpg""]
        
        # List of filters to apply
        filters = [""grayscale"", ""blur"", ""sepia""]
        
        # Generate all combinations
        params = []
        for img in images:
            for f in filters:
                params.append({
                    ""input"": img,
                    ""filter"": f
                })
        
        return params",Generate parameters for each image-filter combination.,"def prep ( self , shared ) : # List of images to process images = [ ""cat.jpg"" , ""dog.jpg"" , ""bird.jpg"" ] # List of filters to apply filters = [ ""grayscale"" , ""blur"" , ""sepia"" ] # Generate all combinations params = [ ] for img in images : for f in filters : params . append ( { ""input"" : img , ""filter"" : f } ) return params",Generate parameters for each image-filter combination.
/echomimic_v2/src/models/whisper/whisper/tokenizer.py,language_token,"def language_token(self) -> int:
        """"""Returns the token id corresponding to the value of the `language` field""""""
        if self.language is None:
            raise ValueError(f""This tokenizer does not have language token configured"")

        additional_tokens = dict(
            zip(
                self.tokenizer.additional_special_tokens,
                self.tokenizer.additional_special_tokens_ids,
            )
        )
        candidate = f""<|{self.language}|>""
        if candidate in additional_tokens:
            return additional_tokens[candidate]

        raise KeyError(f""Language {self.language} not found in tokenizer."")","def language_token(self) -> int:
        """"""Returns the token id corresponding to the value of the `language` field""""""
        if self.language is None:
            raise ValueError(f""This tokenizer does not have language token configured"")

        additional_tokens = dict(
            zip(
                self.tokenizer.additional_special_tokens,
                self.tokenizer.additional_special_tokens_ids,
            )
        )
        candidate = f""<|{self.language}|>""
        if candidate in additional_tokens:
            return additional_tokens[candidate]

        raise KeyError(f""Language {self.language} not found in tokenizer."")",Returns the token id corresponding to the value of the `language` field,Returns the token id corresponding to the value of the `language` field,"def language_token(self) -> int:
        
        if self.language is None:
            raise ValueError(f""This tokenizer does not have language token configured"")

        additional_tokens = dict(
            zip(
                self.tokenizer.additional_special_tokens,
                self.tokenizer.additional_special_tokens_ids,
            )
        )
        candidate = f""<|{self.language}|>""
        if candidate in additional_tokens:
            return additional_tokens[candidate]

        raise KeyError(f""Language {self.language} not found in tokenizer."")",Returns the token id corresponding to the value of the `language` field,"def language_token ( self ) -> int : if self . language is None : raise ValueError ( f""This tokenizer does not have language token configured"" ) additional_tokens = dict ( zip ( self . tokenizer . additional_special_tokens , self . tokenizer . additional_special_tokens_ids , ) ) candidate = f""<|{self.language}|>"" if candidate in additional_tokens : return additional_tokens [ candidate ] raise KeyError ( f""Language {self.language} not found in tokenizer."" )",Returns the token id corresponding to the value of the `language` field
/local-deep-research/src/local_deep_research/advanced_search_system/strategies/source_based_strategy.py,_format_search_results_as_context,"def _format_search_results_as_context(self, search_results):
        """"""Format search results into context for question generation.""""""
        context_snippets = []

        for i, result in enumerate(
            search_results[:10]
        ):  # Limit to prevent context overflow
            title = result.get(""title"", ""Untitled"")
            snippet = result.get(""snippet"", """")
            url = result.get(""link"", """")

            if snippet:
                context_snippets.append(
                    f""Source {i + 1}: {title}\nURL: {url}\nSnippet: {snippet}""
                )

        return ""\n\n"".join(context_snippets)","def _format_search_results_as_context(self, search_results):
        """"""Format search results into context for question generation.""""""
        context_snippets = []

        for i, result in enumerate(
            search_results[:10]
        ):  # Limit to prevent context overflow
            title = result.get(""title"", ""Untitled"")
            snippet = result.get(""snippet"", """")
            url = result.get(""link"", """")

            if snippet:
                context_snippets.append(
                    f""Source {i + 1}: {title}\nURL: {url}\nSnippet: {snippet}""
                )

        return ""\n\n"".join(context_snippets)",Format search results into context for question generation.,Format search results into context for question generation.,"def _format_search_results_as_context(self, search_results):
        
        context_snippets = []

        for i, result in enumerate(
            search_results[:10]
        ):  # Limit to prevent context overflow
            title = result.get(""title"", ""Untitled"")
            snippet = result.get(""snippet"", """")
            url = result.get(""link"", """")

            if snippet:
                context_snippets.append(
                    f""Source {i + 1}: {title}\nURL: {url}\nSnippet: {snippet}""
                )

        return ""\n\n"".join(context_snippets)",Format search results into context for question generation.,"def _format_search_results_as_context ( self , search_results ) : context_snippets = [ ] for i , result in enumerate ( search_results [ : 10 ] ) : # Limit to prevent context overflow title = result . get ( ""title"" , ""Untitled"" ) snippet = result . get ( ""snippet"" , """" ) url = result . get ( ""link"" , """" ) if snippet : context_snippets . append ( f""Source {i + 1}: {title}\nURL: {url}\nSnippet: {snippet}"" ) return ""\n\n"" . join ( context_snippets )",Format search results into context for question generation.
/nv-ingest/client/client_tests/client/test_client.py,tasks,"def tasks():
    """"""Fixture for common tasks.""""""
    return {
        ""split"": SplitTask(),
        ""extract_pdf"": ExtractTask(document_type=""pdf""),
    }","def tasks():
    """"""Fixture for common tasks.""""""
    return {
        ""split"": SplitTask(),
        ""extract_pdf"": ExtractTask(document_type=""pdf""),
    }",Fixture for common tasks.,Fixture for common tasks.,"def tasks():
    
    return {
        ""split"": SplitTask(),
        ""extract_pdf"": ExtractTask(document_type=""pdf""),
    }",Fixture for common tasks.,"def tasks ( ) : return { ""split"" : SplitTask ( ) , ""extract_pdf"" : ExtractTask ( document_type = ""pdf"" ) , }",Fixture for common tasks.
/adk-samples/python/agents/fomc-research/fomc_research/shared_libraries/file_utils.py,create_html_redline,"def create_html_redline(text1: str, text2: str) -> str:
    """"""Creates an HTML redline doc of differences between text1 and text2.""""""
    d = dmp.diff_match_patch()
    diffs = d.diff_main(text2, text1)
    d.diff_cleanupSemantic(diffs)

    html_output = """"
    for op, text in diffs:
        if op == -1:  # Deletion
            html_output += (
                f'<del style=""background-color: #ffcccc;"">{text}</del>'
            )
        elif op == 1:  # Insertion
            html_output += (
                f'<ins style=""background-color: #ccffcc;"">{text}</ins>'
            )
        else:  # Unchanged
            html_output += text

    return html_output","def create_html_redline(text1: str, text2: str) -> str:
    """"""Creates an HTML redline doc of differences between text1 and text2.""""""
    d = dmp.diff_match_patch()
    diffs = d.diff_main(text2, text1)
    d.diff_cleanupSemantic(diffs)

    html_output = """"
    for op, text in diffs:
        if op == -1:  # Deletion
            html_output += (
                f'<del style=""background-color: #ffcccc;"">{text}</del>'
            )
        elif op == 1:  # Insertion
            html_output += (
                f'<ins style=""background-color: #ccffcc;"">{text}</ins>'
            )
        else:  # Unchanged
            html_output += text

    return html_output",Creates an HTML redline doc of differences between text1 and text2.,Creates an HTML redline doc of differences between text1 and text2.,"def create_html_redline(text1: str, text2: str) -> str:
    
    d = dmp.diff_match_patch()
    diffs = d.diff_main(text2, text1)
    d.diff_cleanupSemantic(diffs)

    html_output = """"
    for op, text in diffs:
        if op == -1:  # Deletion
            html_output += (
                f'<del style=""background-color: #ffcccc;"">{text}</del>'
            )
        elif op == 1:  # Insertion
            html_output += (
                f'<ins style=""background-color: #ccffcc;"">{text}</ins>'
            )
        else:  # Unchanged
            html_output += text

    return html_output",Creates an HTML redline doc of differences between text1 and text2.,"def create_html_redline ( text1 : str , text2 : str ) -> str : d = dmp . diff_match_patch ( ) diffs = d . diff_main ( text2 , text1 ) d . diff_cleanupSemantic ( diffs ) html_output = """" for op , text in diffs : if op == - 1 : # Deletion html_output += ( f'<del style=""background-color: #ffcccc;"">{text}</del>' ) elif op == 1 : # Insertion html_output += ( f'<ins style=""background-color: #ccffcc;"">{text}</ins>' ) else : # Unchanged html_output += text return html_output",Creates an HTML redline doc of differences between text1 and text2.
/verl/verl/workers/actor/dp_actor.py,__init__,"def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):
        """"""When optimizer is None, it is Reference Policy""""""
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer

        self.use_remove_padding = self.config.get(""use_remove_padding"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_remove_padding={self.use_remove_padding}"")
        self.use_fused_kernels = self.config.get(""use_fused_kernels"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_fused_kernels={self.use_fused_kernels}"")

        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = (
            torch.compile(verl_F.entropy_from_logits, dynamic=True)
            if self.config.get(""use_torch_compile"", True)  #  use torch compile by default
            else verl_F.entropy_from_logits
        )
        self.device_name = get_device_name()","def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):
        """"""When optimizer is None, it is Reference Policy""""""
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer

        self.use_remove_padding = self.config.get(""use_remove_padding"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_remove_padding={self.use_remove_padding}"")
        self.use_fused_kernels = self.config.get(""use_fused_kernels"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_fused_kernels={self.use_fused_kernels}"")

        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = (
            torch.compile(verl_F.entropy_from_logits, dynamic=True)
            if self.config.get(""use_torch_compile"", True)  #  use torch compile by default
            else verl_F.entropy_from_logits
        )
        self.device_name = get_device_name()","When optimizer is None, it is Reference Policy","When optimizer is None, it is Reference Policy","def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):
        
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer

        self.use_remove_padding = self.config.get(""use_remove_padding"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_remove_padding={self.use_remove_padding}"")
        self.use_fused_kernels = self.config.get(""use_fused_kernels"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_fused_kernels={self.use_fused_kernels}"")

        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = (
            torch.compile(verl_F.entropy_from_logits, dynamic=True)
            if self.config.get(""use_torch_compile"", True)  #  use torch compile by default
            else verl_F.entropy_from_logits
        )
        self.device_name = get_device_name()","When optimizer is None, it is Reference Policy","def __init__ ( self , config , actor_module : nn . Module , actor_optimizer : torch . optim . Optimizer = None ) : super ( ) . __init__ ( config ) self . actor_module = actor_module self . actor_optimizer = actor_optimizer self . use_remove_padding = self . config . get ( ""use_remove_padding"" , False ) if torch . distributed . get_rank ( ) == 0 : print ( f""Actor use_remove_padding={self.use_remove_padding}"" ) self . use_fused_kernels = self . config . get ( ""use_fused_kernels"" , False ) if torch . distributed . get_rank ( ) == 0 : print ( f""Actor use_fused_kernels={self.use_fused_kernels}"" ) self . ulysses_sequence_parallel_size = self . config . ulysses_sequence_parallel_size self . use_ulysses_sp = self . ulysses_sequence_parallel_size > 1 self . compute_entropy_from_logits = ( torch . compile ( verl_F . entropy_from_logits , dynamic = True ) if self . config . get ( ""use_torch_compile"" , True ) #  use torch compile by default else verl_F . entropy_from_logits ) self . device_name = get_device_name ( )","When optimizer is None, it is Reference Policy"
/Kokoro-FastAPI/examples/assorted_checks/test_combinations/test_analyze_combined_voices.py,generate_and_save_audio,"def generate_and_save_audio(voice: str, output_path: str):
    """"""Generate audio using specified voice and save to WAV file.""""""
    print(f""\nGenerating audio for voice: {voice}"")
    start_time = time.time()
    
    # Generate audio using streaming response
    with client.audio.speech.with_streaming_response.create(
        model=""kokoro"",
        voice=voice,
        response_format=""wav"",
        input=text,
    ) as response:
        # Save the audio stream to file
        with open(output_path, ""wb"") as f:
            for chunk in response.iter_bytes():
                f.write(chunk)
    
    duration = time.time() - start_time
    print(f""Generated in {duration:.2f}s"")
    print(f""Saved to {output_path}"")
    return output_path","def generate_and_save_audio(voice: str, output_path: str):
    """"""Generate audio using specified voice and save to WAV file.""""""
    print(f""\nGenerating audio for voice: {voice}"")
    start_time = time.time()
    
    # Generate audio using streaming response
    with client.audio.speech.with_streaming_response.create(
        model=""kokoro"",
        voice=voice,
        response_format=""wav"",
        input=text,
    ) as response:
        # Save the audio stream to file
        with open(output_path, ""wb"") as f:
            for chunk in response.iter_bytes():
                f.write(chunk)
    
    duration = time.time() - start_time
    print(f""Generated in {duration:.2f}s"")
    print(f""Saved to {output_path}"")
    return output_path",Generate audio using specified voice and save to WAV file.,Generate audio using specified voice and save to WAV file.,"def generate_and_save_audio(voice: str, output_path: str):
    
    print(f""\nGenerating audio for voice: {voice}"")
    start_time = time.time()
    
    # Generate audio using streaming response
    with client.audio.speech.with_streaming_response.create(
        model=""kokoro"",
        voice=voice,
        response_format=""wav"",
        input=text,
    ) as response:
        # Save the audio stream to file
        with open(output_path, ""wb"") as f:
            for chunk in response.iter_bytes():
                f.write(chunk)
    
    duration = time.time() - start_time
    print(f""Generated in {duration:.2f}s"")
    print(f""Saved to {output_path}"")
    return output_path",Generate audio using specified voice and save to WAV file.,"def generate_and_save_audio ( voice : str , output_path : str ) : print ( f""\nGenerating audio for voice: {voice}"" ) start_time = time . time ( ) # Generate audio using streaming response with client . audio . speech . with_streaming_response . create ( model = ""kokoro"" , voice = voice , response_format = ""wav"" , input = text , ) as response : # Save the audio stream to file with open ( output_path , ""wb"" ) as f : for chunk in response . iter_bytes ( ) : f . write ( chunk ) duration = time . time ( ) - start_time print ( f""Generated in {duration:.2f}s"" ) print ( f""Saved to {output_path}"" ) return output_path",Generate audio using specified voice and save to WAV file.
/python-sdk/examples/fastmcp/parameter_descriptions.py,greet_user,"def greet_user(
    name: str = Field(description=""The name of the person to greet""),
    title: str = Field(description=""Optional title like Mr/Ms/Dr"", default=""""),
    times: int = Field(description=""Number of times to repeat the greeting"", default=1),
) -> str:
    """"""Greet a user with optional title and repetition""""""
    greeting = f""Hello {title + ' ' if title else ''}{name}!""
    return ""\n"".join([greeting] * times)","def greet_user(
    name: str = Field(description=""The name of the person to greet""),
    title: str = Field(description=""Optional title like Mr/Ms/Dr"", default=""""),
    times: int = Field(description=""Number of times to repeat the greeting"", default=1),
) -> str:
    """"""Greet a user with optional title and repetition""""""
    greeting = f""Hello {title + ' ' if title else ''}{name}!""
    return ""\n"".join([greeting] * times)",Greet a user with optional title and repetition,Greet a user with optional title and repetition,"def greet_user(
    name: str = Field(description=""The name of the person to greet""),
    title: str = Field(description=""Optional title like Mr/Ms/Dr"", default=""""),
    times: int = Field(description=""Number of times to repeat the greeting"", default=1),
) -> str:
    
    greeting = f""Hello {title + ' ' if title else ''}{name}!""
    return ""\n"".join([greeting] * times)",Greet a user with optional title and repetition,"def greet_user ( name : str = Field ( description = ""The name of the person to greet"" ) , title : str = Field ( description = ""Optional title like Mr/Ms/Dr"" , default = """" ) , times : int = Field ( description = ""Number of times to repeat the greeting"" , default = 1 ) , ) -> str : greeting = f""Hello {title + ' ' if title else ''}{name}!"" return ""\n"" . join ( [ greeting ] * times )",Greet a user with optional title and repetition
/OpenManus-RL/verl/utils/dataset/rl_dataset.py,collate_fn,"def collate_fn(data_list: list[dict]) -> dict:
    """"""Collate a batch of data.""""""
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)

    for data in data_list:
        for key, val in data.items():
            if isinstance(val, torch.Tensor):
                tensors[key].append(val)
            else:
                non_tensors[key].append(val)

    for key, val in tensors.items():
        tensors[key] = torch.stack(val, dim=0)

    for key, val in non_tensors.items():
        non_tensors[key] = np.array(val, dtype=object)

    return {**tensors, **non_tensors}","def collate_fn(data_list: list[dict]) -> dict:
    """"""Collate a batch of data.""""""
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)

    for data in data_list:
        for key, val in data.items():
            if isinstance(val, torch.Tensor):
                tensors[key].append(val)
            else:
                non_tensors[key].append(val)

    for key, val in tensors.items():
        tensors[key] = torch.stack(val, dim=0)

    for key, val in non_tensors.items():
        non_tensors[key] = np.array(val, dtype=object)

    return {**tensors, **non_tensors}",Collate a batch of data.,Collate a batch of data.,"def collate_fn(data_list: list[dict]) -> dict:
    
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)

    for data in data_list:
        for key, val in data.items():
            if isinstance(val, torch.Tensor):
                tensors[key].append(val)
            else:
                non_tensors[key].append(val)

    for key, val in tensors.items():
        tensors[key] = torch.stack(val, dim=0)

    for key, val in non_tensors.items():
        non_tensors[key] = np.array(val, dtype=object)

    return {**tensors, **non_tensors}",Collate a batch of data.,"def collate_fn ( data_list : list [ dict ] ) -> dict : tensors = defaultdict ( list ) non_tensors = defaultdict ( list ) for data in data_list : for key , val in data . items ( ) : if isinstance ( val , torch . Tensor ) : tensors [ key ] . append ( val ) else : non_tensors [ key ] . append ( val ) for key , val in tensors . items ( ) : tensors [ key ] = torch . stack ( val , dim = 0 ) for key , val in non_tensors . items ( ) : non_tensors [ key ] = np . array ( val , dtype = object ) return { ** tensors , ** non_tensors }",Collate a batch of data.
/optillm/optillm/plugins/deepthink/reasoning_modules.py,get_modules_by_category,"def get_modules_by_category():
    """"""Categorize modules by their primary focus.""""""
    categories = {
        ""analytical"": [1, 3, 5, 10, 14, 17, 20, 23, 24, 25, 29],
        ""creative"": [2, 4, 11, 30, 34, 35, 37],
        ""systematic"": [9, 13, 16, 18, 22, 31, 33, 36, 38, 39],
        ""collaborative"": [7, 12, 15, 21],
        ""risk_oriented"": [6, 8, 14, 19],
        ""behavioral"": [27, 28],
        ""constraint_focused"": [26, 32]
    }
    
    return {
        category: [REASONING_MODULES[i-1] for i in indices]
        for category, indices in categories.items()
    }","def get_modules_by_category():
    """"""Categorize modules by their primary focus.""""""
    categories = {
        ""analytical"": [1, 3, 5, 10, 14, 17, 20, 23, 24, 25, 29],
        ""creative"": [2, 4, 11, 30, 34, 35, 37],
        ""systematic"": [9, 13, 16, 18, 22, 31, 33, 36, 38, 39],
        ""collaborative"": [7, 12, 15, 21],
        ""risk_oriented"": [6, 8, 14, 19],
        ""behavioral"": [27, 28],
        ""constraint_focused"": [26, 32]
    }
    
    return {
        category: [REASONING_MODULES[i-1] for i in indices]
        for category, indices in categories.items()
    }",Categorize modules by their primary focus.,Categorize modules by their primary focus.,"def get_modules_by_category():
    
    categories = {
        ""analytical"": [1, 3, 5, 10, 14, 17, 20, 23, 24, 25, 29],
        ""creative"": [2, 4, 11, 30, 34, 35, 37],
        ""systematic"": [9, 13, 16, 18, 22, 31, 33, 36, 38, 39],
        ""collaborative"": [7, 12, 15, 21],
        ""risk_oriented"": [6, 8, 14, 19],
        ""behavioral"": [27, 28],
        ""constraint_focused"": [26, 32]
    }
    
    return {
        category: [REASONING_MODULES[i-1] for i in indices]
        for category, indices in categories.items()
    }",Categorize modules by their primary focus.,"def get_modules_by_category ( ) : categories = { ""analytical"" : [ 1 , 3 , 5 , 10 , 14 , 17 , 20 , 23 , 24 , 25 , 29 ] , ""creative"" : [ 2 , 4 , 11 , 30 , 34 , 35 , 37 ] , ""systematic"" : [ 9 , 13 , 16 , 18 , 22 , 31 , 33 , 36 , 38 , 39 ] , ""collaborative"" : [ 7 , 12 , 15 , 21 ] , ""risk_oriented"" : [ 6 , 8 , 14 , 19 ] , ""behavioral"" : [ 27 , 28 ] , ""constraint_focused"" : [ 26 , 32 ] } return { category : [ REASONING_MODULES [ i - 1 ] for i in indices ] for category , indices in categories . items ( ) }",Categorize modules by their primary focus.
/mcp-agent/src/mcp_agent/workflows/router/router_base.py,format_category,"def format_category(
        self, category: RouterCategory, index: int | None = None
    ) -> str:
        """"""Format a category into a readable string.""""""

        index_str = f""{index}. "" if index is not None else "" ""
        category_str = """"

        if isinstance(category, ServerRouterCategory):
            category_str = self._format_server_category(category)
        elif isinstance(category, AgentRouterCategory):
            category_str = self._format_agent_category(category)
        else:
            category_str = self._format_function_category(category)

        return f""{index_str}{category_str}""","def format_category(
        self, category: RouterCategory, index: int | None = None
    ) -> str:
        """"""Format a category into a readable string.""""""

        index_str = f""{index}. "" if index is not None else "" ""
        category_str = """"

        if isinstance(category, ServerRouterCategory):
            category_str = self._format_server_category(category)
        elif isinstance(category, AgentRouterCategory):
            category_str = self._format_agent_category(category)
        else:
            category_str = self._format_function_category(category)

        return f""{index_str}{category_str}""",Format a category into a readable string.,Format a category into a readable string.,"def format_category(
        self, category: RouterCategory, index: int | None = None
    ) -> str:
        

        index_str = f""{index}. "" if index is not None else "" ""
        category_str = """"

        if isinstance(category, ServerRouterCategory):
            category_str = self._format_server_category(category)
        elif isinstance(category, AgentRouterCategory):
            category_str = self._format_agent_category(category)
        else:
            category_str = self._format_function_category(category)

        return f""{index_str}{category_str}""",Format a category into a readable string.,"def format_category ( self , category : RouterCategory , index : int | None = None ) -> str : index_str = f""{index}. "" if index is not None else "" "" category_str = """" if isinstance ( category , ServerRouterCategory ) : category_str = self . _format_server_category ( category ) elif isinstance ( category , AgentRouterCategory ) : category_str = self . _format_agent_category ( category ) else : category_str = self . _format_function_category ( category ) return f""{index_str}{category_str}""",Format a category into a readable string.
/openai-agents-python/src/agents/items.py,tool_call_output_item,"def tool_call_output_item(
        cls, tool_call: ResponseFunctionToolCall, output: str
    ) -> FunctionCallOutput:
        """"""Creates a tool call output item from a tool call and its output.""""""
        return {
            ""call_id"": tool_call.call_id,
            ""output"": output,
            ""type"": ""function_call_output"",
        }","def tool_call_output_item(
        cls, tool_call: ResponseFunctionToolCall, output: str
    ) -> FunctionCallOutput:
        """"""Creates a tool call output item from a tool call and its output.""""""
        return {
            ""call_id"": tool_call.call_id,
            ""output"": output,
            ""type"": ""function_call_output"",
        }",Creates a tool call output item from a tool call and its output.,Creates a tool call output item from a tool call and its output.,"def tool_call_output_item(
        cls, tool_call: ResponseFunctionToolCall, output: str
    ) -> FunctionCallOutput:
        
        return {
            ""call_id"": tool_call.call_id,
            ""output"": output,
            ""type"": ""function_call_output"",
        }",Creates a tool call output item from a tool call and its output.,"def tool_call_output_item ( cls , tool_call : ResponseFunctionToolCall , output : str ) -> FunctionCallOutput : return { ""call_id"" : tool_call . call_id , ""output"" : output , ""type"" : ""function_call_output"" , }",Creates a tool call output item from a tool call and its output.
/adk-samples/python/agents/brand-search-optimization/brand_search_optimization/sub_agents/search_results/agent.py,enter_text_into_element,"def enter_text_into_element(text_to_enter: str, element_id: str) -> str:
    """"""Enters text into an element with the given ID.""""""
    print(
        f""📝 Entering text '{text_to_enter}' into element with ID: {element_id}""
    )  # Added print statement

    try:
        input_element = driver.find_element(By.ID, element_id)
        input_element.send_keys(text_to_enter)
        return (
            f""Entered text '{text_to_enter}' into element with ID: {element_id}""
        )
    except selenium.common.exceptions.NoSuchElementException:
        return ""Element with given ID not found.""
    except selenium.common.exceptions.ElementNotInteractableException:
        return ""Element not interactable, cannot click.""","def enter_text_into_element(text_to_enter: str, element_id: str) -> str:
    """"""Enters text into an element with the given ID.""""""
    print(
        f""📝 Entering text '{text_to_enter}' into element with ID: {element_id}""
    )  # Added print statement

    try:
        input_element = driver.find_element(By.ID, element_id)
        input_element.send_keys(text_to_enter)
        return (
            f""Entered text '{text_to_enter}' into element with ID: {element_id}""
        )
    except selenium.common.exceptions.NoSuchElementException:
        return ""Element with given ID not found.""
    except selenium.common.exceptions.ElementNotInteractableException:
        return ""Element not interactable, cannot click.""",Enters text into an element with the given ID.,Enters text into an element with the given ID.,"def enter_text_into_element(text_to_enter: str, element_id: str) -> str:
    
    print(
        f""📝 Entering text '{text_to_enter}' into element with ID: {element_id}""
    )  # Added print statement

    try:
        input_element = driver.find_element(By.ID, element_id)
        input_element.send_keys(text_to_enter)
        return (
            f""Entered text '{text_to_enter}' into element with ID: {element_id}""
        )
    except selenium.common.exceptions.NoSuchElementException:
        return ""Element with given ID not found.""
    except selenium.common.exceptions.ElementNotInteractableException:
        return ""Element not interactable, cannot click.""",Enters text into an element with the given ID.,"def enter_text_into_element ( text_to_enter : str , element_id : str ) -> str : print ( f""📝 Entering text '{text_to_enter}' into element with ID: {element_id}"" ) # Added print statement try : input_element = driver . find_element ( By . ID , element_id ) input_element . send_keys ( text_to_enter ) return ( f""Entered text '{text_to_enter}' into element with ID: {element_id}"" ) except selenium . common . exceptions . NoSuchElementException : return ""Element with given ID not found."" except selenium . common . exceptions . ElementNotInteractableException : return ""Element not interactable, cannot click.""",Enters text into an element with the given ID.
/LightRAG/lightrag/api/gunicorn_config.py,on_exit,"def on_exit(server):
    """"""
    Executed when Gunicorn is shutting down.
    This is a good place to release shared resources.
    """"""
    print(""="" * 80)
    print(""GUNICORN MASTER PROCESS: Shutting down"")
    print(f""Process ID: {os.getpid()}"")
    print(""="" * 80)

    # Release shared resources
    finalize_share_data()

    print(""="" * 80)
    print(""Gunicorn shutdown complete"")
    print(""="" * 80)","def on_exit(server):
    """"""
    Executed when Gunicorn is shutting down.
    This is a good place to release shared resources.
    """"""
    print(""="" * 80)
    print(""GUNICORN MASTER PROCESS: Shutting down"")
    print(f""Process ID: {os.getpid()}"")
    print(""="" * 80)

    # Release shared resources
    finalize_share_data()

    print(""="" * 80)
    print(""Gunicorn shutdown complete"")
    print(""="" * 80)","Executed when Gunicorn is shutting down.
This is a good place to release shared resources.",Executed when Gunicorn is shutting down.,"def on_exit(server):
    
    print(""="" * 80)
    print(""GUNICORN MASTER PROCESS: Shutting down"")
    print(f""Process ID: {os.getpid()}"")
    print(""="" * 80)

    # Release shared resources
    finalize_share_data()

    print(""="" * 80)
    print(""Gunicorn shutdown complete"")
    print(""="" * 80)",Executed when Gunicorn is shutting down.,"def on_exit ( server ) : print ( ""="" * 80 ) print ( ""GUNICORN MASTER PROCESS: Shutting down"" ) print ( f""Process ID: {os.getpid()}"" ) print ( ""="" * 80 ) # Release shared resources finalize_share_data ( ) print ( ""="" * 80 ) print ( ""Gunicorn shutdown complete"" ) print ( ""="" * 80 )",Executed when Gunicorn is shutting down.
/fastmcp/src/fastmcp/prompts/prompt_manager.py,add_prompt,"def add_prompt(self, prompt: Prompt, key: str | None = None) -> Prompt:
        """"""Add a prompt to the manager.""""""
        key = key or prompt.name

        # Check for duplicates
        existing = self._prompts.get(key)
        if existing:
            if self.duplicate_behavior == ""warn"":
                logger.warning(f""Prompt already exists: {key}"")
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""replace"":
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""error"":
                raise ValueError(f""Prompt already exists: {key}"")
            elif self.duplicate_behavior == ""ignore"":
                return existing
        else:
            self._prompts[key] = prompt
        return prompt","def add_prompt(self, prompt: Prompt, key: str | None = None) -> Prompt:
        """"""Add a prompt to the manager.""""""
        key = key or prompt.name

        # Check for duplicates
        existing = self._prompts.get(key)
        if existing:
            if self.duplicate_behavior == ""warn"":
                logger.warning(f""Prompt already exists: {key}"")
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""replace"":
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""error"":
                raise ValueError(f""Prompt already exists: {key}"")
            elif self.duplicate_behavior == ""ignore"":
                return existing
        else:
            self._prompts[key] = prompt
        return prompt",Add a prompt to the manager.,Add a prompt to the manager.,"def add_prompt(self, prompt: Prompt, key: str | None = None) -> Prompt:
        
        key = key or prompt.name

        # Check for duplicates
        existing = self._prompts.get(key)
        if existing:
            if self.duplicate_behavior == ""warn"":
                logger.warning(f""Prompt already exists: {key}"")
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""replace"":
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""error"":
                raise ValueError(f""Prompt already exists: {key}"")
            elif self.duplicate_behavior == ""ignore"":
                return existing
        else:
            self._prompts[key] = prompt
        return prompt",Add a prompt to the manager.,"def add_prompt ( self , prompt : Prompt , key : str | None = None ) -> Prompt : key = key or prompt . name # Check for duplicates existing = self . _prompts . get ( key ) if existing : if self . duplicate_behavior == ""warn"" : logger . warning ( f""Prompt already exists: {key}"" ) self . _prompts [ key ] = prompt elif self . duplicate_behavior == ""replace"" : self . _prompts [ key ] = prompt elif self . duplicate_behavior == ""error"" : raise ValueError ( f""Prompt already exists: {key}"" ) elif self . duplicate_behavior == ""ignore"" : return existing else : self . _prompts [ key ] = prompt return prompt",Add a prompt to the manager.
/aci/backend/aci/cli/tests/conftest.py,database_setup_and_cleanup,"def database_setup_and_cleanup(db_session: Session) -> Generator[None, None, None]:
    """"""
    Setup and cleanup the database for each test case.
    """"""

    inspector = cast(Inspector, inspect(db_session.bind))

    # Check if all tables defined in models are created in the db
    for table in Base.metadata.tables.values():
        if not inspector.has_table(table.name):
            pytest.exit(f""Table {table} does not exist in the database."")

    clear_database(db_session)
    yield  # This allows the test to run
    clear_database(db_session)","def database_setup_and_cleanup(db_session: Session) -> Generator[None, None, None]:
    """"""
    Setup and cleanup the database for each test case.
    """"""

    inspector = cast(Inspector, inspect(db_session.bind))

    # Check if all tables defined in models are created in the db
    for table in Base.metadata.tables.values():
        if not inspector.has_table(table.name):
            pytest.exit(f""Table {table} does not exist in the database."")

    clear_database(db_session)
    yield  # This allows the test to run
    clear_database(db_session)",Setup and cleanup the database for each test case.,Setup and cleanup the database for each test case.,"def database_setup_and_cleanup(db_session: Session) -> Generator[None, None, None]:
    

    inspector = cast(Inspector, inspect(db_session.bind))

    # Check if all tables defined in models are created in the db
    for table in Base.metadata.tables.values():
        if not inspector.has_table(table.name):
            pytest.exit(f""Table {table} does not exist in the database."")

    clear_database(db_session)
    yield  # This allows the test to run
    clear_database(db_session)",Setup and cleanup the database for each test case.,"def database_setup_and_cleanup ( db_session : Session ) -> Generator [ None , None , None ] : inspector = cast ( Inspector , inspect ( db_session . bind ) ) # Check if all tables defined in models are created in the db for table in Base . metadata . tables . values ( ) : if not inspector . has_table ( table . name ) : pytest . exit ( f""Table {table} does not exist in the database."" ) clear_database ( db_session ) yield # This allows the test to run clear_database ( db_session )",Setup and cleanup the database for each test case.
/nv-ingest/src/util/image_model_validation/cached.py,extract_and_print_results,"def extract_and_print_results(results):
    """"""
    Extract and print the results from the Triton server response.
    """"""
    try:
        text = "" "".join([output[0].decode(""utf-8"") for output in results.as_numpy(""output"")])
        logger.info(text)
        logger.info(json.loads(text))
    except Exception as e:
        logger.info(f""JSON extract failed: {e}"")

    output_data = results.as_numpy(""output"")
    logger.info(output_data)
    return output_data","def extract_and_print_results(results):
    """"""
    Extract and print the results from the Triton server response.
    """"""
    try:
        text = "" "".join([output[0].decode(""utf-8"") for output in results.as_numpy(""output"")])
        logger.info(text)
        logger.info(json.loads(text))
    except Exception as e:
        logger.info(f""JSON extract failed: {e}"")

    output_data = results.as_numpy(""output"")
    logger.info(output_data)
    return output_data",Extract and print the results from the Triton server response.,Extract and print the results from the Triton server response.,"def extract_and_print_results(results):
    
    try:
        text = "" "".join([output[0].decode(""utf-8"") for output in results.as_numpy(""output"")])
        logger.info(text)
        logger.info(json.loads(text))
    except Exception as e:
        logger.info(f""JSON extract failed: {e}"")

    output_data = results.as_numpy(""output"")
    logger.info(output_data)
    return output_data",Extract and print the results from the Triton server response.,"def extract_and_print_results ( results ) : try : text = "" "" . join ( [ output [ 0 ] . decode ( ""utf-8"" ) for output in results . as_numpy ( ""output"" ) ] ) logger . info ( text ) logger . info ( json . loads ( text ) ) except Exception as e : logger . info ( f""JSON extract failed: {e}"" ) output_data = results . as_numpy ( ""output"" ) logger . info ( output_data ) return output_data",Extract and print the results from the Triton server response.
/verl/verl/models/llama/megatron/checkpoint_utils/llama_saver.py,_megatron_calc_global_rank,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    """"""given TP,DP,PP rank to get the global rank.""""""

    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert tp_size * dp_size * pp_size == torch.distributed.get_world_size(), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    # We only support TP-DP-PP grouping, for correctness when resharding
    return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    """"""given TP,DP,PP rank to get the global rank.""""""

    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert tp_size * dp_size * pp_size == torch.distributed.get_world_size(), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    # We only support TP-DP-PP grouping, for correctness when resharding
    return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","given TP,DP,PP rank to get the global rank.","given TP,DP,PP rank to get the global rank.","def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    

    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert tp_size * dp_size * pp_size == torch.distributed.get_world_size(), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    # We only support TP-DP-PP grouping, for correctness when resharding
    return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","given TP,DP,PP rank to get the global rank.","def _megatron_calc_global_rank ( tp_rank : int = 0 , dp_rank : int = 0 , pp_rank : int = 0 ) : tp_size = mpu . get_tensor_model_parallel_world_size ( ) dp_size = mpu . get_data_parallel_world_size ( ) pp_size = mpu . get_pipeline_model_parallel_world_size ( ) assert tp_size * dp_size * pp_size == torch . distributed . get_world_size ( ) , f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}"" # We only support TP-DP-PP grouping, for correctness when resharding return ( pp_rank * dp_size + dp_rank ) * tp_size + tp_rank","given TP,DP,PP rank to get the global rank."
/browser-use/browser_use/agent/gif.py,decode_unicode_escapes_to_utf8,"def decode_unicode_escapes_to_utf8(text: str) -> str:
	""""""Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text)""""""

	if r'\u' not in text:
		# doesn't have any escape sequences that need to be decoded
		return text

	try:
		# Try to decode Unicode escape sequences
		return text.encode('latin1').decode('unicode_escape')
	except (UnicodeEncodeError, UnicodeDecodeError):
		# logger.debug(f""Failed to decode unicode escape sequences while generating gif text: {text}"")
		return text","def decode_unicode_escapes_to_utf8(text: str) -> str:
	""""""Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text)""""""

	if r'\u' not in text:
		# doesn't have any escape sequences that need to be decoded
		return text

	try:
		# Try to decode Unicode escape sequences
		return text.encode('latin1').decode('unicode_escape')
	except (UnicodeEncodeError, UnicodeDecodeError):
		# logger.debug(f""Failed to decode unicode escape sequences while generating gif text: {text}"")
		return text",Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text),Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text),"def decode_unicode_escapes_to_utf8(text: str) -> str:
	

	if r'\u' not in text:
		# doesn't have any escape sequences that need to be decoded
		return text

	try:
		# Try to decode Unicode escape sequences
		return text.encode('latin1').decode('unicode_escape')
	except (UnicodeEncodeError, UnicodeDecodeError):
		# logger.debug(f""Failed to decode unicode escape sequences while generating gif text: {text}"")
		return text",Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text),"def decode_unicode_escapes_to_utf8 ( text : str ) -> str : if r'\u' not in text : # doesn't have any escape sequences that need to be decoded return text try : # Try to decode Unicode escape sequences return text . encode ( 'latin1' ) . decode ( 'unicode_escape' ) except ( UnicodeEncodeError , UnicodeDecodeError ) : # logger.debug(f""Failed to decode unicode escape sequences while generating gif text: {text}"") return text",Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text)
/adk-python/contributing/samples/hello_world_ma/agent.py,check_prime,"def check_prime(nums: list[int]) -> str:
  """"""Check if a given list of numbers are prime.""""""
  primes = set()
  for number in nums:
    number = int(number)
    if number <= 1:
      continue
    is_prime = True
    for i in range(2, int(number**0.5) + 1):
      if number % i == 0:
        is_prime = False
        break
    if is_prime:
      primes.add(number)
  return (
      ""No prime numbers found.""
      if not primes
      else f""{', '.join(str(num) for num in primes)} are prime numbers.""
  )","def check_prime(nums: list[int]) -> str:
  """"""Check if a given list of numbers are prime.""""""
  primes = set()
  for number in nums:
    number = int(number)
    if number <= 1:
      continue
    is_prime = True
    for i in range(2, int(number**0.5) + 1):
      if number % i == 0:
        is_prime = False
        break
    if is_prime:
      primes.add(number)
  return (
      ""No prime numbers found.""
      if not primes
      else f""{', '.join(str(num) for num in primes)} are prime numbers.""
  )",Check if a given list of numbers are prime.,Check if a given list of numbers are prime.,"def check_prime(nums: list[int]) -> str:
  
  primes = set()
  for number in nums:
    number = int(number)
    if number <= 1:
      continue
    is_prime = True
    for i in range(2, int(number**0.5) + 1):
      if number % i == 0:
        is_prime = False
        break
    if is_prime:
      primes.add(number)
  return (
      ""No prime numbers found.""
      if not primes
      else f""{', '.join(str(num) for num in primes)} are prime numbers.""
  )",Check if a given list of numbers are prime.,"def check_prime ( nums : list [ int ] ) -> str : primes = set ( ) for number in nums : number = int ( number ) if number <= 1 : continue is_prime = True for i in range ( 2 , int ( number ** 0.5 ) + 1 ) : if number % i == 0 : is_prime = False break if is_prime : primes . add ( number ) return ( ""No prime numbers found."" if not primes else f""{', '.join(str(num) for num in primes)} are prime numbers."" )",Check if a given list of numbers are prime.
/Kokoro-FastAPI/dev/Test Threads.py,worker_task,"def worker_task(thread_id, args):
    """"""Worker task for each thread""""""
    for i in range(args.iterations):
        iteration = i + 1
        test_id = f""{thread_id:02d}_{iteration:02d}""
        text = generate_test_sentence(thread_id, iteration)
        success = request_tts(
            args.url, test_id, text, args.voice, args.output_dir, args.debug
        )

        if not success:
            log_message(
                f""Thread {thread_id}: Iteration {iteration} failed"",
                args.debug,
                is_error=True,
            )

        # Small delay between iterations to avoid overwhelming the API
        time.sleep(0.1)","def worker_task(thread_id, args):
    """"""Worker task for each thread""""""
    for i in range(args.iterations):
        iteration = i + 1
        test_id = f""{thread_id:02d}_{iteration:02d}""
        text = generate_test_sentence(thread_id, iteration)
        success = request_tts(
            args.url, test_id, text, args.voice, args.output_dir, args.debug
        )

        if not success:
            log_message(
                f""Thread {thread_id}: Iteration {iteration} failed"",
                args.debug,
                is_error=True,
            )

        # Small delay between iterations to avoid overwhelming the API
        time.sleep(0.1)",Worker task for each thread,Worker task for each thread,"def worker_task(thread_id, args):
    
    for i in range(args.iterations):
        iteration = i + 1
        test_id = f""{thread_id:02d}_{iteration:02d}""
        text = generate_test_sentence(thread_id, iteration)
        success = request_tts(
            args.url, test_id, text, args.voice, args.output_dir, args.debug
        )

        if not success:
            log_message(
                f""Thread {thread_id}: Iteration {iteration} failed"",
                args.debug,
                is_error=True,
            )

        # Small delay between iterations to avoid overwhelming the API
        time.sleep(0.1)",Worker task for each thread,"def worker_task ( thread_id , args ) : for i in range ( args . iterations ) : iteration = i + 1 test_id = f""{thread_id:02d}_{iteration:02d}"" text = generate_test_sentence ( thread_id , iteration ) success = request_tts ( args . url , test_id , text , args . voice , args . output_dir , args . debug ) if not success : log_message ( f""Thread {thread_id}: Iteration {iteration} failed"" , args . debug , is_error = True , ) # Small delay between iterations to avoid overwhelming the API time . sleep ( 0.1 )",Worker task for each thread
/mcp-agent/src/mcp_agent/workflows/parallel/fan_out.py,_annotate_span_for_generation_message,"def _annotate_span_for_generation_message(
        self,
        span: trace.Span,
        message: MessageParamT | str | List[MessageParamT],
    ) -> None:
        """"""Annotate the span with the message content.""""""
        if not self.context.tracing_enabled:
            return
        if isinstance(message, str):
            span.set_attribute(""message.content"", message)
        elif isinstance(message, list):
            for i, msg in enumerate(message):
                if isinstance(msg, str):
                    span.set_attribute(f""message.{i}.content"", msg)
                else:
                    span.set_attribute(f""message.{i}"", str(msg))
        else:
            span.set_attribute(""message"", str(message))","def _annotate_span_for_generation_message(
        self,
        span: trace.Span,
        message: MessageParamT | str | List[MessageParamT],
    ) -> None:
        """"""Annotate the span with the message content.""""""
        if not self.context.tracing_enabled:
            return
        if isinstance(message, str):
            span.set_attribute(""message.content"", message)
        elif isinstance(message, list):
            for i, msg in enumerate(message):
                if isinstance(msg, str):
                    span.set_attribute(f""message.{i}.content"", msg)
                else:
                    span.set_attribute(f""message.{i}"", str(msg))
        else:
            span.set_attribute(""message"", str(message))",Annotate the span with the message content.,Annotate the span with the message content.,"def _annotate_span_for_generation_message(
        self,
        span: trace.Span,
        message: MessageParamT | str | List[MessageParamT],
    ) -> None:
        
        if not self.context.tracing_enabled:
            return
        if isinstance(message, str):
            span.set_attribute(""message.content"", message)
        elif isinstance(message, list):
            for i, msg in enumerate(message):
                if isinstance(msg, str):
                    span.set_attribute(f""message.{i}.content"", msg)
                else:
                    span.set_attribute(f""message.{i}"", str(msg))
        else:
            span.set_attribute(""message"", str(message))",Annotate the span with the message content.,"def _annotate_span_for_generation_message ( self , span : trace . Span , message : MessageParamT | str | List [ MessageParamT ] , ) -> None : if not self . context . tracing_enabled : return if isinstance ( message , str ) : span . set_attribute ( ""message.content"" , message ) elif isinstance ( message , list ) : for i , msg in enumerate ( message ) : if isinstance ( msg , str ) : span . set_attribute ( f""message.{i}.content"" , msg ) else : span . set_attribute ( f""message.{i}"" , str ( msg ) ) else : span . set_attribute ( ""message"" , str ( message ) )",Annotate the span with the message content.
/nexa-sdk/nexa/gguf/streamlit/streamlit_image_chat.py,load_local_model,"def load_local_model(local_path: str):
    """"""Load local model with default parameters.""""""
    try:
        nexa_model = NexaImageInference(
            model_path=""local_model"",
            local_path=local_path
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None","def load_local_model(local_path: str):
    """"""Load local model with default parameters.""""""
    try:
        nexa_model = NexaImageInference(
            model_path=""local_model"",
            local_path=local_path
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None",Load local model with default parameters.,Load local model with default parameters.,"def load_local_model(local_path: str):
    
    try:
        nexa_model = NexaImageInference(
            model_path=""local_model"",
            local_path=local_path
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None",Load local model with default parameters.,"def load_local_model ( local_path : str ) : try : nexa_model = NexaImageInference ( model_path = ""local_model"" , local_path = local_path ) # update options after successful local model load update_model_options ( specified_run_type , model_map ) return nexa_model except Exception as e : st . error ( f""Error loading local model: {str(e)}"" ) return None",Load local model with default parameters.
/NLWeb/code/llm/gemini.py,_build_messages,"def _build_messages(cls, prompt: str, schema: Dict[str, Any]) -> List[str]:
        """"""Construct the message sequence for JSON-schema enforcement.""""""
        return [
            f""Provide a valid JSON response matching this schema: {json.dumps(schema)}"",
            prompt
        ]","def _build_messages(cls, prompt: str, schema: Dict[str, Any]) -> List[str]:
        """"""Construct the message sequence for JSON-schema enforcement.""""""
        return [
            f""Provide a valid JSON response matching this schema: {json.dumps(schema)}"",
            prompt
        ]",Construct the message sequence for JSON-schema enforcement.,Construct the message sequence for JSON-schema enforcement.,"def _build_messages(cls, prompt: str, schema: Dict[str, Any]) -> List[str]:
        
        return [
            f""Provide a valid JSON response matching this schema: {json.dumps(schema)}"",
            prompt
        ]",Construct the message sequence for JSON-schema enforcement.,"def _build_messages ( cls , prompt : str , schema : Dict [ str , Any ] ) -> List [ str ] : return [ f""Provide a valid JSON response matching this schema: {json.dumps(schema)}"" , prompt ]",Construct the message sequence for JSON-schema enforcement.
/mcp-agent/src/mcp_agent/logging/event_progress.py,__str__,"def __str__(self) -> str:
        """"""Format the progress event for display.""""""
        base = f""{self.action.ljust(11)}. {self.target}""
        if self.details:
            base += f"" - {self.details}""
        if self.agent_name:
            base = f""[{self.agent_name}] {base}""
        return base","def __str__(self) -> str:
        """"""Format the progress event for display.""""""
        base = f""{self.action.ljust(11)}. {self.target}""
        if self.details:
            base += f"" - {self.details}""
        if self.agent_name:
            base = f""[{self.agent_name}] {base}""
        return base",Format the progress event for display.,Format the progress event for display.,"def __str__(self) -> str:
        
        base = f""{self.action.ljust(11)}. {self.target}""
        if self.details:
            base += f"" - {self.details}""
        if self.agent_name:
            base = f""[{self.agent_name}] {base}""
        return base",Format the progress event for display.,"def __str__ ( self ) -> str : base = f""{self.action.ljust(11)}. {self.target}"" if self . details : base += f"" - {self.details}"" if self . agent_name : base = f""[{self.agent_name}] {base}"" return base",Format the progress event for display.
/ragaai-catalyst/ragaai_catalyst/tracers/agentic_tracing/utils/system_monitor.py,track_memory_usage,"def track_memory_usage(self) -> Optional[float]:
        """"""Track memory usage in MB""""""
        try:
            memory_usage = psutil.Process().memory_info().rss
            return memory_usage / (1024 * 1024)  # Convert to MB
        except Exception as e:
            logger.warning(f""Failed to track memory usage: {str(e)}"")
            return None","def track_memory_usage(self) -> Optional[float]:
        """"""Track memory usage in MB""""""
        try:
            memory_usage = psutil.Process().memory_info().rss
            return memory_usage / (1024 * 1024)  # Convert to MB
        except Exception as e:
            logger.warning(f""Failed to track memory usage: {str(e)}"")
            return None",Track memory usage in MB,Track memory usage in MB,"def track_memory_usage(self) -> Optional[float]:
        
        try:
            memory_usage = psutil.Process().memory_info().rss
            return memory_usage / (1024 * 1024)  # Convert to MB
        except Exception as e:
            logger.warning(f""Failed to track memory usage: {str(e)}"")
            return None",Track memory usage in MB,"def track_memory_usage ( self ) -> Optional [ float ] : try : memory_usage = psutil . Process ( ) . memory_info ( ) . rss return memory_usage / ( 1024 * 1024 ) # Convert to MB except Exception as e : logger . warning ( f""Failed to track memory usage: {str(e)}"" ) return None",Track memory usage in MB
/verl/verl/utils/model.py,normalize_model_name,"def normalize_model_name(name, pp_rank, vpp_rank, transformer_config, layer_name=""layers""):
    """"""
    Transform the model name in each model_chunk in each pp stage into the name in inference engine
    """"""
    from verl.utils.megatron_utils import get_transformer_layer_offset

    layer_offset = get_transformer_layer_offset(pp_rank, vpp_rank, transformer_config)

    if layer_name in name:  # belong to an intermediate layer
        split_name = name.split(""."")
        # find the num next to split_name
        for i, name in enumerate(split_name):
            if name == layer_name:
                break
        layer_num_idx = i + 1
        # check the name
        assert len(split_name) >= layer_num_idx + 1, f""split_name = {split_name}""
        assert split_name[layer_num_idx].isdigit(), f""split_name = {split_name}""
        # increment layer_num_idx by layer_offset
        split_name[layer_num_idx] = str(int(split_name[layer_num_idx]) + layer_offset)
        name = ""."".join(split_name)  # weight name in inference_tp_model
    return name","def normalize_model_name(name, pp_rank, vpp_rank, transformer_config, layer_name=""layers""):
    """"""
    Transform the model name in each model_chunk in each pp stage into the name in inference engine
    """"""
    from verl.utils.megatron_utils import get_transformer_layer_offset

    layer_offset = get_transformer_layer_offset(pp_rank, vpp_rank, transformer_config)

    if layer_name in name:  # belong to an intermediate layer
        split_name = name.split(""."")
        # find the num next to split_name
        for i, name in enumerate(split_name):
            if name == layer_name:
                break
        layer_num_idx = i + 1
        # check the name
        assert len(split_name) >= layer_num_idx + 1, f""split_name = {split_name}""
        assert split_name[layer_num_idx].isdigit(), f""split_name = {split_name}""
        # increment layer_num_idx by layer_offset
        split_name[layer_num_idx] = str(int(split_name[layer_num_idx]) + layer_offset)
        name = ""."".join(split_name)  # weight name in inference_tp_model
    return name",Transform the model name in each model_chunk in each pp stage into the name in inference engine,Transform the model name in each model_chunk in each pp stage into the name in inference engine,"def normalize_model_name(name, pp_rank, vpp_rank, transformer_config, layer_name=""layers""):
    
    from verl.utils.megatron_utils import get_transformer_layer_offset

    layer_offset = get_transformer_layer_offset(pp_rank, vpp_rank, transformer_config)

    if layer_name in name:  # belong to an intermediate layer
        split_name = name.split(""."")
        # find the num next to split_name
        for i, name in enumerate(split_name):
            if name == layer_name:
                break
        layer_num_idx = i + 1
        # check the name
        assert len(split_name) >= layer_num_idx + 1, f""split_name = {split_name}""
        assert split_name[layer_num_idx].isdigit(), f""split_name = {split_name}""
        # increment layer_num_idx by layer_offset
        split_name[layer_num_idx] = str(int(split_name[layer_num_idx]) + layer_offset)
        name = ""."".join(split_name)  # weight name in inference_tp_model
    return name",Transform the model name in each model_chunk in each pp stage into the name in inference engine,"def normalize_model_name ( name , pp_rank , vpp_rank , transformer_config , layer_name = ""layers"" ) : from verl . utils . megatron_utils import get_transformer_layer_offset layer_offset = get_transformer_layer_offset ( pp_rank , vpp_rank , transformer_config ) if layer_name in name : # belong to an intermediate layer split_name = name . split ( ""."" ) # find the num next to split_name for i , name in enumerate ( split_name ) : if name == layer_name : break layer_num_idx = i + 1 # check the name assert len ( split_name ) >= layer_num_idx + 1 , f""split_name = {split_name}"" assert split_name [ layer_num_idx ] . isdigit ( ) , f""split_name = {split_name}"" # increment layer_num_idx by layer_offset split_name [ layer_num_idx ] = str ( int ( split_name [ layer_num_idx ] ) + layer_offset ) name = ""."" . join ( split_name ) # weight name in inference_tp_model return name",Transform the model name in each model_chunk in each pp stage into the name in inference engine
/agenticSeek/sources/tools/PyInterpreter.py,interpreter_feedback,"def interpreter_feedback(self, output:str) -> str:
        """"""
        Provide feedback based on the output of the code execution
        """"""
        if self.execution_failure_check(output):
            feedback = f""[failure] Error in execution:\n{output}""
        else:
            feedback = ""[success] Execution success, code output:\n"" + output
        return feedback","def interpreter_feedback(self, output:str) -> str:
        """"""
        Provide feedback based on the output of the code execution
        """"""
        if self.execution_failure_check(output):
            feedback = f""[failure] Error in execution:\n{output}""
        else:
            feedback = ""[success] Execution success, code output:\n"" + output
        return feedback",Provide feedback based on the output of the code execution,Provide feedback based on the output of the code execution,"def interpreter_feedback(self, output:str) -> str:
        
        if self.execution_failure_check(output):
            feedback = f""[failure] Error in execution:\n{output}""
        else:
            feedback = ""[success] Execution success, code output:\n"" + output
        return feedback",Provide feedback based on the output of the code execution,"def interpreter_feedback ( self , output : str ) -> str : if self . execution_failure_check ( output ) : feedback = f""[failure] Error in execution:\n{output}"" else : feedback = ""[success] Execution success, code output:\n"" + output return feedback",Provide feedback based on the output of the code execution
/KAG/kag/common/parser/logic_node_parser.py,__repr__,"def __repr__(self) -> str:
        """"""Generate debug-friendly string representation""""""
        output_str = f""sub question: {self.sub_question}""
        if self.chunks:
            output_str += f""\nretrieved chunks:\n{self.chunks}""
        if self.spo:
            output_str += f""\nretrieved spo:\n{self.spo}""
        if self.summary:
            output_str += f""\nsummary:\n{self.summary}""
        return output_str","def __repr__(self) -> str:
        """"""Generate debug-friendly string representation""""""
        output_str = f""sub question: {self.sub_question}""
        if self.chunks:
            output_str += f""\nretrieved chunks:\n{self.chunks}""
        if self.spo:
            output_str += f""\nretrieved spo:\n{self.spo}""
        if self.summary:
            output_str += f""\nsummary:\n{self.summary}""
        return output_str",Generate debug-friendly string representation,Generate debug-friendly string representation,"def __repr__(self) -> str:
        
        output_str = f""sub question: {self.sub_question}""
        if self.chunks:
            output_str += f""\nretrieved chunks:\n{self.chunks}""
        if self.spo:
            output_str += f""\nretrieved spo:\n{self.spo}""
        if self.summary:
            output_str += f""\nsummary:\n{self.summary}""
        return output_str",Generate debug-friendly string representation,"def __repr__ ( self ) -> str : output_str = f""sub question: {self.sub_question}"" if self . chunks : output_str += f""\nretrieved chunks:\n{self.chunks}"" if self . spo : output_str += f""\nretrieved spo:\n{self.spo}"" if self . summary : output_str += f""\nsummary:\n{self.summary}"" return output_str",Generate debug-friendly string representation
/local-deep-research/src/local_deep_research/setup_data_dir.py,setup_data_dir,"def setup_data_dir():
    """"""Set up the data directory for the application.""""""
    # Get the project root directory (3 levels up from this file)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "".."", ""..""))

    # Define the data directory path
    data_dir = os.path.join(project_root, ""data"")

    # Create the data directory if it doesn't exist
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f""Created data directory at: {data_dir}"")
    else:
        print(f""Data directory already exists at: {data_dir}"")

    # Return the path to the data directory
    return data_dir","def setup_data_dir():
    """"""Set up the data directory for the application.""""""
    # Get the project root directory (3 levels up from this file)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "".."", ""..""))

    # Define the data directory path
    data_dir = os.path.join(project_root, ""data"")

    # Create the data directory if it doesn't exist
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f""Created data directory at: {data_dir}"")
    else:
        print(f""Data directory already exists at: {data_dir}"")

    # Return the path to the data directory
    return data_dir",Set up the data directory for the application.,Set up the data directory for the application.,"def setup_data_dir():
    
    # Get the project root directory (3 levels up from this file)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "".."", ""..""))

    # Define the data directory path
    data_dir = os.path.join(project_root, ""data"")

    # Create the data directory if it doesn't exist
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f""Created data directory at: {data_dir}"")
    else:
        print(f""Data directory already exists at: {data_dir}"")

    # Return the path to the data directory
    return data_dir",Set up the data directory for the application.,"def setup_data_dir ( ) : # Get the project root directory (3 levels up from this file) current_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) project_root = os . path . abspath ( os . path . join ( current_dir , "".."" , "".."" ) ) # Define the data directory path data_dir = os . path . join ( project_root , ""data"" ) # Create the data directory if it doesn't exist if not os . path . exists ( data_dir ) : os . makedirs ( data_dir ) print ( f""Created data directory at: {data_dir}"" ) else : print ( f""Data directory already exists at: {data_dir}"" ) # Return the path to the data directory return data_dir",Set up the data directory for the application.
/PocketFlow/cookbook/pocketflow-voice-chat/utils/audio_utils.py,play_audio_data,"def play_audio_data(audio_data, sample_rate):
    """"""Plays in-memory audio data (NumPy array).""""""
    try:
        print(f""Playing in-memory audio data (Sample rate: {sample_rate} Hz, Duration: {len(audio_data)/sample_rate:.2f}s)"")
        sd.play(audio_data, sample_rate)
        sd.wait()
        print(""Playback from memory finished."")
    except Exception as e:
        print(f""Error playing in-memory audio: {e}"")","def play_audio_data(audio_data, sample_rate):
    """"""Plays in-memory audio data (NumPy array).""""""
    try:
        print(f""Playing in-memory audio data (Sample rate: {sample_rate} Hz, Duration: {len(audio_data)/sample_rate:.2f}s)"")
        sd.play(audio_data, sample_rate)
        sd.wait()
        print(""Playback from memory finished."")
    except Exception as e:
        print(f""Error playing in-memory audio: {e}"")",Plays in-memory audio data (NumPy array).,Plays in-memory audio data (NumPy array).,"def play_audio_data(audio_data, sample_rate):
    
    try:
        print(f""Playing in-memory audio data (Sample rate: {sample_rate} Hz, Duration: {len(audio_data)/sample_rate:.2f}s)"")
        sd.play(audio_data, sample_rate)
        sd.wait()
        print(""Playback from memory finished."")
    except Exception as e:
        print(f""Error playing in-memory audio: {e}"")",Plays in-memory audio data (NumPy array).,"def play_audio_data ( audio_data , sample_rate ) : try : print ( f""Playing in-memory audio data (Sample rate: {sample_rate} Hz, Duration: {len(audio_data)/sample_rate:.2f}s)"" ) sd . play ( audio_data , sample_rate ) sd . wait ( ) print ( ""Playback from memory finished."" ) except Exception as e : print ( f""Error playing in-memory audio: {e}"" )",Plays in-memory audio data (NumPy array).
/adk-python/src/google/adk/cli/utils/__init__.py,create_empty_state,"def create_empty_state(
    agent: BaseAgent, initialized_states: Optional[dict[str, Any]] = None
) -> dict[str, Any]:
  """"""Creates empty str for non-initialized states.""""""
  non_initialized_states = {}
  _create_empty_state(agent, non_initialized_states)
  for key in initialized_states or {}:
    if key in non_initialized_states:
      del non_initialized_states[key]
  return non_initialized_states","def create_empty_state(
    agent: BaseAgent, initialized_states: Optional[dict[str, Any]] = None
) -> dict[str, Any]:
  """"""Creates empty str for non-initialized states.""""""
  non_initialized_states = {}
  _create_empty_state(agent, non_initialized_states)
  for key in initialized_states or {}:
    if key in non_initialized_states:
      del non_initialized_states[key]
  return non_initialized_states",Creates empty str for non-initialized states.,Creates empty str for non-initialized states.,"def create_empty_state(
    agent: BaseAgent, initialized_states: Optional[dict[str, Any]] = None
) -> dict[str, Any]:
  
  non_initialized_states = {}
  _create_empty_state(agent, non_initialized_states)
  for key in initialized_states or {}:
    if key in non_initialized_states:
      del non_initialized_states[key]
  return non_initialized_states",Creates empty str for non-initialized states.,"def create_empty_state ( agent : BaseAgent , initialized_states : Optional [ dict [ str , Any ] ] = None ) -> dict [ str , Any ] : non_initialized_states = { } _create_empty_state ( agent , non_initialized_states ) for key in initialized_states or { } : if key in non_initialized_states : del non_initialized_states [ key ] return non_initialized_states",Creates empty str for non-initialized states.
/mcp-agent/examples/usecases/reliable_conversation/src/utils/test_runner.py,_display_quality_summary,"def _display_quality_summary(self, metrics: Dict[str, Any]):
        """"""Display quality metrics in test context""""""
        overall_score = metrics.get(""overall_score"", 0)
        issues = metrics.get(""issues"", [])

        # Use formatter for consistent display
        quality_display = self.formatter.format_quality_score(overall_score, issues)
        self.console.print(f""[dim]{quality_display}[/dim]"")

        # Highlight specific test concerns
        if metrics.get(""premature_attempt""):
            self.console.print(
                ""  [yellow]⚠ Test detected premature answer attempt[/yellow]""
            )

        verbosity = metrics.get(""verbosity"", 0)
        if verbosity > 0.7:
            self.console.print(
                f""  [yellow]⚠ High verbosity detected ({verbosity:.0%})[/yellow]""
            )","def _display_quality_summary(self, metrics: Dict[str, Any]):
        """"""Display quality metrics in test context""""""
        overall_score = metrics.get(""overall_score"", 0)
        issues = metrics.get(""issues"", [])

        # Use formatter for consistent display
        quality_display = self.formatter.format_quality_score(overall_score, issues)
        self.console.print(f""[dim]{quality_display}[/dim]"")

        # Highlight specific test concerns
        if metrics.get(""premature_attempt""):
            self.console.print(
                ""  [yellow]⚠ Test detected premature answer attempt[/yellow]""
            )

        verbosity = metrics.get(""verbosity"", 0)
        if verbosity > 0.7:
            self.console.print(
                f""  [yellow]⚠ High verbosity detected ({verbosity:.0%})[/yellow]""
            )",Display quality metrics in test context,Display quality metrics in test context,"def _display_quality_summary(self, metrics: Dict[str, Any]):
        
        overall_score = metrics.get(""overall_score"", 0)
        issues = metrics.get(""issues"", [])

        # Use formatter for consistent display
        quality_display = self.formatter.format_quality_score(overall_score, issues)
        self.console.print(f""[dim]{quality_display}[/dim]"")

        # Highlight specific test concerns
        if metrics.get(""premature_attempt""):
            self.console.print(
                ""  [yellow]⚠ Test detected premature answer attempt[/yellow]""
            )

        verbosity = metrics.get(""verbosity"", 0)
        if verbosity > 0.7:
            self.console.print(
                f""  [yellow]⚠ High verbosity detected ({verbosity:.0%})[/yellow]""
            )",Display quality metrics in test context,"def _display_quality_summary ( self , metrics : Dict [ str , Any ] ) : overall_score = metrics . get ( ""overall_score"" , 0 ) issues = metrics . get ( ""issues"" , [ ] ) # Use formatter for consistent display quality_display = self . formatter . format_quality_score ( overall_score , issues ) self . console . print ( f""[dim]{quality_display}[/dim]"" ) # Highlight specific test concerns if metrics . get ( ""premature_attempt"" ) : self . console . print ( ""  [yellow]⚠ Test detected premature answer attempt[/yellow]"" ) verbosity = metrics . get ( ""verbosity"" , 0 ) if verbosity > 0.7 : self . console . print ( f""  [yellow]⚠ High verbosity detected ({verbosity:.0%})[/yellow]"" )",Display quality metrics in test context
/aci/backend/aci/server/dependencies.py,get_request_context,"def get_request_context(
    db_session: Annotated[Session, Depends(yield_db_session)],
    api_key_id: Annotated[UUID, Depends(validate_api_key)],
    agent: Annotated[Agent, Depends(validate_agent)],
    project: Annotated[Project, Depends(validate_project_quota)],
) -> RequestContext:
    """"""
    Returns a RequestContext object containing the DB session,
    the validated API key ID, and the project ID.
    """"""
    logger.info(
        ""populating request context"",
        extra={""api_key_id"": api_key_id, ""project_id"": project.id, ""agent_id"": agent.id},
    )
    return RequestContext(
        db_session=db_session,
        api_key_id=api_key_id,
        project=project,
        agent=agent,
    )","def get_request_context(
    db_session: Annotated[Session, Depends(yield_db_session)],
    api_key_id: Annotated[UUID, Depends(validate_api_key)],
    agent: Annotated[Agent, Depends(validate_agent)],
    project: Annotated[Project, Depends(validate_project_quota)],
) -> RequestContext:
    """"""
    Returns a RequestContext object containing the DB session,
    the validated API key ID, and the project ID.
    """"""
    logger.info(
        ""populating request context"",
        extra={""api_key_id"": api_key_id, ""project_id"": project.id, ""agent_id"": agent.id},
    )
    return RequestContext(
        db_session=db_session,
        api_key_id=api_key_id,
        project=project,
        agent=agent,
    )","Returns a RequestContext object containing the DB session,
the validated API key ID, and the project ID.","Returns a RequestContext object containing the DB session, the validated API key ID, and the project ID.","def get_request_context(
    db_session: Annotated[Session, Depends(yield_db_session)],
    api_key_id: Annotated[UUID, Depends(validate_api_key)],
    agent: Annotated[Agent, Depends(validate_agent)],
    project: Annotated[Project, Depends(validate_project_quota)],
) -> RequestContext:
    
    logger.info(
        ""populating request context"",
        extra={""api_key_id"": api_key_id, ""project_id"": project.id, ""agent_id"": agent.id},
    )
    return RequestContext(
        db_session=db_session,
        api_key_id=api_key_id,
        project=project,
        agent=agent,
    )","Returns a RequestContext object containing the DB session, the validated API key ID, and the project ID.","def get_request_context ( db_session : Annotated [ Session , Depends ( yield_db_session ) ] , api_key_id : Annotated [ UUID , Depends ( validate_api_key ) ] , agent : Annotated [ Agent , Depends ( validate_agent ) ] , project : Annotated [ Project , Depends ( validate_project_quota ) ] , ) -> RequestContext : logger . info ( ""populating request context"" , extra = { ""api_key_id"" : api_key_id , ""project_id"" : project . id , ""agent_id"" : agent . id } , ) return RequestContext ( db_session = db_session , api_key_id = api_key_id , project = project , agent = agent , )","Returns a RequestContext object containing the DB session, the validated API key ID, and the project ID."
/python-sdk/src/mcp/server/sse.py,__init__,"def __init__(self, endpoint: str) -> None:
        """"""
        Creates a new SSE server transport, which will direct the client to POST
        messages to the relative or absolute URL given.
        """"""

        super().__init__()
        self._endpoint = endpoint
        self._read_stream_writers = {}
        logger.debug(f""SseServerTransport initialized with endpoint: {endpoint}"")","def __init__(self, endpoint: str) -> None:
        """"""
        Creates a new SSE server transport, which will direct the client to POST
        messages to the relative or absolute URL given.
        """"""

        super().__init__()
        self._endpoint = endpoint
        self._read_stream_writers = {}
        logger.debug(f""SseServerTransport initialized with endpoint: {endpoint}"")","Creates a new SSE server transport, which will direct the client to POST
messages to the relative or absolute URL given.","Creates a new SSE server transport, which will direct the client to POST messages to the relative or absolute URL given.","def __init__(self, endpoint: str) -> None:
        

        super().__init__()
        self._endpoint = endpoint
        self._read_stream_writers = {}
        logger.debug(f""SseServerTransport initialized with endpoint: {endpoint}"")","Creates a new SSE server transport, which will direct the client to POST messages to the relative or absolute URL given.","def __init__ ( self , endpoint : str ) -> None : super ( ) . __init__ ( ) self . _endpoint = endpoint self . _read_stream_writers = { } logger . debug ( f""SseServerTransport initialized with endpoint: {endpoint}"" )","Creates a new SSE server transport, which will direct the client to POST messages to the relative or absolute URL given."
/KAG/kag/common/benchmarks/evaluate.py,evaForSimilarity,"def evaForSimilarity(self, predictionlist: List[str], goldlist: List[str]):
        """"""
        evaluate the similarity between prediction and gold #TODO
        """"""
        # data_samples = {
        #     'question': [],
        #     'answer': predictionlist,
        #     'ground_truth': goldlist
        # }
        # dataset = Dataset.from_dict(data_samples)
        # run_config = RunConfig(timeout=240, thread_timeout=240, max_workers=16)
        # embeddings = embedding_factory(self.embedding_factory, run_config)
        #
        # score = evaluate(dataset, metrics=[answer_similarity], embeddings = embeddings, run_config=run_config)
        # return np.average(score.to_pandas()[['answer_similarity']])
        return {""similarity"": 0.0}","def evaForSimilarity(self, predictionlist: List[str], goldlist: List[str]):
        """"""
        evaluate the similarity between prediction and gold #TODO
        """"""
        # data_samples = {
        #     'question': [],
        #     'answer': predictionlist,
        #     'ground_truth': goldlist
        # }
        # dataset = Dataset.from_dict(data_samples)
        # run_config = RunConfig(timeout=240, thread_timeout=240, max_workers=16)
        # embeddings = embedding_factory(self.embedding_factory, run_config)
        #
        # score = evaluate(dataset, metrics=[answer_similarity], embeddings = embeddings, run_config=run_config)
        # return np.average(score.to_pandas()[['answer_similarity']])
        return {""similarity"": 0.0}",evaluate the similarity between prediction and gold #TODO,evaluate the similarity between prediction and gold #TODO,"def evaForSimilarity(self, predictionlist: List[str], goldlist: List[str]):
        
        # data_samples = {
        #     'question': [],
        #     'answer': predictionlist,
        #     'ground_truth': goldlist
        # }
        # dataset = Dataset.from_dict(data_samples)
        # run_config = RunConfig(timeout=240, thread_timeout=240, max_workers=16)
        # embeddings = embedding_factory(self.embedding_factory, run_config)
        #
        # score = evaluate(dataset, metrics=[answer_similarity], embeddings = embeddings, run_config=run_config)
        # return np.average(score.to_pandas()[['answer_similarity']])
        return {""similarity"": 0.0}",evaluate the similarity between prediction and gold #TODO,"def evaForSimilarity ( self , predictionlist : List [ str ] , goldlist : List [ str ] ) : # data_samples = { #     'question': [], #     'answer': predictionlist, #     'ground_truth': goldlist # } # dataset = Dataset.from_dict(data_samples) # run_config = RunConfig(timeout=240, thread_timeout=240, max_workers=16) # embeddings = embedding_factory(self.embedding_factory, run_config) # # score = evaluate(dataset, metrics=[answer_similarity], embeddings = embeddings, run_config=run_config) # return np.average(score.to_pandas()[['answer_similarity']]) return { ""similarity"" : 0.0 }",evaluate the similarity between prediction and gold #TODO
/local-deep-research/src/local_deep_research/web/routes/history_routes.py,get_markdown,"def get_markdown(research_id):
    """"""Get markdown export for a specific research""""""
    conn = get_db_connection()
    conn.row_factory = lambda cursor, row: {
        column[0]: row[idx] for idx, column in enumerate(cursor.description)
    }
    cursor = conn.cursor()
    cursor.execute(""SELECT * FROM research_history WHERE id = ?"", (research_id,))
    result = cursor.fetchone()
    conn.close()

    if not result or not result.get(""report_path""):
        return jsonify({""status"": ""error"", ""message"": ""Report not found""}), 404

    try:
        with open(result[""report_path""], ""r"", encoding=""utf-8"") as f:
            content = f.read()
        return jsonify({""status"": ""success"", ""content"": content})
    except Exception as e:
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500","def get_markdown(research_id):
    """"""Get markdown export for a specific research""""""
    conn = get_db_connection()
    conn.row_factory = lambda cursor, row: {
        column[0]: row[idx] for idx, column in enumerate(cursor.description)
    }
    cursor = conn.cursor()
    cursor.execute(""SELECT * FROM research_history WHERE id = ?"", (research_id,))
    result = cursor.fetchone()
    conn.close()

    if not result or not result.get(""report_path""):
        return jsonify({""status"": ""error"", ""message"": ""Report not found""}), 404

    try:
        with open(result[""report_path""], ""r"", encoding=""utf-8"") as f:
            content = f.read()
        return jsonify({""status"": ""success"", ""content"": content})
    except Exception as e:
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500",Get markdown export for a specific research,Get markdown export for a specific research,"def get_markdown(research_id):
    
    conn = get_db_connection()
    conn.row_factory = lambda cursor, row: {
        column[0]: row[idx] for idx, column in enumerate(cursor.description)
    }
    cursor = conn.cursor()
    cursor.execute(""SELECT * FROM research_history WHERE id = ?"", (research_id,))
    result = cursor.fetchone()
    conn.close()

    if not result or not result.get(""report_path""):
        return jsonify({""status"": ""error"", ""message"": ""Report not found""}), 404

    try:
        with open(result[""report_path""], ""r"", encoding=""utf-8"") as f:
            content = f.read()
        return jsonify({""status"": ""success"", ""content"": content})
    except Exception as e:
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500",Get markdown export for a specific research,"def get_markdown ( research_id ) : conn = get_db_connection ( ) conn . row_factory = lambda cursor , row : { column [ 0 ] : row [ idx ] for idx , column in enumerate ( cursor . description ) } cursor = conn . cursor ( ) cursor . execute ( ""SELECT * FROM research_history WHERE id = ?"" , ( research_id , ) ) result = cursor . fetchone ( ) conn . close ( ) if not result or not result . get ( ""report_path"" ) : return jsonify ( { ""status"" : ""error"" , ""message"" : ""Report not found"" } ) , 404 try : with open ( result [ ""report_path"" ] , ""r"" , encoding = ""utf-8"" ) as f : content = f . read ( ) return jsonify ( { ""status"" : ""success"" , ""content"" : content } ) except Exception as e : return jsonify ( { ""status"" : ""error"" , ""message"" : str ( e ) } ) , 500",Get markdown export for a specific research
/airweave/backend/airweave/platform/embedding_models/local_text2vec.py,model_post_init,"def model_post_init(self, __context) -> None:
        """"""Post initialization hook to set the inference URL from settings.

        This runs after Pydantic validation but before the model is used.
        """"""
        super().model_post_init(__context)
        self.inference_url = settings.TEXT2VEC_INFERENCE_URL
        logger.info(f""Text2Vec model using inference URL: {self.inference_url}"")","def model_post_init(self, __context) -> None:
        """"""Post initialization hook to set the inference URL from settings.

        This runs after Pydantic validation but before the model is used.
        """"""
        super().model_post_init(__context)
        self.inference_url = settings.TEXT2VEC_INFERENCE_URL
        logger.info(f""Text2Vec model using inference URL: {self.inference_url}"")","Post initialization hook to set the inference URL from settings.

This runs after Pydantic validation but before the model is used.",Post initialization hook to set the inference URL from settings.,"def model_post_init(self, __context) -> None:
        
        super().model_post_init(__context)
        self.inference_url = settings.TEXT2VEC_INFERENCE_URL
        logger.info(f""Text2Vec model using inference URL: {self.inference_url}"")",Post initialization hook to set the inference URL from settings.,"def model_post_init ( self , __context ) -> None : super ( ) . model_post_init ( __context ) self . inference_url = settings . TEXT2VEC_INFERENCE_URL logger . info ( f""Text2Vec model using inference URL: {self.inference_url}"" )",Post initialization hook to set the inference URL from settings.
/blender-mcp/src/blender_mcp/server.py,connect,"def connect(self) -> bool:
        """"""Connect to the Blender addon socket server""""""
        if self.sock:
            return True
            
        try:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.connect((self.host, self.port))
            logger.info(f""Connected to Blender at {self.host}:{self.port}"")
            return True
        except Exception as e:
            logger.error(f""Failed to connect to Blender: {str(e)}"")
            self.sock = None
            return False","def connect(self) -> bool:
        """"""Connect to the Blender addon socket server""""""
        if self.sock:
            return True
            
        try:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.connect((self.host, self.port))
            logger.info(f""Connected to Blender at {self.host}:{self.port}"")
            return True
        except Exception as e:
            logger.error(f""Failed to connect to Blender: {str(e)}"")
            self.sock = None
            return False",Connect to the Blender addon socket server,Connect to the Blender addon socket server,"def connect(self) -> bool:
        
        if self.sock:
            return True
            
        try:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.connect((self.host, self.port))
            logger.info(f""Connected to Blender at {self.host}:{self.port}"")
            return True
        except Exception as e:
            logger.error(f""Failed to connect to Blender: {str(e)}"")
            self.sock = None
            return False",Connect to the Blender addon socket server,"def connect ( self ) -> bool : if self . sock : return True try : self . sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . sock . connect ( ( self . host , self . port ) ) logger . info ( f""Connected to Blender at {self.host}:{self.port}"" ) return True except Exception as e : logger . error ( f""Failed to connect to Blender: {str(e)}"" ) self . sock = None return False",Connect to the Blender addon socket server
/Hunyuan3D-2/hy3dgen/texgen/hunyuanpaint/pipeline.py,append_dims,"def append_dims(x, target_dims):
    """"""Appends dimensions to the end of a tensor until it has target_dims dimensions.""""""
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f""input has {x.ndim} dims but target_dims is {target_dims}, which is less"")
    return x[(...,) + (None,) * dims_to_append]","def append_dims(x, target_dims):
    """"""Appends dimensions to the end of a tensor until it has target_dims dimensions.""""""
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f""input has {x.ndim} dims but target_dims is {target_dims}, which is less"")
    return x[(...,) + (None,) * dims_to_append]",Appends dimensions to the end of a tensor until it has target_dims dimensions.,Appends dimensions to the end of a tensor until it has target_dims dimensions.,"def append_dims(x, target_dims):
    
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f""input has {x.ndim} dims but target_dims is {target_dims}, which is less"")
    return x[(...,) + (None,) * dims_to_append]",Appends dimensions to the end of a tensor until it has target_dims dimensions.,"def append_dims ( x , target_dims ) : dims_to_append = target_dims - x . ndim if dims_to_append < 0 : raise ValueError ( f""input has {x.ndim} dims but target_dims is {target_dims}, which is less"" ) return x [ ( ... , ) + ( None , ) * dims_to_append ]",Appends dimensions to the end of a tensor until it has target_dims dimensions.
/local-deep-research/src/local_deep_research/web/routes/api_routes.py,api_delete_resource,"def api_delete_resource(research_id, resource_id):
    """"""
    Delete a resource from a research project
    """"""
    try:
        # Delete the resource
        success = delete_resource(resource_id)

        if success:
            return jsonify(
                {""status"": ""success"", ""message"": ""Resource deleted successfully""}
            )
        else:
            return jsonify({""status"": ""error"", ""message"": ""Resource not found""}), 404
    except Exception as e:
        logger.error(f""Error deleting resource: {str(e)}"")
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500","def api_delete_resource(research_id, resource_id):
    """"""
    Delete a resource from a research project
    """"""
    try:
        # Delete the resource
        success = delete_resource(resource_id)

        if success:
            return jsonify(
                {""status"": ""success"", ""message"": ""Resource deleted successfully""}
            )
        else:
            return jsonify({""status"": ""error"", ""message"": ""Resource not found""}), 404
    except Exception as e:
        logger.error(f""Error deleting resource: {str(e)}"")
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500",Delete a resource from a research project,Delete a resource from a research project,"def api_delete_resource(research_id, resource_id):
    
    try:
        # Delete the resource
        success = delete_resource(resource_id)

        if success:
            return jsonify(
                {""status"": ""success"", ""message"": ""Resource deleted successfully""}
            )
        else:
            return jsonify({""status"": ""error"", ""message"": ""Resource not found""}), 404
    except Exception as e:
        logger.error(f""Error deleting resource: {str(e)}"")
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500",Delete a resource from a research project,"def api_delete_resource ( research_id , resource_id ) : try : # Delete the resource success = delete_resource ( resource_id ) if success : return jsonify ( { ""status"" : ""success"" , ""message"" : ""Resource deleted successfully"" } ) else : return jsonify ( { ""status"" : ""error"" , ""message"" : ""Resource not found"" } ) , 404 except Exception as e : logger . error ( f""Error deleting resource: {str(e)}"" ) return jsonify ( { ""status"" : ""error"" , ""message"" : str ( e ) } ) , 500",Delete a resource from a research project
/OpenManus-RL/verl/utils/reward_score/countdown.py,evaluate_equation,"def evaluate_equation(equation_str):
    """"""Safely evaluate the arithmetic equation using eval() with precautions.""""""
    try:
        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace
        allowed_pattern = r'^[\d+\-*/().\s]+$'
        if not re.match(allowed_pattern, equation_str):
            raise ValueError(""Invalid characters in equation."")

        # Evaluate the equation with restricted globals and locals
        result = eval(equation_str, {""__builtins__"": None}, {})
        return result
    except Exception as e:
        return None","def evaluate_equation(equation_str):
    """"""Safely evaluate the arithmetic equation using eval() with precautions.""""""
    try:
        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace
        allowed_pattern = r'^[\d+\-*/().\s]+$'
        if not re.match(allowed_pattern, equation_str):
            raise ValueError(""Invalid characters in equation."")

        # Evaluate the equation with restricted globals and locals
        result = eval(equation_str, {""__builtins__"": None}, {})
        return result
    except Exception as e:
        return None",Safely evaluate the arithmetic equation using eval() with precautions.,Safely evaluate the arithmetic equation using eval() with precautions.,"def evaluate_equation(equation_str):
    
    try:
        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace
        allowed_pattern = r'^[\d+\-*/().\s]+$'
        if not re.match(allowed_pattern, equation_str):
            raise ValueError(""Invalid characters in equation."")

        # Evaluate the equation with restricted globals and locals
        result = eval(equation_str, {""__builtins__"": None}, {})
        return result
    except Exception as e:
        return None",Safely evaluate the arithmetic equation using eval() with precautions.,"def evaluate_equation ( equation_str ) : try : # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace allowed_pattern = r'^[\d+\-*/().\s]+$' if not re . match ( allowed_pattern , equation_str ) : raise ValueError ( ""Invalid characters in equation."" ) # Evaluate the equation with restricted globals and locals result = eval ( equation_str , { ""__builtins__"" : None } , { } ) return result except Exception as e : return None",Safely evaluate the arithmetic equation using eval() with precautions.
/web-ui/src/webui/components/browser_use_agent_tab.py,_handle_done,"def _handle_done(webui_manager: WebuiManager, history: AgentHistoryList):
    """"""Callback when the agent finishes the task (success or failure).""""""
    logger.info(
        f""Agent task finished. Duration: {history.total_duration_seconds():.2f}s, Tokens: {history.total_input_tokens()}""
    )
    final_summary = ""**Task Completed**\n""
    final_summary += f""- Duration: {history.total_duration_seconds():.2f} seconds\n""
    final_summary += f""- Total Input Tokens: {history.total_input_tokens()}\n""  # Or total tokens if available

    final_result = history.final_result()
    if final_result:
        final_summary += f""- Final Result: {final_result}\n""

    errors = history.errors()
    if errors and any(errors):
        final_summary += f""- **Errors:**\n```\n{errors}\n```\n""
    else:
        final_summary += ""- Status: Success\n""

    webui_manager.bu_chat_history.append(
        {""role"": ""assistant"", ""content"": final_summary}
    )","def _handle_done(webui_manager: WebuiManager, history: AgentHistoryList):
    """"""Callback when the agent finishes the task (success or failure).""""""
    logger.info(
        f""Agent task finished. Duration: {history.total_duration_seconds():.2f}s, Tokens: {history.total_input_tokens()}""
    )
    final_summary = ""**Task Completed**\n""
    final_summary += f""- Duration: {history.total_duration_seconds():.2f} seconds\n""
    final_summary += f""- Total Input Tokens: {history.total_input_tokens()}\n""  # Or total tokens if available

    final_result = history.final_result()
    if final_result:
        final_summary += f""- Final Result: {final_result}\n""

    errors = history.errors()
    if errors and any(errors):
        final_summary += f""- **Errors:**\n```\n{errors}\n```\n""
    else:
        final_summary += ""- Status: Success\n""

    webui_manager.bu_chat_history.append(
        {""role"": ""assistant"", ""content"": final_summary}
    )",Callback when the agent finishes the task (success or failure).,Callback when the agent finishes the task (success or failure).,"def _handle_done(webui_manager: WebuiManager, history: AgentHistoryList):
    
    logger.info(
        f""Agent task finished. Duration: {history.total_duration_seconds():.2f}s, Tokens: {history.total_input_tokens()}""
    )
    final_summary = ""**Task Completed**\n""
    final_summary += f""- Duration: {history.total_duration_seconds():.2f} seconds\n""
    final_summary += f""- Total Input Tokens: {history.total_input_tokens()}\n""  # Or total tokens if available

    final_result = history.final_result()
    if final_result:
        final_summary += f""- Final Result: {final_result}\n""

    errors = history.errors()
    if errors and any(errors):
        final_summary += f""- **Errors:**\n```\n{errors}\n```\n""
    else:
        final_summary += ""- Status: Success\n""

    webui_manager.bu_chat_history.append(
        {""role"": ""assistant"", ""content"": final_summary}
    )",Callback when the agent finishes the task (success or failure).,"def _handle_done ( webui_manager : WebuiManager , history : AgentHistoryList ) : logger . info ( f""Agent task finished. Duration: {history.total_duration_seconds():.2f}s, Tokens: {history.total_input_tokens()}"" ) final_summary = ""**Task Completed**\n"" final_summary += f""- Duration: {history.total_duration_seconds():.2f} seconds\n"" final_summary += f""- Total Input Tokens: {history.total_input_tokens()}\n"" # Or total tokens if available final_result = history . final_result ( ) if final_result : final_summary += f""- Final Result: {final_result}\n"" errors = history . errors ( ) if errors and any ( errors ) : final_summary += f""- **Errors:**\n```\n{errors}\n```\n"" else : final_summary += ""- Status: Success\n"" webui_manager . bu_chat_history . append ( { ""role"" : ""assistant"" , ""content"" : final_summary } )",Callback when the agent finishes the task (success or failure).
/mcp/src/aws-support-mcp-server/tests/conftests.py,minimal_service_data,"def minimal_service_data() -> Dict[str, Any]:
    """"""Return a dictionary with minimal service data.""""""
    return {
        ""code"": ""amazon-elastic-compute-cloud-linux"",
        ""name"": ""Amazon Elastic Compute Cloud (Linux)"",
        ""categories"": [],
    }","def minimal_service_data() -> Dict[str, Any]:
    """"""Return a dictionary with minimal service data.""""""
    return {
        ""code"": ""amazon-elastic-compute-cloud-linux"",
        ""name"": ""Amazon Elastic Compute Cloud (Linux)"",
        ""categories"": [],
    }",Return a dictionary with minimal service data.,Return a dictionary with minimal service data.,"def minimal_service_data() -> Dict[str, Any]:
    
    return {
        ""code"": ""amazon-elastic-compute-cloud-linux"",
        ""name"": ""Amazon Elastic Compute Cloud (Linux)"",
        ""categories"": [],
    }",Return a dictionary with minimal service data.,"def minimal_service_data ( ) -> Dict [ str , Any ] : return { ""code"" : ""amazon-elastic-compute-cloud-linux"" , ""name"" : ""Amazon Elastic Compute Cloud (Linux)"" , ""categories"" : [ ] , }",Return a dictionary with minimal service data.
/VideoLingo/core/asr_backend/audio_preprocess.py,get_audio_duration,"def get_audio_duration(audio_file: str) -> float:
    """"""Get the duration of an audio file using ffmpeg.""""""
    cmd = ['ffmpeg', '-i', audio_file]
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    _, stderr = process.communicate()
    output = stderr.decode('utf-8', errors='ignore')
    
    try:
        duration_str = [line for line in output.split('\n') if 'Duration' in line][0]
        duration_parts = duration_str.split('Duration: ')[1].split(',')[0].split(':')
        duration = float(duration_parts[0])*3600 + float(duration_parts[1])*60 + float(duration_parts[2])
    except Exception as e:
        print(f""[red]❌ Error: Failed to get audio duration: {e}[/red]"")
        duration = 0
    return duration","def get_audio_duration(audio_file: str) -> float:
    """"""Get the duration of an audio file using ffmpeg.""""""
    cmd = ['ffmpeg', '-i', audio_file]
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    _, stderr = process.communicate()
    output = stderr.decode('utf-8', errors='ignore')
    
    try:
        duration_str = [line for line in output.split('\n') if 'Duration' in line][0]
        duration_parts = duration_str.split('Duration: ')[1].split(',')[0].split(':')
        duration = float(duration_parts[0])*3600 + float(duration_parts[1])*60 + float(duration_parts[2])
    except Exception as e:
        print(f""[red]❌ Error: Failed to get audio duration: {e}[/red]"")
        duration = 0
    return duration",Get the duration of an audio file using ffmpeg.,Get the duration of an audio file using ffmpeg.,"def get_audio_duration(audio_file: str) -> float:
    
    cmd = ['ffmpeg', '-i', audio_file]
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    _, stderr = process.communicate()
    output = stderr.decode('utf-8', errors='ignore')
    
    try:
        duration_str = [line for line in output.split('\n') if 'Duration' in line][0]
        duration_parts = duration_str.split('Duration: ')[1].split(',')[0].split(':')
        duration = float(duration_parts[0])*3600 + float(duration_parts[1])*60 + float(duration_parts[2])
    except Exception as e:
        print(f""[red]❌ Error: Failed to get audio duration: {e}[/red]"")
        duration = 0
    return duration",Get the duration of an audio file using ffmpeg.,"def get_audio_duration ( audio_file : str ) -> float : cmd = [ 'ffmpeg' , '-i' , audio_file ] process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) _ , stderr = process . communicate ( ) output = stderr . decode ( 'utf-8' , errors = 'ignore' ) try : duration_str = [ line for line in output . split ( '\n' ) if 'Duration' in line ] [ 0 ] duration_parts = duration_str . split ( 'Duration: ' ) [ 1 ] . split ( ',' ) [ 0 ] . split ( ':' ) duration = float ( duration_parts [ 0 ] ) * 3600 + float ( duration_parts [ 1 ] ) * 60 + float ( duration_parts [ 2 ] ) except Exception as e : print ( f""[red]❌ Error: Failed to get audio duration: {e}[/red]"" ) duration = 0 return duration",Get the duration of an audio file using ffmpeg.
/ai-hedge-fund/src/utils/docker.py,is_ollama_available,"def is_ollama_available(ollama_url: str) -> bool:
    """"""Check if Ollama service is available in Docker environment.""""""
    try:
        response = requests.get(f""{ollama_url}/api/version"", timeout=5)
        if response.status_code == 200:
            return True
            
        print(f""{Fore.RED}Cannot connect to Ollama service at {ollama_url}.{Style.RESET_ALL}"")
        print(f""{Fore.YELLOW}Make sure the Ollama service is running in your Docker environment.{Style.RESET_ALL}"")
        return False
    except requests.RequestException as e:
        print(f""{Fore.RED}Error connecting to Ollama service: {e}{Style.RESET_ALL}"")
        return False","def is_ollama_available(ollama_url: str) -> bool:
    """"""Check if Ollama service is available in Docker environment.""""""
    try:
        response = requests.get(f""{ollama_url}/api/version"", timeout=5)
        if response.status_code == 200:
            return True
            
        print(f""{Fore.RED}Cannot connect to Ollama service at {ollama_url}.{Style.RESET_ALL}"")
        print(f""{Fore.YELLOW}Make sure the Ollama service is running in your Docker environment.{Style.RESET_ALL}"")
        return False
    except requests.RequestException as e:
        print(f""{Fore.RED}Error connecting to Ollama service: {e}{Style.RESET_ALL}"")
        return False",Check if Ollama service is available in Docker environment.,Check if Ollama service is available in Docker environment.,"def is_ollama_available(ollama_url: str) -> bool:
    
    try:
        response = requests.get(f""{ollama_url}/api/version"", timeout=5)
        if response.status_code == 200:
            return True
            
        print(f""{Fore.RED}Cannot connect to Ollama service at {ollama_url}.{Style.RESET_ALL}"")
        print(f""{Fore.YELLOW}Make sure the Ollama service is running in your Docker environment.{Style.RESET_ALL}"")
        return False
    except requests.RequestException as e:
        print(f""{Fore.RED}Error connecting to Ollama service: {e}{Style.RESET_ALL}"")
        return False",Check if Ollama service is available in Docker environment.,"def is_ollama_available ( ollama_url : str ) -> bool : try : response = requests . get ( f""{ollama_url}/api/version"" , timeout = 5 ) if response . status_code == 200 : return True print ( f""{Fore.RED}Cannot connect to Ollama service at {ollama_url}.{Style.RESET_ALL}"" ) print ( f""{Fore.YELLOW}Make sure the Ollama service is running in your Docker environment.{Style.RESET_ALL}"" ) return False except requests . RequestException as e : print ( f""{Fore.RED}Error connecting to Ollama service: {e}{Style.RESET_ALL}"" ) return False",Check if Ollama service is available in Docker environment.
/agenticSeek/sources/agents/mcp_agent.py,get_api_keys,"def get_api_keys(self) -> dict:
        """"""
        Returns the API keys for the tools.
        """"""
        api_key_mcp_finder = os.getenv(""MCP_FINDER_API_KEY"")
        if not api_key_mcp_finder or api_key_mcp_finder == """":
            pretty_print(""MCP Finder disabled."", color=""warning"")
            self.enabled = False
        return {
            ""mcp_finder"": api_key_mcp_finder
        }","def get_api_keys(self) -> dict:
        """"""
        Returns the API keys for the tools.
        """"""
        api_key_mcp_finder = os.getenv(""MCP_FINDER_API_KEY"")
        if not api_key_mcp_finder or api_key_mcp_finder == """":
            pretty_print(""MCP Finder disabled."", color=""warning"")
            self.enabled = False
        return {
            ""mcp_finder"": api_key_mcp_finder
        }",Returns the API keys for the tools.,Returns the API keys for the tools.,"def get_api_keys(self) -> dict:
        
        api_key_mcp_finder = os.getenv(""MCP_FINDER_API_KEY"")
        if not api_key_mcp_finder or api_key_mcp_finder == """":
            pretty_print(""MCP Finder disabled."", color=""warning"")
            self.enabled = False
        return {
            ""mcp_finder"": api_key_mcp_finder
        }",Returns the API keys for the tools.,"def get_api_keys ( self ) -> dict : api_key_mcp_finder = os . getenv ( ""MCP_FINDER_API_KEY"" ) if not api_key_mcp_finder or api_key_mcp_finder == """" : pretty_print ( ""MCP Finder disabled."" , color = ""warning"" ) self . enabled = False return { ""mcp_finder"" : api_key_mcp_finder }",Returns the API keys for the tools.
/morphik-core/examples/batch_operations.py,create_sample_files,"def create_sample_files():
    """"""Create temporary text files for demonstration""""""
    files = []
    sample_texts = [
        ""Artificial Intelligence is transforming various industries."",
        ""Machine Learning models require significant amounts of data."",
        ""Natural Language Processing enables computers to understand human language."",
        ""Computer Vision systems can detect objects in images and videos."",
        ""Reinforcement Learning is used in robotics and game-playing AI."",
    ]

    # Create temporary directory
    temp_dir = tempfile.mkdtemp()
    print(f""Created temporary directory: {temp_dir}"")

    # Create text files
    for i, text in enumerate(sample_texts):
        file_path = os.path.join(temp_dir, f""sample_{i+1}.txt"")
        with open(file_path, ""w"") as f:
            f.write(text)
        files.append(file_path)

    return temp_dir, files","def create_sample_files():
    """"""Create temporary text files for demonstration""""""
    files = []
    sample_texts = [
        ""Artificial Intelligence is transforming various industries."",
        ""Machine Learning models require significant amounts of data."",
        ""Natural Language Processing enables computers to understand human language."",
        ""Computer Vision systems can detect objects in images and videos."",
        ""Reinforcement Learning is used in robotics and game-playing AI."",
    ]

    # Create temporary directory
    temp_dir = tempfile.mkdtemp()
    print(f""Created temporary directory: {temp_dir}"")

    # Create text files
    for i, text in enumerate(sample_texts):
        file_path = os.path.join(temp_dir, f""sample_{i+1}.txt"")
        with open(file_path, ""w"") as f:
            f.write(text)
        files.append(file_path)

    return temp_dir, files",Create temporary text files for demonstration,Create temporary text files for demonstration,"def create_sample_files():
    
    files = []
    sample_texts = [
        ""Artificial Intelligence is transforming various industries."",
        ""Machine Learning models require significant amounts of data."",
        ""Natural Language Processing enables computers to understand human language."",
        ""Computer Vision systems can detect objects in images and videos."",
        ""Reinforcement Learning is used in robotics and game-playing AI."",
    ]

    # Create temporary directory
    temp_dir = tempfile.mkdtemp()
    print(f""Created temporary directory: {temp_dir}"")

    # Create text files
    for i, text in enumerate(sample_texts):
        file_path = os.path.join(temp_dir, f""sample_{i+1}.txt"")
        with open(file_path, ""w"") as f:
            f.write(text)
        files.append(file_path)

    return temp_dir, files",Create temporary text files for demonstration,"def create_sample_files ( ) : files = [ ] sample_texts = [ ""Artificial Intelligence is transforming various industries."" , ""Machine Learning models require significant amounts of data."" , ""Natural Language Processing enables computers to understand human language."" , ""Computer Vision systems can detect objects in images and videos."" , ""Reinforcement Learning is used in robotics and game-playing AI."" , ] # Create temporary directory temp_dir = tempfile . mkdtemp ( ) print ( f""Created temporary directory: {temp_dir}"" ) # Create text files for i , text in enumerate ( sample_texts ) : file_path = os . path . join ( temp_dir , f""sample_{i+1}.txt"" ) with open ( file_path , ""w"" ) as f : f . write ( text ) files . append ( file_path ) return temp_dir , files",Create temporary text files for demonstration
