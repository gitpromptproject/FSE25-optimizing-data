{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aaef98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import openai\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a86ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee389c8",
   "metadata": {},
   "source": [
    "SYS_PROMPT = \"\"\"You are an expert Python and Java developer. Your task is to evaluate how closely a method’s code aligns with its corresponding documentation comment. Assign a score in the real interval [-1, 1], with the following interpretation:\n",
    "  \n",
    "  • -1 -> The documentation does not reflect any actual behavior of the method. It is misleading, unrelated, or entirely incorrect.\n",
    "Find an example in Java right below:\n",
    "\n",
    "\n",
    "Code: public TestResult start(Class testCase) {{\n",
    "  try {{\n",
    "    Test suite = getTest(testCase);\n",
    "    if (null != suite) {{\n",
    "      return doRun(suite);\n",
    "    }} else {{\n",
    "      runFailed(testCase, \"could not create/run JUnit test suite\");\n",
    "    }}\n",
    "  }} catch (Exception e) {{\n",
    "    runFailed(testCase, \"could not create/run JUnit test suite: \" + e.getMessage());\n",
    "  }}\n",
    "  return null;\n",
    "}}\n",
    "\n",
    "Comment: returns the number of the object\n",
    "Score: -0.81\n",
    "********************************************************************\n",
    "\n",
    "  • 0 -> The documentation is loosely related to the method but fails to capture its core behavior. It may reference unrelated or peripheral actions, or be too vague to be helpful.\n",
    "Find an example in Java right below:\n",
    "\n",
    "\n",
    "Code: protected void logoutAllUsers() {{\n",
    "  synchronized (usersMutex) {{\n",
    "    UserModule um;\n",
    "    Iterator<UserModule> itr = userIdMap.values().iterator();\n",
    "    while (itr.hasNext()) {{\n",
    "      um = itr.next();\n",
    "      if (um != null) {{\n",
    "        this.getSystemHistorian().addUserLogoffEvent(um.getUserId(), new Date());\n",
    "        um.logout();\n",
    "      }}\n",
    "    }}\n",
    "    userIdMap.clear();\n",
    "    sessionsMap.clear();\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Comment: adds a new node to the list of the list\n",
    "Score: 0.0006\n",
    "********************************************************************\n",
    "\n",
    "  • 1 -> The documentation is a precise and accurate summary of the method’s behavior. It reflects all key functional aspects using terminology that clearly maps to the code logic.\n",
    "Find an example in Java right below:\n",
    "\n",
    "\n",
    "Code: private String substituteSubString(String input, String find, String replace) {{\n",
    "  int find_length = find.length();\n",
    "  int replace_length = replace.length();\n",
    "\n",
    "  StringBuilder output = new StringBuilder(input);\n",
    "  int index = input.indexOf(find);\n",
    "  int outputOffset = 0;\n",
    "\n",
    "  while (index > -1) {{\n",
    "    output.replace(index + outputOffset, index + outputOffset + find_length, replace);\n",
    "    outputOffset = outputOffset + (replace_length - find_length);\n",
    "    index = input.indexOf(find, index + find_length);\n",
    "  }}\n",
    "\n",
    "  return output.toString();\n",
    "}}\n",
    "\n",
    "Comment: substitutes the string with the given string\n",
    "Score: 0.99\n",
    "********************************************************************\n",
    "\n",
    "Use the template below to output the score. Output only the real value representing the calculated_score, without any additional text or formatting.\n",
    "SCORE=\"Real value representing the calculated_score\"\\n\n",
    "\"\"\"\n",
    "\n",
    "BASE_PROMPT = \"\"\"Code: {}\n",
    "Comment: {}\n",
    "Score: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Prompt 1 (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"You are an expert Python and Java developer. Your task is to evaluate how closely a method’s code aligns with its corresponding documentation comment. Assign a score in the real interval [-1, 1], with the following interpretation:\n",
    "\n",
    "• -1 → The documentation does not reflect any actual behavior of the method. It is misleading, unrelated, or entirely incorrect.\n",
    "Find an example in Java right below:\n",
    "\n",
    "Code: public TestResult start(Class testCase) {{\n",
    "  try {{\n",
    "    Test suite = getTest(testCase);\n",
    "    if (null != suite) {{\n",
    "      return doRun(suite);\n",
    "    }} else {{\n",
    "      runFailed(testCase, \"could not create/run JUnit test suite\");\n",
    "    }}\n",
    "  }} catch (Exception e) {{\n",
    "    runFailed(testCase, \"could not create/run JUnit test suite: \" + e.getMessage());\n",
    "  }}\n",
    "  return null;\n",
    "}}\n",
    "\n",
    "Comment: returns the number of the object  \n",
    "Score: -0.81  \n",
    "********************************************************************\n",
    "\n",
    "• 0 → The documentation is loosely related to the method but fails to capture its core behavior. It may reference unrelated or peripheral actions, or be too vague to be helpful.\n",
    "Example:\n",
    "\n",
    "Code: protected void logoutAllUsers() {{\n",
    "  synchronized (usersMutex) {{\n",
    "    UserModule um;\n",
    "    Iterator<UserModule> itr = userIdMap.values().iterator();\n",
    "    while (itr.hasNext()) {{\n",
    "      um = itr.next();\n",
    "      if (um != null) {{\n",
    "        this.getSystemHistorian().addUserLogoffEvent(um.getUserId(), new Date());\n",
    "        um.logout();\n",
    "      }}\n",
    "    }}\n",
    "    userIdMap.clear();\n",
    "    sessionsMap.clear();\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Comment: adds a new node to the list of the list\n",
    "Score: 0.0006\n",
    "********************************************************************\n",
    "\n",
    "\n",
    "Code: private String deriveDivision(String exp, String divider) {{\n",
    "  if (divider == null || exp == null) return exp;\n",
    "  if (exp.contains(divider)) return exp.split(divider)[0];\n",
    "  return exp;\n",
    "}}\n",
    "\n",
    "Comment: description of the method  \n",
    "Score: 0.17  \n",
    "********************************************************************\n",
    "\n",
    "Code: public void setHandout2pdf(String aFile) {{\n",
    "  this.handout2pdf = aFile;\n",
    "}}\n",
    "\n",
    "Comment: sets the <UNK>  \n",
    "Score: 0.36  \n",
    "********************************************************************\n",
    "\n",
    "• 1 → The documentation is a precise and accurate summary of the method’s behavior. It reflects all key functional aspects using terminology that clearly maps to the code logic.\n",
    "Example:\n",
    "\n",
    "Code: private String substituteSubString(String input, String find, String replace) {{\n",
    "  int find_length = find.length();\n",
    "  int replace_length = replace.length();\n",
    "\n",
    "  StringBuilder output = new StringBuilder(input);\n",
    "  int index = input.indexOf(find);\n",
    "  int outputOffset = 0;\n",
    "\n",
    "  while (index > -1) {{\n",
    "    output.replace(index + outputOffset, index + outputOffset + find_length, replace);\n",
    "    outputOffset = outputOffset + (replace_length - find_length);\n",
    "    index = input.indexOf(find, index + find_length);\n",
    "  }}\n",
    "\n",
    "  return output.toString();\n",
    "}}\n",
    "\n",
    "Comment: substitutes the string with the given string  \n",
    "Score: 0.99  \n",
    "********************************************************************\n",
    "\n",
    "Code: protected void applyText() {{\n",
    "  anchor.setTitle(descriptionField.getText());\n",
    "}}\n",
    "\n",
    "Comment: sets the value of the code property  \n",
    "Score: 0.48  \n",
    "********************************************************************\n",
    "\n",
    "Use the template below to output the score. Output **only** the real value representing the calculated score, without any additional text or formatting.\n",
    "\n",
    "SCORE=\"Real value representing the calculated_score\"\n",
    "\"\"\"\n",
    "\n",
    "BASE_PROMPT = \"\"\"Code: {}\n",
    "Comment: {}\n",
    "Score: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Prompt 2 (Below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"You are an expert Python and Java developer. Your task is to evaluate how closely a method’s code aligns with its corresponding documentation comment. Assign a score in the real interval [-1, 1], with the following interpretation:\n",
    "\n",
    "• -1 → The documentation is completely misleading, inaccurate, or unrelated to the code’s actual behavior.\n",
    "• 0 → The documentation is vague or only partially related to the main logic of the code.\n",
    "• 1 → The documentation is a clear, complete, and accurate summary of what the method does.\n",
    "\n",
    "Here are several examples for reference:\n",
    "\n",
    "###  Misleading Documentation (Score ~ -1)\n",
    "\n",
    "Code: public TestResult start(Class testCase) {{\n",
    "  try {{\n",
    "    Test suite = getTest(testCase);\n",
    "    if (null != suite) {{\n",
    "      return doRun(suite);\n",
    "    }} else {{\n",
    "      runFailed(testCase, \"could not create/run JUnit test suite\");\n",
    "    }}\n",
    "  }} catch (Exception e) {{\n",
    "    runFailed(testCase, \"could not create/run JUnit test suite: \" + e.getMessage());\n",
    "  }}\n",
    "  return null;\n",
    "}}\n",
    "\n",
    "Comment: returns the number of the object  \n",
    "Score: -0.81  \n",
    "********************************************************************\n",
    "\n",
    "###  Vague or Generic Documentation (Score ~ 0.05–0.2)\n",
    "\n",
    "Code: private void unmark() throws YamlException, IOException {{\n",
    "  for (Object key : marked.keySet()) {{\n",
    "    Stacks stacks = (Stacks) marked.get(key);\n",
    "    stacks.unmark();\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Comment: <UNK> the current state of all stacks  \n",
    "Score: 0.05  \n",
    "********************************************************************\n",
    "\n",
    "Code: private String deriveDivision(String exp, String divider) {{\n",
    "  if (divider == null || exp == null) return exp;\n",
    "  if (exp.contains(divider)) return exp.split(divider)[0];\n",
    "  return exp;\n",
    "}}\n",
    "\n",
    "Comment: description of the method  \n",
    "Score: 0.17  \n",
    "********************************************************************\n",
    "\n",
    "###  Partially Aligned (Score ~ 0.3–0.5)\n",
    "\n",
    "Code: public void setHandout2pdf(String aFile) {{\n",
    "  this.handout2pdf = aFile;\n",
    "}}\n",
    "\n",
    "Comment: sets the <UNK>  \n",
    "Score: 0.36  \n",
    "********************************************************************\n",
    "\n",
    "Code: protected void applyText() {{\n",
    "  anchor.setTitle(descriptionField.getText());\n",
    "}}\n",
    "\n",
    "Comment: sets the value of the code property  \n",
    "Score: 0.47  \n",
    "********************************************************************\n",
    "\n",
    "###  High Alignment (Score ~ 0.9–1)\n",
    "\n",
    "Code: private String substituteSubString(String input, String find, String replace) {{\n",
    "  int find_length = find.length();\n",
    "  int replace_length = replace.length();\n",
    "\n",
    "  StringBuilder output = new StringBuilder(input);\n",
    "  int index = input.indexOf(find);\n",
    "  int outputOffset = 0;\n",
    "\n",
    "  while (index > -1) {{\n",
    "    output.replace(index + outputOffset, index + outputOffset + find_length, replace);\n",
    "    outputOffset = outputOffset + (replace_length - find_length);\n",
    "    index = input.indexOf(find, index + find_length);\n",
    "  }}\n",
    "\n",
    "  return output.toString();\n",
    "}}\n",
    "\n",
    "Comment: substitutes the string with the given string  \n",
    "Score: 0.99  \n",
    "********************************************************************\n",
    "\n",
    "Output your final score using this exact format, and **only** this format:\n",
    "\n",
    "SCORE=\"Real value representing the calculated_score\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de23df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/user/projects/prompt-project/SIDE_p/scripts/training_SIDE/annotation_with_metrics.csv')\n",
    "df.head()\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de609e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to query GPT-4o ===\n",
    "def scoreCodeCommentPair(method, comment):\n",
    "    # Surround the code with triple question marks as per system prompt instruction    \n",
    "    #print(\"SYS_PROMPT:\", SYS_PROMPT)  # Debugging line to check the system prompt\n",
    "    #print(f\"BASE_PROMPT Prompt: {prompt}\")  # Debugging line to check the prompt\n",
    "    \n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": BASE_PROMPT.format(method, comment)}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        return \"API_ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79829eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresList = []\n",
    "inspectList = []\n",
    "for idx,row in tqdm(df.iterrows()):\n",
    "    code = row['codeFunctions']\n",
    "    comment = row['codeComment']\n",
    "    result = scoreCodeCommentPair(code, comment)\n",
    "    \n",
    "    '''try:\n",
    "        score = float(result.split('SCORE=\"')[1].split('\"')[0] if 'SCORE=\"' in result else \"No score found\")\n",
    "    except (IndexError, ValueError):\n",
    "        score = \"Invalid score format\"\n",
    "        inspectList.append((idx))'''\n",
    "    match = re.search(r'SCORE=\"?([0-9.]+)\"?', result)\n",
    "    if match:\n",
    "        score = float(match.group(1))   \n",
    "    else:\n",
    "        score = None\n",
    "        inspectList.append(idx)\n",
    "    #print(score)\n",
    "    #print(result)\n",
    "    scoresList.append(score)\n",
    "    \n",
    "print(\"Total None score found for index:\", len(inspectList))\n",
    "df['SIDE_GPT'] = scoresList\n",
    "df.to_csv('data_with_GPT_scores-2.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7204e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspectList = [9, 21, 36, 56, 58, 62, 78, 91, 99, 123, 141, 142, 151, 193, 228, 237, 239, 248, 280, 284, 287, 288, 298, 306, 321, 342, 348, 352, 366, 372, 395, 397, 404, 418, 427, 438, 445, 447, 458, 463, 483, 498]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s9/400cmz7n0ll775p3_mld28v80000gn/T/ipykernel_40467/272980305.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspectList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'codeFunctions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'codeComment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "for index in inspectList:\n",
    "    \n",
    "    code = df.iloc[index]['codeFunctions']\n",
    "    comment = df.iloc[index]['codeComment']\n",
    "\n",
    "    result = scoreCodeCommentPair(code, comment)\n",
    "    \n",
    "    # try:\n",
    "    #     score = float(result.split('SCORE=\"')[1].split('\"')[0] if 'SCORE=\"' in result else \"No score found\")\n",
    "    # except (IndexError, ValueError):\n",
    "    #     score = \"Invalid score format\"\n",
    "    #     print(f\"Error processing index {index}\")\n",
    "    score = float(result.split('SCORE=\"')[1].split('\"')[0] if 'SCORE=\"' in result else \"No score found\")\n",
    "    df.at[index, 'SIDE_GPT'] = score\n",
    "df.to_csv('data_with_GPT_scores_fixed.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
